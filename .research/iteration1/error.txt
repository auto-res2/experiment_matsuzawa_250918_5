Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/src/main.py", line 11, in <module>
    from . import preprocess
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/src/preprocess.py", line 140
    final_labeled_data.append((f"<LID:unk}> {text}", reward, safety_label))
                                                   ^
SyntaxError: f-string: single '}' is not allowed
2025-09-18 15:34:33,236 - INFO - Loading configuration from config/smoke_test.yaml
2025-09-18 15:34:33,278 - INFO - Environment set up with random seed: 42
2025-09-18 15:34:33,278 - INFO - ====== Starting Phase 1: Data Preprocessing ======
2025-09-18 15:34:33,278 - INFO - Starting preprocessing...
2025-09-18 15:34:33,280 - INFO - fastText language identification model not found. Downloading...
2025-09-18 15:34:35,160 - INFO - fastText model downloaded successfully.
Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
2025-09-18 15:34:35,471 - INFO - Loading Alpaca dataset (as PromptSource substitute)
Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]Generating train split:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32000/52002 [00:00<00:00, 314372.88 examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:00<00:00, 218434.61 examples/s]
2025-09-18 15:34:41,855 - INFO - Loading LAION-Aesthetics dataset (as Stable Diffusion logs substitute)
Generating train split:   0%|          | 0/591753 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 591753/591753 [00:00<00:00, 2428646.75 examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 591753/591753 [00:00<00:00, 1521722.61 examples/s]
2025-09-18 15:34:48,625 - INFO - Loading DETOX dataset for safety labels
Downloading data:   0%|          | 0/27 [00:00<?, ?files/s]Downloading data:   4%|â–Ž         | 1/27 [00:09<03:59,  9.22s/files]Downloading data:   7%|â–‹         | 2/27 [00:18<03:58,  9.52s/files]Downloading data:  11%|â–ˆ         | 3/27 [00:27<03:40,  9.17s/files]Downloading data:  15%|â–ˆâ–        | 4/27 [00:36<03:29,  9.12s/files]Downloading data:  19%|â–ˆâ–Š        | 5/27 [00:46<03:24,  9.29s/files]Downloading data:  22%|â–ˆâ–ˆâ–       | 6/27 [00:55<03:11,  9.14s/files]Downloading data:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [01:04<03:01,  9.09s/files]Downloading data:  30%|â–ˆâ–ˆâ–‰       | 8/27 [01:13<02:51,  9.04s/files]2025-09-18 15:36:26,315 - INFO - Loading configuration from config/smoke_test.yaml
2025-09-18 15:36:26,354 - INFO - Environment set up with random seed: 42
2025-09-18 15:36:26,354 - INFO - ====== Starting Phase 1: Data Preprocessing ======
2025-09-18 15:36:26,354 - INFO - Starting preprocessing...
Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
2025-09-18 15:36:26,536 - INFO - Loading Alpaca dataset (as PromptSource substitute)
2025-09-18 15:36:29,481 - INFO - Loading LAION-Aesthetics dataset (as Stable Diffusion logs substitute)
2025-09-18 15:36:32,167 - INFO - Loading DETOX dataset for safety labels
Downloading data:   0%|          | 0/27 [00:00<?, ?files/s]Downloading data:   4%|â–Ž         | 1/27 [00:00<00:04,  5.44files/s]Downloading data:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:06<00:13,  1.37files/s]Downloading data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:15<00:32,  1.94s/files]Downloading data:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:24<00:49,  3.09s/files]Downloading data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:32<01:01,  4.10s/files]Downloading data:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:43<01:16,  5.50s/files]Downloading data:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:52<01:21,  6.24s/files]Downloading data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [01:01<01:23,  6.97s/files]Downloading data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [01:10<01:22,  7.50s/files]Downloading data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [01:18<01:17,  7.74s/files]Downloading data:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [01:28<01:16,  8.47s/files]Downloading data:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [01:37<01:07,  8.50s/files]Downloading data:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [01:46<01:01,  8.79s/files]Downloading data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [02:01<01:02, 10.45s/files]Downloading data:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [02:10<00:50, 10.03s/files]Downloading data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [02:18<00:38,  9.59s/files]Downloading data:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [02:28<00:28,  9.45s/files]Downloading data:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [02:37<00:18,  9.34s/files]Downloading data:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [02:47<00:09,  9.64s/files]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:14<00:00, 50.67s/files]Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:14<00:00, 11.64s/files]
Generating train split:   0%|          | 0/1949977 [00:00<?, ? examples/s]Generating train split:   1%|          | 10000/1949977 [00:00<02:02, 15804.80 examples/s]Generating train split:   1%|          | 20000/1949977 [00:00<01:23, 23240.87 examples/s]Generating train split:   2%|â–         | 30000/1949977 [00:01<01:12, 26402.16 examples/s]Generating train split:   2%|â–         | 40000/1949977 [00:01<01:07, 28399.85 examples/s]Generating train split:   3%|â–Ž         | 50000/1949977 [00:01<01:02, 30295.24 examples/s]Generating train split:   3%|â–Ž         | 60000/1949977 [00:02<01:01, 30756.45 examples/s]Generating train split:   4%|â–         | 82222/1949977 [00:02<00:59, 31545.24 examples/s]Generating train split:   5%|â–         | 92222/1949977 [00:03<01:27, 21213.87 examples/s]Generating train split:   6%|â–‹         | 122222/1949977 [00:03<00:47, 38685.68 examples/s]Generating train split:   7%|â–‹         | 132222/1949977 [00:04<00:47, 37959.94 examples/s]Generating train split:   8%|â–Š         | 154444/1949977 [00:04<00:50, 35850.46 examples/s]Generating train split:   8%|â–Š         | 164444/1949977 [00:05<00:49, 35979.40 examples/s]Generating train split:   9%|â–‰         | 174444/1949977 [00:06<01:28, 19968.64 examples/s]Generating train split:  11%|â–ˆ         | 214444/1949977 [00:06<00:42, 40720.37 examples/s]Generating train split:  12%|â–ˆâ–        | 226666/1949977 [00:07<00:50, 34330.04 examples/s]Generating train split:  12%|â–ˆâ–        | 236666/1949977 [00:07<00:48, 35494.61 examples/s]Generating train split:  13%|â–ˆâ–Ž        | 246666/1949977 [00:07<00:46, 36804.11 examples/s]Generating train split:  13%|â–ˆâ–Ž        | 256666/1949977 [00:08<01:11, 23594.65 examples/s]Generating train split:  15%|â–ˆâ–        | 288888/1949977 [00:08<00:37, 44343.82 examples/s]Generating train split:  16%|â–ˆâ–Œ        | 308888/1949977 [00:09<00:49, 33356.61 examples/s]Generating train split:  16%|â–ˆâ–‹        | 318888/1949977 [00:09<00:48, 33895.78 examples/s]Generating train split:  17%|â–ˆâ–‹        | 328888/1949977 [00:10<00:46, 34775.84 examples/s]Generating train split:  17%|â–ˆâ–‹        | 338888/1949977 [00:11<01:16, 21020.60 examples/s]Generating train split:  19%|â–ˆâ–‰        | 371110/1949977 [00:12<00:53, 29554.16 examples/s]Generating train split:  20%|â–ˆâ–‰        | 381110/1949977 [00:12<00:49, 31616.33 examples/s]Generating train split:  20%|â–ˆâ–ˆ        | 391110/1949977 [00:12<00:48, 32331.67 examples/s]Generating train split:  21%|â–ˆâ–ˆ        | 401110/1949977 [00:12<00:45, 33823.70 examples/s]Generating train split:  21%|â–ˆâ–ˆ        | 411110/1949977 [00:13<00:44, 34754.77 examples/s]Generating train split:  22%|â–ˆâ–ˆâ–       | 421110/1949977 [00:14<01:28, 17272.45 examples/s]Generating train split:  23%|â–ˆâ–ˆâ–Ž       | 443332/1949977 [00:15<01:05, 22905.37 examples/s]Generating train split:  23%|â–ˆâ–ˆâ–Ž       | 453332/1949977 [00:15<00:58, 25716.70 examples/s]Generating train split:  24%|â–ˆâ–ˆâ–       | 463332/1949977 [00:15<00:52, 28203.25 examples/s]Generating train split:  24%|â–ˆâ–ˆâ–       | 473332/1949977 [00:15<00:49, 29900.41 examples/s]Generating train split:  25%|â–ˆâ–ˆâ–       | 483332/1949977 [00:16<00:46, 31354.27 examples/s]Generating train split:  25%|â–ˆâ–ˆâ–Œ       | 493332/1949977 [00:16<00:43, 33235.77 examples/s]Generating train split:  26%|â–ˆâ–ˆâ–Œ       | 503332/1949977 [00:17<01:10, 20501.41 examples/s]Generating train split:  26%|â–ˆâ–ˆâ–‹       | 515554/1949977 [00:17<01:05, 21969.93 examples/s]Generating train split:  27%|â–ˆâ–ˆâ–‹       | 525554/1949977 [00:17<00:57, 24803.63 examples/s]Generating train split:  27%|â–ˆâ–ˆâ–‹       | 535554/1949977 [00:18<00:51, 27272.96 examples/s]Generating train split:  28%|â–ˆâ–ˆâ–Š       | 545554/1949977 [00:18<00:44, 31258.39 examples/s]Generating train split:  28%|â–ˆâ–ˆâ–Š       | 555554/1949977 [00:18<00:43, 32383.92 examples/s]Generating train split:  29%|â–ˆâ–ˆâ–‰       | 565554/1949977 [00:19<00:41, 33555.16 examples/s]Generating train split:  30%|â–ˆâ–ˆâ–‰       | 577776/1949977 [00:19<00:56, 24427.18 examples/s]Generating train split:  30%|â–ˆâ–ˆâ–ˆ       | 587776/1949977 [00:20<01:02, 21783.34 examples/s]Generating train split:  31%|â–ˆâ–ˆâ–ˆ       | 597776/1949977 [00:20<00:57, 23579.55 examples/s]Generating train split:  31%|â–ˆâ–ˆâ–ˆ       | 607776/1949977 [00:21<00:52, 25566.07 examples/s]Generating train split:  32%|â–ˆâ–ˆâ–ˆâ–      | 617776/1949977 [00:21<00:46, 28837.25 examples/s]Generating train split:  32%|â–ˆâ–ˆâ–ˆâ–      | 627776/1949977 [00:21<00:41, 31584.73 examples/s]Generating train split:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 637776/1949977 [00:21<00:40, 32801.21 examples/s]Generating train split:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 649998/1949977 [00:23<01:36, 13494.54 examples/s]Generating train split:  34%|â–ˆâ–ˆâ–ˆâ–      | 659998/1949977 [00:24<01:29, 14404.23 examples/s]Generating train split:  34%|â–ˆâ–ˆâ–ˆâ–      | 669998/1949977 [00:24<01:09, 18492.24 examples/s]Generating train split:  35%|â–ˆâ–ˆâ–ˆâ–      | 679998/1949977 [00:24<00:59, 21282.19 examples/s]Generating train split:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 689998/1949977 [00:24<00:49, 25655.50 examples/s]Generating train split:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 699998/1949977 [00:25<00:41, 29861.18 examples/s]Generating train split:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 709998/1949977 [00:25<00:36, 34006.35 examples/s]Generating train split:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 722220/1949977 [00:26<01:04, 18946.71 examples/s]Generating train split:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 732220/1949977 [00:27<01:07, 17971.97 examples/s]Generating train split:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 742220/1949977 [00:27<00:56, 21431.69 examples/s]Generating train split:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 752220/1949977 [00:27<00:46, 25928.00 examples/s]Generating train split:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 762220/1949977 [00:27<00:41, 28819.97 examples/s]Generating train split:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 772220/1949977 [00:28<00:36, 32611.57 examples/s]Generating train split:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 782220/1949977 [00:28<00:33, 35151.48 examples/s]Generating train split:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 804441/1949977 [00:30<01:00, 18933.32 examples/s]Generating train split:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 844441/1949977 [00:30<00:28, 39262.63 examples/s]Generating train split:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 876662/1949977 [00:30<00:23, 46051.11 examples/s]Generating train split:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 886662/1949977 [00:31<00:34, 30691.32 examples/s]Generating train split:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 926662/1949977 [00:31<00:20, 48818.38 examples/s]Generating train split:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 948883/1949977 [00:32<00:22, 44084.08 examples/s]Generating train split:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 958883/1949977 [00:32<00:22, 43413.76 examples/s]Generating train split:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 968883/1949977 [00:33<00:34, 28271.99 examples/s]Generating train split:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1008883/1949977 [00:33<00:18, 51028.99 examples/s]Generating train split:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1021104/1949977 [00:34<00:22, 40825.79 examples/s]Generating train split:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1031104/1949977 [00:34<00:22, 40190.88 examples/s]Generating train split:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1041104/1949977 [00:35<00:23, 38920.45 examples/s]Generating train split:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1051104/1949977 [00:36<00:37, 23864.28 examples/s]Generating train split:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1071104/1949977 [00:36<00:27, 32019.05 examples/s]Generating train split:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1093325/1949977 [00:36<00:26, 32663.37 examples/s]Generating train split:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1103325/1949977 [00:37<00:25, 33692.29 examples/s]Generating train split:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1113325/1949977 [00:37<00:23, 36019.38 examples/s]Generating train split:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1123325/1949977 [00:37<00:23, 35906.38 examples/s]Generating train split:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1133325/1949977 [00:39<01:00, 13476.18 examples/s]Generating train split:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1165546/1949977 [00:40<00:33, 23293.20 examples/s]Generating train split:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1175546/1949977 [00:40<00:30, 25071.29 examples/s]Generating train split:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1185546/1949977 [00:40<00:27, 27954.03 examples/s]Generating train split:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1195546/1949977 [00:41<00:24, 30534.87 examples/s]Generating train split:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1205546/1949977 [00:41<00:22, 33112.40 examples/s]Generating train split:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1215546/1949977 [00:42<00:40, 17941.54 examples/s]Generating train split:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1237767/1949977 [00:43<00:30, 23533.60 examples/s]Generating train split:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1247767/1949977 [00:43<00:26, 26313.86 examples/s]Generating train split:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1257767/1949977 [00:43<00:24, 28576.14 examples/s]Generating train split:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1267767/1949977 [00:43<00:22, 30904.16 examples/s]Generating train split:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1277767/1949977 [00:44<00:20, 32743.42 examples/s]Generating train split:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1287767/1949977 [00:44<00:22, 29154.63 examples/s]Generating train split:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1297767/1949977 [00:45<00:33, 19676.83 examples/s]Generating train split:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1309988/1949977 [00:46<00:30, 20698.52 examples/s]Generating train split:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1319988/1949977 [00:46<00:26, 23769.54 examples/s]Generating train split:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1329988/1949977 [00:46<00:22, 27621.75 examples/s]Generating train split:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1339988/1949977 [00:46<00:21, 27900.85 examples/s]Generating train split:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1349988/1949977 [00:47<00:19, 30767.42 examples/s]Generating train split:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1359988/1949977 [00:47<00:17, 32890.56 examples/s]Generating train split:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1369988/1949977 [00:48<00:30, 19239.13 examples/s]Generating train split:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1382209/1949977 [00:48<00:28, 19981.99 examples/s]Generating train split:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1392209/1949977 [00:49<00:23, 23352.06 examples/s]Generating train split:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1402209/1949977 [00:49<00:21, 25978.52 examples/s]Generating train split:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1412209/1949977 [00:49<00:18, 28839.39 examples/s]Generating train split:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1422209/1949977 [00:49<00:16, 31460.10 examples/s]Generating train split:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1432209/1949977 [00:50<00:15, 33161.67 examples/s]Generating train split:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1442209/1949977 [00:51<00:26, 19234.12 examples/s]Generating train split:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1454430/1949977 [00:51<00:24, 20277.80 examples/s]Generating train split:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1464430/1949977 [00:52<00:20, 23960.51 examples/s]Generating train split:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1474430/1949977 [00:52<00:17, 27601.58 examples/s]Generating train split:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1484430/1949977 [00:52<00:15, 30462.44 examples/s]Generating train split:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1494430/1949977 [00:52<00:13, 34031.49 examples/s]Generating train split:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1504430/1949977 [00:52<00:12, 35942.23 examples/s]Generating train split:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1516651/1949977 [00:54<00:22, 19404.77 examples/s]Generating train split:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1526651/1949977 [00:54<00:22, 19104.56 examples/s]Generating train split:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1536651/1949977 [00:54<00:18, 22927.98 examples/s]Generating train split:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1546651/1949977 [00:55<00:15, 26263.62 examples/s]Generating train split:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1556651/1949977 [00:55<00:12, 31074.43 examples/s]Generating train split:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1566651/1949977 [00:55<00:11, 33746.01 examples/s]Generating train split:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1576651/1949977 [00:55<00:10, 35059.28 examples/s]Generating train split:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1598872/1949977 [00:57<00:17, 20556.05 examples/s]Generating train split:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1638872/1949977 [00:57<00:07, 42368.91 examples/s]Generating train split:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1658872/1949977 [00:57<00:05, 52098.25 examples/s]Generating train split:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1671093/1949977 [00:58<00:06, 40981.40 examples/s]Generating train split:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1681093/1949977 [00:59<00:12, 20723.62 examples/s]Generating train split:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1721093/1949977 [00:59<00:05, 39925.57 examples/s]Generating train split:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1743314/1949977 [01:00<00:05, 39491.85 examples/s]Generating train split:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1763314/1949977 [01:02<00:07, 24783.32 examples/s]Generating train split:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1803314/1949977 [01:02<00:03, 41243.44 examples/s]Generating train split:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1825535/1949977 [01:02<00:03, 37762.06 examples/s]Generating train split:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1845535/1949977 [01:04<00:04, 26077.14 examples/s]Generating train split:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1887756/1949977 [01:05<00:01, 34655.14 examples/s]Generating train split:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1897756/1949977 [01:05<00:01, 35678.97 examples/s]Generating train split:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1907756/1949977 [01:05<00:01, 35582.80 examples/s]Generating train split:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1917756/1949977 [01:05<00:00, 35527.93 examples/s]Generating train split:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1927756/1949977 [01:07<00:01, 20563.75 examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1949977/1949977 [01:07<00:00, 28833.53 examples/s]
2025-09-18 15:43:00,282 - ERROR - Failed to load DETOX dataset. Aborting. Error: 'text'
2025-09-18 15:44:29,024 - INFO - Loading configuration from config/smoke_test.yaml
2025-09-18 15:44:29,071 - INFO - Environment set up with random seed: 42
2025-09-18 15:44:29,071 - INFO - ====== Starting Phase 1: Data Preprocessing ======
2025-09-18 15:44:29,071 - INFO - Starting preprocessing...
Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
2025-09-18 15:44:29,247 - INFO - Loading Alpaca dataset (as PromptSource substitute)
2025-09-18 15:44:32,236 - INFO - Loading LAION-Aesthetics dataset (as Stable Diffusion logs substitute)
2025-09-18 15:44:35,182 - INFO - Loading DETOX dataset for safety labels
2025-09-18 15:44:38,192 - INFO - Total processed examples: 768
2025-09-18 15:44:38,372 - INFO - Total examples with language ID: 768
2025-09-18 15:44:38,387 - INFO - Saved datasets to data/processed/smoke_test
2025-09-18 15:44:38,387 - INFO - Train size: 614, Val size: 77, Test size: 77
2025-09-18 15:44:38,387 - INFO - Preprocessing finished successfully.
2025-09-18 15:44:38,402 - INFO - ====== Finished Phase 1: Data Preprocessing ======
2025-09-18 15:44:38,402 - INFO - 
====== Starting Phase 2: Model Training ======
2025-09-18 15:44:38,402 - INFO - Starting training process...
2025-09-18 15:44:38,403 - INFO - Loading preprocessed data.
2025-09-18 15:44:38,405 - CRITICAL - An unhandled exception occurred during the pipeline: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL src.preprocess.PreprocessedDataset was not an allowed global by default. Please use `torch.serialization.add_safe_globals([src.preprocess.PreprocessedDataset])` or the `torch.serialization.safe_globals([src.preprocess.PreprocessedDataset])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
Traceback (most recent call last):
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/src/main.py", line 69, in main
    train.run_training(config)
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/src/train.py", line 145, in run_training
    train_data = torch.load(os.path.join(processed_dir, 'train_data.pt'))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/.venv/lib/python3.11/site-packages/torch/serialization.py", line 1529, in load
    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL src.preprocess.PreprocessedDataset was not an allowed global by default. Please use `torch.serialization.add_safe_globals([src.preprocess.PreprocessedDataset])` or the `torch.serialization.safe_globals([src.preprocess.PreprocessedDataset])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
2025-09-18 15:45:41,355 - INFO - Loading configuration from config/smoke_test.yaml
2025-09-18 15:45:41,395 - INFO - Environment set up with random seed: 42
2025-09-18 15:45:41,395 - INFO - ====== Starting Phase 1: Data Preprocessing ======
2025-09-18 15:45:41,395 - INFO - Starting preprocessing...
Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.
2025-09-18 15:45:41,579 - INFO - Loading Alpaca dataset (as PromptSource substitute)
2025-09-18 15:45:46,238 - INFO - Loading LAION-Aesthetics dataset (as Stable Diffusion logs substitute)
2025-09-18 15:45:48,990 - INFO - Loading DETOX dataset for safety labels
2025-09-18 15:45:52,471 - INFO - Total processed examples: 768
2025-09-18 15:45:52,723 - INFO - Total examples with language ID: 768
2025-09-18 15:45:52,738 - INFO - Saved datasets to data/processed/smoke_test
2025-09-18 15:45:52,738 - INFO - Train size: 614, Val size: 77, Test size: 77
2025-09-18 15:45:52,738 - INFO - Preprocessing finished successfully.
2025-09-18 15:45:52,756 - INFO - ====== Finished Phase 1: Data Preprocessing ======
2025-09-18 15:45:52,756 - INFO - 
====== Starting Phase 2: Model Training ======
2025-09-18 15:45:52,756 - INFO - Starting training process...
2025-09-18 15:45:52,757 - INFO - Loading preprocessed data.
2025-09-18 15:45:52,763 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-09-18 15:45:56,552 - INFO - Phase 1: Training Safety-Unified Head and Reward Model.
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.10s/it]Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.10s/it]
2025-09-18 15:45:57,761 - CRITICAL - An unhandled exception occurred during the pipeline: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
Traceback (most recent call last):
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/src/main.py", line 69, in main
    train.run_training(config)
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/src/train.py", line 177, in run_training
    safety_logits, latent_embeddings = safety_head(embeddings)
                                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/src/train.py", line 36, in forward
    latent = torch.relu(self.projector(x))
                        ^^^^^^^^^^^^^^^^^
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/pt-80-1-a-13/_work/experiment_matsuzawa_250918_5/experiment_matsuzawa_250918_5/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
