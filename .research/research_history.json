{
  "research_topic": "Test-Time Adaptationを収束速度に関して改善したい",
  "queries": [
    "Test-Time Adaptation convergence",
    "Rapid test-time adaptation",
    "TTA optimization",
    "Online adaptation acceleration",
    "Test-Time learning rate adaptation"
  ],
  "research_study_list": [
    {
      "title": "Improved Test-Time Adaptation for Domain Generalization",
      "abstract": "The main challenge in domain generalization (DG) is to handle the\ndistribution shift problem that lies between the training and test data. Recent\nstudies suggest that test-time training (TTT), which adapts the learned model\nwith test data, might be a promising solution to the problem. Generally, a TTT\nstrategy hinges its performance on two main factors: selecting an appropriate\nauxiliary TTT task for updating and identifying reliable parameters to update\nduring the test phase. Both previous arts and our experiments indicate that TTT\nmay not improve but be detrimental to the learned model if those two factors\nare not properly considered. This work addresses those two factors by proposing\nan Improved Test-Time Adaptation (ITTA) method. First, instead of heuristically\ndefining an auxiliary objective, we propose a learnable consistency loss for\nthe TTT task, which contains learnable parameters that can be adjusted toward\nbetter alignment between our TTT task and the main prediction task. Second, we\nintroduce additional adaptive parameters for the trained model, and we suggest\nonly updating the adaptive parameters during the test phase. Through extensive\nexperiments, we show that the proposed two strategies are beneficial for the\nlearned model (see Figure 1), and ITTA could achieve superior performance to\nthe current state-of-the-art methods on several DG benchmarks. Code is\navailable at https://github.com/liangchen527/ITTA.",
      "full_text": "Improved Test-Time Adaptation for Domain Generalization Liang Chen1 Yong Zhang2* Yibing Song3 Ying Shan2 Lingqiao Liu1∗ 1 The University of Adelaide 2 Tencent AI Lab 3 AI3 Institute, Fudan University {liangchen527, zhangyong201303, yibingsong.cv}@gmail.com yingsshan@tencent.com lingqiao.liu@adelaide.edu.au Abstract The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Gen- erally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for up- dating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments in- dicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly consid- ered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, in- stead of heuristically defining an auxiliary objective, we pro- pose a learnable consistency loss for the TTT task, which con- tains learnable parameters that can be adjusted toward bet- ter alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adap- tive parameters during the test phase. Through extensive ex- periments, we show that the proposed two strategies are ben- eficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-art methods on several DG benchmarks. Code is available at https://github.com/liangchen527/ITTA. 1. Introduction Recent years have witnessed the rapid development of deep learning models, which often assume the training and test data are from the same domain and follow the same distribution. However, this assumption does not always hold in real-world scenarios. Distribution shift among the source and target domains is ubiquitous in related areas [35], such as autonomous driving or object recognition tasks, resulting *Corresponding authors. This work is done when L. Chen is an intern in Tencent AI Lab. 0.5 1.1 0.5 1.2 0.5 0.5 0.5 1.4 0.4 0.4 0.4 0.3 art cartoon photo sketch 79.9 75.4 94.4 75.8 83.3 76.0 94.4 76.7 84.7 78.0 94.5 78.2 Figure 1. Performance improvements from the proposed two strate- gies (i.e. introducing a learnable consistency loss and including additional adaptive parameters to improve TTT) for the baseline model (i.e. ResNet18 [30] with existing augmentation strategy [75]). Experiments are conducted on the PACS dataset [37] with the leave- one-out setting. Following [27], we use 60 sets of random seeds and hyper-parameters for each target domain. The reported average accuracy and error bars verify the effectiveness of our method. in poor performances for delicately designed models and hindering the further application of deep learning techniques. Domain generalization (DG) [2,8,16,23,24,31,38 –40,40, 44, 47, 51, 52, 69], designed to generalize a learned model to unseen target domains, has attracted a great deal of attention in the research community. The problem can be traced back to a decade ago [7], and various approaches have been pro- posed to push the DG boundary ever since. Those efforts in- clude invariant representation learning [28,47,49,58], adver- sarial learning [23,40,44,69], augmentation [9,41,42,66,75], or meta-learning [2, 16, 38, 39]. Despite successes on certain occasions, a recent study [27] shows that, under a rigorous evaluation protocol, most of these arts are inferior to the baseline empirical risk minimization (ERM) method [61]. This finding is not surprising, as most current arts strive to decrease the distribution shift only through the training data while overlooking the contributions from test samples. Recently, the test-time training (TTT) technique [60] has been gaining momentum for easing the distribution shift problem. TTT lies its success in enabling dynamic tuning of the pretrained model with the test samples via an auxil- iary TTT task, which seems to be a promising effort when arXiv:2304.04494v2  [cs.CV]  16 Apr 2023confronting data from different domains. However, TTT is not guaranteed to improve the performance. Previous arts [46, 63] indicate that selecting an appropriate auxiliary TTT task is crucial, and an inappropriate one that does not align with the main loss may deteriorate instead of improv- ing the performance. Meanwhile, it is pointed out in [63] that identifying reliable parameters to update is also essential for generalization, which is in line with our experimental findings in Sec. 5.3. Both of these two tasks are non-trivial, and there are limited efforts made to address them. This paper aims to improve the TTT strategy for better DG. First, different from previous works that empirically define auxiliary objectives and assume they are aligned with the main task, our work does not make such assumptions. Instead, we suggest learning an appropriate auxiliary loss for test-time updating. Specifically, encouraged by recent successes in multi-view consistency learning [13,26,29], we propose to augment the consistency loss by adding learn- able parameters based on the original implementation, where the parameters can be adjusted to assure our TTT task can be more aligned with the main task and are updated by en- forcing the two tasks share the same optimization direction. Second, considering that identifying reliable parameters to update is an everlasting job given the growing size of current deep models, we suggest introducing new adaptive param- eters after each block during the test phase, and we only tune the new parameters by the learned consistency loss while leaving the original parameters unchanged. Through extensive evaluations on the current benchmark [27], we illustrate that the learnable consistency loss performs more effectively than the self-supervised TTT tasks adopted in previous arts [60, 63], and by tuning only the new adaptive parameters, our method is superior to existing strategies that update all the parameters or part of them. This work aims to ease the distribution shift problem by improving TTT, and the main contributions are three-fold: • We introduce a learnable consistency loss for test-time adaptation, which can be enforced to be more aligned with the main loss by tuning its learnable parameters. • We introduce new adaptive parameters for the trained model and only update them during the test phase. • We conduct experiments on various DG benchmarks and illustrate that our ITTA performs competitively against current arts under the rigorous setting [27] for both the multi-source and single-source DG tasks. 2. Related Works 2.1. Domain Generalization. Being able to generalize to new environments while de- ploying is a challenging and practical requirement for cur- rent deep models. Existing DG approaches can be roughly categorized into three types. (1) Invariant representation learning: The pioneering work [5] theoretically proves that if the features remain invariant across different domains, then they are general and transferable to different domains. Guided by this finding, [47] uses maximum mean discrep- ancy (MMD) to align the learned features, and [25] proposes to use a multi-domain reconstruction auto-encoder to obtain invariant features. More recently, [58] suggests maximiz- ing the inner product of gradients from different domains to enforce invariance, and a similar idea is proposed in [52] where these gradients are expected to be similar to their mean values. (2) Optimization algorithms: Among the different optimization techniques adopted in DG, prevail- ing approaches resort to adversarial learning [23, 40, 44, 69] and meta-learning [2, 16, 38, 39]. Adversarial training is often used to enforce the learned features to be agnostic about the domain information. In [23], a domain-adversarial neural network (DANN) is implemented by asking the main- stream feature to maximize the domain classification loss. This idea is also adopted in [44], where adversarial training and an MMD constraint are employed to update an auto- encoder. Meanwhile, the meta-learning technique is used to simulate the distribution shifts between seen and unseen environments [2, 16, 38, 39], and most of these works are developed based on the MAML framework [20]. (3) Aug- mentation: Most augmentation skills applied in the general- ization tasks are operated in the feature level [34, 41, 48, 75] except for [11,66,68] which mix images [68] or its phase [66] to synthesize new data. To enable contrastive learning, we incorporate an existing augmentation strategy [75] in our framework. This method originated from AdaIN [32], which synthesizes new domain information by mixing the statistics of the features. Similar ideas can be found in [42, 48]. 2.2. Test-Time Training and Adaptation Test-Time Training (TTT) is first introduced in [60]. The basic paradigm is to employ a test-time task besides the main task during the training phase and update the pre- trained model using the test data with only the test-time objective before the final prediction step. The idea is empir- ically proved effective [60] and further developed in other related areas [3, 10, 12, 14, 21, 22, 43, 56, 63, 65, 73, 74]. Most current works focus on finding auxiliary tasks for updat- ing during the test phase, and the efforts derive from self- supervion [3, 10, 21, 22, 43, 60], meta-learning [65, 73, 74], information entropy [63], pseudo-labeling [12, 14], to name a few. However, not all empirically selected test-time tasks are effective. A recent study [46] indicates that only when the auxiliary loss aligns with the main loss can TTT improve the trained model. Inspired by that, we propose a learnable consistency loss and enforce alignment between the two ob- jectives. Results show that our strategy can be beneficial for the trained model (see Figure 1).subtract Figure 2. Training process of ITTA. We use x from the source domain as input for the feature extractor fθ(·) to obtain the repre- sentation z and its augmented version z′, where the augmentation skill from [75] is applied. The classifier fϕ(·) and weight subnet- work fw(·) are used to compute the main loss Lmain and learnable consistency loss Lwcont. Please refer to our text for details. Meanwhile, [63] suggests that auxiliary loss is not the only factor that affects the performance. Selecting reliable parameters to update is also crucial within the TTT frame- work. Given the large size of current models, correctly iden- tifying these parameters may require tremendous amounts of effort. To this end, instead of heuristically selecting candi- dates, we propose to include new adaptive parameters for up- dating during the test phase. Experimental results show that the proposed method can obtain comparable performances against existing skills. 3. Methodology In the task of DG, we are often given access to data from S (S ≥ 1) source domains Ds = {D1, D2, ..., DS} and expect a model to make good prediction on unseen target domains Dt = {D1, D2, ..., DT } (T ≥ 1). Our method aims to improve the test-time training (TTT) strategy for better DG. The improvements are two-fold. First, we pro- pose a learnable consistency loss for the TTT task, which could be enforced to align with the main objective by tuning its learnable weights. Second, we suggest including addi- tional adaptive parameters and only updating these adaptive parameters during the test phase. 3.1. A Learnable Consistency Loss for TTT The TTT strategies have shown promising performances when dealing with distribution shift problems [43, 63]. How- ever, their successes are depended on the empirically selected auxiliary TTT tasks, which may deteriorate the performances if chosen improperly. Motivated by the recent successes in multi-view consistency learning [13, 26, 29], we suggest adopting a consistency loss in our TTT task. Note that the naive consistency loss is still not guaranteed to be effective as prior art [46] indicates that only when the auxiliary loss aligns with the main loss, can TTT improves the perfor- mance. To this end, we propose to augment the auxiliary loss with learnable parameters that could be adjusted toward a better alignment between the TTT and main tasks. In our case, we make the adopted consistency loss learnable by introducing a weight subnetwork that allows flexible ways Algorithm 1 Pseudo code of the training phase of ITTA in a PyTorch-like style. # fθ, fϕ, fw: feature extractor, classifier, weight subnetwork # α, 0: weight paramter, all zero tensor # training process for x, yin training loader: # load a minibatch with N samples def forward process(x, y): z, z′ = fθ.forward(x) # computing losses Lmain = CrossEntropyLoss(fϕ.forward(z), y) Lmain+ =CrossEntropyLoss(fϕ.forward(z′), y) Lwcont = MSELoss(fw.forward(z − z′), 0) return Lmain, Lwcont # SGD update: feature extractor and classifier Lmain, Lwcont = forward process(x, y) ([fθ.params, fϕ.params]).zero grad() (Lmain + αLwcont).backward() update( \u0002 fθ.params, fϕ.params \u0003 ) # compute objectives for updating weight subnetwork Lmain, Lwcont = forward process(x, y) Lmain.backward() ˆgmain = fθ.params.grad.clone().normalize() fθ.params.zero grad() Lwcont.backward() ˆgwcont = fθ.params.grad.clone().normalize() # SGD update: weight subnetwork MSELoss(ˆgmain, ˆgwcont).backward() fw.params.zero grad() update(fw.params) to measure the consistency between two views of the same instance. We first introduce the pipeline of our training framework. Given the D dimensional representation z ∈ RD1 and its corresponding augmented version z′ that are obtained from a feature extractor (i.e. {z, z′} = fθ(x), where x is an input image from Ds, and fθ(·) is the feature extractor parame- terized by θ. In our implementation, we use the existing augmentation method [75] to obtain z′ by modifying the intermediate activation in fθ(x). We show in our supplemen- tary material that our framework can also thrive with other augmentation strategies), our learnable consistency loss is given by, Lwcont = ∥fw(z − z′)∥, (1) where ∥ · ∥denotes the L2 norm; fw(·) is the weight sub- network parameterized by w. To make the training process more stable and potentially achieve better performance, we apply a dimension-wise nonlinear function to map each di- mension of z − z′ before calculating the L2 norm. That is, ∀h ∈ RD, fw(h) is implemented by stacking layers of a nonlinear function: ReLU(a ∗ h + b), where a ∈ RD and b ∈ RD are the weight and bias from the nonlinear function, 1We omit the batch dimensions of the variables for simplicity.… … subtract Figure 3. Test adaptation process of ITTA. Different from that in the training stage, we include additional adaptive parameters fΘ after each block of the feature extractor fθ. For each test sample x, the intermediate representations zi and z′i obtained from fi θ are passed to fi Θ before going to the next block fi+1 θ . We use the learnable consistency loss Lwcont as the objective to update fΘ. Please refer to our text for details. and different layers of a, bform the parameter w in fw. In effect, this creates a piecewise-linear mapping function for h: depending on the value of h, the output could be 0, a constant, or a scaling-and-shifted version of h. More studies about the design of fw are provided in our supplementary material. Compared to the naive consistency learning with- out fw, our Lwcont can be more flexible with an adjustable fw, which we show in the following is the key for learning an appropriate loss in the improved TTT framework. Combining Lwcont with the main loss Lmain which applies the cross-entropy loss (CE) for both the origi- nal and augmented inputs ( i.e. Lmain = CE(fϕ(z), y) + CE(fϕ(z′), y), where fϕ is the classifier parameterized by ϕ, and y is the corresponding label), the objective for the feature extractor and classifier can be formulated into, min{θ,ϕ} Lmain + αLwcont, (2) where α is the weight parameter that balances the contri- butions from the two terms. A simple illustration of the workflow is shown in Figure 2. From Eq. (2), the expected gradients for the feature ex- tractor from Lmain and Lwcont can be represented as, \u001a gmain = ∇θ(CE(fϕ(z), y) + CE(fϕ(z′), y)), (3) gwcont = ∇θ∥fw(z − z′)∥. (4) We observe that the direction of gwcont is also determined by the weight subnetwork fw(·), which should be close with gmain to ensure alignment between Lmain and Lwcont [46, 60]. To this end, we propose a straightforward solution by enforcing equality between the normalized versions of gmain and gwcont, and we use this term as the objective for updating fw(·), which gives, min w Lalign, s.t. Lalign = ∥ˆgmain − ˆgwcont∥, (5) where ˆgmain = gmain−Egmain σgmain , and similar for ˆgwcont. In our implementation, we update {θ, ϕ} and w in an alternative manner. Pseudo code of the training process are shown in Algorithm 1. Algorithm 2 Pseudo code of the test phase of ITTA in a PyTorch-like style. # fθ, fϕ: feature extractor, classifier # fw, fΘ: weight subnetwork, additional adaptive blocks # m, 0: total number of blocks in fθ, all zero tensor # test process for x in test loader: # load a test batch def forward process(x): z1, z′1 = f1 Θ.forward((f1 θ .forward(x))) # first blocks for i in range(2, m + 1): # the following m − 1 blocks zi, z′i = fi θ.forward(zi−1), fi θ.forward(z′i−1) zi, z′i = fi Θ.forward(zi), fi Θ.forward(z′i) return zi, z′i # test adaptation phase: SGD update additional adaptive parameters z, z′ = forward process(x) Lwcont = MSELoss(fw.forward(z − z′), 0) fΘ.params.zero grad() Lwcont.backward() update(fΘ.params) # final prediction z, = forward process(x) result = fϕ.forward(z) 3.2. Including Additional Adaptive Parameters Selecting expressive and reliable parameters to update during the test phase is also essential in the TTT frame- work [63]. Some strategies decide to update all the parame- ters from the feature extractor [3, 43], while others use only the parameters from the specific layers for updating [63, 71]. Given the fact that the sizes of current deep models are often very large and still growing, exhaustively trying different combinations among the millions of candidates seems to be an everlasting job. As there are no consensuses on which parameter should be updated, we suggest another easy alter- native in this work. Specifically, assuming there are a total of m blocks in the pretrained feature extractor fθ(·), and the i-th block can be denoted as fi θ(·). Then the intermediate representation zi from fi θ(·) can be formulated as, zi = fi θ(zi−1), s.t. z1 = f1 θ (x). (6) We propose to include additional adaptive blockfΘ that is parameterized by Θ after each block of fθ during the test- time adaptation phase, which reformulates Eq. (6) into, zi = fi Θ(fi θ(zi−1)), s.t. z1 = f1 Θ(f1 θ (x)), (7) where fΘ(·) does not change the dimension and sizes of the intermediate representations. In our work, we use a structure similar to fw to implement fΘ. Note zm is simplified as z in this phase, and the same process is applied for obtaining z′. Then, in the test-time adaptation phase, we suggest only updating the new adaptive parameters via the learned con- sistency loss. The optimization process can be written as,Table 1. Multi sources domain generalization. Experiments are conducted on the DomainBed benchmark [27]. All methods are examined for 60 trials in each unseen domain. Top5 accumulates the number of datasets where a method achieves the top 5 performances. The score here accumulates the numbers of the dataset where a specific art obtains larger accuracy than ERM on account of the variance. Best results are colored as red. Among the 22 methods compared, less than a quarter outperforms ERM in most datasets (Score ≥ 3). PACS VLCS OfficeHome TerraInc DomainNet Avg. Top5↑ Score↑ MMD [40] 81.3 ± 0.8 74.9 ± 0.5 59.9 ± 0.4 42.0 ± 1.0 7.9 ± 6.2 53.2 1 2 RSC [33] 80.5 ± 0.2 75.4 ± 0.3 58.4 ± 0.6 39.4 ± 1.3 27.9 ± 2.0 56.3 0 1 IRM [1] 80.9 ± 0.5 75.1 ± 0.1 58.0 ± 0.1 38.4 ± 0.9 30.4 ± 1.0 56.6 0 1 ARM [72] 80.6 ± 0.5 75.9 ± 0.3 59.6 ± 0.3 37.4 ± 1.9 29.9 ± 0.1 56.7 0 0 DANN [23] 79.2 ± 0.3 76.3 ± 0.2 59.5 ± 0.5 37.9 ± 0.9 31.5 ± 0.1 56.9 1 1 GroupGRO [55] 80.7 ± 0.4 75.4 ± 1.0 60.6 ± 0.3 41.5 ± 2.0 27.5 ± 0.1 57.1 0 1 CDANN [44] 80.3 ± 0.5 76.0 ± 0.5 59.3 ± 0.4 38.6 ± 2.3 31.8 ± 0.2 57.2 0 0 VREx [36] 80.2 ± 0.5 75.3 ± 0.6 59.5 ± 0.1 43.2 ± 0.3 28.1 ± 1.0 57.3 1 1 CAD [53] 81.9 ± 0.3 75.2 ± 0.6 60.5 ± 0.3 40.5 ± 0.4 31.0 ± 0.8 57.8 1 2 CondCAD [53] 80.8 ± 0.5 76.1 ± 0.3 61.0 ± 0.4 39.7 ± 0.4 31.9 ± 0.7 57.9 0 1 MTL [6] 80.1 ± 0.8 75.2 ± 0.3 59.9 ± 0.5 40.4 ± 1.0 35.0 ± 0.0 58.1 0 0 ERM [61] 79.8 ± 0.4 75.8 ± 0.2 60.6 ± 0.2 38.8 ± 1.0 35.3 ± 0.1 58.1 1 - MixStyle [75] 82.6 ± 0.4 75.2 ± 0.7 59.6 ± 0.8 40.9 ± 1.1 33.9 ± 0.1 58.4 1 1 MLDG [38] 81.3 ± 0.2 75.2 ± 0.3 60.9 ± 0.2 40.1 ± 0.9 35.4 ± 0.0 58.6 1 1 Mixup [68] 79.2 ± 0.9 76.2 ± 0.3 61.7 ± 0.5 42.1 ± 0.7 34.0 ± 0.0 58.6 2 2 Fishr [52] 81.3 ± 0.3 76.2 ± 0.3 60.9 ± 0.3 42.6 ± 1.0 34.2 ± 0.3 59.0 2 2 SagNet [48] 81.7 ± 0.6 75.4 ± 0.8 62.5 ± 0.3 40.6 ± 1.5 35.3 ± 0.1 59.1 1 2 SelfReg [34] 81.8 ± 0.3 76.4 ± 0.7 62.4 ± 0.1 41.3 ± 0.3 34.7 ± 0.2 59.3 2 3 Fish [58] 82.0 ± 0.3 76.9 ± 0.2 62.0 ± 0.6 40.2 ± 0.6 35.5 ± 0.0 59.3 3 4 CORAL [59] 81.7 ± 0.0 75.5 ± 0.4 62.4 ± 0.4 41.4 ± 1.8 36.1 ± 0.2 59.4 2 3 SD [51] 81.9 ± 0.3 75.5 ± 0.4 62.9 ± 0.2 42.0 ± 1.0 36.3 ± 0.2 59.7 4 4 Ours 83.8 ± 0.3 76.9 ± 0.6 62.0 ± 0.2 43.2 ± 0.5 34.9 ± 0.1 60.2 4 4 min Θ ∥fw(z − z′)∥, s.t. {z, z′} = fΘ(fθ(x)). (8) Note that different from the training phase, x in this stage is from the target domain Dt, and we use the online setting in [60] for updating. A simple illustration of the test adaptation pipeline is shown in Figure 3. For the final step, we use the original representation ob- tained from the pretrained feature extractor and the adapted adaptive parameters for prediction. Pseudo code of the test stage are shown in Algorithm 2. 4. Experiments 4.1. Settings Datasets. We evalute ITTA on five benchmark datasets: PACS [37] which consists of 9,991 images from 7 cate- gories. This dataset is probably the most widely-used DG benchmark owing to its large distributional shift across 4 do- mains including art painting, cartoon, photo, and sketch; VLCS [18] contains 10,729 images of 5 classes from 4 different datasets (i.e. domains) including PASCAL VOC 2007 [17], LabelMe [54], Caltech [19], and Sun [64] where each dataset is considered a domain in DG;OfficeHome [62] is composed of 15,588 images from 65 classes in office and home environments, and those images can be categorized into 4 domains (i.e. artistic, clipart, product, and real world); TerraInc [4] has 24,788 images from 10 classes. Those images are wild animals taken from 4 different locations (i.e. domains) including L100, L38, L43, and L46; Domain- Net [50] which contains 586,575 images from 345 classes, and the images in it can be depicted in 6 styles (i.e. clipart, infograph, painting, quickdraw, real, and sketch). Implementation details. For all the experiments, we use the ImageNet [15] pretrained ResNet18 [30] backbone that with 4 blocks as the feature extractor fθ, which could en- large the gaps in DG compared to larger models [70]. Corre- spondingly, we also include 4 blocks of additional adaptive parameters (i.e. fΘ), and each block is implemented with 5 layers of learnable parameters with weight initialized as all ones and bias initialized as all zeros. For the weight subnet- work fw, we use 10 layers of learnable parameters with the initialization skill similar to that of fΘ. The classifier fϕ is an MLP layer provided by the Domainbed benchmark [27]. For the weight parameter α in Eq. (2), we set it to be 1 for all experiments (please refer to our supplementary material for analysis). The random seeds, learning rates, batch size, and augmentation skills are all dynamically set for all the compared arts according to [27].Table 2. Single source domain generalization. Experiments are conducted on the PACS dataset [37]. Here A, C, P, and S are the art, cartoon, photo, and sketch domains in PACS. A→C represents models trained on the art domain and tested on the cartoon domain, and similar for others. All methods are examined for 60 trials in each unseen domain. Best results are colored as red. A→C A →P A →S C →A C →P C →S P →A P →C P →S S →A S →C S →P Avg. RSC 66.3 ±1.3 88.2±0.6 57.2±3.1 65.8±1.5 82.4±0.6 68.7±2.5 60.5±2.0 41.3±6.0 53.1±2.8 53.8±1.6 65.9±0.7 48.4±1.9 62.6 Fish 67.1 ±0.5 89.2±1.8 57.0±0.2 66.7±1.0 85.6±0.4 64.5±3.6 55.1±2.1 33.9±2.3 51.2±4.2 59.1±3.2 67.1±0.9 58.4±1.2 62.9 CDANN 66.5±1.7 92.2±0.6 65.0±0.9 70.6±0.1 82.9±1.4 67.7±3.0 60.6±0.3 42.2±6.4 46.9±9.9 51.4±2.3 60.7±1.2 51.9±0.4 63.2 SelfReg 63.9±1.9 90.1±1.0 56.8±2.2 70.2±2.3 85.4±0.3 70.2±2.2 60.9±2.6 38.8±4.0 50.5±3.2 54.5±4.7 66.2±1.2 51.7±4.1 63.3 DANN 67.5 ±1.6 91.2±1.3 67.5±1.3 70.6±1.0 81.4±0.4 66.6±1.1 54.1±2.3 33.5±2.7 52.8±2.3 53.8±1.7 64.4±0.7 58.9±0.8 63.5 CAD 67.1 ±1.5 89.6±0.4 60.2±0.2 67.7±3.1 83.7±1.4 70.2±2.6 60.6±2.6 38.3±3.7 53.8±3.2 50.7±1.6 65.8±1.3 54.4±1.7 63.5 GroupGRO66.5±1.2 90.5±1.5 58.9±2.5 70.8±0.9 85.7±1.2 69.7±1.8 62.3±2.1 41.1±2.7 48.2±4.1 54.8±0.5 65.2±1.6 53.9±1.4 64.0 MTL 67.3 ±1.0 90.1±1.0 58.9±0.7 70.2±1.8 84.2±2.2 71.9±0.7 58.3±2.7 38.5±2.7 52.8±1.5 55.4±3.1 66.1±1.3 55.2±2.6 64.1 IRM 67.5 ±1.8 93.0±0.5 62.9±4.7 67.6±1.3 83.8±0.4 68.9±0.8 63.7±1.8 39.9±3.7 49.0±5.4 54.9±1.4 63.1±2.1 54.9±1.4 64.1 ARM 66.0 ±2.4 91.2±0.7 58.7±6.9 70.6±0.8 84.2±1.0 69.1±0.9 59.2±1.8 42.1±5.6 52.1±3.0 60.0±0.6 62.9±3.3 53.8±2.0 64.2 Mixup 65.5 ±0.8 87.8±0.3 57.2±1.0 71.4±1.1 83.1±1.8 68.0±3.0 59.6±1.7 37.2±2.7 56.5±3.8 55.0±2.2 66.2±1.5 62.7±4.2 64.2 CORAL 66.8±0.5 90.3±0.7 61.5±1.9 67.9±2.1 85.4±0.3 70.4±1.3 55.9±2.9 40.4±4.9 49.8±8.5 55.8±2.1 67.6±0.9 58.9±3.8 64.2 SD 67.1 ±1.3 91.7±1.2 63.7±4.1 70.3±0.9 84.4±0.7 69.4±2.3 57.5±2.5 42.6±0.8 47.7±1.7 55.9±2.4 65.7±0.8 55.8±2.1 64.3 MMD 67.1 ±1.4 88.0±0.8 63.6±1.6 70.0±1.1 83.6±0.2 70.2±1.0 58.8±2.6 40.3±1.0 52.3±2.4 57.4±1.9 68.7±0.9 52.7±3.7 64.4 MLDG 67.3±2.0 90.8±0.5 64.4±0.9 70.8±1.0 84.2±0.3 69.7±1.8 61.6±1.0 41.3±5.1 50.4±0.2 49.9±2.5 66.8±0.4 58.7±3.4 64.7 CondCAD66.9±1.4 92.3±0.7 60.8±4.5 71.0±0.6 84.7±1.1 72.6±0.5 61.2±1.5 40.7±3.6 55.7±1.6 52.3±1.7 64.2±0.4 55.3±1.2 64.8 ERM 67.3 ±0.7 91.7±0.9 60.1±4.7 70.4±0.6 82.3±2.7 68.1±0.9 59.6±1.8 44.7±2.8 56.5±2.7 52.8±2.3 68.1±0.7 58.4±0.9 65.0 VREx 67.1 ±1.5 91.0±1.0 62.6±3.5 71.1±2.4 84.1±0.9 71.7±1.3 62.4±3.1 37.7±3.3 53.6±2.3 60.6±1.6 66.7±0.8 57.5±1.4 65.5 Fishr 67.9 ±1.9 92.7±0.3 62.4±4.7 71.2±0.5 83.4±0.6 70.2±1.1 60.0±2.3 42.7±3.2 57.1±3.9 55.7±3.7 68.4±1.0 62.0±3.1 66.1 SagNet 67.6±1.4 92.3±0.5 59.5±1.7 71.8±0.3 82.8±0.6 69.9±1.8 62.5±2.5 45.2±2.5 64.1±2.0 55.8±1.1 65.7±1.4 55.9±3.5 66.1 MixStyle 68.5±2.0 91.2±1.6 65.1±0.7 73.2±1.3 85.0±0.8 71.7±1.5 63.6±1.7 46.3±1.1 51.6±3.7 54.2±1.5 67.0±3.4 58.3±1.4 66.3 Ours 68.9 ±0.6 92.4±0.1 62.5±0.6 75.3±0.4 85.9±0.3 70.2±1.4 66.5±1.1 52.2±2.7 63.8±1.1 57.6±3.7 68.0±1.3 57.9±2.0 68.4 Training and evaluation details. For all the compared methods, we conduct 60 trials on each source domain, and each with 5,000 iteration steps. During the training stage, we split the examples from training domains to 8:2 (train:val) where the training and validation samples are dynamically selected among different training trials. During test, we select the model that performs the best in the validation samples and test it on the target domains. The strategy is referred to as the “training-domain validate set” model selec- tion method in [27]. For each domain in different datasets, the final performance is the average accuracy from the 60 trials. 4.2. Multi-Source Generalization In these experiments, all five benchmark datasets afore- mentioned are used for evaluation, and the leave-one-out strategy is adopted for training (i.e. with S = |Ds ∪Dt|2 −1, and T = 1). Results are shown in Table 1. We note that ERM method obtains favorable performance against existing arts. In fact, as a strong baseline, ERM is superior to half of the methods in the term of average accuracy, and only 5 arts (i.e. SelfReg [34], Fish [58], CORAL [59], SD [51], and ours) among the compared 22 methods outperforms ERM in most datasets (i.e. with Score ≥ 3). In comparison, the proposed ITTA is more effective than all other models on average. In particular, ITTA achieves the best performances in 3 out of the 5 benchmarks (i.e. PACS, VLCS, and TerraInc datasets) and 4 in the top 5. Note that although our method does not obtain the best performances in the OfficeHome and DomainNet benchmarks, it still outperforms more than half 2We use | · |to denote the number of domains in the environment. of the existing models. The results validate the effectiveness of our method when tested in the multi-source setting. We present results of average accuracy in each domain from different datasets in the supplementary material. Please refer to it for details. 4.3. Single-Source Generalization In these experiments, we adopt the widely-used PACS [37] benchmark for evaluation, and the models are trained on one domain while tested on the remaining three (i.e. with S = 1, and T = 3). Although some approaches, such as MLDG [38] and Fishr [52], may require more than one domain information for their trainings, we can simu- late multi-domain information using only the source domain, and thus the experimental settings are still feasible for them. Compared to the multi-source generalization task, the single- source generalization is considered more difficult due to the limited domain information during the training phase. Evalu- ation results are presented in Table 2. We note that the ERM method outperforms most state-of-the-art models, and only 5 models, including VREx [36], Fishr [52], SagNet [48], MixStyle [75], and the proposed ITTA, can obtain better re- sults than ERM in the term of average accuracy. Meanwhile, our method achieves the best performances when trained in 5 out of the 12 source domain, and it obtains the best perfor- mance on average, leading more than 2% than the second best (i.e. MixStyle [75]) and 3% the ERM method. In line with the findings in [27], we notice that the naive ERM method [61] can indeed perform favorably against most existing models under rigorous evaluation protocol. As a matter of fact, the proposed method is the only one that consistently outperforms ERM in both the multi-sourceTable 3. Evaluations of different TTT-based models in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch Baseline 79.9 ±0.5 75.4±1.1 94.4±0.5 75.8±1.2 81.4±0.5 TTT [60] 81.5±0.8 77.6±0.6 94.3±0.2 78.4±0.7 83.0±0.2 MT3 [3] 82.0 ±1.0 76.5±1.0 94.1±0.2 77.7±1.3 82.6±0.6 TENT [63] 80.2±0.9 77.2±0.8 94.4±0.2 77.4±0.1 82.3±0.5 Ours 84.7 ±0.4 78.0±0.4 94.5±0.4 78.2±0.3 83.8±0.3 and single-source settings. These results indicate that DG remains challenging for current efforts that aim to ease the distribution shift only through training data, and using the proposed improved TTT strategy may be a promising direc- tion for solving DG. 5. Analysis All experiments in this section are conducted on the widely-used PACS benchmark [37] with the leave-one-out strategy. The experimental settings are the same as that illus- trated in Sec. 4.1. Please refer to our supplementary material for more analysis. 5.1. Compared with Other TTT-Based Models Using test-time adaptation to ease the distribution shift problem has been explored in previous works, such as the original TTT method [60] and MT3 [3]. Their differences lie in that TTT uses a rotation estimation task for the test-time objective, and MT3 adopts a contrastive loss for the task and implements the overall framework using MAML [20]. There is also a recently proposed TENT [63] that aims to minimize the entropy of the final results by tuning the parameters from the batch normalization (BN) layers. To analyze the overall effectiveness of our method, we compare ITTA with these arts using the same baseline (i.e. ResNet18 [30] backbone with the existing augmentation skill [75]). Results are shown in Table 3. We observe that all the com- pared TTT-based methods can improve the baseline model in almost all target domains except for the “Photo” domain, which might be due to the ImageNet pretraining [67]. This phenomenon demonstrates that the TTT strategy may be a promising effort for easing the distribution shift problem. Meanwhile, we observe that the proposed ITTA is superior to all other approaches in most target domains and leads in the term of average accuracy. The main reason is that compared to the empirically designed TTT tasks adopted in previous works, the proposed learnable consistency loss is enforced to be more aligned with the main loss, thus more suitable for the test-time adaptation task [46]. Meanwhile, compared to the strategies that update the original param- eters from the trained model, the adaptation of the newly included parameters is also more effective for the overall (a) Input (b) Ours w/o fw (c) Ours (d) Main Figure 4. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels from the four target do- mains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). Ours w/o fw is the naive consis- tency loss with fw disabled in Eq. (1). The proposed learnable consistency loss can align well with the main classification task. TTT framework. In the following, we provide more analysis to support these claims. 5.2. Effectiveness of the Learnable Consistency Loss To examine the effectiveness of our learnable consistency loss, we conduct ablation studies by comparing our method with the following variants. (1) Ours w/o fw: we disable fw when computing the learnable consistency loss in Eq. (1), which uses the naive consistency loss for the auxiliary TTT task. (2) Ours w/ Ent.: after training the model using the baseline settings (i.e. ResNet18 with the augmentation strat- egy [75]), we use the entropy minimization task in [63] for the TTT task. (3) Ours w/ Rot.: we use the rotation estimation task in [60] for the TTT task. To ensure fair com- parisons, we use the same baseline settings and include the same additional adaptive parameters for all the variants. Results are shown in the 4th to 6th rows Table 4. We find that the results from the naive consistency loss ( i.e. Ours w/o fw) are slightly better than that from the other two specially-designed objectives (i.e. Ours w/ Ent. and Ours w/ Rot.) on average. Besides the possibility of deteriorating the performance [46], our results indicate that empirically select- ing a TTT task may also be far from optimal. Meanwhile, we observe that when enabling fw, the proposed learnable consistency loss is superior to that withoutfw in all target do-Table 4. Comparison between different TTT tasks and parameter selecting strategies in the unseen domain from the PACS benchmark [37]. Here the “Ent.”, “Rot.”, and “Lwcont” denotes the entropy minimization task in [63], the rotation estimation task in [60], and the proposed learnable consistency objective, the “All”, “BN”, and “Ada.” are the strategies that update all the parameters, parameters from the batch normalization layer, and the proposed strategy that updates only the new additional adaptive parameters. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model TTT tasks Param selectings Target domain Avg.Ent. Rot. Lwcont All BN Ada. Art Cartoon Photo Sketch Ours − − ✓ − − ✓ 84.7±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 Ours w/ofw − − − − − ✓ 83.1±0.4 74.6 ±0.6 94.0 ±0.5 78.0 ±0.8 82.5 ±0.1 Ours w/ Ent. ✓ − − − − ✓ 79.9±2.4 77.3 ±0.3 94.8 ±0.8 77.6 ±0.4 82.4 ±0.8 Ours w/ Rot. − ✓ − − − ✓ 81.1±1.0 75.2 ±0.5 94.9 ±0.3 77.3 ±0.6 82.1 ±0.3 Ours w/o TTT − − ✓ − − − 83.3±0.5 76.0 ±0.5 94.4 ±0.5 76.7 ±1.4 82.8 ±0.3 Ours w/ All − − ✓ ✓ − − 83.0±0.7 77.0 ±1.4 94.5 ±0.7 77.4 ±0.9 83.0 ±0.2 Ours w/ BN − − ✓ − ✓ − 81.8±0.5 75.6 ±0.3 94.4 ±0.3 77.9 ±1.1 82.4 ±0.5 mains, and it leads in the term of average accuracy among the variants compared, illustrating its advantage against other adopted TTT tasks. These results are not surprising. By comparing the Grad-CAM [57] visualizations from the main classification task with the learnable and naive consistency losses in Figure 4, we find that the proposed learnable objec- tive can well align with the main loss when fw is enabled as the hot zones activated by these two tasks are similar, which guarantees the improvement for the test-time adapta- tion [46, 60]. Please refer to our supplementary material for more visualizations. 5.3. Effectiveness of the Adaptive Parameters We compare ITTA with three variants to demonstrate the effectiveness of the proposed additional adaptive parameters. (1) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model. (2) Ours w/ ALL: similar to the updating strategy in the original TTT method [60], we update all the parameters from the feature extractor during the test phase. (3) Ours w/ BN: following the suggestion from TENT [63], only parameters from the BN layers of the feature extractor are updated. Note the same pretrained model is shared for all variants in these experiments, and the objectives during the test adaptation phase are to minimize the same learned consistency loss. We list the results in the last three rows in Table 4. We observe that when only updating parameters from the BN layers, the performance is inferior to the strategy without test-time adaptation, and updating all the parameters does not ensure improvements in all target domains. The observations are in line with the findings in [63] that selecting reliable parameters to update is essential in the TTT system and may also interact with the choice of the TTT task. In comparison, when including additional adaptive parameters for updating, the pretrained model can be boosted in all environments. The results validate that our adaptive parameters are more effective than that selected with existing strategies [60, 63] when applied with the proposed learnable test-time objective. 5.4. Limitation Although the proposed learned loss can bring satisfaction improvements, we are aware that the lunch is not free. When the weight subnetwork fw is disabled, updating the joint loss in Eq. (2) only costs 1 forward and 1 backward. However, in order to update fw, we have to compute the second-order derivative in Eq. (5), which will require 1 more forward and 3 more backward processes, bringing extra burden to the system. Our future efforts aim to simplify the overall optimization process and reduce the cost for ITTA. 6. Conclusion In this paper, we aim to improve the current TTT strategy for alleviating the distribution shift problem in DG. First, given that the auxiliary TTT task plays a vital role in the over- all framework, and an empirically selecting one that does not align with the main task may potentially deteriorate instead of improving the performance, we propose a learnable con- sistency loss that can be enforced to be more aligned with the main loss by adjusting its learnable parameters. This strategy is ensured to improve the model and shows favorable perfor- mance against some specially-designed objectives. Second, considering that selecting reliable and effective parameters to update during the test phase is also essential while exhaus- tively trying different combinations may require tremendous effort, we propose a new alternative by including new ad- ditional adaptive parameters for adaptation during the test phase. This alternative is shown to outperform some pre- vious parameter selecting strategies via our experimental findings. By conducting extensive experiments under a rig- orous evaluation protocol, we show that our method can achieve superior performance against existing arts in both the multi-source and single-source DG tasks. Acknowledgements. Liang Chen is supported by the ChinaScholarship Council (CSC Student ID 202008440331). References [1] Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. 5, 15, 16, 17 [2] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel- lappa. Metareg: Towards domain generalization using meta- regularization. In NeurIPS, 2018. 1, 2, 14, 15 [3] Alexander Bartler, Andre B¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In AISTATS, 2022. 2, 4, 7 [4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018. 5, 17 [5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NeurIPS, 2006. 2 [6] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017. 5, 15, 16, 17 [7] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generaliz- ing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011. 1 [8] Chaoqi Chen, Jiongcheng Li, Xiaoguang Han, Xiaoqing Liu, and Yizhou Yu. Compound domain generalization via meta- knowledge encoding. In CVPR, 2022. 1 [9] Chaoqi Chen, Luyao Tang, Feng Liu, Gangming Zhao, Yue Huang, and Yizhou Yu. Mix and reason: Reasoning over se- mantic topology with data mixing for domain generalization. In NeurIPS, 2022. 1 [10] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, 2022. 2 [11] Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and Jue Wang. Self-supervised learning of adversarial example: Towards good generalizations for deepfake detection. In CVPR, 2022. 2 [12] Liang Chen, Yong Zhang, Yibing Song, Jue Wang, and Lingqiao Liu. Ost: Improving generalization of deepfake detection via one-shot test-time training. In NeurIPS, 2022. 2, 12 [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof- frey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3 [14] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, 2022. 2 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5 [16] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, 2019. 1, 2 [17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 88(2):303–338, 2010. 5 [18] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013. 5, 16 [19] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener- ative visual models from few training examples: An incre- mental bayesian approach tested on 101 object categories. In CVPR worksho, 2004. 5 [20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 2, 7 [21] Francois Fleuret et al. Uncertainty reduction for model adap- tation in semantic segmentation. In CVPR, 2021. 2 [22] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 2 [23] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marc- hand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 17(1):2096–2030, 2016. 1, 2, 5, 15, 16, 17 [24] Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE TPAMI, 39(7):1414–1430, 2016. 1 [25] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In ICCV, 2015. 2 [26] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer- sch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 2, 3 [27] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021. 1, 2, 5, 6, 14, 15, 16, 17 [28] Sivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, et al. Unsupervised domain general- ization by learning a bridge across domains. In CVPR, 2022. 1 [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual repre- sentation learning. In CVPR, 2020. 2, 3 [30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 5, 7, 14 [31] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In UAI, 2020. 1 [32] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. 2 [33] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In ECCV, 2020. 5, 15, 16, 17[34] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regular- ization for domain generalization. In ICCV, 2021. 2, 5, 6, 15, 16, 17 [35] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribu- tion shifts. In ICML, 2021. 1 [36] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In ICML, 2021. 5, 6, 15, 16, 17 [37] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In ICCV, 2017. 1, 5, 6, 7, 8, 12, 13, 14, 15 [38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, 2018. 1, 2, 5, 6, 15, 16, 17 [39] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic training for domain generalization. In ICCV, 2019. 1, 2 [40] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In CVPR, 2018. 1, 2, 5, 15, 16, 17 [41] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple feature augmentation for domain generalization. In ICCV, 2021. 1, 2, 12, 14 [42] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out- of-distribution generalization. In ICLR, 2022. 1, 2 [43] Yizhuo Li, Miao Hao, Zonglin Di, Nitesh Bharadwaj Gun- davarapu, and Xiaolong Wang. Test-time personalization with a transformer for human pose estimation. In NeurIPS, 2021. 2, 3, 4 [44] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza- tion via conditional invariant adversarial networks. In ECCV, 2018. 1, 2, 5, 15, 16, 17 [45] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heterogeneous do- main generalization. In ICML, 2019. 14, 15 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In NeurIPS, 2021. 2, 3, 4, 7, 8, 12, 14, 15 [47] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant feature representation. In ICML, 2013. 1, 2 [48] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, 2021. 2, 5, 6, 15, 16, 17 [49] Prashant Pandey, Mrigank Raman, Sumanth Varambally, and Prathosh Ap. Generalization on unseen domains via inference- time label-preserving target projections. In CVPR, 2021. 1 [50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, 2019. 5, 17 [51] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient star- vation: A learning proclivity in neural networks. In NeurIPS, 2021. 1, 5, 6, 15, 16, 17 [52] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution gen- eralization. In ICML, 2022. 1, 2, 5, 6, 15, 16, 17 [53] Yangjun Ruan, Yann Dubois, and Chris J Maddison. Optimal representations for covariate shift. In ICLR, 2022. 5, 15, 16, 17 [54] Bryan C Russell, Antonio Torralba, Kevin P Murphy, and William T Freeman. Labelme: a database and web-based tool for image annotation. IJCV, 77(1):157–173, 2008. 5 [55] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst- case generalization. In ICLR, 2020. 5, 15, 16, 17 [56] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 2 [57] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad- cam: Visual explanations from deep networks via gradient- based localization. In ICCV, 2017. 7, 8, 11, 13 [58] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In ICLR, 2021. 1, 2, 5, 6, 15, 16, 17 [59] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016. 5, 6, 15, 16, 17 [60] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, 2020. 1, 2, 4, 5, 7, 8, 11, 12, 13 [61] Vladimir Vapnik. The nature of statistical learning theory . Springer science & business media, 1999. 1, 5, 6, 15, 16, 17 [62] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017. 5, 16 [63] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 2, 3, 4, 7, 8, 11, 12, 13 [64] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recog- nition from abbey to zoo. In CVPR, 2010. 5 [65] Zehao Xiao, Xiantong Zhen, Ling Shao, and Cees GM Snoek. Learning to generalize across domains on single test samples. In ICLR, 2022. 2 [66] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generaliza- tion. In CVPR, 2021. 1, 2 [67] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 7[68] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020. 2, 5, 15, 16, 17 [69] Fu-En Yang, Yuan-Chia Cheng, Zu-Yun Shiau, and Yu- Chiang Frank Wang. Adversarial teacher-student representa- tion learning for domain generalization. In NeurIPS, 2021. 1, 2 [70] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu. Ood-bench: Quantifying and understanding two dimensions of out-of- distribution generalization. In CVPR, 2022. 5 [71] Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 4 [72] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: A meta-learning approach for tackling group distri- bution shift. arXiv preprint arXiv:2007.02931, 2020. 5, 15, 16, 17 [73] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: Learning to adapt to domain shift. NeurIPS, 2021. 2 [74] Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, and Jin Tang. Meta-dmoe: Adapting to domain shift by meta- distillation from mixture-of-experts. In NeurIPS, 2022. 2 [75] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 1, 2, 3, 5, 6, 7, 12, 15, 16, 17 Appendix In this supplementary material, we provide, 1. Resource usage for ITTA in Section 7. 2. Grad-CAM visualizations of different loss terms in Section 8. 3. Parameter analysis of ITTA in Section 9; 4. Using a different augmentation skill for ITTA in Sec- tion 10. 5. Using different updating steps or a strategy for ITTA during the test phase in Section 11. 6. Using different network structures for the learnable consistency loss and adaptive parameters in Section 12. 7. Comparisons with other related methods in Section 13. 8. Detailed experimental results in the DomainBed bench- mark in Section 14. 7. Resource Usage Comparisons Between ITTA and the Baseline Model Requiring extra resources for our ITTA is a common lim- itation for existing test-time-based arts. To further evaluate our method, in this section, we compare FLOPS, model size, and inference time in Table 5. We compare only with ERM as most existing methods utilize the same network during in- ferences. We note that compare to the baseline model, ITTA requires extra Flops and processing time, this is because the adaptation process uses extra forward and backward steps during the test phase. While the parameters between the two models are similar because the newly included adaptive blocks are much smaller in size compared to the original model. Table 5. Resource comparisons during testing. Here inc. and exc. columns in ITTA indicate to include and exclude the TTA phase. Model Flops (G) Params (M) Time (s) Baseline 1.82 11.18 0.004 ITTA (inc.| exc.) 6.12 | 1.83 14.95 | 14.94 0.021 | 0.005 8. Grad-CAM Visualizations of Different Self- Supervised Objectives In Section 5 of the manuscript, we provide Grad-CAM [57] visualizations of our learnable consistency and the main losses to illustrate their alignment. To further show the differences between several TTT tasks [60, 63], we present more visual examples in this section. Results are shown in Figure 5. We observe that the entropy minimization [63] and rotation estimation [60] objectives do not activate the same regions as the main loss. As shown in the first row, for the class label of giraffe, both the main loss and our learned loss can correctly locate the two giraffes in the image, while the rotation estimation task can only locate one target, the same observation can be found when the learned weightsare disabled in our loss term. Meanwhile, although the two objects can be found for the entropy minimization task, the corresponding hot region does not align with that of the main loss. Similar phenomena can be observed in other samples. These visual examples demonstrate that our learned objective can better align with the main task than the TTT tasks adopted in previous works [60, 63], explaining why using the proposed learnable consistency loss can better improve TTT. 9. Parameter Analysis In this section, we analyze the hyper-parameter used in ITTA. We use the weight parameterα to balance the contri- butions from the main loss and weighted consistency loss (i.e. Lmain + αLwcont in Eq. (2) of our manuscript). To analyze the sensitivity of ITTA regarding different values of α, we conduct ablation studies in the PACS benchmark [37]. Results are listed in Table 6. We observe that the proposed ITTA can obtain favorable performances when α is in the range of 0.1 to 10, and it performs the best on average when setting as 1. We thus fix the parameter as 1 in all experi- ments. 10. A Different Augmentation Skill for ITTA In our manuscript, we use the existing augmentation strat- egy from [75] to obtain the augmented feature. In this sec- tion, we replace this implementation with that from [41] to further verify if our ITTA can still thrive with another aug- mentation skill. Different from [75] that mixes the statics of the feature to synthesize new information, [41] uses an affine transformation to create new features, where the weight for the transformation is sampled from a normal distribution with the mean value of one and standard value of zero, and the bias for the transformation is sampled from a normal distribution with the mean and standard values both zero. Experiments are conducted on the PACS benchmark [37] with the leave-one-out strategy. We compare ITTA with several different variants. (1) Ours w/o fw & TTT: this variant is the baseline model which uses the naive consistency loss for training and does not include TTT during the test phase. (2) Ours w/o fw: we disable the fw in our consistency loss, which uses the naive consistency loss for the test-time updating. (3) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model when replacing the augmentation strategy. We also compare these variants with the ERM method to show their effectivenesses. Results are listed in Table 7. We observe that ERM per- forms favorably against the baseline model, indicating that this augmentation strategy may not be beneficial for the training process. Meanwhile, we observe that when fw is disabled, the performances seem to decrease in 3 out of 4 target domains, and the average accuracy is also inferior to the baseline (i.e. Ours w/o fw & TTT). This result is in line with the finding in [46] that an inappropriate TTT task may deteriorate the performance. In comparison, we note that the performances are both improved when fw is enabled (i.e. Ours w/o TTT and Ours), which once again demonstrates that the proposed learnable consistency loss can improve the trained model. Moreover, we can also observe that when combining fw and TTT, our model is superior to other vari- ants and the ERM method. These results demonstrate that the proposed two strategies can improve the current TTT framework despite a less effective augmentation strategy. 11. Different Updating Steps or Strategies for ITTA In the manuscript, we use one TTT step for ITTA before during the testing step. In this section, we conduct experi- ments to evaluate the performances of ITTA with different TTT steps. Experiments are conducted on the PACS bench- mark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper- parameter settings. Results are listed in Table 8. We observe that the average accuracies of using more TTT steps are not improved greatly while the computational times are propor- tional to the TTT steps. To this end, we use one TTT step for ITTA as a compromise between accuracy and efficiency. We use the online setting from TTT [60] for all arts, which assumes test samples arrive sequentially and updates the adaptive blocks based on the states optimized from a previous sample. In this section, we also test ITTA in an episodic manner (i.e. Epi) [12]. Results in Table 8 suggest that while the episodic updating strategy performs slightly worse than the current scheme, and it still outperforms the baseline. 12. Different Network Structures for the Learnable Consistency Loss and Adaptive Parameters In our implementation, we use 10 layers of learnable pa- rameters for fw, and we use 5 layers of learnable parameters for fΘ after each block. In this section, we evaluate our ITTA with different network structures for these two mod- ules. Specifically, we compare the original implementation with the variants that use 1, 5, and 15 layers for fw and 1, 10, and 15 layers for fΘ to evaluate the performances of dif- ferent structures. Similarly, we conduct experiments on the PACS benchmark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper-parameter settings. Evaluation results are listed in Table 9. We observe that their differences in the average accuracy are rather subtle on account of the variances. To(a) Input (b) Entropy (c) Rotation (d) Ours w/o fw (e) Ours (f) Main Figure 5. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels (i.e. giraffe, elephant, house, and horse from top to bottom) from the four target domains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). “Entropy” and “Rotation” here denote the entropy minimization and rotation estimation tasks in [63] and [60]. Ours w/o fw is the learnable consistency loss in Eq. (1) in the manuscript (i.e. ∥fw(z − z′)∥) when fw is disabled. The proposed learnable consistency loss can align well with the main classification task. Table 6. Sensitivity analysis of ITTA regarding different values ofα in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Values Target domain Avg.Art Cartoon Photo Sketch α = 0.1 83.9 ± 0.7 76.2 ± 1.1 94.8 ± 0.2 78.8 ± 0.8 83.4 ± 0.2 α = 1 (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 α = 10 83.9 ± 0.5 77.4 ± 0.6 94.2 ± 0.7 77.3 ± 0.8 83.2 ± 0.3 α = 100 81.5 ± 1.2 77.0 ± 0.6 92.6 ± 0.7 78.9 ± 2.1 82.5 ± 0.9 this end, we use the original implementation with 10 layers of learnable parameters for fw and 5 layers of learnable pa- rameters for fΘ, which performs relatively better than other variants. Since the adaptive blocks fΘ are attached after each layer of the network, one may wonder how the varying locations of the adaptive blocks affect the performance of ITTA. To answer this question, we further conduct experiments by adding the adaptive blocks after different layers of the orig- inal network. Denoting as Loc = lan given the n layers in the original network, we note that the model performs less effectively when the adaptive block is placed after the 1st layer of the network, and using all four adaptive blocks (i.e. ours) is more effective than other alternatives. 13. Comparisons with Other Related Methods Apart from the proposed ITTA, some other works also propose to include learnable parameters in their auxiliaryTable 7. Performances of our method with another augmentation strategy from [41] in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch ERM 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 Ours w/o fw & TTT 74.9 ± 0.4 74.1 ± 0.8 90.6 ± 0.3 79.7 ± 0.7 79.8 ± 0.4 Ours w/o fw 77.1 ± 1.0 73.6 ± 1.1 89.9 ± 0.4 78.4 ± 0.8 79.7 ± 0.2 Ours w/o TTT 77.5 ± 0.3 73.2 ± 0.6 92.4 ± 0.4 78.0 ± 1.0 80.3 ± 0.3 Ours (w/ fw & TTT) 79.2 ± 0.8 74.9 ± 1.1 92.2 ± 0.3 76.9 ± 0.7 80.8 ± 0.4 Table 8. Evaluations of ITTA in the unseen domain from PACS [37] with different TTT steps and updating strategies during the testing phase. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. The time consumption (TC) is computed using one image with the size of 224 × 224. Epi. denotes updating ITTA in an episodic manner. Steps Target domain Avg. TCArt Cartoon Photo Sketch 1 step (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 2.4 ms 2 step 84.2 ± 0.9 77.5 ± 0.6 94.4 ± 0.4 79.1 ± 1.0 83.8 ± 0.1 4.2 ms 3 step 84.5 ± 1.2 77.6 ± 0.6 94.0 ± 0.6 79.3 ± 0.1 83.9 ± 0.3 6.1 ms Epi. 83.6 ± 0.7 77.9 ± 0.5 95.2 ± 0.1 76.6 ± 0.5 83.3 ± 0.4 losses. Examples include MetaReg [2] and Feature-Critic [45] which both suggest using meta-learning to produce more general models. The main difference between these arts and ITTA is that parameters in the auxiliary loss from [2,45] are gradually refined by episode training, and they are updated via a gradient alignment step in ITTA (see Sec. 3.1 in the manuscript), which is much simpler. In this sec- tion, we compare ITTA with these two arts in the PACS dataset [37] using the same settings aforementioned. Be- cause MetaReg [2] does not release codes, we thus directly cite the data from their paper in the comparison. Different from others, the results in [2] are averaged by 5 trials accord- ing to their paper, which is much less than our experimental settings. Meanwhile, we also compare with TTT++ [46] which suggests storing the momentum of the features from the source domain and enforcing the similarity between mo- mentums of features from the source and target domains. We use the same setting in Section 5.1 from the manuscript to evaluate TTT++. Results are listed in Table 10. We observe that our method consistently outperforms that from [2,45,46] for both the cases with and without TTT, indicating that the proposed learnable consistency loss and updating method is not only simpler but also more effective than the losses in [2, 45]. 14. Detailed Results in the DomainBed Bench- mark [27] this section presents the average accuracy in each domain from different datasets. As shown in Table 11, 12, 13, 14, and 15, these results are detailed illustrations of the results in Table 2 in our manuscript. For all the experiments, we use the “training-domain validate set” as the model selection method. A total of 22 methods are examined for 60 trials in each unseen domain, and all methods are trained with the leave-one-out strategy using the ResNet18 [30] backbones.Table 9. Performances of our method with different network structures for the consistency loss (i.e. fw) and adaptive parameters (i.e. fΘ) in the unseen domain from PACS [37]. Here ‘Loc=lan’ locates the adaptive block after the n-th layer of the model (‘la4’ is the last layer). The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Structures Target domain Avg.Art Cartoon Photo Sketch Structures offw 1 layer 83.5 ±1.2 76.0 ±1.0 95.3 ±0.2 78.7 ±1.5 83.4 ±0.4 5 layers 83.7 ±0.6 76.8 ±0.9 94.6 ±0.3 78.8 ±0.3 83.5 ±0.3 10 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 15 layers 84.1 ±0.4 75.8 ±0.2 94.3 ±0.3 79.5 ±0.4 83.4 ±0.2 Structures offΘ 1 layer 84.0 ±0.6 77.4 ±0.5 94.4 ±0.5 78.3 ±0.4 83.5 ±0.3 5 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 10 layers 84.8 ±0.3 76.0 ±0.6 94.1 ±0.5 78.3 ±0.1 83.3 ±0.3 15 layers 83.9 ±0.8 76.0 ±0.5 93.8 ±0.4 78.7 ±1.4 83.1 ±0.6 Locations offΘ Loc=la1 83.4±0.7 76.8 ±0.3 94.4 ±0.3 77.8 ±0.3 83.1 ±0.3 Loc=la2 83.4±0.6 77.7 ±0.6 94.2 ±0.5 78.0 ±0.5 83.3 ±0.3 Loc=la3 84.0±0.4 77.5 ±0.3 94.4 ±0.1 77.8 ±0.1 83.4 ±0.2 Loc=la4 84.1±0.7 77.8 ±0.5 94.8 ±0.2 76.9 ±1.5 83.4 ±0.4 Table 10. Compare with learnable losses in [2, 45] in the unseen domain from PACS [37]. The reported accuracies ( %) and standard deviations are computed from 60 trials in each target domain except for [2] where the numbers are directly cited from their paper. Model Target domain Avg.Art Cartoon Photo Sketch MetaReg [2] 83.7 ± 0.2 77.2 ± 0.3 95.5 ± 0.2 70.3 ± 0.3 81.7 Feture-Critic [45] 78.4 ± 1.6 75.4 ± 1.2 92.6 ± 0.5 73.3 ± 1.4 80.0 ± 0.3 TTT++ [46] 84.3 ± 0.1 78.4 ± 0.5 93.8 ± 1.3 73.2 ± 3.2 82.4 ± 1.1 Ours w/o TTT 83.3 ± 0.5 76.0 ± 0.5 94.4 ± 0.5 76.7 ± 1.4 82.8 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 Table 11. Average accuracies on the PACS [37] datasets using the default hyper-parameter settings in DomainBed [27]. art cartoon photo sketch Average ERM [61] 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 IRM [1] 76.9 ± 2.6 75.1 ± 0.7 94.3 ± 0.4 77.4 ± 0.4 80.9 ± 0.5 GroupGRO [55] 77.7 ± 2.6 76.4 ± 0.3 94.0 ± 0.3 74.8 ± 1.3 80.7 ± 0.4 Mixup [68] 79.3 ± 1.1 74.2 ± 0.3 94.9 ± 0.3 68.3 ± 2.7 79.2 ± 0.9 MLDG [38] 78.4 ± 0.7 75.1 ± 0.5 94.8 ± 0.4 76.7 ± 0.8 81.3 ± 0.2 CORAL [59] 81.5 ± 0.5 75.4 ± 0.7 95.2 ± 0.5 74.8 ± 0.4 81.7 ± 0.0 MMD [40] 81.3 ± 0.6 75.5 ± 1.0 94.0 ± 0.5 74.3 ± 1.5 81.3 ± 0.8 DANN [23] 79.0 ± 0.6 72.5 ± 0.7 94.4 ± 0.5 70.8 ± 3.0 79.2 ± 0.3 CDANN [44] 80.4 ± 0.8 73.7 ± 0.3 93.1 ± 0.6 74.2 ± 1.7 80.3 ± 0.5 MTL [6] 78.7 ± 0.6 73.4 ± 1.0 94.1 ± 0.6 74.4 ± 3.0 80.1 ± 0.8 SagNet [48] 82.9 ± 0.4 73.2 ± 1.1 94.6 ± 0.5 76.1 ± 1.8 81.7 ± 0.6 ARM [72] 79.4 ± 0.6 75.0 ± 0.7 94.3 ± 0.6 73.8 ± 0.6 80.6 ± 0.5 VREx [36] 74.4 ± 0.7 75.0 ± 0.4 93.3 ± 0.3 78.1 ± 0.9 80.2 ± 0.5 RSC [33] 78.5 ± 1.1 73.3 ± 0.9 93.6 ± 0.6 76.5 ± 1.4 80.5 ± 0.2 SelfReg [34] 82.5 ± 0.8 74.4 ± 1.5 95.4 ± 0.5 74.9 ± 1.3 81.8 ± 0.3 MixStyle [75] 82.6 ± 1.2 76.3 ± 0.4 94.2 ± 0.3 77.5 ± 1.3 82.6 ± 0.4 Fish [58] 80.9 ± 1.0 75.9 ± 0.4 95.0 ± 0.4 76.2 ± 1.0 82.0 ± 0.3 SD [51] 83.2 ± 0.6 74.6 ± 0.3 94.6 ± 0.1 75.1 ± 1.6 81.9 ± 0.3 CAD [53] 83.9 ± 0.8 74.2 ± 0.4 94.6 ± 0.4 75.0 ± 1.2 81.9 ± 0.3 CondCAD [53] 79.7 ± 1.0 74.2 ± 0.9 94.6 ± 0.4 74.8 ± 1.4 80.8 ± 0.5 Fishr [52] 81.2 ± 0.4 75.8 ± 0.8 94.3 ± 0.3 73.8 ± 0.6 81.3 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3Table 12. Average accuracies on the VLCS [18] datasets using the default hyper-parameter settings in DomainBed [27]. Caltech LabelMe Sun VOC Average ERM [61] 97.7 ± 0.3 62.1 ± 0.9 70.3 ± 0.9 73.2 ± 0.7 75.8 ± 0.2 IRM [1] 96.1 ± 0.8 62.5 ± 0.3 69.9 ± 0.7 72.0 ± 1.4 75.1 ± 0.1 GroupGRO [55] 96.7 ± 0.6 61.7 ± 1.5 70.2 ± 1.8 72.9 ± 0.6 75.4 ± 1.0 Mixup [68] 95.6 ± 1.5 62.7 ± 0.4 71.3 ± 0.3 75.4 ± 0.2 76.2 ± 0.3 MLDG [38] 95.8 ± 0.5 63.3 ± 0.8 68.5 ± 0.5 73.1 ± 0.8 75.2 ± 0.3 CORAL [59] 96.5 ± 0.3 62.8 ± 0.1 69.1 ± 0.6 73.8 ± 1.0 75.5 ± 0.4 MMD [40] 96.0 ± 0.8 64.3 ± 0.6 68.5 ± 0.6 70.8 ± 0.1 74.9 ± 0.5 DANN [23] 97.2 ± 0.1 63.3 ± 0.6 70.2 ± 0.9 74.4 ± 0.2 76.3 ± 0.2 CDANN [44] 95.4 ± 1.2 62.6 ± 0.6 69.9 ± 1.3 76.2 ± 0.5 76.0 ± 0.5 MTL [6] 94.4 ± 2.3 65.0 ± 0.6 69.6 ± 0.6 71.7 ± 1.3 75.2 ± 0.3 SagNet [48] 94.9 ± 0.7 61.9 ± 0.7 69.6 ± 1.3 75.2 ± 0.6 75.4 ± 0.8 ARM [72] 96.9 ± 0.5 61.9 ± 0.4 71.6 ± 0.1 73.3 ± 0.4 75.9 ± 0.3 VREx [36] 96.2 ± 0.0 62.5 ± 1.3 69.3 ± 0.9 73.1 ± 1.2 75.3 ± 0.6 RSC [33] 96.2 ± 0.0 63.6 ± 1.3 69.8 ± 1.0 72.0 ± 0.4 75.4 ± 0.3 SelfReg [34] 95.8 ± 0.6 63.4 ± 1.1 71.1 ± 0.6 75.3 ± 0.6 76.4 ± 0.7 MixStyle [75] 97.3 ± 0.3 61.6 ± 0.1 70.4 ± 0.7 71.3 ± 1.9 75.2 ± 0.7 Fish [58] 97.4 ± 0.2 63.4 ± 0.1 71.5 ± 0.4 75.2 ± 0.7 76.9 ± 0.2 SD [51] 96.5 ± 0.4 62.2 ± 0.0 69.7 ± 0.9 73.6 ± 0.4 75.5 ± 0.4 CAD [53] 94.5 ± 0.9 63.5 ± 0.6 70.4 ± 1.2 72.4 ± 1.3 75.2 ± 0.6 CondCAD [53] 96.5 ± 0.8 62.6 ± 0.4 69.1 ± 0.2 76.0 ± 0.2 76.1 ± 0.3 Fishr [52] 97.2 ± 0.6 63.3 ± 0.7 70.4 ± 0.6 74.0 ± 0.8 76.2 ± 0.3 Ours 96.9 ± 1.2 63.7 ± 1.1 72.0 ± 0.3 74.9 ± 0.8 76.9 ± 0.6 Table 13. Average accuracies on the OfficeHome [62] datasets using the default hyper-parameter settings in DomainBed [27]. art clipart product real Average ERM [61] 52.2 ± 0.2 48.7 ± 0.5 69.9 ± 0.5 71.7 ± 0.5 60.6 ± 0.2 IRM [1] 49.7 ± 0.2 46.8 ± 0.5 67.5 ± 0.4 68.1 ± 0.6 58.0 ± 0.1 GroupGRO [55] 52.6 ± 1.1 48.2 ± 0.9 69.9 ± 0.4 71.5 ± 0.8 60.6 ± 0.3 Mixup [68] 54.0 ± 0.7 49.3 ± 0.7 70.7 ± 0.7 72.6 ± 0.3 61.7 ± 0.5 MLDG [38] 53.1 ± 0.3 48.4 ± 0.3 70.5 ± 0.7 71.7 ± 0.4 60.9 ± 0.2 CORAL [59] 55.1 ± 0.7 49.7 ± 0.9 71.8 ± 0.2 73.1 ± 0.5 62.4 ± 0.4 MMD [40] 50.9 ± 1.0 48.7 ± 0.3 69.3 ± 0.7 70.7 ± 1.3 59.9 ± 0.4 DANN [23] 51.8 ± 0.5 47.1 ± 0.1 69.1 ± 0.7 70.2 ± 0.7 59.5 ± 0.5 CDANN [44] 51.4 ± 0.5 46.9 ± 0.6 68.4 ± 0.5 70.4 ± 0.4 59.3 ± 0.4 MTL [6] 51.6 ± 1.5 47.7 ± 0.5 69.1 ± 0.3 71.0 ± 0.6 59.9 ± 0.5 SagNet [48] 55.3 ± 0.4 49.6 ± 0.2 72.1 ± 0.4 73.2 ± 0.4 62.5 ± 0.3 ARM [72] 51.3 ± 0.9 48.5 ± 0.4 68.0 ± 0.3 70.6 ± 0.1 59.6 ± 0.3 VREx [36] 51.1 ± 0.3 47.4 ± 0.6 69.0 ± 0.4 70.5 ± 0.4 59.5 ± 0.1 RSC [33] 49.0 ± 0.1 46.2 ± 1.5 67.8 ± 0.7 70.6 ± 0.3 58.4 ± 0.6 SelfReg [34] 55.1 ± 0.8 49.2 ± 0.6 72.2 ± 0.3 73.0 ± 0.3 62.4 ± 0.1 MixStyle [75] 50.8 ± 0.6 51.4 ± 1.1 67.6 ± 1.3 68.8 ± 0.5 59.6 ± 0.8 Fish [58] 54.6 ± 1.0 49.6 ± 1.0 71.3 ± 0.6 72.4 ± 0.2 62.0 ± 0.6 SD [51] 55.0 ± 0.4 51.3 ± 0.5 72.5 ± 0.2 72.7 ± 0.3 62.9 ± 0.2 CAD [53] 52.1 ± 0.6 48.3 ± 0.5 69.7 ± 0.3 71.9 ± 0.4 60.5 ± 0.3 CondCAD [53] 53.3 ± 0.6 48.4 ± 0.2 69.8 ± 0.9 72.6 ± 0.1 61.0 ± 0.4 Fishr [52] 52.6 ± 0.9 48.6 ± 0.3 69.9 ± 0.6 72.4 ± 0.4 60.9 ± 0.3 Ours 54.4 ± 0.2 52.3 ± 0.8 69.5 ± 0.3 71.7 ± 0.2 62.0 ± 0.2Table 14. Average accuracies on the TerraInc [4] datasets using the default hyper-parameter settings in DomainBed [27]. L100 L38 L43 L46 Average ERM [61] 42.1 ± 2.5 30.1 ± 1.2 48.9 ± 0.6 34.0 ± 1.1 38.8 ± 1.0 IRM [1] 41.8 ± 1.8 29.0 ± 3.6 49.6 ± 2.1 33.1 ± 1.5 38.4 ± 0.9 GroupGRO [55] 45.3 ± 4.6 36.1 ± 4.4 51.0 ± 0.8 33.7 ± 0.9 41.5 ± 2.0 Mixup [68] 49.4 ± 2.0 35.9 ± 1.8 53.0 ± 0.7 30.0 ± 0.9 42.1 ± 0.7 MLDG [38] 39.6 ± 2.3 33.2 ± 2.7 52.4 ± 0.5 35.1 ± 1.5 40.1 ± 0.9 CORAL [59] 46.7 ± 3.2 36.9 ± 4.3 49.5 ± 1.9 32.5 ± 0.7 41.4 ± 1.8 MMD [40] 49.1 ± 1.2 36.4 ± 4.8 50.4 ± 2.1 32.3 ± 1.5 42.0 ± 1.0 DANN [23] 44.3 ± 3.6 28.0 ± 1.5 47.9 ± 1.0 31.3 ± 0.6 37.9 ± 0.9 CDANN [44] 36.9 ± 6.4 32.7 ± 6.2 51.1 ± 1.3 33.5 ± 0.5 38.6 ± 2.3 MTL [6] 45.2 ± 2.6 31.0 ± 1.6 50.6 ± 1.1 34.9 ± 0.4 40.4 ± 1.0 SagNet [48] 36.3 ± 4.7 40.3 ± 2.0 52.5 ± 0.6 33.3 ± 1.3 40.6 ± 1.5 ARM [72] 41.5 ± 4.5 27.7 ± 2.4 50.9 ± 1.0 29.6 ± 1.5 37.4 ± 1.9 VREx [36] 48.0 ± 1.7 41.1 ± 1.5 51.8 ± 1.5 32.0 ± 1.2 43.2 ± 0.3 RSC [33] 42.8 ± 2.4 32.2 ± 3.8 49.6 ± 0.9 32.9 ± 1.2 39.4 ± 1.3 SelfReg [34] 46.1 ± 1.5 34.5 ± 1.6 49.8 ± 0.3 34.7 ± 1.5 41.3 ± 0.3 MixStyle [75] 50.6 ± 1.9 28.0 ± 4.5 52.1 ± 0.7 33.0 ± 0.2 40.9 ± 1.1 Fish [58] 46.3 ± 3.0 29.0 ± 1.1 52.7 ± 1.2 32.8 ± 1.0 40.2 ± 0.6 SD [51] 45.5 ± 1.9 33.2 ± 3.1 52.9 ± 0.7 36.4 ± 0.8 42.0 ± 1.0 CAD [53] 43.1 ± 2.6 31.1 ± 1.9 53.1 ± 1.6 34.7 ± 1.3 40.5 ± 0.4 CondCAD [53] 44.4 ± 2.9 32.9 ± 2.5 50.5 ± 1.3 30.8 ± 0.5 39.7 ± 0.4 Fishr [52] 49.9 ± 3.3 36.6 ± 0.9 49.8 ± 0.2 34.2 ± 1.3 42.6 ± 1.0 Ours 51.7 ± 2.4 37.6 ± 0.6 49.9 ± 0.6 33.6 ± 0.6 43.2 ± 0.5 Table 15. Average accuracies on the DomainNet [50] datasets using the default hyper-parameter settings in DomainBed [27]. clip info paint quick real sketch Average ERM [61] 50.4 ± 0.2 14.0 ± 0.2 40.3 ± 0.5 11.7 ± 0.2 52.0 ± 0.2 43.2 ± 0.3 35.3 ± 0.1 IRM [1] 43.2 ± 0.9 12.6 ± 0.3 35.0 ± 1.4 9.9 ± 0.4 43.4 ± 3.0 38.4 ± 0.4 30.4 ± 1.0 GroupGRO [55] 38.2 ± 0.5 13.0 ± 0.3 28.7 ± 0.3 8.2 ± 0.1 43.4 ± 0.5 33.7 ± 0.0 27.5 ± 0.1 Mixup [68] 48.9 ± 0.3 13.6 ± 0.3 39.5 ± 0.5 10.9 ± 0.4 49.9 ± 0.2 41.2 ± 0.2 34.0 ± 0.0 MLDG [38] 51.1 ± 0.3 14.1 ± 0.3 40.7 ± 0.3 11.7 ± 0.1 52.3 ± 0.3 42.7 ± 0.2 35.4 ± 0.0 CORAL [59] 51.2 ± 0.2 15.4 ± 0.2 42.0 ± 0.2 12.7 ± 0.1 52.0 ± 0.3 43.4 ± 0.0 36.1 ± 0.2 MMD [40] 16.6 ± 13.3 0.3 ± 0.0 12.8 ± 10.4 0.3 ± 0.0 17.1 ± 13.7 0.4 ± 0.0 7.9 ± 6.2 DANN [23] 45.0 ± 0.2 12.8 ± 0.2 36.0 ± 0.2 10.4 ± 0.3 46.7 ± 0.3 38.0 ± 0.3 31.5 ± 0.1 CDANN [44] 45.3 ± 0.2 12.6 ± 0.2 36.6 ± 0.2 10.3 ± 0.4 47.5 ± 0.1 38.9 ± 0.4 31.8 ± 0.2 MTL [6] 50.6 ± 0.2 14.0 ± 0.4 39.6 ± 0.3 12.0 ± 0.3 52.1 ± 0.1 41.5 ± 0.0 35.0 ± 0.0 SagNet [48] 51.0 ± 0.1 14.6 ± 0.1 40.2 ± 0.2 12.1 ± 0.2 51.5 ± 0.3 42.4 ± 0.1 35.3 ± 0.1 ARM [72] 43.0 ± 0.2 11.7 ± 0.2 34.6 ± 0.1 9.8 ± 0.4 43.2 ± 0.3 37.0 ± 0.3 29.9 ± 0.1 VREx [36] 39.2 ± 1.6 11.9 ± 0.4 31.2 ± 1.3 10.2 ± 0.4 41.5 ± 1.8 34.8 ± 0.8 28.1 ± 1.0 RSC [33] 39.5 ± 3.7 11.4 ± 0.8 30.5 ± 3.1 10.2 ± 0.8 41.0 ± 1.4 34.7 ± 2.6 27.9 ± 2.0 SelfReg [34] 47.9 ± 0.3 15.1 ± 0.3 41.2 ± 0.2 11.7 ± 0.3 48.8 ± 0.0 43.8 ± 0.3 34.7 ± 0.2 MixStyle [75] 49.1 ± 0.4 13.4 ± 0.0 39.3 ± 0.0 11.4 ± 0.4 47.7 ± 0.3 42.7 ± 0.1 33.9 ± 0.1 Fish [58] 51.5 ± 0.3 14.5 ± 0.2 40.4 ± 0.3 11.7 ± 0.5 52.6 ± 0.2 42.1 ± 0.1 35.5 ± 0.0 SD [51] 51.3 ± 0.3 15.5 ± 0.1 41.5 ± 0.3 12.6 ± 0.2 52.9 ± 0.2 44.0 ± 0.4 36.3 ± 0.2 CAD [53] 45.4 ± 1.0 12.1 ± 0.5 34.9 ± 1.1 10.2 ± 0.6 45.1 ± 1.6 38.5 ± 0.6 31.0 ± 0.8 CondCAD [53] 46.1 ± 1.0 13.3 ± 0.4 36.1 ± 1.4 10.7 ± 0.2 46.8 ± 1.3 38.7 ± 0.7 31.9 ± 0.7 Fishr [52] 47.8 ± 0.7 14.6 ± 0.2 40.0 ± 0.3 11.9 ± 0.2 49.2 ± 0.7 41.7 ± 0.1 34.2 ± 0.3 Ours 50.7 ± 0.7 13.9 ± 0.4 39.4 ± 0.5 11.9 ± 0.2 50.2 ± 0.3 43.5 ± 0.1 34.9 ± 0.1",
      "meta_data": {
        "arxiv_id": "2304.04494v2",
        "authors": [
          "Liang Chen",
          "Yong Zhang",
          "Yibing Song",
          "Ying Shan",
          "Lingqiao Liu"
        ],
        "published_date": "2023-04-10T10:12:38Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04494v2.pdf",
        "github_url": "https://github.com/liangchen527/ITTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the distribution shift problem in domain generalization (DG) by improving Test-Time Training (TTT) with a novel Improved Test-Time Adaptation (ITTA) method. ITTA proposes a learnable consistency loss, which includes adjustable parameters for better alignment with the main prediction task, and introduces additional adaptive parameters to the trained model, updating only these new parameters during the test phase. Through extensive experiments, ITTA achieves superior performance on several DG benchmarks compared to current state-of-the-art methods in both multi-source and single-source DG tasks.",
        "methodology": "The proposed ITTA method improves TTT through two main strategies. First, a learnable consistency loss (Lwcont) is introduced for the TTT auxiliary task. Instead of a heuristically defined objective, Lwcont uses a weight subnetwork (fw) with learnable parameters to measure consistency between original and augmented feature representations. These parameters are adjusted by enforcing the normalized gradients of Lwcont and the main classification loss (Lmain) to be equal, ensuring alignment between the TTT task and the main objective. Second, additional adaptive parameters (fΘ) are included as adaptive blocks after each block of the pre-trained feature extractor (fθ). During the test-time adaptation phase, only these newly introduced adaptive parameters (fΘ) are updated using the learned consistency loss, while the original model parameters (fθ, fϕ) remain unchanged.",
        "experimental_setup": "ITTA was evaluated on five benchmark datasets: PACS, VLCS, OfficeHome, TerraInc, and DomainNet. The experiments followed the rigorous DomainBed benchmark protocol, including 60 trials for each unseen domain, 5,000 iteration steps during training, and using a ResNet18 backbone pre-trained on ImageNet. The leave-one-out strategy was adopted for multi-source DG, and single-source DG was evaluated on PACS. Model selection used the 'training-domain validate set' method. Comparisons were made against 22 existing DG methods (e.g., ERM, MMD, MixStyle, Fishr) and other TTT-based models (TTT, MT3, TENT). Ablation studies were conducted on PACS to analyze the effectiveness of the learnable consistency loss and adaptive parameters.",
        "limitations": "The main limitation of ITTA is the computational burden introduced by the learnable consistency loss. While updating the joint loss without the weight subnetwork (fw) costs 1 forward and 1 backward pass, updating fw requires computing second-order derivatives in the gradient alignment step, which demands an additional forward pass and three backward passes, leading to extra computational cost for the system.",
        "future_research_directions": "Future research will focus on simplifying the overall optimization process of ITTA and reducing its computational cost, particularly regarding the updates involving the weight subnetwork (fw).",
        "experimental_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.autograd as autograd\n\nimport copy\nimport numpy as np\nfrom collections import defaultdict, OrderedDict\n\nfrom domainbed import networks\nfrom domainbed import HSIC\nfrom domainbed.lib.misc import (\n    random_pairs_of_minibatches, ParamDict, MovingAverage, l2_between_dicts\n)\n\n\nclass ITTA(Algorithm):\n    \"\"\"\n    Improved Test-Time Adaptation (ITTA)\n    \"\"\"\n\n    def __init__(self, input_shape, num_classes, num_domains, hparams):\n        super(ITTA, self).__init__(input_shape, num_classes, num_domains,\n                                  hparams)\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.featurizer = networks.ResNet_ITTA(input_shape, self.hparams)\n        self.classifier = networks.Classifier(\n            self.featurizer.n_outputs,\n            num_classes,\n            self.hparams['nonlinear_classifier'])\n        self.test_mapping = networks.MappingNetwork() #specialized for resnet18\n        self.test_optimizer = torch.optim.Adam(self.test_mapping.parameters(), lr=self.hparams[\"lr\"]*0.1)\n        self.optimizer = torch.optim.Adam([\n            {'params': self.featurizer.parameters()},\n            {'params': self.classifier.parameters()}],\n            lr=self.hparams[\"lr\"],\n            weight_decay=self.hparams['weight_decay']\n        )\n        self.MSEloss = nn.MSELoss()\n        self.adaparams = networks.Adaparams() #specialized for resnet18\n        self.adaparams_optimizer = torch.optim.Adam(self.adaparams.parameters(), lr=self.hparams[\"lr\"]*0.1)\n\n    def _get_grads(self, loss):\n        self.optimizer.zero_grad()\n        loss.backward(inputs=list(self.featurizer.parameters()),\n                          retain_graph=True, create_graph=True)\n        dict = OrderedDict(\n            [\n                (name, weights.grad.clone().view(weights.grad.size(0),-1))\n                for name, weights in self.featurizer.named_parameters()\n            ]\n        )\n\n        return dict\n\n    def update(self, minibatches, unlabeled=None):\n        all_x = torch.cat([x for x,y in minibatches])\n        all_y = torch.cat([y for x,y in minibatches])\n        ############################# this is for network update\n        #############################\n        z_ori, z_aug = self.featurizer(all_x)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + \\\n                   F.cross_entropy(self.classifier(z_aug), all_y)\n        loss = loss_reg + loss_cla\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        ############################# this is for adaparams update\n        #############################\n        z_ori, z_aug = self.featurizer(all_x)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + \\\n                   F.cross_entropy(self.classifier(z_aug), all_y)\n        dict_reg = self._get_grads(loss_reg)\n        dict_cla = self._get_grads(loss_cla)\n        penalty = l2_between_dicts(dict_reg, dict_cla, normalize=True) * 0.1\n        self.adaparams_optimizer.zero_grad()\n        penalty.backward(inputs=list(self.adaparams.parameters()))\n        self.adaparams_optimizer.step()\n\n        return {'loss': loss_cla.item(), 'reg': loss_reg.item()}\n\n    def test_adapt(self, x):\n        z_ori, z_aug = self.featurizer(x)\n        z_ori, z_aug = self.test_mapping.fea1(z_ori), self.test_mapping.fea1(z_aug)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.test_mapping.fea2(z_ori), self.test_mapping.fea2(z_aug)\n        z_ori, z_aug = self.featurizer.fea3(z_ori), self.featurizer.fea3(z_aug)\n        z_ori, z_aug = self.test_mapping.fea3(z_ori), self.test_mapping.fea3(z_aug)\n        z_ori, z_aug = self.featurizer.fea4(z_ori), self.featurizer.fea4(z_aug)\n        z_ori, z_aug = self.test_mapping.fea4(z_ori), self.test_mapping.fea4(z_aug)\n        z_ori, z_aug = self.featurizer.flat(z_ori), self.featurizer.flat(z_aug)\n        ########## small lr for large datasets\n        loss_reg = self.MSEloss(self.adaparams(z_aug-z_ori), torch.zeros_like(z_ori)) * self.hparams['ada_lr']\n        self.test_optimizer.zero_grad()\n        loss_reg.backward(inputs=list(self.test_mapping.parameters()))\n        self.test_optimizer.step()\n\n    def predict(self, x):\n        z_ori, z_aug = self.featurizer(x)\n        z_ori = self.test_mapping.fea1(z_ori)\n        z_ori, z_aug = self.featurizer.fea2(z_ori,z_aug)\n        z_ori = self.test_mapping.fea2(z_ori)\n        z_ori = self.featurizer.fea3(z_ori)\n        z_ori = self.test_mapping.fea3(z_ori)\n        z_ori = self.featurizer.fea4(z_ori)\n        z_ori = self.test_mapping.fea4(z_ori)\n        z_ori = self.featurizer.flat(z_ori)\n        return self.classifier(z_ori)\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models\nimport random\nfrom domainbed.lib import wide_resnet\nimport copy\n\nclass MappingNetwork(torch.nn.Module):\n    def __init__(self, depth=5):\n        super().__init__()\n        self.depth = depth\n        self.weight1 = nn.ParameterList()\n        self.bias1 = nn.ParameterList()\n        self.weight2 = nn.ParameterList()\n        self.bias2 = nn.ParameterList()\n        self.weight3 = nn.ParameterList()\n        self.bias3 = nn.ParameterList()\n        self.weight4 = nn.ParameterList()\n        self.bias4 = nn.ParameterList()\n        for i in range(depth):\n            self.weight1.append(nn.Parameter(torch.ones((64,56,56))))\n            self.bias1.append(nn.Parameter(torch.zeros((64,56,56))))\n\n            self.weight2.append(nn.Parameter(torch.ones((128,28,28))))\n            self.bias2.append(nn.Parameter(torch.zeros((128,28,28))))\n\n            self.weight3.append(nn.Parameter(torch.ones((256,14,14))))\n            self.bias3.append(nn.Parameter(torch.zeros((256,14,14))))\n\n            self.weight4.append(nn.Parameter(torch.ones((512, 7, 7))))\n            self.bias4.append(nn.Parameter(torch.zeros((512, 7, 7))))\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def fea1(self, x):\n        for i in range(self.depth-1):\n            x = self.relu(self.weight1[i] * x + self.bias1[i])\n        x = self.weight1[i+1] * x + self.bias1[i+1]\n        return x\n\n    def fea2(self, x):\n        for i in range(self.depth - 1):\n            x = self.relu(self.weight2[i] * x + self.bias2[i])\n        x = self.weight2[i + 1] * x + self.bias2[i + 1]\n        return x\n\n    def fea3(self, x):\n        for i in range(self.depth - 1):\n            x = self.relu(self.weight3[i] * x + self.bias3[i])\n        x = self.weight3[i + 1] * x + self.bias3[i + 1]\n        return x\n\n    def fea4(self, x):\n        for i in range(self.depth-1):\n            x = self.relu(self.weight4[i] * x + self.bias4[i])\n        x = self.weight4[i+1] * x + self.bias4[i+1]\n        return x\n\nclass Adaparams(nn.Module):\n    def __init__(self, depth=10):\n        super(Adaparams, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.depth = depth\n        self.weight = nn.ParameterList()\n        self.bias = nn.ParameterList()\n        for i in range(depth):\n            self.weight.append(nn.Parameter(torch.ones(512)))\n            self.bias.append(nn.Parameter(torch.zeros(512)))\n\n    def forward(self, x):\n        for i in range(self.depth-1):\n            x = self.relu(self.weight[i] * x + self.bias[i])\n        x = self.weight[i+1] * x + self.bias[i+1]\n        return x\n        \n\nclass ResNet_ITTA(torch.nn.Module):\n    \"\"\"ResNet with the softmax chopped off and the batchnorm frozen\"\"\"\n    def __init__(self, input_shape, hparams):\n        super(ResNet_ITTA, self).__init__()\n        if hparams['resnet18']:\n            self.network = torchvision.models.resnet18(pretrained=True)\n            self.n_outputs = 512\n        else:\n            self.network = torchvision.models.resnet18(pretrained=True)\n            self.n_outputs = 2048\n\n        nc = input_shape[0]\n        if nc != 3:\n            tmp = self.network.conv1.weight.data.clone()\n\n            self.network.conv1 = nn.Conv2d(\n                nc, 64, kernel_size=(7, 7),\n                stride=(2, 2), padding=(3, 3), bias=False)\n\n            for i in range(nc):\n                self.network.conv1.weight.data[:, i, :, :] = tmp[:, i % 3, :, :]\n\n        # save memory\n        self.network.fc = Identity()\n        self.isaug = True\n        self.freeze_bn()\n        self.hparams = hparams\n        self.dropout = nn.Dropout(hparams['resnet_dropout'])\n        self.eps = 1e-6\n\n    def mixstyle(self, x):\n        alpha = 0.1\n        beta = torch.distributions.Beta(alpha, alpha)\n        B = x.size(0)\n        mu = x.mean(dim=[2, 3], keepdim=True)\n        var = x.var(dim=[2, 3], keepdim=True)\n        sig = (var + self.eps).sqrt()\n        mu, sig = mu.detach(), sig.detach()\n        x_normed = (x - mu) / sig\n        lmda = beta.sample((B, 1, 1, 1))\n        lmda = lmda.to(x.device)\n        perm = torch.randperm(B)\n        mu2, sig2 = mu[perm], sig[perm]\n        mu_mix = mu * lmda + mu2 * (1 - lmda)\n        sig_mix = sig * lmda + sig2 * (1 - lmda)\n        return x_normed * sig_mix + mu_mix\n\n    def fea_forward(self, x):\n        x = self.fea3(x)\n        x = self.fea4(x)\n\n        x = self.flat(x)\n        return x\n\n    def fea2(self, x, aug_x):\n        x = self.network.layer2(x)\n        aug_x = self.network.layer2(aug_x)\n        if not self.isaug:\n            aug_x = self.mixstyle(aug_x)\n        return x, aug_x\n\n    def fea3(self, x):\n        x = self.network.layer3(x)\n        return x\n\n    def fea4(self, x):\n        x = self.network.layer4(x)\n        return x\n\n    def flat(self, x):\n        x = self.network.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.network.fc(x)\n        x = self.dropout(x)\n        return x\n\n    def forward(self, x):\n        \"\"\"Encode x into a feature vector of size n_outputs.\"\"\"\n        x = self.network.conv1(x)\n        x = self.network.bn1(x)\n        x = self.network.relu(x)\n        x = self.network.maxpool(x)\n\n        x = self.network.layer1(x)\n        if random.random() > 0.5:\n            self.isaug = True\n            aug_x = self.mixstyle(x)\n        else:\n            self.isaug = False\n            aug_x = x\n\n        return x, aug_x\n\n    def train(self, mode=True):\n        \"\"\"\n        Override the default train() to freeze the BN parameters\n        \"\"\"\n        super().train(mode)\n        self.freeze_bn()\n\n    def freeze_bn(self):\n        for m in self.network.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n",
        "experimental_info": "The ITTA (Improved Test-Time Adaptation) method enhances TTT (Test-Time Training) through two main strategies:\n\n1.  **Learnable Consistency Loss (Lwcont):**\n    *   A weight subnetwork `fw` (implemented as `self.adaparams = networks.Adaparams()`) with learnable parameters is used to measure consistency between original (`z_ori`) and augmented (`z_aug`) feature representations. The consistency loss (`loss_reg`) is calculated as `loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))`. `Adaparams` is a simple MLP-like network with a configurable `depth` (default 10) acting on 512-dimensional features.\n    *   The parameters of `self.adaparams` are adjusted by enforcing the normalized gradients of `loss_reg` (`dict_reg`) and the main classification loss (`loss_cla`) to be equal. This is achieved through `penalty = l2_between_dicts(dict_reg, dict_cla, normalize=True) * 0.1`, where `l2_between_dicts` computes the L2 distance between the flattened and normalized gradients of the two losses with respect to the featurizer's parameters. This `penalty` is then backpropagated through `self.adaparams.parameters()` using `self.adaparams_optimizer` (with `lr=self.hparams[\"lr\"]*0.1`).\n\n2.  **Adaptive Parameters (fΘ):**\n    *   Additional adaptive parameters (`fΘ`) are introduced as adaptive blocks after each block of the pre-trained feature extractor (`fθ`). These are implemented as `self.test_mapping = networks.MappingNetwork()`, specifically designed for ResNet-18. `MappingNetwork` consists of four sets of `nn.ParameterList` (weights and biases), each applied after a ResNet layer (layer1, layer2, layer3, layer4 effectively `fea1`, `fea2`, `fea3`, `fea4`). It has a configurable `depth` (default 5).\n    *   During the test-time adaptation phase (`test_adapt` function), only these newly introduced adaptive parameters (`self.test_mapping.parameters()`) are updated using a learned consistency loss. The original model parameters (`fθ` which is `self.featurizer`, and `fϕ` which is `self.classifier`) remain unchanged (indicated by `network.featurizer.requires_grad_(False)` and `network.classifier.requires_grad_(False)` during evaluation in `misc.accuracy_tsc`).\n    *   The loss for updating `test_mapping` during `test_adapt` is `loss_reg = self.MSEloss(self.adaparams(z_aug-z_ori), torch.zeros_like(z_ori)) * self.hparams['ada_lr']`, where `z_ori` and `z_aug` are features processed through `test_mapping` blocks. `self.test_optimizer` (with `lr=self.hparams[\"lr\"]*0.1`) updates `test_mapping`'s parameters.\n\n**Feature Extractor and Augmentation:**\n*   The feature extractor `fθ` is `networks.ResNet_ITTA`, a modified ResNet-18 (or 50) initialized with pre-trained weights. It freezes BatchNorm layers during training (`freeze_bn`).\n*   `ResNet_ITTA` implements a `mixstyle` function for data augmentation, applying style randomization (mean and variance mixing) to feature maps (specifically after `network.layer1`). A random choice is made whether to apply `mixstyle` or use the original feature. The `alpha` for the Beta distribution in `mixstyle` is 0.1.\n\n**Hyperparameters:**\n*   `ada_lr`: Learning rate multiplier for `loss_reg` during `test_adapt` (updating `test_mapping`). It's set to `0.1` for the `DomainNet` dataset and `1e-6` for other datasets.\n*   `lr`: Base learning rate for `self.optimizer` (updating `featurizer` and `classifier` parameters during training).\n*   `test_optimizer` and `adaparams_optimizer` both use `self.hparams[\"lr\"]*0.1` as their learning rate.\n*   `resnet18`: Boolean, determines if ResNet-18 (`n_outputs=512`) or ResNet-50 (`n_outputs=2048`) is used for the featurizer.\n*   `resnet_dropout`: Dropout rate applied in the featurizer.\n*   `weight_decay`: Weight decay for `self.optimizer`.\n*   `nonlinear_classifier`: Boolean, determines if the classifier is a simple linear layer or an MLP.\n\n**Training Process:**\n*   The `update` method of `ITTA` performs two main updates:\n    1.  Updates the featurizer (`self.featurizer`) and classifier (`self.classifier`) using a combined loss of `loss_reg` (from `adaparams`) and `loss_cla` (cross-entropy on original and augmented features). `self.optimizer` is used here.\n    2.  Updates `self.adaparams` by aligning the gradients of `loss_reg` and `loss_cla` with respect to the featurizer's parameters. `self.adaparams_optimizer` is used here.\n*   During evaluation, `misc.accuracy_tsc` calls `algorithm.test_adapt(x)` for a single step (1 iteration) before predicting, allowing for online adaptation of `self.test_mapping`."
      }
    },
    {
      "title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
      "abstract": "Test-time adaptation (TTA) addresses distribution shifts for streaming test\ndata in unsupervised settings. Currently, most TTA methods can only deal with\nminor shifts and rely heavily on heuristic and empirical studies.\n  To advance TTA under domain shifts, we propose the novel problem setting of\nactive test-time adaptation (ATTA) that integrates active learning within the\nfully TTA setting.\n  We provide a learning theory analysis, demonstrating that incorporating\nlimited labeled test instances enhances overall performances across test\ndomains with a theoretical guarantee. We also present a sample entropy\nbalancing for implementing ATTA while avoiding catastrophic forgetting (CF). We\nintroduce a simple yet effective ATTA algorithm, known as SimATTA, using\nreal-time sample selection techniques. Extensive experimental results confirm\nconsistency with our theoretical analyses and show that the proposed ATTA\nmethod yields substantial performance improvements over TTA methods while\nmaintaining efficiency and shares similar effectiveness to the more demanding\nactive domain adaptation (ADA) methods. Our code is available at\nhttps://github.com/divelab/ATTA",
      "full_text": "Published as a conference paper at ICLR 2024 ACTIVE TEST-TIME ADAPTATION : T HEORETICAL ANALYSES AND AN ALGORITHM Shurui Gui∗ Texas A&M University College Station, TX 77843 shurui.gui@tamu.edu Xiner Li* Texas A&M University College Station, TX 77843 lxe@tamu.edu Shuiwang Ji Texas A&M University College Station, TX 77843 sji@tamu.edu ABSTRACT Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies. To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting. We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques. Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods. Our code is available at https://github.com/divelab/ATTA. 1 I NTRODUCTION Deep learning has achieved remarkable success across various fields, attaining high accuracy in numerous applications (Krizhevsky et al., 2017; Simonyan and Zisserman, 2014). Nonetheless, When training and test data follow distinct distributions, models often experience significant performance degradation during test. This phenomenon, known as the distribution shift or out-of-distribution (OOD) problem, is extensively studied within the context of both domain generalization (DG) (Gulra- jani and Lopez-Paz, 2020; Koh et al., 2021; Gui et al., 2022) and domain adaptation (DA) (Ganin et al., 2016; Sun and Saenko, 2016). While these studies involve intensive training of models with considerable generalization abilities towards target domains, they overlook an important application property; namely, continuous adaptivity to real-time streaming data under privacy, resource, and efficiency constraints. This gap leads to the emergence of test-time adaptation (TTA) tasks, targeting on-the-fly adaptation to continuous new domains during the test phase or application deployment. The study of TTA encompasses two main categories; namely test-time training (TTT) methods (Sun et al., 2020; Liu et al., 2021c) and fully test-time adaptation (FTTA) (Niu et al., 2023; Wang et al., 2021). The TTT pipeline incorporates retraining on the source data, whereas FTTA methods adapt arbitrary pre-trained models to the given test mini-batch by conducting entropy minimization, without access to the source data. Nevertheless, most TTA methods can only handle corrupted distribution shifts (Hendrycks and Dietterich, 2019b) (e.g., Gaussian noise,) and rely heavily on human intuition or empirical studies. To bridge this gap, our paper focuses on tackling significant domain distribution shifts in real time with theoretical insights. We investigate FTTA, which is more general and adaptable than TTT, particularly under data ac- cessibility, privacy, and efficiency constraints. Traditional FTTA aims at adapting a pre-trained model to streaming test-time data from diverse domains under unsupervised settings. However, recent works (Lin et al., 2022; Pearl, 2009) prove that it is theoretically infeasible to achieve OOD generalization without extra information such as environment partitions. Since utilizing environment partitions requires heavy pretraining, contradicting the nature of TTA, we are motivated to incorporate extra information in a different way,i.e., integrating a limited number of labeled test-time samples to alleviate distribution shifts, following the active learning (AL) paradigm (Settles, 2009). To this end, we propose the novel problem setting of active test-time adaptation (ATTA) by incorporating ∗Equal contributions 1 arXiv:2404.05094v1  [cs.LG]  7 Apr 2024Published as a conference paper at ICLR 2024 AL within FTTA. ATTA faces two major challenges; namely, catastrophic forgetting (CF) (Kemker et al., 2018; Li and Hoiem, 2017) and real-time active sample selection. CF problem arises when a model continually trained on a sequence of domains experiences a significant performance drop on previously learned domains, due to the inaccessibility of the source data and previous test data. Real-time active sample selection requires AL algorithms to select informative samples from a small buffer of streaming test data for annotation, without a complete view of the test distribution. In this paper, we first formally define the ATTA setting. We then provide its foundational analysis under the learning theory’s paradigm to guarantee the mitigation of distribution shifts and avoid CF. Aligned with our empirical validations, while the widely used entropy minimization (Wang et al., 2021; Grandvalet and Bengio, 2004) can cause CF, it can conversely become the key to preventing CF problems with our sample selection and balancing techniques. Building on the analyses, we then introduce a simple yet effective ATTA algorithm, SimATTA, incorporating balanced sample selections and incremental clustering. Finally, we conducted a comprehensive experimental study to evaluate the proposed ATTA settings with three different settings in the order of low to high requirement restrictiveness, i.e., TTA, Enhanced TTA, and Active Domain Adaptation (ADA). Intensive experiments indicate that ATTA jointly equips with the efficiency of TTA and the effectiveness of ADA, rendering an uncompromising real-time distribution adaptation direction. Comparison to related studies. Compared to TTA methods, ATTA requires extra active labels, but the failure of TTA methods (Sec. 5.1) and the theoretical proof of Lin et al. (2022); Pearl (2009) justify its necessity and rationality. Compared to active online learning, ATTA focuses on lightweight real-time fine-tuning without round-wise re-trainings as Saran et al. (2023) and emphasizes the importance of CF avoidance instead of resetting models and losing learned distributions. In fact, active online learning is partially similar to our enhanced TTA setting (Sec. 5.2. Compared to ADA methods (Prabhu et al., 2021; Ning et al., 2021), ATTA does not presuppose access to source data, model parameters, or pre-collected target samples. Furthermore, without this information, ATTA can still perform on par with ADA methods (Sec. 5.3). The recent source-free active domain adaptation (SFADA) method SALAD (Kothandaraman et al., 2023) still requires access to model parameter gradients, pre-collected target data, and training of additional networks. Our ATTA, in contrast, with non-regrettable active sample selection on streaming data, is a much lighter and more realistic approach distinct from ADA and SFADA. More related-work discussions are provided in Appx. C. 2 T HE ACTIVE TEST-TIME ADAPTATION FORMULATION TTA methods aim to solve distribution shifts by dynamically optimizing a pre-trained model based on streaming test data. We introduce the novel problem setting of Active Test-Time Adaptation (ATTA), which incorporates active learning during the test phase. In ATTA, the model continuously selects the most informative instances from the test batch to be labeled by an explicit or implicit oracle (e.g., human annotations, self-supervised signals) and subsequently learned by the model, aiming to improve future adaptations. Considering the labeling costs in real-world applications, a “budget” is established for labeled test instances. The model must effectively manage this budget distribution and ensure that the total number of label requests throughout the test phase does not surpass the budget. We now present a formal definition of the ATTA problem. Consider a pre-trained modelf(x; ϕ) with parameters ϕ trained on the source dataset DS = (x, y)|DS|, with each data sample x ∈ Xand a label y ∈ Y. We aim to adapt model parameters θ, initialized as ϕ, to an unlabeled test-time data stream. The streaming test data exhibit distribution shifts from the source data and varies continuously with time, forming multiple domains to which we must continuously adapt. The test phase commences at time step t = 1 and the streaming test data is formulated in batches. The samples are then actively selected, labeled (by the oracle) and collected as Dte(t) = ActAlg(Ute(t)), where ActAlg(·) denotes an active selection/labeling algorithm. The labeled samples Dte(t) are subsequently incorporated into the ATTA training setDtr(t). Finally, we conclude time step t by performing ATTA training, updating model parameters θ(t) using Dtr(t), with θ(t) initialized as the previous final state θ(t − 1). Definition 1 (The ATTA problem). Given a model f(x; θ), with parameters θ, initialized with parameters θ(0) = ϕ obtained by pre-training on source domain data, and streaming test data batches Ute(t) continually changing over time, the ATTA task aims to optimize the model at any time stept (with test phase commencing at t = 1) as θ(t)∗ := argmin θ(t) (E(x,y,t)∈Dtr(t)[ℓCE (f(x; θ(t)), y)] + E(x,t)∈Ute(t)[ℓU (f(x; θ(t)))]), (1) 2Published as a conference paper at ICLR 2024 where Dtr(t) = ( ∅, t = 0 Dtr(t − 1) ∪ Dte(t), t ≥ 1, s.t. |Dtr(t)| ≤ B, (2) Dte(t) = ActAlg(Ute(t)) is actively selected and labeled, ℓCE is the cross entropy loss, ℓU is an unsupervised learning loss, and B is the budget. 3 T HEORETICAL STUDIES In this section, we conduct an in-depth theoretical analysis of TTA based on learning theories. We mainly explore two questions: How can significant distribution shifts be effectively addressed under the TTA setting? How can we simultaneously combat the issue of CF? Sec. 3.1 provides a solution with theoretical guarantees to the first question, namely, active TTA (ATTA), along with the conditions under which distribution shifts can be well addressed. Sec. 3.2 answers the second question with an underexplored technique, i.e., selective entropy minimization, building upon the learning bounds established in Sec. 3.1. We further validate these theoretical findings through experimental analysis. Collectively, we present a theoretically supported ATTA solution that effectively tackles both distribution shift and CF. 3.1 A LLEVIATING DISTRIBUTION SHIFTS THROUGH ACTIVE TEST-TIME ADAPTATION Traditional TTA is performed in unsupervised or self-supervised context. In contrast, ATTA introduces supervision into the adaptation setting. In this subsection, we delve into learning bounds and establish generalization bounds to gauge the efficacy of ATTA in solving distribution shifts. We scrutinize the influence of active learning and evidence that the inclusion of labeled test instances markedly enhances overall performances across incremental test domains. Following Kifer et al. (2004), we examine statistical guarantees for binary classification. A hypothesis is a function h : X → {0, 1}, which can serve as the prediction function within this context. In the ATTA setting, the mapping ofh varies with time as h(x, t). We use H∆H-distance following Ben- David et al. (2010), which essentially provides a measure to quantify the distribution shift between two distributions D1 and D2, and can also be applied between datasets. The probability that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} according to distribution D is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source data is inaccessible under ATTA settings, we consider the existence of source dataset DS for accurate theoretical analysis. Thus, we initialize Dtr as Dtr(0) = DS. For every time step t, the test and training data can be expressed asUte(t) and Dtr(t) = DS ∪Dte(1) ∪Dte(2) ∪···∪ Dte(t). Building upon two lemmas (provided in Appx. D), we establish bounds on domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesish at time t. Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), ··· , Ute(t), ··· , Si are unlabeled samples of sizem sampled from each of thet+1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λ = (λ0, ··· , λt). If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w = (w0, ··· , wt) on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. The adaptation performance on a test domain is majorly bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. Further theoretical analysis are in Appx. D. 3Published as a conference paper at ICLR 2024 Figure 1: (a) Empirical validation of Thm. 1. We train a series of models on N = 2000 samples from the PACS (Li et al., 2017) dataset given differentλ0 and w0 and display the test domain loss of each model. Red points are the test loss minimums given a fixed λ0. The orange line is the reference where w0 = λ0. We observe that w0 with loss minimums are located closed to the orange line but slightly smaller than λ0, which validates our findings in Eq. (4). (b) Empirical analysis with an uncertainty balancing. Given source pre-trained models, we fine-tune the models on 500 samples with different λ0 and w0, and display the combined error surface of test and source error. Although a small λ0 is good for test domain error, it can lead to non-trivial source error exacerbation. Therefore, we can observe that the global loss minimum (green X) locates in a relatively high-λ0 region. If we consider the multiple test data distributions as a single test domain,i.e., St i=1 Ute(i), Thm. 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . Given the optimal test/source hypothesis h∗ T (t) = arg minh∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), we have |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (3.a), with approximatelyB = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (4) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. The following theorem offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (5) Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. All proofs are provided in Appx. E. Finally, we support the theoretical findings with experimental analysis and show the numerical results of applying the principles on real-world datasets, as shown in Fig. 1. For rigorous analysis, note that our theoretical results rest on the underlying condition that N should at least be of the same scale as d, according to the principles of VC-dimension theory. The empirical alignment of our experiments with the theoretical framework can be attributed to the assumption that fine-tuning a model is roughly equivalent to learning a model with a relatively small d. Experiment details and other validations can be found in Appx. H. 4Published as a conference paper at ICLR 2024 3.2 M ITIGATING CATASTROPHIC FORGETTING WITH BALANCED ENTROPY MINIMIZATION Catastrophic forgetting (CF), within the realm of Test-Time Adaptation (TTA), principally manifests as significant declines in overall performance, most notably in the source domain. Despite the lack of well-developed learning theories for analyzing training with series data, empirical studies have convincingly illustrated the crucial role of data sequential arrangement in model learning, thereby accounting for the phenomenon of CF. Traditionally, the mitigation of CF in adaptation tasks involves intricate utilization of source domain data. However, under FTTA settings, access to the source dataset is unavailable, leaving the problem of CF largely unexplored in the data-centric view. Table 1: Correlation analysis of high/low en- tropy samples and domains. We use a source pre-trained model to select samples with low- est/highest entropy, and 1.retrain the model on 2000 samples; 2.fine-tune the model on 300 sam- ples. We report losses on source/test domains for each setting, showing that low-entropy samples form distributions close to the source domain. Sample type Retrain Fine-tune ϵS ϵT ϵS ϵT Low entropy 0.5641 0.8022 0.0619 1.8838 High entropy 2.5117 0.3414 0.8539 0.7725 To overcome this challenge of source dataset ab- sence, we explore the acquisition of “source-like” data. In TTA scenarios, it is generally assumed that the amount of source data is considerably large. We also maintain this assumption in ATTA, practically assuming the volume of source data greatly surpasses the test-time budget. As a re- sult, we can safely assume that the pre-trained model is well-trained on abundant source do- main data DS. Given this adequately trained source model, we can treat it as a “true” source data labeling function f(x; ϕ). The model es- sentially describes a distribution, Dϕ,S(X, Y) = {(x, ˆy) ∈ (X, Y) | ˆy = f(x; ϕ), x∈ DS}. The entropy of the model prediction is defined as H(ˆy) = −P c p(ˆyc) logp(ˆyc), ˆy = f(x; ϕ), where c denotes the class. Lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction, which can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model recognizes the sample as being similar to those it was trained on. Thus entropy can be used as an indicator of how closely a sample x aligns with the model distribution Dϕ,S. Since the model distribution is approximately the source distribution, selecting (and labeling) low-entropy samples using f(x; ϕ) essentially provides an estimate of sampling from the source dataset. Therefore, in place of the inaccessible DS, we can feasibly include the source-like dataset into the ATTA training data at each time stept: Dϕ,S(t) = {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el}, (6) where el is the entropy threshold. The assumption that Dϕ,S(t) is an approximation of DS can be empirically validated, as shown by the numerical results on PACS in Tab. 1. In contrast, high-entropy test samples typically deviate more from the source data, from which we select Dte(t) for active labeling. Following the notations in Thm. 1, we are practically minimizing the empirical weighted error of hypothesis h(t) as ˆϵ′ w(h(t)) = tX j=0 wjˆϵj(h(t)) = w0 λ0N X x∈Dϕ,S(t) |h(x, t) − f(x; ϕ)| + tX j=1 wj λjN X x,y∈Dte(j) |h(x, t) − y|. (7) By substituting DS with Dϕ,S(t) in Thm. 1, the bounds of Thm. 1 continue to hold for the test domains. In the corollary below, we bound the source error for practical ATTA at each time stept. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Thm. 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Thm. 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Further analysis and proofs are in Appx. D and E. The following corollary provides direct theoretical support that our strategy conditionally reduces the error bound on the source domain. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Thm. 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight 5Published as a conference paper at ICLR 2024 <latexit sha1_base64=\"NxhXSyFABPQk4q8627/odirDspg=\">AAAB9XicbVDLSgMxFM34rPVVdekmWARXZab4WhbcuKzYF7S1ZNI7bWgmMyR3lDL0P9y4UMSt/+LOvzHTdqGtBwKHc87l3hw/lsKg6347K6tr6xubua389s7u3n7h4LBhokRzqPNIRrrlMwNSKKijQAmtWAMLfQlNf3ST+c1H0EZEqobjGLohGygRCM7QSg/3mIWFGtAaGOwVim7JnYIuE29OimSOaq/w1elHPAlBIZfMmLbnxthNmUbBJUzyncRAzPiIDaBtqWIhmG46vXpCT63Sp0Gk7VNIp+rviZSFxoxD3yZDhkOz6GXif147weC6mwoVJwiKzxYFiaQY0awC2hcaOMqxJYxrYW+lfMg042iLytsSvMUvL5NGueRdli7uysXK+byOHDkmJ+SMeOSKVMgtqZI64USTZ/JK3pwn58V5dz5m0RVnPnNE/sD5/AFnsJJq</latexit> Streaming Test <latexit sha1_base64=\"a41BOKrutEYSWO9+8CjkPZKHvb8=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69BIvgqSTiR48FLx4r2A9oQ9lsN+3SzSbuToQQ+ie8eFDEq3/Hm//GTZuDtj4YeLw3w8w8PxZco+N8W6W19Y3NrfJ2ZWd3b/+genjU0VGiKGvTSESq5xPNBJesjRwF68WKkdAXrOtPb3O/+8SU5pF8wDRmXkjGkgecEjRSbzAhmKWzyrBac+rOHPYqcQtSgwKtYfVrMIpoEjKJVBCt+64To5cRhZwKNqsMEs1iQqdkzPqGShIy7WXze2f2mVFGdhApUxLtufp7IiOh1mnom86Q4EQve7n4n9dPMGh4GZdxgkzSxaIgETZGdv68PeKKURSpIYQqbm616YQoQtFElIfgLr+8SjoXdfe6fnV/WWs2ijjKcAKncA4u3EAT7qAFbaAg4Ble4c16tF6sd+tj0Vqyiplj+APr8wfpIY/e</latexit> ˆy <latexit sha1_base64=\"SJEOE2ZYxLL1SU/QahOlMH6fop4=\">AAAB8HicbVBNSwMxEM3Wr1q/qh69BItQL2VX/Oix4MVjBbettEvJptk2NMkuyaxQlv4KLx4U8erP8ea/MW33oK0PBh7vzTAzL0wEN+C6305hbX1jc6u4XdrZ3ds/KB8etUycasp8GotYd0JimOCK+cBBsE6iGZGhYO1wfDvz209MGx6rB5gkLJBkqHjEKQErPfr9DNi0Cuf9csWtuXPgVeLlpIJyNPvlr94gpqlkCqggxnQ9N4EgIxo4FWxa6qWGJYSOyZB1LVVEMhNk84On+MwqAxzF2pYCPFd/T2REGjORoe2UBEZm2ZuJ/3ndFKJ6kHGVpMAUXSyKUoEhxrPv8YBrRkFMLCFUc3srpiOiCQWbUcmG4C2/vEpaFzXvunZ1f1lp1PM4iugEnaIq8tANaqA71EQ+okiiZ/SK3hztvDjvzseiteDkM8foD5zPH2KnkB4=</latexit> U te ( t ) <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model SimATTA <latexit sha1_base64=\"bhVea6W/pzUPuDRNfs2xbDF7qAk=\">AAAB73icbVC7SgNBFL3rM8ZX1NJmMAhWYTf4KgM2FhYRzAOSJcxOZpMhs7PrzF0hhPyEjYUitv6OnX/jbLKFJh4YOJxzD3PvCRIpDLrut7Oyura+sVnYKm7v7O7tlw4OmyZONeMNFstYtwNquBSKN1Cg5O1EcxoFkreC0U3mt564NiJWDzhOuB/RgRKhYBSt1L6jQRYd9Eplt+LOQJaJl5My5Kj3Sl/dfszSiCtkkhrT8dwE/QnVKJjk02I3NTyhbEQHvGOpohE3/mS275ScWqVPwljbp5DM1N+JCY2MGUeBnYwoDs2il4n/eZ0Uw2t/IlSSIlds/lGYSoIxyY4nfaE5Qzm2hDIt7K6EDammDG1FRVuCt3jyMmlWK95l5eK+Wq6d53UU4BhO4Aw8uIIa3EIdGsBAwjO8wpvz6Lw4787HfHTFyTNH8AfO5w/1SI/i</latexit> Labeling <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"ipQ+JKlINPDcPjrbUYUkqyyzp40=\">AAAB+nicbVC7TsMwFHXKq5RXCiOLRYXEQpVUvMZKLIxF0IfURpXj3LRWHSeyHVBV+iksDCDEypew8Te4aQZoOZKlo3Puy8dPOFPacb6twsrq2vpGcbO0tb2zu2eX91sqTiWFJo15LDs+UcCZgKZmmkMnkUAin0PbH13P/PYDSMVica/HCXgRGQgWMkq0kfp2+S6bdNqQoCUxQ4K+XXGqTga8TNycVFCORt/+6gUxTSMQmnKiVNd1Eu1NiNSMcpiWeqmChNARGUDXUEEiUN4kO32Kj40S4DCW5gmNM/V3x4RESo0j31RGRA/VojcT//O6qQ6vvAkTSapB0PmiMOVYx3iWAw6YBKr52BBCJTO3YjokklBt0iqZENzFLy+TVq3qXlTPb2uV+lkeRxEdoiN0glx0ieroBjVQE1H0iJ7RK3qznqwX6936mJcWrLznAP2B9fkDSAyT+w==</latexit> Source-Pretrained <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model <latexit sha1_base64=\"5LNAmmVR/AN9Lc2T+FRV/is2yz8=\">AAAB8nicbVDLSgNBEJyNrxhfUY9eBoPgKewGX8eACB48RDAP2CxhdjKbDJmdWWZ6lbDkM7x4UMSrX+PNv3GS7EETCxqKqm66u8JEcAOu++0UVlbX1jeKm6Wt7Z3dvfL+QcuoVFPWpEoo3QmJYYJL1gQOgnUSzUgcCtYOR9dTv/3ItOFKPsA4YUFMBpJHnBKwkn+nnvCNBK2Sca9ccavuDHiZeDmpoByNXvmr21c0jZkEKogxvucmEGREA6eCTUrd1LCE0BEZMN9SSWJmgmx28gSfWKWPI6VtScAz9fdERmJjxnFoO2MCQ7PoTcX/PD+F6CrIuExSYJLOF0WpwKDw9H/c55pREGNLCNXc3orpkGhCwaZUsiF4iy8vk1at6l1Uz+9rlfpZHkcRHaFjdIo8dInq6BY1UBNRpNAzekVvDjgvzrvzMW8tOPnMIfoD5/MHKbiRJQ==</latexit> Low Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"wuZucU3JbeEJSquG2WgqGdYMCR8=\">AAAB83icbVDLSgMxFL3js9ZX1aWbYBFclZnia1kQocsK9gHtUDJppg3NJCHJCGXob7hxoYhbf8adf2PazkJbD1w4nHMv994TKc6M9f1vb219Y3Nru7BT3N3bPzgsHR23jEw1oU0iudSdCBvKmaBNyyynHaUpTiJO29H4bua3n6g2TIpHO1E0TPBQsJgRbJ3Uq7PhCN0Lq6Wa9Etlv+LPgVZJkJMy5Gj0S1+9gSRpQoUlHBvTDXxlwwxrywin02IvNVRhMsZD2nVU4ISaMJvfPEXnThmgWGpXwqK5+nsiw4kxkyRynQm2I7PszcT/vG5q49swY0KllgqyWBSnHFmJZgGgAdOUWD5xBBPN3K2IjLDGxLqYii6EYPnlVdKqVoLrytVDtVy7zOMowCmcwQUEcAM1qEMDmkBAwTO8wpuXei/eu/exaF3z8pkT+APv8wfIYpF9</latexit> High Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"1BO6D/gzkeZNQ7HNIaph5NqELCI=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPF17LgRncV7AOmQ8mkd9rQTDIkGaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3kHtPmHCmjet+O6W19Y3NrfJ2ZWd3b/+genjU0TJVFNpUcql6IdHAmYC2YYZDL1FA4pBDN5zc5n73CZRmUjyaaQJBTEaCRYwSYyX/XlAFMQhD+KBac+vuHHiVeAWpoQKtQfWrP5Q0zdOUE619z01MkBFlGOUwq/RTDQmhEzIC31JBYtBBNl95hs+sMsSRVPYJg+fq70RGYq2ncWgnY2LGetnLxf88PzXRTZAxkaQGBF18FKUcG4nz+/GQKaCGTy0hVDG7K6Zjogg1tqWKLcFbPnmVdBp176p++dCoNS+KOsroBJ2ic+Sha9REd6iF2ogiiZ7RK3pzjPPivDsfi9GSU2SO0R84nz9y2ZFU</latexit> Incremental <latexit sha1_base64=\"Jmobmj50NeE6y3ftB4xt5xZD5Eg=\">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKewGX8dALh4jmAcmS5id9CZDZmeXmVkhLP6FFw+KePVvvPk3TpI9aGJBQ1HVTXdXkAiujet+O4W19Y3NreJ2aWd3b/+gfHjU1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfM7j6g0j+W9mSboR3QkecgZNVZ6aIhUG1Rcjgblilt15yCrxMtJBXI0B+Wv/jBmaYTSMEG17nluYvyMKsOZwKdSP9WYUDahI+xZKmmE2s/mFz+RM6sMSRgrW9KQufp7IqOR1tMosJ0RNWO97M3E/7xeasIbP+MySQ1KtlgUpoKYmMzeJ0OukBkxtYQyxe2thI2posymoEs2BG/55VXSrlW9q+rlXa1Sv8jjKMIJnMI5eHANdbiFJrSAgYRneIU3RzsvzrvzsWgtOPnMMfyB8/kDzgaQ+A==</latexit> Clustering <latexit sha1_base64=\"c4xrXg0yZYBSSDLHCxlf45OWNzg=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBA8hd2Aj2PAi8eI5gHJEmYnnWTIzOwyMyuEJR/hxYMiXv0eb/6Nk2QPmljQUFR1090VJYIb6/vf3tr6xubWdmGnuLu3f3BYOjpumjjVDBssFrFuR9Sg4AoblluB7UQjlZHAVjS+nfmtJ9SGx+rRThIMJR0qPuCMWie1HqhMBJpeqexX/DnIKglyUoYc9V7pq9uPWSpRWSaoMZ3AT2yYUW05EzgtdlODCWVjOsSOo4pKNGE2P3dKzp3SJ4NYu1KWzNXfExmVxkxk5DoltSOz7M3E/7xOagc3YcZVklpUbLFokApiYzL7nfS5RmbFxBHKNHe3EjaimjLrEiq6EILll1dJs1oJriqX99VyrZrHUYBTOIMLCOAaanAHdWgAgzE8wyu8eYn34r17H4vWNS+fOYE/8D5/AF7Wj40=</latexit> Samples <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"KzBZ8R84UC9mpPFQBWeRHFxcqjw=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKVI8FLx4rmLbQhrLZbNq1m92wuxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzwpQzbVz32yltbG5t75R3K3v7B4dH1eOTjpaZItQnkkvVC7GmnAnqG2Y47aWK4iTktBtObud+94kqzaR4MNOUBgkeCRYzgo2VOn4aYUOH1ZpbdxdA68QrSA0KtIfVr0EkSZZQYQjHWvc9NzVBjpVhhNNZZZBpmmIywSPat1TghOogX1w7QxdWiVAslS1h0EL9PZHjROtpEtrOBJuxXvXm4n9ePzPxTZAzkWaGCrJcFGccGYnmr6OIKUoMn1qCiWL2VkTGWGFibEAVG4K3+vI66TTqXrPevG/UWldFHGU4g3O4BA+uoQV30AYfCDzCM7zCmyOdF+fd+Vi2lpxi5hT+wPn8AYuwjxQ=</latexit> Update <latexit sha1_base64=\"y2NH6tDs2GygUDqZYglGwvR4SpA=\">AAAB+nicbVBNSwMxEJ2tX7V+bfXoJVgEQSi7PVSPFS8eK9oPaEvJptk2NMkuSVYpa3+KFw+KePWXePPfmLZ70NYHA4/3ZpiZF8ScaeN5305ubX1jcyu/XdjZ3ds/cIuHTR0litAGiXik2gHWlDNJG4YZTtuxolgEnLaC8fXMbz1QpVkk780kpj2Bh5KFjGBjpb5bvMMi5lSjc3QlyShSuu+WvLI3B1olfkZKkKHed7+6g4gkgkpDONa643ux6aVYGUY4nRa6iaYxJmM8pB1LJRZU99L56VN0apUBCiNlSxo0V39PpFhoPRGB7RTYjPSyNxP/8zqJCS97KZNxYqgki0VhwpGJ0CwHNGCKEsMnlmCimL0VkRFWmBibVsGG4C+/vEqalbJfLVdvK6Wal8WRh2M4gTPw4QJqcAN1aACBR3iGV3hznpwX5935WLTmnGzmCP7A+fwBUnKTWg==</latexit> Samples + Anchors <latexit sha1_base64=\"u0BDOcH87PXd3DsT+o414+7cHnI=\">AAAB7XicbZC7SgNBFIbPxluMt6ilIINBsAq7FjGdARvLBMwFkhBmZ2eTMbMzy8ysEJaU9jYWitj6Cql8CDufwZdwcik0+sPAx/+fw5xz/JgzbVz308msrK6tb2Q3c1vbO7t7+f2DhpaJIrROJJeq5WNNORO0bpjhtBUriiOf06Y/vJrmzTuqNJPixoxi2o1wX7CQEWys1eiQQBrdyxfcojsT+gveAgqX75Pa1/3xpNrLf3QCSZKICkM41rrtubHpplgZRjgd5zqJpjEmQ9ynbYsCR1R309m0Y3RqnQCFUtknDJq5PztSHGk9inxbGWEz0MvZ1PwvaycmLHdTJuLEUEHmH4UJR0ai6eooYIoSw0cWMFHMzorIACtMjD1Qzh7BW175LzTOi16pWKq5hUoZ5srCEZzAGXhwARW4hirUgcAtPMATPDvSeXRenNd5acZZ9BzCLzlv33Yvk3g=</latexit> ··· <latexit sha1_base64=\"+7L/8ObZcl+JIZaSFhVO3t+lUUE=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2bvizjWb9YcituBrJMvDkp1Y6CDPV+8as7iFgScoVMUmM6nhtjL6UaBZN8UugmhseUPdIh71iqaMhNL82unZBTqwxIEGlbCkmm/p5IaWjMOPRtZ0hxZBa9qfif10kwuOqlQsUJcsVmi4JEEozI9HUyEJozlGNLKNPC3krYiGrK0AZUsCF4iy8vk+Z5xatWqnc2jQuYIQ/HcAJl8OASanALdWgAgwd4hld4cyLnxXl3PmatOWc+cwh/4Hz+AFjYkTs=</latexit> D l ( t ) <latexit sha1_base64=\"9C0bB8PYImk9DX0HLfGvGd44PFA=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2b/qiMZ/1iya24Gcgy8eakVDsKMtT7xa/uIGJJyBUySY3peG6MvZRqFEzySaGbGB5T9kiHvGOpoiE3vTS7dkJOrTIgQaRtKSSZ+nsipaEx49C3nSHFkVn0puJ/XifB4KqXChUnyBWbLQoSSTAi09fJQGjOUI4toUwLeythI6opQxtQwYbgLb68TJrnFa9aqd7ZNC5ghjwcwwmUwYNLqMEt1KEBDB7gGV7hzYmcF+fd+Zi15pz5zCH8gfP5A1K8kTc=</latexit> D h ( t ) <latexit sha1_base64=\"eNrtnhPGeU8n4BRDMStm5cjQ4ts=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDbbTbt0s4m7E6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSKQw6Lrfzsbm1vbObmGvuH9weHRcOjltmzjVjLdYLGPdDajhUijeQoGSdxPNaRRI3gkmjbnfeeLaiFg94DThfkRHSoSCUbRS1zNIGlTKQansVtwFyDrxclKGHM1B6as/jFkacYVMUmN6npugn1GNgkk+K/ZTwxPKJnTEe5YqGnHjZ4t7Z+TSKkMSxtqWQrJQf09kNDJmGgW2M6I4NqveXPzP66UY3vqZUEmKXLHlojCVBGMyf54MheYM5dQSyrSwtxI2ppoytBEVbQje6svrpF2teLVK7b5arl/ncRTgHC7gCjy4gTrcQRNawEDCM7zCm/PovDjvzseydcPJZ87gD5zPH1Naj3k=</latexit> 1st Call <latexit sha1_base64=\"mxsL+XuWb2hqFND+pzTctrB1rcY=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDababt0s4m7G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfc7T6g0j+WDmSboR3Qk+ZAzaqzUrcqQNKgQg1LZrbgLkHXi5aQMOZqD0lc/jFkaoTRMUK17npsYP6PKcCZwVuynGhPKJnSEPUsljVD72eLeGbm0SkiGsbIlDVmovycyGmk9jQLbGVEz1qveXPzP66VmeOtnXCapQcmWi4apICYm8+dJyBUyI6aWUKa4vZWwMVWUGRtR0Ybgrb68TtrViler1O6r5fp1HkcBzuECrsCDG6jDHTShBQwEPMMrvDmPzovz7nwsWzecfOYM/sD5/AE0o49l</latexit> 2nd Call <latexit sha1_base64=\"oSA1OFmXXL9y3PJtqoVxTIG9mto=\">AAAB8HicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaElCY2UwkQ8DF7K3zMGGvb3L7p6REH6FjYXG2Ppz7Pw3LnCFgi+Z5OW9mczMCxLBtXHdbye3sbm1vZPfLeztHxweFY9PWjpOFcMmi0WsOgHVKLjEpuFGYCdRSKNAYDsY1+d++xGV5rG8N5ME/YgOJQ85o8ZKD7f4ZEidCtEvltyyuwBZJ15GSpCh0S9+9QYxSyOUhgmqdddzE+NPqTKcCZwVeqnGhLIxHWLXUkkj1P50cfCMXFhlQMJY2ZKGLNTfE1MaaT2JAtsZUTPSq95c/M/rpia89qdcJqlByZaLwlQQE5P592TAFTIjJpZQpri9lbARVZQZm1HBhuCtvrxOWpWyVy1X7yqlWiWLIw9ncA6X4MEV1OAGGtAEBhE8wyu8Ocp5cd6dj2VrzslmTuEPnM8fSFeQCA==</latexit> Next Call Figure 2: Overview of the SimATTA framework. and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (8) Corollary 4 validates that the selected low-entropy samples can mitigate the CF problem under the assumption that these samples are source-like, which is also empirically validated in Fig. 1. Note that our strategy employs entropy minimization in a selective manner, aiming to solve CF rather than the main adaptation issue. While many FTTA works use entropy minimization to adapt across domains without guarantees, our use is more theoretically-sound. 4 A N ATTA ALGORITHM Building on our theoretical findings, we introduce a simple yet effective ATTA method, known as SimATTA, that innovatively integrates incremental clustering and selective entropy minimization techniques, as illustrated in Fig. 2. We start with an overview of our methodology, including the learning framework and the comprehensive sample selection strategies. We then proceed to discuss the details of the incremental clustering technique designed for real-time sample selections. 4.1 A LGORITHM OVERVIEW Let (x, y) be a labeled sample and f(·; θ) be our neural network, where ˆy = f(x; θ) and θ represents the parameters. We have a model pre-trained on source domains with the pre-trained parameters ϕ. We initialize model parameters as θ(0) = ϕ and aim to adapt the model f(·; θ) in real-time. During the test phase, the model continuously predicts labels for streaming-in test data and concurrently gets fine-tuned. We perform sample selection to enable active learning. As discussed in Sec. 3.2, we empirically consider informative high-entropy samples for addressing distribution shifts and source-like low-entropy samples to mitigate CF. As shown in Alg. 1, at each time step t, we first partition unlabeled test samples Ute(t) into high entropy and low entropy datasets, Uh(t) and Ul(t), using an entropy threshold. The source-pretrained model f(·; ϕ) is frozen to predict pseudo labels for low entropy data. We obtain labeled low-entropy data Dl(t) by labeling Ul(t) with f(·; ϕ) and combining it with Dl(t − 1). In contrast, the selection of high-entropy samples for active labeling is less straightforward. Since the complete test dataset is inaccessible for analyzing the target domain distribution, real-time sample selection is required. We design an incremental clustering sample selection technique to reduce sample redundancy and increase distribution coverage, detailed in Sec. 4.2. The incremental clustering algorithm outputs the labeled test samples Dh(t), also referred to as anchors, given Dh(t −1) and Uh(t). After sample selection, the model undergoes test-time training using the labeled test anchors Dh(t) and pseudo-labeled source-like anchors Dl(t). Following the analyses in Sec. 3.1, the training weights and sample numbers should satisfy w(t) ≈ λ(t) for Dh(t) and Dl(t) for optimal results. The analyses and results in Sec. 3.2 further indicate that balancing the source and target ratio is the key to mitigating CF. However, when source-like samples significantly outnumber test samples, the optimal w(t) for test domains can deviate from λ(t) according to Eq. (4). 4.2 I NCREMENTAL CLUSTERING We propose incremental clustering, a novel continual clustering technique designed to select informa- tive samples in unsupervised settings under the ATTA framework. The primary goal of this strategy is to store representative samples for distributions seen so far. Intuitively, we apply clusters to cover all seen distributions while adding new clusters to cover newly seen distributions. During this process with new clusters added, old clusters may be merged due to the limit of the cluster budget. Since 6Published as a conference paper at ICLR 2024 Algorithm 1 SIMATTA: A SIMPLE ATTA ALGORITHM Require: A fixed source pre-trained model f(·; ϕ) and a real-time adapting model f(·; θ(t)) with θ(0) = ϕ. Streaming test data Ute(t) at time step t. Entropy of predictions H(ˆy) = −P c p(ˆyc) logp(ˆyc). Low entropy and high entropy thresholds el and eh. The number of cluster centroid budget NC (t) at time step t. Centroid increase number k. Learning step size η. 1: for t = 1, . . . , Tdo 2: Model inference on Ute(t) using f(·; θ(t − 1)). 3: Dl(t) ← Dl(t − 1) ∪ {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el} 4: Uh(t) ← {x|x ∈ Ute(t), H(f(x; θ)) > eh} 5: Dh(t) ← Dh(t − 1) ∪ {(x, y)|∀x ∈ IC(Dh(t − 1), Uh(t), NC(t)), y= Oracle(x)} 6: λ(t) ← |Dl(t)|/(|Dl(t)| + |Dh(t)|), |Dh(t)|/(|Dl(t)| + |Dh(t)|) 7: w(t) ← GetW(λ(t)) ▷ Generally, GetW(λ(t)) = λ(t) is a fair choice. 8: θ(t) ← θ(t − 1) 9: for (xl, yl) in Dl and (xh, yh) in Dh do 10: θ(t) ← θ(t) − ηw0∇ℓCE (f(xl; θ(t)), yl) − η(1 − w0)∇ℓCE (f(xh; θ(t)), yh) 11: end for 12: NC (t + 1) ← UpdateCentroidNum(NC (t)) ▷ Naive choice: NC (t + 1) ← NC (t) + k. 13: end for clusters cannot be stored efficiently, we store the representative samples of clusters, named anchors, instead. In this work, we adopt weighted K-means (Krishna and Murty, 1999) as our base clustering method due to its popularity and suitability for new setting explorations. When we apply clustering with new samples, a previously selected anchor should not weigh the same as new samples since the anchor is a representation of a cluster,i.e., a representation of many samples. Instead, the anchor should be considered as a barycenter with a weight of the sum of its cluster’s sample weights. For a newly added cluster, its new anchor has the weight of the whole cluster. For clusters containing multiple old anchors, i.e., old clusters, the increased weights are distributed equally among these anchors. These increased weights are contributed by new samples that are close to these old anchors. Intuitively, this process of clustering is analogous to the process of planet formation. Where there are no planets, new planets (anchors) will be formed by the aggregation of the surrounding material (samples). Where there are planets, the matter is absorbed by the surrounding planets. This example is only for better understanding without specific technical meanings. Specifically, we provide the detailed Alg. 2 for incremental clustering. In each iteration, we apply weighted K-Means for previously selected anchors Danc and the new streaming-in unlabeled data Unew. We first extract all sample features using the model from the previous step f(·; θ(t − 1)), and then cluster these weighted features. The initial weights of the new unlabeled samples are 1, while anchors inherit weights from previous iterations. After clustering, clusters including old anchors are old clusters, while clusters only containing new samples are newly formed ones. For each new cluster, we select the centroid-closest sample as the new anchor to store. As shown in line 10 of Alg. 2, for both old and new clusters, we distribute the sample weights in this cluster as its anchors’ weights. With incremental clustering, although we can control the number of clusters in each iteration, we cannot control the number of new clusters/new anchors. This indirect control makes the increase of new anchors adaptive to the change of distributions, but it also leads to indirect budget control. Therefore, in experimental studies, we set the budget limit, but the actual anchor budget will not reach this limit. The overall extra storage requirement is O(B) since the number of saved unlabeled samples is proportional to the number of saved labeled samples (anchors). 5 E XPERIMENTAL STUDIES In this study, we aim to validate the effectiveness of our proposed method, as well as explore the various facets of the ATTA setting. Specifically, we design experiments around the following research questions: RQ1: Can TTA methods address domain distribution shifts? RQ2: Is ATTA as efficient as TTA? RQ3: How do the components of SimATTA perform? RQ4: Can ATTA perform on par with stronger Active Domain Adaptation (ADA) methods? We compare ATTA with three settings, TTA (Tab. 2), enhanced TTA (Tab. 3 and 5), and ADA (Tab. 4). Datasets. To assess the OOD performance of the TTA methods, we benchmark them using datasets from DomainBed (Gulrajani and Lopez-Paz, 2020) and Hendrycks and Dietterich (2019a). We employ PACS (Li et al., 2017), VLCS (Fang et al., 2013), Office-Home (Venkateswara et al., 2017), and Tiny-ImageNet-C datasets for our evaluations. For each dataset, we designate one domain as 7Published as a conference paper at ICLR 2024 Table 2: TTA comparisons on PACS and VLCS.This table includes the two data stream mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. PACS Domain-wise data stream Post-adaptation Random data stream Post-adaptation P →A→ →C→ →S P A C S →1→ →2→ →3→ →4 P A C S BN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 Tent (steps=1) N/A 67.29 64.59 44.67 97.60 66.85 64.08 42.58 56.35 54.09 51.83 48.58 97.19 63.53 60.75 41.56Tent (steps=10) N/A 67.38 57.85 20.23 62.63 34.52 40.57 13.59 47.36 31.01 22.84 20.33 50.78 23.68 20.95 19.62EATA N/A 67.04 64.72 50.27 98.62 66.50 62.46 48.18 57.31 56.06 58.17 59.78 98.62 69.63 65.70 54.26CoTTA N/A 65.48 62.12 53.17 98.62 65.48 63.10 53.78 56.06 54.33 57.16 57.42 98.62 65.97 62.97 54.62SAR (steps=1) N/A 66.75 63.82 49.58 98.32 66.94 62.93 45.74 56.78 56.35 56.68 56.70 98.44 68.16 64.38 52.53SAR (steps=10) N/A 69.38 68.26 49.02 96.47 62.16 56.19 54.62 53.51 51.15 51.78 45.60 94.13 56.64 56.02 36.37 SimATTA (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00SimATTA (B ≤500) N/A 77.93 76.02 76.30 98.62 88.33 83.49 83.74 68.46 78.22 80.91 85.49 99.16 86.67 84.77 87.71 VLCS Domain-wise data stream Post-adaptation Random data stream Post-adaptation C →L→ →S→ →V C L S V →1→ →2→ →3→ →4 C L S V BN w/o adapt 100.00 33.55 41.10 49.05 100.00 33.55 41.10 49.05 41.23 41.23 41.23 41.23 100.00 33.55 41.10 49.05BN w/ adapt 85.16 37.31 33.27 52.16 85.16 37.31 33.27 52.16 40.91 40.91 40.91 40.91 85.16 37.31 33.27 52.16 Tent (steps=1) N/A 38.55 34.40 53.88 84.73 43.86 33.61 53.11 44.85 44.29 47.38 44.98 85.30 43.49 37.81 53.35Tent (steps=10) N/A 45.41 31.44 32.32 42.54 37.65 27.79 33.12 46.13 42.31 43.51 39.48 52.01 40.32 33.64 40.37EATA N/A 37.24 33.15 52.58 84.10 37.69 32.39 52.49 43.77 42.48 43.34 41.55 83.32 36.67 31.47 52.55CoTTA N/A 37.39 32.54 52.25 82.12 37.65 33.12 52.90 43.69 42.14 43.21 42.32 81.98 37.99 33.52 53.23SAR (steps=1) N/A 36.18 34.43 52.46 83.96 39.72 36.53 52.37 43.64 43.04 44.20 41.93 85.09 40.70 36.44 53.02SAR (steps=10) N/A 35.32 34.10 51.66 82.12 41.49 33.94 53.08 43.56 42.05 42.53 41.16 85.09 37.58 33.12 52.01 SimATTA (B ≤300) N/A 62.61 65.08 74.38 99.93 69.50 66.67 77.34 62.33 69.33 73.20 71.93 99.93 69.43 72.46 80.39SimATTA (B ≤500) N/A 63.52 68.01 76.13 99.51 70.56 73.10 78.35 62.29 70.45 73.50 72.02 99.43 70.29 72.55 80.18 the source domain and arrange the samples from the other domains to form the test data stream. For DomainBed datasets, we adopt two stream order strategies. The first order uses a domain-wise data stream, i.e., we finish streaming samples from one domain before starting streaming another domain. The second order is random, where we shuffle samples from all target domains and partition them into four splits 1, 2, 3, and 4, as shown in Tab. 2. More dataset details are provided in Appx. G.1. Baselines. For baseline models, we start with the common source-only models, which either utilize pre-calculated batch statistics (BN w/o adapt) or test batch statistics (BN w/ adapt). For comparison with other TTA methods, we consider four state-of-the-art TTA methods: Tent (Wang et al., 2021), EATA (Niu et al., 2022), CoTTA (Wang et al., 2022a), and SAR (Niu et al., 2023). The three of them except Tent provide extra design to avoid CF. To compare with ADA methods, we select algorithms that are partially comparable with our method, i.e., they should be efficient (e.g., uncertainty-based) without the requirements of additional networks. Therefore, we adopt random, entropy (Wang and Shang, 2014), k-means (Krishna and Murty, 1999), and CLUE (Prabhu et al., 2021) for comparisons. Settings. For TTA, we compare with general TTA baselines in streaming adaptation using the two aforementioned data streaming orders, domain-wise and random. We choose P in PACS and C in VLCS as source domains. For domain-wise data stream, we use order A → C → S for PACS and L → S → V for VLCS. We report the real-time adaptation accuracy results for each split of the data stream, as well as the accuracy on each domain after all adaptations through the data stream (under “post-adaptation” columns). Enhanced TTA is built on TTA with access to extra random sample labels. TTA baselines are further fine-tuned with these random samples. To further improve enhanced TTA, we use long-term label storage and larger unlabeled sample pools. To its extreme where the model can access the whole test set samples, the setting becomes similar to ADA, thus we also use ADA methods for comparisons. ADA baselines have access to all samples in the pre-collected target datasets but not source domain data, whereas our method can only access the streaming test data. 5.1 T HE FAILURE OF TEST-TIME ADAPTATION The failure of TTA methods on domain distribution shifts is one of the main motivations of the ATTA setting. As shown in Tab. 2, TTA methods cannot consistently outperform eventhe simplest baseline \"BN w/ adapt\" which uses test time batch statistics to make predictions, evidencing that current TTA methods cannot solve domain distribution shifts (RQ1). Additionally, Tent (step=10) exhibits significant CF issues, where \"step=10\" indicates 10 test-time training updates, i.e., 10 gradient backpropagation iterations. This failure of TTA methods necessitates the position of ATTA. In contrast, SimATTA, with a budget B less than 300, outperforms all TTA methods on both source and target domains by substantial margins. Moreover, compared to the source-only baselines, our method improves the target domain performances significantly with negligible source performance loss, showing that ATTA is a more practically effective setting for real-world distribution shifts. 5.2 E FFICIENCY & ENHANCED TTA SETTING COMPARISONS To validate the efficiency of ATTA and broaden the dataset choice, we conduct this study on Tiny- ImageNet-C which, though does not focus on domain shifts, is much larger than PACS and VLCS. we 8Published as a conference paper at ICLR 2024 Table 3: Comparisons with Enhanced TTA on Tiny-ImageNet-C (severity level 5). Tiny-ImageNet-C Time (sec)Noise Blur Weather Digital Gauss. Shot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Contr. Elastic Pixel JPEG Avg. Tent (step=1) 68.83 9.32 11.97 8.86 10.43 7.00 12.20 14.34 13.58 15.46 13.55 3.99 13.31 17.79 18.61 12.17Tent (step=10) 426.90 0.86 0.63 0.52 0.52 0.55 0.54 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.54EATA 93.14 3.98 3.33 2.18 4.80 2.37 11.02 11.41 14.06 15.26 9.65 1.36 9.88 14.24 12.12 8.26CoTTA 538.78 5.63 7.12 6.31 8.05 5.74 9.68 10.55 11.75 12.00 11.15 4.17 5.35 7.82 8.90 8.16SAR (step=1) 113.76 8.90 3.11 1.67 1.55 1.47 1.35 1.19 1.03 1.04 0.93 0.83 1.00 0.74 0.77 1.83SAR (step=10) 774.11 2.67 3.26 2.38 1.64 1.85 2.49 3.16 3.81 2.72 3.12 0.81 3.47 4.04 1.76 2.66 SimATTA (step=10) 736.289.68 19.40 12.14 30.28 17.03 42.36 43.10 31.96 40.08 29.243.21 34.56 45.24 45.74 28.86 enhance the TTA setting by fine-tuning baselines on randomly selected labeled samples. Specifically, the classifier of ResNet18-BN is pre-adapted to the brightness corruption (source domain) before test-time adapting. SimATTA’s label budget is around 4,000, while all other TTA methods have budget 4,500 for randomly selected labeled samples. The data stream order is shown in Tab. 3. Time is measured across all corrupted images in the Noise and Blur noise types, and the values represent the average time cost for adapting 10,000 images. The results clearly evidence the efficiency of ATTA (RQ2), while substantially outperforming all enhanced TTA baselines. Simply accessing labeled samples cannot benefit TTA methods to match ATTA. With 10 training updates (step=10) for each batch, FTTA methods would suffer from severe CF problem. In contrast, ATTA covers a statistically significant distribution, achieving stronger performances with 10 training updates or even more steps till approximate convergences. In fact, longer training on Tent (step=10) leads to worse results (compared to step=1), which further motivates the design of the ATTA setting. The reason for higher absolute time cost in Tab. 3 is due to differences in training steps. In this experiment, SimATTA has a training step of 10, and similar time cost as SAR per step. Note that if the enhanced TTA setting is further improved to maintain distributions with a balanced CF mitigation strategy and an incremental clustering design, the design approaches ATTA. Specifically, we compare SimATTA with its variants as the ablation study (RQ3) in Appx. I.2. 5.3 C OMPARISONS TO A STRONGER SETTING : ACTIVE DOMAIN ADAPTATION Table 4: Comparisons to ADA baselines. Source domains are denoted as \"(S)\". Results are average accuracies (with standard deviations). PACS P (S) A C S Random (B= 300) 96.21 (0.80) 81.19 (0.48) 80.75 (1.27) 84.34 (0.18)Entropy (B= 300) 96.31 (0.64)88.00 (1.46)82.48 (1.71) 80.55 (1.01)Kmeans (B= 300) 93.71 (1.50) 79.31 (4.01) 79.64 (1.44) 83.92 (0.65)CLUE (B= 300) 96.69 (0.17)83.97 (0.57)84.77 (0.88) 86.91 (0.26) SimATTA (B ≤300) 98.89 (0.09)84.69 (0.22)83.09 (0.83)83.76 (2.24) VLCS C (S) L S V Random (B= 300) 96.21 (1.65) 66.67 (1.70) 70.72 (0.30) 72.14 (1.71)Entropy (B= 300) 97.74 (1.56) 69.29 (2.26)69.25 (4.77) 75.26 (3.07)Kmeans (B= 300) 98.61 (0.27)67.57 (1.64)70.77 (0.01)74.49 (0.97)CLUE (B= 300) 85.70 (10.09) 65.29 (1.49) 69.42 (2.64) 69.09 (6.05) SimATTA (B ≤300) 99.93 (0.00) 69.47 (0.03)69.57 (2.90)78.87 (1.53) In addtion to the above comparisons with (en- hanced) TTA, which necessitate the requirement of extra information in the ATTA setting, we com- pare ATTA with a stronger setting Active Domain Adaptation (ADA) to demonstrate another supe- riority of ATTA, i.e., weaker requirements for comparable performances (RQ4). ADA baselines are able to choose the global best active samples, while ATTA has to choose samples from a small sample buffer (e.g., a size of 100) and discard the rest. Tab. 4 presents the post-adaptation model per- formance results. All ADA results are averaged from 3 random runs, while ATTA results are the post-adaptation performances averaged from the two data stream orders. As can be observed, despite the lack of a pre-collected target dataset, SimATTA produces better or competitive results against ADA methods. Moreover, without source data access, SimATTA’s design for CF allows it to maintain superior source domain performances over ADA methods. Further experimental studies including the Office-Home dataset are provided in Appx. I. In conclusion, the significant improvement compared to weaker settings (TTA, enhanced TTA) and the comparable performance with the stronger setting, ADA, rendering ATTA a setting that is as efficient as TTA and as effective as ADA. This implies its potential is worthy of future explorations. 6 C ONCLUSION AND DISCUSSION There’s no denying that OOD generalization can be extremely challenging without certain information, often relying on various assumptions easily compromised by different circumstances. Thus, it’s prudent to seek methods to achieve significant improvements with minimal cost, e.g., DG methods leveraging environment partitions and ATTA methods using budgeted annotations. As justified in our theoretical and experimental studies, ATTA stands as a robust approach to achieve real-time OOD generalization. Although SimATTA sets a strong baseline for ATTA, there’s considerable scope for further investigation within the ATTA setting. One potential direction involves developing alternatives to prevent CF in ATTA scenarios. While selective entropy minimization on low-entropy samples has prove to be empirically effective, it relies on the quality of the pre-trained model and training on incorrectly predicted low-entropy samples may reinforce the errors. It might not be cost-effective to expend annotation budgets on low-entropy samples, but correcting them could be a viable alternative solution. We anticipate that our work will spur numerous further explorations in this field. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGMENTS This work was supported in part by National Science Foundation grant IIS-2006861 and National Institutes of Health grant U01AG070112. REFERENCES Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marchand. Domain- adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014. Lucas Baier, Tim Schlör, Jakob Schöffer, and Niklas Kühl. Detecting concept drift with neural network model uncertainty. In Hawaii International Conference on System Sciences, 2021. URL https://api.semanticscholar.org/CorpusID:235731947. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79:151–175, 2010. Davide Cacciarelli and Murat Kulahci. A survey on online active learning, 2023. Cheng Chen, Quande Liu, Yueming Jin, Qi Dou, and Pheng-Ann Heng. Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pages 225–235. Springer, 2021. Li Chen, Tutian Tang, Zhitian Cai, Yang Li, Penghao Wu, Hongyang Li, Jianping Shi, Junchi Yan, and Yu Qiao. Level 2 autonomous driving on a single device: Diving into the devils of openpilot. arXiv preprint arXiv:2206.08176, 2022a. Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu, and Yueting Zhuang. Self-supervised noisy label learning for source-free unsupervised domain adaptation. In 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS) , pages 10185–10192. IEEE, 2022b. Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. Advances in Neural Information Processing Systems, 33:21061–21071, 2020. David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129–145, 1996. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. Yuhe Ding, Lijun Sheng, Jian Liang, Aihua Zheng, and Ran He. Proxymix: Proxy-based mixup training with label refinery for source-free domain adaptation. arXiv preprint arXiv:2205.14566, 2022. Cian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Schölkopf. Source-free adaptation to measurement shift via bottom-up feature restoration. arXiv preprint arXiv:2107.05446, 2021. Jiahao Fan, Hangyu Zhu, Xinyu Jiang, Long Meng, Chen Chen, Cong Fu, Huan Yu, Chenyun Dai, and Wei Chen. Unsupervised domain adaptation by statistics alignment for deep sleep staging networks. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30:205–216, 2022. Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE International Conference on Computer Vision, pages 1657–1664, 2013. Yuqi Fang, Pew-Thian Yap, Weili Lin, Hongtu Zhu, and Mingxia Liu. Source-free unsupervised domain adaptation: A survey. arXiv preprint arXiv:2301.00265, 2022. Francois Fleuret et al. Uncertainty reduction for model adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9613–9623, 2021. 10Published as a conference paper at ICLR 2024 Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180–1189. PMLR, 2015. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342, 2021. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. Shurui Gui, Chaoyue Wang, Qihua Chen, and Dacheng Tao. Featureflow: Robust video interpolation via structure-to-texture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14004–14013, 2020. Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=8hHg-zs_p-h. Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. March 2019a. doi: 10.48550/ARXIV .1903.12261. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019b. Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Semisupervised svm batch mode active learning with applications to image retrieval. ACM Transactions on Information Systems (TOIS), 27(3):1–29, 2009. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853–17862, 2023. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. Advances in Neural Information Processing Systems, 34:3635–3649, 2021. Masato Ishii and Masashi Sugiyama. Source-free domain adaptation via distributional alignment by matching batch normalization statistics. arXiv preprint arXiv:2101.10842, 2021. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021. Suyog Dutt Jain and Kristen Grauman. Active image segmentation propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2864–2873, 2016. Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4893–4902, 2019. Ashish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. Active learning with gaussian processes for object categorization. In 2007 IEEE 11th international conference on computer vision, pages 1–8. IEEE, 2007. Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu. Test-time adaptable neural networks for robust medical image segmentation. Medical Image Analysis, 68:101907, 2021. 11Published as a conference paper at ICLR 2024 Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In VLDB, volume 4, pages 180–191. Toronto, Canada, 2004. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521–3526, 2017. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal- subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637–5664. PMLR, 2021. Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, and Dinesh Manocha. Salad: Source-free active label-agnostic domain adaptation for classification, segmentation and detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 382–391, 2023. K Krishna and M Narasimha Murty. Genetic k-means algorithm. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 29(3):433–439, 1999. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu- tional neural networks. Communications of the ACM, 60(6):84–90, 2017. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrap- olation (REx). In International Conference on Machine Learning , pages 5815–5826. PMLR, 2021. Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 615–625, 2021. David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148–156. Elsevier, 1994. Aodong Li, Alex Boyd, Padhraic Smyth, and Stephan Mandt. Detecting and adapting to irregular distribution shifts in bayesian online learning. Advances in neural information processing systems, 34:6816–6828, 2021a. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542–5550, 2017. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9641–9650, 2020. Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsupervised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8474–8481, 2021b. Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Jian Liang, Dapeng Hu, Ran He, and Jiashi Feng. Distill and fine-tune: Effective adaptation from a black-box source model. arXiv preprint arXiv:2104.01539, 1(3), 2021. Jian Liang, Dapeng Hu, Jiashi Feng, and Ran He. Dine: Domain adaptation from single and multiple black-box predictors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8003–8013, 2022. 12Published as a conference paper at ICLR 2024 Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without environment partition? Advances in Neural Information Processing Systems, 35:24529–24542, 2022. Xiaofeng Liu, Fangxu Xing, Chao Yang, Georges El Fakhri, and Jonghye Woo. Adapting off-the- shelf source segmenter for target medical image segmentation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24, pages 549–559. Springer, 2021a. Xinyu Liu and Yixuan Yuan. A source-free domain adaptive polyp detection framework with style diversification flow. IEEE Transactions on Medical Imaging, 41(7):1897–1908, 2022. Yuang Liu, Wei Zhang, Jun Wang, and Jianyong Wang. Data-free knowledge transfer: A survey. arXiv preprint arXiv:2112.15278, 2021b. Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021c. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97–105. PMLR, 2015. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017. Chaochao Lu, Yuhuai Wu, José Miguel Hernández-Lobato, and Bernhard Schölkopf. Invariant causal representation learning for out-of-distribution generalization. In International Conference on Learning Representations, 2021. Xinhong Ma, Junyu Gao, and Changsheng Xu. Active universal domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8968–8977, 2021. Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, and Dongmei Zhang. Source free unsupervised graph domain adaptation. arXiv preprint arXiv:2112.00955, 2021. Christoforos Mavrogiannis, Francesca Baldini, Allan Wang, Dapeng Zhao, Pete Trautman, Aaron Steinfeld, and Jean Oh. Core challenges of social robot navigation: A survey. ACM Transactions on Human-Robot Interaction, 12(3):1–39, 2023. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Munan Ning, Donghuan Lu, Dong Wei, Cheng Bian, Chenglang Yuan, Shuang Yu, Kai Ma, and Yefeng Zheng. Multi-anchor active domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9112–9122, 2021. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In International conference on machine learning, pages 16888–16905. PMLR, 2022. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. InThe Eleventh International Con- ference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE transactions on neural networks, 22(2):199–210, 2010. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 13Published as a conference paper at ICLR 2024 Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53–69, 2015. Judea Pearl. Causality. Cambridge university press, 2009. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947–1012, 2016. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017. Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, and Judy Hoffman. Active domain adaptation via clustering uncertainty-weighted embeddings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8505–8514, 2021. Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. arXiv preprint arXiv:2010.05761, 2020. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal- ization help optimization? Advances in neural information processing systems, 31, 2018. Akanksha Saran, Safoora Yousefi, Akshay Krishnamurthy, John Langford, and Jordan T. Ash. Streaming active learning with deep neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 30005–30021. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr. press/v202/saran23a.html. Harald Schafer, Eder Santana, Andrew Haden, and Riccardo Biasini. A commute in data: The comma2k19 dataset, 2018. Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for informa- tion extraction. In Advances in Intelligent Data Analysis: 4th International Conference, IDA 2001 Cascais, Portugal, September 13–15, 2001 Proceedings 4, pages 309–318. Springer, 2001. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. Burr Settles. Active learning literature survey. 2009. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji, and Manmohan Chandraker. Active adversarial domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 739–748, 2020. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pages 443–450. Springer, 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229–9248. PMLR, 2020. 14Published as a conference paper at ICLR 2024 Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7472–7481, 2018. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pages 4068–4076, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167–7176, 2017. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018–5027, 2017. Sudheendra Vijayanarasimhan and Ashish Kapoor. Visual recognition and detection under bounded computational resources. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1006–1013. IEEE, 2010. Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint conference on neural networks (IJCNN), pages 112–119. IEEE, 2014. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test- time adaptation by entropy minimization. InInternational Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312: 135–153, 2018. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022a. Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. Cross-domain contrastive learning for unsupervised domain adaptation. IEEE Transactions on Multimedia , 2022b. Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 11(5):1–46, 2020. Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, Xinjing Cheng, and Guoren Wang. Active learning for domain adaptation: An energy-based approach. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8708–8716, 2022. Zhao Xu, Kai Yu, V olker Tresp, Xiaowei Xu, and Jizhi Wang. Representative sampling for text classification using support vector machines. In Advances in Information Retrieval: 25th European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14–16, 2003. Proceedings 25, pages 393–407. Springer, 2003. Baoyao Yang, Hao-Wei Yeh, Tatsuya Harada, and Pong C Yuen. Model-induced generalization error bound for information-theoretic representation learning in source-data-free unsupervised domain adaptation. IEEE Transactions on Image Processing, 31:419–432, 2021a. Guanglei Yang, Hao Tang, Zhun Zhong, Mingli Ding, Ling Shao, Nicu Sebe, and Elisa Ricci. Transformer-based source-free domain adaptation. arXiv preprint arXiv:2105.14138, 2021b. Jianfei Yang, Xiangyu Peng, Kai Wang, Zheng Zhu, Jiashi Feng, Lihua Xie, and Yang You. Divide to adapt: Mitigating confirmation bias for domain adaptation of black-box predictors. arXiv preprint arXiv:2205.14467, 2022. H Yao, Yuhong Guo, and Chunsheng Yang. Source-free unsupervised domain adaptation with surrogate data generation. In Proceedings of NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021. 15Published as a conference paper at ICLR 2024 Hao-Wei Yeh, Baoyao Yang, Pong C Yuen, and Tatsuya Harada. Sofa: Source-data-free feature alignment for unsupervised domain adaptation. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 474–483, 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Hu Yu, Jie Huang, Yajing Liu, Qi Zhu, Man Zhou, and Feng Zhao. Source-free domain adaptation for real-world image dehazing. In Proceedings of the 30th ACM International Conference on Multimedia, pages 6645–6654, 2022. Haojian Zhang, Yabin Zhang, Kui Jia, and Lei Zhang. Unsupervised domain adaptation of black-box source models. arXiv preprint arXiv:2101.02839, 2021. Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems, 35:38629–38642, 2022a. Yifan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Adanpc: Exploring non-parametric classifier for test-time adaptation. In International Conference on Machine Learning, pages 41647–41676. PMLR, 2023. Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2339–2348, 2022b. Bowen Zhao, Chen Chen, and Shu-Tao Xia. Delta: degradation-free fully test-time adaptation. arXiv preprint arXiv:2301.13018, 2023a. Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In International Conference on Machine Learning (ICML), 2023b. Chunting Zhou, Xuezhe Ma, Paul Michel, and Graham Neubig. Examining and combating spurious features under distribution shift. In International Conference on Machine Learning, pages 12857– 12867. PMLR, 2021. 16Published as a conference paper at ICLR 2024 Active Test-Time Adaptation: Foundational Analyses and An Algorithm Supplementary Material A B ROADER IMPACTS The field of domain generalization primarily concentrates on enhancing a model’s generalization abilities by preparing it thoroughly before deployment. However, it is equally important for deep learning applications to have the capacity for real-time adaptation, as no amount of preparation can account for all possible scenarios. Consequently, domain generalization and test-time adaptation are complementary strategies: the former is more weighty and extensive, while the latter is more agile, lightweight and privacy-friendly. This work delves into the development of a real-time model adaptation strategy that can be applied to any pre-trained models, including large language models, to enhance their adaptive capabilities. Our research does not involve any human subjects or dataset releases, nor does it raise any ethical concerns. Since this work does not directly tie to specific applications, we do not foresee any immediate negative societal impacts. Nonetheless, we acknowledge that any technological advancement may carry potential risks, and we encourage the continued assessment of the broader impacts of real-time adaptation methodologies in various contexts. B FAQ & D ISCUSSIONS To facilitate the reviewing process, we summarize the answers to the questions that arose during the discussion of an earlier version of this paper. The major updates of this version are reorganized theoretical studies, incremental clustering details, experimental reorganization, and additional datasets and settings . We include more related field comparisons to distinguish different settings. We also cover the position of this paper in literature and the main claims of this paper. Finally, we will frankly acknowledge the limitations of this paper, explain and justify the scope of coverage, and provide possible future directions. Q1: What is the relationship between the proposed ATTA protocol and stream based active learning (Saran et al., 2023)? A: We would like to discuss the difference between our work and the referenced work. 1. Real-time Training Distinction: Saran et al. (2023) doesn’t operate in real-time capacity. This is evident from their experiments, where their model is trained only after completing a round. In contrast, our work involves training the model post each batch. This positions Saran et al. (2023)’s work as an intrinsic active learning technique, while our approach leans towards TTA methods. 2. Continual Training Nuance: Following the point above, Saran et al. (2023) stands out of the scope of continual training. As they mentioned ‘each time new data are acquired, the ResNet is reset to the ImageNet pre-trained weights before being updated‘, Saran et al. (2023) starts afresh with each iteration and is out of scope for CF discussions. Contrarily, our model is continuously trained on varying distributions, compelling us to address the CF issue while preserving advantages derived from various stored distributions. 3. Comparative Complexity: Given the aforementioned distinctions, it’s evident that our task presents a greater challenge compared to theirs. In addition, we have included comparisons with stronger active learning settings in Sec. 5.3. Q2: What are the insights from the theoretically foundational analysis? A: 1. It sets a well-defined formulation and grounded theoretical framework for the ATTA setting. 2. While entropy minimizations can cause CF, balancing the learning rate and number of high/low entropy samples is conversely the key solution to both distribution shifts and 17Published as a conference paper at ICLR 2024 CF by corresponding benefits. Though adding low-entropy data is intuitive, it is crucial in that this simple operation can make methods either too conservative or too aggressive without the correct balancing conditions. 3. The studies in Sec. 3.1 directly present a feasible and guaranteed solution for imple- menting ATTA to tackle shifts while avoiding CF. The aligned empirical validations of Sec. 3.2 also instruct the implementation of SimATTA. Q3: In test-time adaptation, one important issue is that the number of testing samples in a batch may be small, which means the sample size m will also be very small. May it affect the theorem and make them become very loose? A: We consider this issue jointly from theoretical and empirical validations. 1. It is true that the theoretical bounds can be loose given a small size of m unlabeled test samples. This situation of the error bound is mathematically ascribed to the quotient between the VC-dimension d of the hypothesis class and m. Under the VC-dimension theory, the ResNet18 model we adopt should have d ≫ m. However, practically we perform fine-tuning on pre-trained models instead of training from scratch, which significantly reduces the scale of parameter update. In this case, an assumption can be established that fine-tuning a model is roughly equivalent to learning a model with a relatively small d (Appx. H). This assumption is potentially underpinned by the empirical alignment of our validation experiments with the theoretical framework (Fig. 1). To this end, experiments indicate thatd and m are practically of similar scale for our settings. This prevents our theoretical bounds from being very loose and meaningless in reality. 2. Regarding cases that our assumption does not apply, this issue would appear inevitable, since it is rigorously inherent in the estimation error of our streaming and varying test distributions. The distribution of a test stream can be hardly monitored when only a limited batch is allowed, which we consider as a limitation of TTA settings. Moreover, this issue directly implies the necessity of using a buffer for unlabeled samples. A good practice is to maintain a relatively comparable sample buffer scale. Q4: What distribution shifts can ATTA solve? A: We would like to follow (but not limited to) the work (Zhao et al., 2023b) to discuss the distribution shifts ATTA can solve. 1. As elucidated in Sec. 3.1 and Sec. 5, ATTA can solve domain generalization shifts. Domain generalization shifts include complex shifts on the joint data distribution P(X, Y), given X as the covariates and Y as the label variable. Since P(X, Y) = P(X)P(Y |X), ATTA can handle covariate shift (P(X)), label shift (P(Y )), and conditional shift (P(Y |X)). The shifts on both covariate and conditional distributions can cover the shift on labels, but they (covariate + conditional shifts) are more complicated than pure label shifts, where only the marginal label distribution changes while the conditional distribution remains. Note that the conditional shifts are generally caused by spurious correlations, where the independent causal mechanism assumption (Pearl, 2009) holds or no concept drifts exist. 2. In our framework, the distribution support of X at different time steps can be different, but we don’t cover the situation where the support of Y changes, i.e., class-incremental problems. Q5: It is unclear how many samples are selected in each minibatch of testing samples. How the total budget is distributed across the whole testing data stream? A: The number of selected samples for each minibatch is decided jointly by the incremental clustering and the cluster centroid number NC (t). Intuitively, this sample selection is a dynamic process, with NC (t) restricting the budget and incremental clustering performing sample selection. For each batch, we increase applicable clustering centroids as a maximum limit, while the exact number of the selected samples is given by the incremental clustering by how many clusters are located in the scope of new distributions. e.g., if the incoming batch does not introduce new data distributions, then we select zero samples even with increased NC (t). In contrast, if the incoming batch contains data located in multiple new distributions, the incremental clustering tends to select more samples than the NC (t) limit, thus forcing to merging of multiple previous clusters into one new cluster. 18Published as a conference paper at ICLR 2024 The incremental clustering is detailed in Sec. 4.2, and NC (t) is naively increased by a constant hyper-parameter k. Therefore, the budget is adaptively distributed according to the data streaming distribution with budgets controlled by k, which is also the reason why we compare methods under a budget limit. Q6: Could compared methods have access to a few ground-truth labels as well? Making other algorithms be able to use the same amount of ground-truth labels randomly will produce fairer comparisons. A: 1. The enhanced TTA setting is exactly the setup we provide to produce fairer comparisons. See Tab. 3 and Tab. 5 for comparison results. 2. ATTA also compares to a stronger setting ADA which can access the whole test datasets multiple times. Table 5: The table demonstrates the comparisons on PACS where all enhanced TTA baselines have 300 budgets to randomly select labeled samples. The training steps of these labeled samples are the same as the original TTA method training steps. For accumulated sample selection, please refer to our ablation studies. Method Domain-wise data stream A VG Random data stream A VG P→ →A→ →C→ →S P A C S 1 2 3 4 P A C S Source onlyBN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 TTA Tent (steps=1) N/A 70.07 68.43 64.42 97.72 74.17 72.61 68.92 61.20 62.36 66.59 67.32 98.14 74.37 70.26 66.07Tent (steps=10) N/A 76.27 63.78 49.35 59.46 38.62 48.46 55.03 56.20 53.22 52.55 55.55 58.32 47.56 60.75 58.00EATA N/A 69.53 66.94 61.42 98.56 69.38 66.60 64.83 60.34 59.81 64.38 65.02 98.68 73.78 68.30 59.74CoTTA N/A 66.55 63.14 59.91 90.12 61.67 66.68 67.68 57.26 57.36 63.46 65.64 92.22 71.53 70.44 62.41SAR (steps=1) N/A 66.60 63.78 50.34 98.38 67.87 64.04 49.48 57.21 56.06 56.78 57.14 98.38 68.80 64.59 53.02SAR (steps=10) N/A 69.09 66.55 49.07 96.23 62.50 59.34 46.53 49.76 52.74 48.51 49.06 95.39 57.13 54.61 38.76 Ours (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00 Q7: What is the position of ATTA? A: Comparisons with different settings are challenging. In this work, the design of our experiments (Sec. 5) is to overcome this challenge by comparing both weaker settings and stronger settings. While the significant performance over weaker settings renders the necessity of extra information, the comparable performance with stronger settings provides the potential to relax restricted requirements. Intuitively, ATTA is the most cost-effective option in the consideration of both efficiency and effectiveness. We further provide the following ATTA summary: ATTA, which incorporates active learning in FTTA, is the light, real-time, source-free, widely applicable setting to achieve high generalization performances for test-time adaptation. 1. Necessity: From the causality perspective, new information is necessary (Lin et al., 2022; Pearl, 2009; Peters et al., 2017) to attain generalizable over distribution shifts which are insurmountable within the current TTA framework. 2. Effectiveness: Compared to FTTA methods, ATTA produces substantially better perfor- mances, on-par with the costly active domain adaptation (ADA) methods as shown in Table 3 in the paper. 3. Efficiency: Relative to ADA methods, ATTA possesses superior efficiency, similar to general FTTA methods, as shown in Tab. 3. 4. Applicability: ATTA is a model-agnostic setting. (1) Compared to domain generalization methods, ATTA do not require re-training and has the potential to apply to any pre-trained models. One interesting future direction is designing ATTA methods for large language models (LLMs), where re-trainings are extremely expensive and source data may be in- accessible. (2) Compared to FTTA methods, ATTA can protect model parameters from corrupting while learning new distributions by fine-tuning pre-trained models, rendering it more feasible and practical. In comparison with existing works, ATTA is motivated to mitigate the limitations of previous settings: 1. FTTA: Limited generalization performance. 19Published as a conference paper at ICLR 2024 2. TTT: Not source-free; limited generalization performance. 3. ADA & domain adaptation/generalization: Expensive re-trainings; limited applicability to pre-trained models. 4. Online active learning: It does not maintain and protect adaptation performances for multiple distributions in one model and does not consider the CF problem. Q8: What is the potential practical utility of ATTA? A: 1. Empirically, our method can generally finish a round of sample selection/training of 100 frames in 5s, i.e., 20 frames per sec, which is more than enough to handle multiple practical situations. Experiments on time complexity are provided in Tab. 3, where SimATTA has comparable time efficiency. 2. As a case analysis, the autopilot system (Hu et al., 2023; Chen et al., 2022a) presents an application scenario requiring high-speed low-latency adaptations, while these adaptations are largely underexplored. When entering an unknown environment, e.g., a construction section, a system of ATTA setting can require the driver to take over the wheel. During the period of manual operation when the driver is handling the wheel, steering signals are generated, and the in-car system quickly adaptations. The system doesn’t need to record 60 frames per second, since only the key steering operations and the corresponding dash cam frames are necessary, which can be handled by ATTA algorithms processing at 20 frames per sec. In this case, the human annotations are necessary and indirect. ATTA makes use of this information and adapts in the short term instead of collecting videos and having a long-round fine-tuning (Schafer et al., 2018). 3. In addition, many scenarios applicable for ATTA are less speed-demanding than the case above. One example is a personalized chatbot that subtly prompts and gathers user labels during user interaction. In a home decoration setting, applications can request that users scan a few crucial areas to ensure effective adaptation. Social robots (Mavrogiannis et al., 2023), e.g., vacuum robots, often require users to label critical obstacles they’ve encountered. 4. Compared with ADA, ATTA stands out as the tailored solution for the above scenarios. It does not require intensive retraining or server-dependent fine-tuning, offering both speed and computational efficiency. Meanwhile, akin to other TTA methods, ATTA also ensures user privacy. While it might marginally exceed the cost of standard TTA methods, the superior generalization ability makes it a compelling choice and justifies the additional expense. Q9: What can be covered by this paper? A: This paper endeavors to establish the foundational framework for a novel setting referred to as ATTA. We target (1) positioning the ATTA setting, (2) solving the two major and basic challenges of ATTA,i.e., the mitigation of distribution shifts and the avoidance of catastrophic forgetting (CF). We achieve the first goal by building the problem formulation and analyses, and further providing extensive qualitative and well-organized experimental comparisons with TTA, enhanced TTA, and ADA settings. These efforts position ATTA as the most cost-effective option between TTA and ADA, where ATTA inherits the efficiency of TTA and the effectiveness of ADA. With our theoretical analyses and the consistent algorithm design, we validate the success of our second goal through significant empirical performances. Q10: What are not covered by this paper? A: Constructing a new setting involves multifaceted complexities. Although there are various potential applications discussed above including scaling this setting up for large models and datasets, we cannot cover them in this single piece of work. There are three main reasons. First, the topics covered by a single paper are limited. Formally establishing ATTA setting and addressing its major challenges of ATTA takes precedence over exploring practical applications. Secondly, given the interrelations between ATTA and other settings, our experimental investigations are predominantly comparative, utilizing the most representative datasets from TTA and domain adaptation to showcase persuasive results. Thirdly, many practical applications necessitate task-specific configurations, rendering them unsuitable for establishing a universal learning setting. While the current focus is on laying down the foundational aspects of ATTA, the exploration of more specialized applications remains a prospective avenue for future work in the ATTA domain. 20Published as a conference paper at ICLR 2024 C R ELATED WORKS The development of deep learning witnesses various applications (He et al., 2016; Gui et al., 2020). To tackle OOD problem, various domain generalization works emerge (Krueger et al., 2021; Sagawa et al., 2019). C.1 U NSUPERVISED DOMAIN ADAPTATION Unsupervised Domain Adaptation (UDA) (Pan et al., 2010; Patel et al., 2015; Wilson and Cook, 2020; Wang and Deng, 2018) aims at mitigating distribution shifts between a source domain and a target domain, given labeled source domain samples and unlabeled target samples. UDA methods generally rely on feature alignment techniques to eliminate distribution shifts by aligning feature distributions between source and target domains. Typical feature alignment techniques include discrepancy minimization (Long et al., 2015; Sun and Saenko, 2016; Kang et al., 2019) and adversarial training (Ganin and Lempitsky, 2015; Tsai et al., 2018; Ajakan et al., 2014; Ganin et al., 2016; Tzeng et al., 2015; 2017). Nevertheless, alignments are normally not guaranteed to be correct, leading to the alignment distortion problem as noted by Ning et al. (2021). Source-free Unsupervised Domain Adaptation (SFUDA) (Fang et al., 2022; Liu et al., 2021b) algorithms aim to adapt a pre-trained model to unlabeled target domain samples without access to source samples. Based on whether the algorithm can access model parameters, these algorithms are categorized into white-box and black-box methods. White-box SFUDA typically considers data recovery (generation) and fine-tuning methods. The former focuses on recovering source- like data (Ding et al., 2022; Yao et al., 2021), e.g., training a Generative Adversarial Network (GAN) (Kurmi et al., 2021; Li et al., 2020), while the latter employs various techniques (Mao et al., 2021), such as knowledge distillation (Chen et al., 2022b; Liu and Yuan, 2022; Yang et al., 2021b; Yu et al., 2022), statistics-based domain alignment (Ishii and Sugiyama, 2021; Liu et al., 2021a; Fan et al., 2022; Eastwood et al., 2021), contrastive learning (Huang et al., 2021; Wang et al., 2022b), and uncertainty-based adaptation (Gawlikowski et al., 2021; Fleuret et al., 2021; Chen et al., 2021; Li et al., 2021b). Black-box SFUDA cannot access model parameters and often relies on self-supervised knowledge distillation (Liang et al., 2022; 2021), pseudo-label denoising (Zhang et al., 2021; Yang et al., 2022), or generative distribution alignment (Yeh et al., 2021; Yang et al., 2021a). C.2 T EST-TIME ADAPTATION Test-time Adaptation (TTA), especially Fully Test-time Adaptation (FTTA) algorithms (Wang et al., 2021; Iwasawa and Matsuo, 2021; Karani et al., 2021; Nado et al., 2020; Schneider et al., 2020; Wang et al., 2022a; Zhao et al., 2023a; Niu et al., 2022; Zhang et al., 2022a; Niu et al., 2023; You et al., 2021; Zhang et al., 2022b), can be considered as realistic and lightweight methods for domain adaptation. Built upon black-box SFUDA, FTTA algorithms eliminate the requirement of a pre-collected target dataset and the corresponding training phase. Instead, they can only access an unlabeled data stream and apply real-time adaptation and training. In addition to FTTA, Test-time Training (TTT) (Sun et al., 2020; Liu et al., 2021c) often relies on appending the original network with a self-supervised task. TTT methods require retraining on the source dataset to transfer information through the self-supervised task. Although they do not access the source dataset during the test-time adaptation phase, TTT algorithms are not off-the-shelf source-free methods. TTA is a promising and critical direction for real-world applications, but current entropy minimization-based methods can be primarily considered as feature calibrations that require high-quality pseudo-labels. This requirement, however, can be easily violated under larger distribution shifts. Current TTA algorithms, inheriting UDA drawbacks, cannot promise good feature calibration results, which can be detrimental in real-world deployments. For instance, entropy minimization on wrongly predicted target domain samples with relatively low entropy can only exacerbate spurious correla- tions (Chen et al., 2020). Without extra information, this problem may be analogous to applying causal inference without intervened distributions, which is intrinsically unsolvable (Peters et al., 2016; Pearl, 2009). This paper aims to mitigate this issue with minimal labeled target domain samples. To minimize the cost, we tailor active learning techniques for TTA settings. It is worth noting that a recent work AdaNPC (Zhang et al., 2023) is essentially a domain gener- alization method with a TTA phase attached, while our ATTA is built based on the FTTA setting. Specifically, Current FTTA methods and our work cannot access the source domain. In contrast, 21Published as a conference paper at ICLR 2024 AdaNPC accesses source data to build its memory bank, circumventing the catastrophic forgetting problem. Furthermore, AdaNPC requires multiple source domains and training before performing TTA. Thus AdaNPC uses additional information on domain labels and retraining resources for its memory bank, undermining the merits of FTTA. Regarding theoretical bounds, their target domain is bounded by source domain error and model estimations (in big-O expression), while we consider active sample learning and time variables for varying test distributions. C.3 C ONTINUAL DOMAIN ADAPTATION Many domain adaptation methods focus on improving target domain performance, neglecting the performance on the source domain, which leads to the CF problem (Kemker et al., 2018; Kirkpatrick et al., 2017; Li and Hoiem, 2017; Lopez-Paz and Ranzato, 2017; De Lange et al., 2021; Wang et al., 2022a; Niu et al., 2022). This issue arises when a neural network, after being trained on a sequence of domains, experiences a significant degradation in its performance on previously learned domains as it continues to learn new domains. Continual learning, also known as lifelong learning, addresses this problem. Recent continual domain adaptation methods have made significant progress by employing gradient regularization, random parameter restoration, buffer sample mixture, and more. Although the CF problem is proposed in the continual learning field, it can occur in any source-free OOD settings since the degradation caused by CF is attributed to the network’s parameters being updated to optimize performance on new domains, which may interfere with the representations learned for previous domains. C.4 A CTIVE DOMAIN ADAPTATION Active Domain Adaptation (ADA) (Prabhu et al., 2021; Ning et al., 2021; Su et al., 2020; Ma et al., 2021; Xie et al., 2022) extends semi-supervised domain adaptation with active learning strate- gies (Cohn et al., 1996; Settles, 2009), aiming to maximize target domain performance with a limited annotation budget. Therefore, the key challenge of active learning algorithms is selecting the most informative unlabeled data in target domains (Kapoor et al., 2007). Sample selection strategies are of- ten based on uncertainty (Lewis and Catlett, 1994; Scheffer et al., 2001), diversity (Jain and Grauman, 2016; Hoi et al., 2009), representativeness (Xu et al., 2003), expected error minimization (Vijaya- narasimhan and Kapoor, 2010), etc. Among these methods, uncertainty and diversity-based methods are simple and computationally efficient, making them the most suitable choices to tailor for TTA settings. Adapting these strategies is non-trivial because, compared to typical active domain adaptation, our proposed Active Test-time Adaptation (ATTA) setting does not provide access to source data, model parameters, or pre-collected target samples. This requirement demands that our active sample selection algorithm select samples for annotation during data streaming. Consequently, this active sampling selection process is non-regrettable, i.e., we can only meet every sample once in a short period. To avoid possible confusion, compared to the recent Source-free Active Domain Adaptation (SFADA) method SALAD (Kothandaraman et al., 2023), we do not require access to model parameter gradients, training additional neural networks, or pre-collected target datasets. Therefore, our ATTA setting is quite different, much lighter, and more realistic than ADA and SFADA. C.5 A CTIVE ONLINE LEARNING The most related branch of active online learning (AOL) (Cacciarelli and Kulahci, 2023) is active online learning on drifting data stream (Zhou et al., 2021; Baier et al., 2021; Li et al., 2021a). Generally, these methods include two components, namely, detection and adaptation. Compared with ATTA, there are several distinctions. First, this line of studies largely focuses on the distribution shift detection problem, while ATTA focuses on multi-domain adaptations. Second, AOL on drifting data stream aims to detect and adapt to one current distribution in the stream, without considering preserving the adaptation abilities of multiple past distributions by maintaining and fine-tuning the original pre-trained models. In contrast, ATTA’s goal is to achieve the OOD generalization optimums adaptable across multiple source and target distributions, leading to the consideration of CF problems. Third, while AOL requires one-by-one data input and discard, ATTA maintains a buffer for incoming data before selection decisions. This is because ATTA targets maintaining the original model without corrupting and replacing it, such that making statistically meaningful and high-quality decisions is 22Published as a conference paper at ICLR 2024 critical for ATTA. In contrast, AOL allows resetting and retraining new models, whose target is more lean to cost saving and one-by-one manner. D F URTHER THEORETICAL STUDIES In this section, we refine the theoretical studies with supplement analysis and further results. We use the H-divergence and H∆H-distance definitions following (Ben-David et al., 2010). Definition 2 (H-divergence). For a function class H and two distributions D1 and D2 over a domain X, the H-divergence between D1 and D2 is defined as dH(D1, D2) = sup h∈H |Px∼D1 [h(x) = 1] − Px∼D2 [h(x) = 1]|. The H∆H-distance is defined base on H-divergence. We use the H∆H-distance definition follow- ing (Ben-David et al., 2010). Definition 3 (H∆H-distance). For two distributions D1 and D2 over a domain X and a hypothesis class H, the H∆H-distance between D1 and D2 w.r.t. H is defined as dH∆H(D1, D2) = sup h,h′∈H Px∼D1 [h(x) ̸= h′(x)] + Px∼D2 [h(x) ̸= h′(x)]. (9) The H∆H-distance essentially provides a measure to quantify the distribution shift between two distributions. It measures the maximum difference of the disagreement between two hypotheses in H for two distributions, providing a metrics to quantify the distribution shift between D1 and D2. H-divergence and H∆H-distance have the advantage that they can be applied between datasets, i.e., estimated from finite samples. Specifically, let S1, S2 be unlabeled samples of size m sampled from D1 and D2; then we have estimated H∆H-distance ˆdH(S1, S2). This estimation can be bounded based on Theorem 3.4 of Kifer et al. (2004), which we state here for completeness. Theorem 5. Let A be a collection of subsets of some domain measure space, and assume that the VC-dimension is some finite d. Let P1 and P2 be probability distributions over that domain and S1, S2 finite samples of sizes m1, m2 drawn i.i.d. according P1, P2 respectively. Then Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16, (10) where Pm1+m2 is the m1 + m2’th power of P - the probability that P induces over the choice of samples. Theorem 5 bounds the probability for relativized discrepancy, and its applications in below lemmas and Theorem 1 help us bound the quantified distribution shifts between domains. The probability, according to a distribution D, that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source domain dataset is inaccessible under ATTA settings, we consider the existence of the source dataset DS for the purpose of accurate theoretical analysis. Thus, we initialize Dtr(0) as DS, i.e., Dtr(0) = DS. For every time step t, the test and training data can be expressed as Ute(t) and Dtr(t) = DS ∪ Dte(1) ∪ Dte(2) ∪ ··· ∪Dte(t). (11) We use N to denote the total number of samples in Dtr(t) and λ = (λ0, λ1, ··· , λt) to represent the ratio of sample numbers in each component subset. In particular, we have |DS| |Dtr(t)| = λ0, |Dte(1)| |Dtr(t)| = λ1, ··· , |Dte(t)| |Dtr(t)| = λt, (12) where Pt i=0 λi = 1. Therefore, at time step t, the model has been trained on labeled data Dtr(t), which contains t + 1 components consisting of a combination of data from the source domain and multiple test-time domains. For each domain the model encounters, DS, Ute(1), Ute(2), ··· , Ute(t), let ϵj(h(t)) denote the error of hypothesis h at time t on the jth domain. Specifically, ϵ0(h(t)) = ϵS(h(t)) represents the error of h(t) on the source data DS, and ϵj(h(t)) for j ≥ 1 denotes the error of h(t) on test data Ute(j). Our optimization minimizes a convex combination of training error over the labeled samples from all domains. Formally, given the vector w = (w0, w1, ··· , wt) of domain error 23Published as a conference paper at ICLR 2024 weights with Pt j=0 wj = 1 and the sample number from each component Nj = λjN, we minimize the empirical weighted error of h(t) as ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj Nj X Nj |h(x, t) − g(x)|. (13) Note that w, λ and N are also functions of t, which we omit for simplicity. We now establish two lemmas as the preliminary for Theorem 1. In the following lemma, we bound the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)). Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. In the following lemma, we provide an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. The proofs for both lemmas are provided in Appx. E. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Lemma 6 bounds the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)), which is majorly influenced by the estimatedH∆H-distance and the quality of discrepancy estimation. During the ATTA process, the streaming test data can form multiple domains and distributions. However, if we consider all data during the test phase as a single test domain,i.e., St i=1 Ute(i), we can simplify Lemma 6 to obtain an upper bound for the test error ϵT as |ϵw(h(t)) − ϵT (h(t))| ≤w0  1 2 ˆdH∆H(S0, ST ) + 2 s 2d log(2m) + log 2 δ m + γ  , (14) where γ = min h∈H{ϵ0(h(t)) + ϵT (h(t))}, and ST is sampled from St i=1 Ute(i). To understand Lamma 7, we need to understand Hoeffding’s Inequality, which we state below as a Proposition for completeness. Proposition 8 (Hoeffding’s Inequality). Let X be a set, D1, . . . , Dt be probability distributions on X, and f1, . . . , ft be real-valued functions on X such that fi : X → [ai, bi] for i = 1, . . . , t. Then for any ϵ >0, P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! (15) where E[fi(x)] is the expected value of fi(x). Lamma 7 provides an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Theorem 1 essentially bounds the performance of ATTA on the source and each test domains. The adaptation performance on a test domain is majorly 24Published as a conference paper at ICLR 2024 bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. If we consider the multiple data distributions during the test phase as a single test domain, i.e., St i=1 Ute(i), Theorem 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . With the optimal test/source hypothesis h∗ T (t) = arg min h∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (16.a), with approximately B = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (17) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. When the budget is sufficiently large or the source data amount is not sufficiently large compared to the distribution shift A, the optimal w∗ 0 for the test error bound is w∗ 0 = 0, i.e., using no source data since possible error reduction from the data addition is always less than the error increase caused by large divergence between the source data and the test data. Theorem 2 offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Following Theorem 1, when no active learning is included during TTA,i.e., w0 = λ0 = 1, the upper boundw0A+ q w2 0 λ0 + (1−w0)2 1−λ0 B ≥ A+B; when enabling ATTA, withw0 = λ0 ̸= 1, we can easily achieve an upper bound w0A + B < A+ B. Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. Entropy quantifies the amount of information contained in a probability distribution. In the context of a classification model, lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction. When a model assigns low entropy to a sample, this high confidence can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model “recognizes” the sample as being similar to those it was trained on, hence the high confidence in its prediction. While entropy is not a direct measure of distributional distance, it can be used as an indicator of how closely a sample aligns with the model’s learned distribution. This interpretation is more about model confidence and the implied proximity rather than a strict mathematical measure of distributional distance. The pre-trained model is well-trained on abundant source domain data, and thus the model distribution is approximately the source distribution. Selecting low-entropy samples using essentially provides an estimate of sampling from the source dataset. Thus, Dϕ,S(t), based on well-aligned with the model’s learned distribution is an approximation of DS. When we consider the CF problem and feasibly include the source-like dataset Dϕ,S(t) into the ATTA training data in place of the inaccessible DS in Eq. (11), we can also derive bounds on the domain errors under this practical ATTA setting when minimizing the empirical weighted errorϵ′ w(h(t)) using the hypothesis h at time t, similar to Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domainsDϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is 25Published as a conference paper at ICLR 2024 N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵ′ w(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Other derived results following Theorem 1 also apply for this practical ATTA setting. Further empirical validations for our theoretical results are provided in Appx. H. E P ROOFS This section presents comprehensive proofs for all the lemmas, theorems, and corollaries mentioned in this paper, along with the derivation of key intermediate results. Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Proof. First we prove that given unlabeled samples of size m S1, S2 sampled from two distributions D1 and D2, we have dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (18) We start with Theorem 3.4 of Kifer et al. (2004): Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16. (19) In Eq. 19, ’d’ is the VC-dimension of a collection of subsets of some domain measure space A, while in our case, d is the VC-dimension of hypothesis space H. Following (Ben-David et al., 2010), the H∆H space is the set of disagreements between every two hypotheses inH, which can be represented as a linear threshold network of depth 2 with 2 hidden units. Therefore, the VC-dimension of H∆H is at most twice the VC-dimension of H, and the VC-dimension of our domain measure space is 2d for Eq. 19 to hold. Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2m)2de−m1ϵ2/16 + (2m)2de−m2ϵ2/16. We rewrite the inequality as δ (2m)2d = e−m1ϵ2/16 + e−m2ϵ2/16; taking the logarithm of both sides, we get log δ (2m)2d = −m1 ϵ2 16 + log(1 +e−(m1−m2) ϵ2 16 ). 26Published as a conference paper at ICLR 2024 Assuming m1 = m2 = m and defining a = ϵ2 16 , we have log δ (2m)2d = −ma + log 2; rearranging the equation, we then get ma + log(δ/2) = 2d log(2m). Now, we can solve for a: a = 2d log(2m) + log 2 δ m . Recall that a = ϵ2 16 , so we get: ϵ = 4√a ϵ = 4 s 2d log(2m) + log 2 δ m . With probability of at least 1 − δ, we have |ϕA(S1, S2) − ϕA(P1, P2)| ≤4 s 2d log(2m) + log 2 δ m ; therefore, dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (20) Now we prove Lemma 6. We use the triangle inequality for classification error in the derivation. For the domain error of hypothesis h at time t on the jth domain ϵj(h(t)), given the definition of ϵw(h(t)), |ϵw(h(t)) − ϵj(h(t))| = | tX i=0 wiϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi|ϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(|ϵi(h(t)) − ϵi(h(t), h∗ i (t))| + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + |ϵj(h(t), h∗ i (t)) − ϵj(h(t))|) ≤ tX i=0 wi(ϵi(h∗ i (t)) + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + ϵj(h∗ i (t))) ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|), where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. By the definition of H∆H-distance and our proved Eq. 20, |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| ≤sup h,h′∈H |ϵi(h(t), h′(t)) − ϵj(h(t), h′(t))| = sup h,h′∈H Px∼Di[h(x) ̸= h′(x)] + Px∼Dj [h(x) ̸= h′(x)] = 1 2dH∆H(Di, Dj) ≤ 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m , 27Published as a conference paper at ICLR 2024 where Di, Dj denote the ith and jth domain. Therefore, |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|) ≤ tX i=0 wi(γi + 1 2dH∆H(Di, Dj)) ≤ tX i=0 wi(γi + 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m ). Since ϵi(h(t)) − ϵj(h(t)) = 0 when i = j, we derive |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. This completes the proof. Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Proof. We apply Hoeffding’s Inequality in our proof: P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! . (21) In the jth domain, there are λjN samples. With the true labeling function g(x), for each of the λjN samples x, let there be a real-valued function fi(x) fi(x) = wj λj |h(x, t) − g(x)|, where fi(x) ∈ [0, wj λj ]. Incorporating all the domains, we get ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj λjN X λjN |h(x, t) − g(x)| = 1 N tX j=0 λjNX i=1 fi(x), which corresponds to the 1 t Pt i=1 fi(x) part in Hoeffding’s Inequality. Due to the linearity of expectations, we can calculate the sum of expectations as 1 N tX j=0 λjNX i=1 E[fi(x)] = 1 N ( tX j=0 λjN wj λj ϵj(h(t))) = tX j=0 wjϵj(h(t)) = ϵw(h(t)), which corresponds to the 1 t Pt i=1 Ex∼Di[fi(x)] part in Hoeffding’s Inequality. Therefore, we can apply Hoeffding’s Inequality as P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2N2ϵ2/( NX i=0 range2(fi(x))) ! = 2 exp   −2N2ϵ2/( tX j=0 λjN(wj λj )2) ! = 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . This completes the proof. 28Published as a conference paper at ICLR 2024 Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. Proof. First we prove that for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (22) We apply Theorem 3.2 of Kifer et al. (2004) and Lemma 7, P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . We rewrite the inequality as δ (2N)d = e −2Nϵ2/(Pt j=0 w2 j λj ) , taking the logarithm of both sides, we get log δ (2N)d = −2Nϵ2/( tX j=0 w2 j λj ). Rearranging the equation, we then get ϵ2 = ( tX j=0 w2 j λj )d log(2N) − log(δ) 2N . Therefore, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (23) 29Published as a conference paper at ICLR 2024 Based on Eq. 23, we now prove Theorem 1. For the empirical domain error of hypothesis h at time t on the jth domain ϵj(ˆh(t)), applying Lemma 6, Eq. 23, and the definition of h∗ j (t), we get ϵj(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ j (t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵj(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   = ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k where k > 0, we have the assumption that k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H(D(k′), Ute(t + k)) ≤ δD. Here, we slightly abuse the notation D(k′) to represent Ds if k′ = 0 and Ute(k′) if k′ > 0. Then we get ϵt+k(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, St+k) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2( ˆdH∆H(Si, Sk′ ) + ˆdH∆H(Sk′ , St+k)) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   30Published as a conference paper at ICLR 2024 ≤ ˆϵw(h∗ t+k(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵt+k(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   = ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. with probability of at least 1−δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 , γi = minh∈H{ϵi(h(t))+ ϵt+k(h(t))}, and 0 ≤ δD ≪ +∞. This completes the proof. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (24) Proof. From Theorem 1, we can derive the bound for the test error where the test-time data are considered as a single test domain: |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t) = w0( ˆdH∆H(S0, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N ; and we simplify the above equation as |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (25) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Since we have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (26) 31Published as a conference paper at ICLR 2024 where Formula 26 obtains the minimum value if and only if w0 = λ0; when enabling ATTA with any λ0 ̸= 1, we can get EBT (w, λ, N, t) = w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≥ w0A + B, (27) where the minimum value EBT (w, λ, N, t)min = w0A + B can be obtained with condition w0 = λ0 ̸= 1. When no active learning is included, i.e., for weight and sample ratio vectors w′ and λ′, w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, we have EBT (w′, λ′, N, t) = w′ 0A + s w′2 0 λ′ 0 + (1 − w′ 0)2 1 − λ′ 0 B = A + B. (28) Since for EBT (w, λ, N, t)min = w0A + B, w0 < 1 and A, B >0 hold, we derive EBT (w, λ, N, t)min = w0A + B < A+ B = EBT (w′, λ′, N, t). (29) This completes the proof. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Theorem 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Theorem 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Proof. For the empirical source error on DS of hypothesis h at time t, similar to Theorem 1, we apply Lemma 6, Eq. 23 to get ϵS(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ S(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵS(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   32Published as a conference paper at ICLR 2024 = ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. This completes the proof. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Theorem 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (30) Proof. From Theorem 1, considering the test-time data as a single test domain, we can derive the bound for the source error on DS: |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N , where ST is sampled fromSt i=1 Ute(i), γ = minh∈H{ϵ0(h(t))+ϵS(h(t))}, and γ′ = minh∈H{ϵT (h(t))+ ϵS(h(t))}. We have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (31) where the equality and the minimum value are obtained if and only if w0 = λ0. When Dϕ,S(t) is not included,i.e., with the weight and sample ratio vectorsw′ and λ′ s.t. w′ 0 = λ′ 0 = 0, using the empirical gap term B = 2 q d log(2N)−log(δ) 2N , we have EBS(w′, λ′, N, t) = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + s w2 0 λ0 + (1 − w0)2 1 − λ0 B = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B. When Dϕ,S(t) is included with λ0 ̸= 0, EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≤ w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B, 33Published as a conference paper at ICLR 2024 Algorithm 2 INCREMENTAL CLUSTERING (IC) Require: Given previously selected anchors, new unlabeled samples, and the cluster budget as Danc, Unew, and NC . Global anchor weights wanc = (wanc 1 , . . . , wanc |Danc|)⊤. 1: For simplicity, we consider anchor weights wanc as a global vector. 2: function IC(Danc, Unew, NC ) 3: wsp ← Concat(wanc, 1⊤ |Unew|) ▷ Assign all new samples with weight 1. 4: Φ ← Extract the features from the penultimate layer of model f on x ∈ Danc ∪ Unew in order. 5: clusters ← Weighted-K-Means(Φ, wsp, NC) 6: new_clusters ← {clusteri | ∀clusteri ∈ clusters, ∀x ∈ Danc, x /∈ clustersi} 7: Xnew_anchors ← {the closest sample x to the centroid of clusteri | ∀clusteri ∈ new_clusters} 8: Xanchors ← {x ∈ Danc} ∪Xnew_anchors 9: wanc ← Concat(wanc, 0⊤ |Xnew_anchors|) ▷ Initialize new anchor weights. 10: for wanc i ∈ wanc, wanc i ← wanc i + # sample of clusterj # anchor in clusterj , wanc i ∈ clusterj ▷ Weight accumulation. 11: Return Xanchors 12: end function where the minimum value can be obtained with condition w0 = λ0 ̸= 0. In practical learning scenarios, we generally assume adaptation tasks are solvable; therefore, there should be a prediction function that performs well on two distinct domains. In this case, γ and γ′ should be relatively small, so we can assume γ ≈ γ′. If ˆdH∆H(S0, SS) < ˆdH∆H(SS, ST ), then we have EBS(w, λ, N, t)min = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B < ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B = EBS(w′, λ′, N, t). Therefore, we derive EBS(w, λ, N, t)min < EBS(w′, λ′, N, t). (32) This completes the proof. F I NCREMENTAL CLUSTERING F.1 A LGORITHM DETAILS We provide the detailed algorithm for incremental clustering as Alg. 2. F.2 V ISUALIZATION To better illustrate the incremental clustering algorithm, we provide visualization results on PACS to demonstrate the process. As shown in Fig. 3, the initial step of IC is a normal K-Means clustering step, and ten anchors denoted as \"X\" are selected. The weights of all samples in a clusters is aggregated into the corresponding anchor’s weight. Therefore, these ten samples (anchors) are given larger sizes visually (i.e., larger weights) than that of other new test samples in the first IC step (Fig. 4). During the first IC step, several distributions are far away from the existed anchors and form clusters 1,7,9 and 10, which leads to 4 new selected anchors. While the number of cluster centroid is only increased by 1, 4 of the existing anchors are clustered into the same cluster 8 (purple). Thus IC produces 4 new anchors instead of 1. Similarly, in the second IC step (Fig. 5), the new streaming-in test samples introduce a new distribution; IC produces 3 new clusters (4, 8, and 11) and the corresponding number of anchors to cover them. The number of centroid is only increased by 1, which implies that there are two original-cluster-merging events. More IC step visualization results are provided in Fig. 6 and 7. 34Published as a conference paper at ICLR 2024 Figure 3: Initial IC step: normal clustering. Left: Clustering results. Right: Selecting new anchors. Figure 4: The first IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 5: The second IC step. Left: Weighted clustering results. Right: Selecting new anchors. 35Published as a conference paper at ICLR 2024 Figure 6: The third IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 7: The fourth IC step. Left: Weighted clustering results. Right: Selecting new anchors. 36Published as a conference paper at ICLR 2024 G E XPERIMENT DETAILS In this section, we provide more experimental details including the details of the datasets and training settings. G.1 D ETAILS ABOUT THE DATASETS We adopt datasets PACS, VLCS, and Office-Home from DomainBed (Gulrajani and Lopez-Paz, 2020) with the same domain splits. All available licenses are mentioned below. • PACS (Li et al., 2017) includes four domains: art, cartoons, photos, and sketches. PACS is a 7-class classification dataset with 9,991 images of dimension (3, 224, 224). • VLCS (Fang et al., 2013) contains photographic domains: Caltech101, LabelMe, SUN09, and VOC2007. This dataset includes 10,729 images of dimension (3, 224, 224) with 5 classes. • Office-Home (Venkateswara et al., 2017) is a 65-class dataset, including domains: art, clipart, product, and real. VLCS includes 10,729 images of dimension (3, 224, 244). (License) • Tiny-ImageNet-C is a 200-class dataset, including 15 corrupt types. Tiny-ImageNet-C includes 150,000 images of dimension (3, 224, 244). Since the class number 200 is less than ImageNet (1000), the model’s last layer classifier needs to be adapted. In this work, we use the brightness corruption domain to adapt. In the source pretraining phase, we adopt the most ImageNet-like domain as our source domain. For PACS and Office-Home, we use domains \"photos\" and \"real\" as the source domains, respectively, while for VLCS, Caltech101 is assigned to apply the source pretraining. We freeze the random seeds to generate the sample indices order for the two test data streams, namely, the domain-wise data stream and the random data stream. For PACS, the domain-wise data stream inputs samples from domain art, cartoons, to sketches, while we shuffle all samples from these three domains in the random data stream. For VLCS, we stream the domains in the order: LabelMe, SUN09, and VOC2007, as the domain-wise data stream. For Office-Home, the domain-wise data stream order becomes art, clipart, and product. G.2 T RAINING AND OPTIMIZATION SETTINGS In this section, we extensively discuss the model architectures, optimization settings, and method settings. G.2.1 A RCHITECTURES PACS & VLCS. We adopt ResNet-18 as our model encoder followed by a linear classifier. The initial parameters of ResNet-18 are ImageNet pre-trained weights. In our experiment, we remove the Dropout layer since we empirically found that using the Dropout layer might degrade the optimization process when the sample number is small. The specific implementation of the network is closely aligned with the implementation in DomainBed (Gulrajani and Lopez-Paz, 2020). Office-Home. We employ ResNet-50 as our model encoder for Office-Home. Except for the architecture, the other model settings are aligned with the ResNet-18. Tiny-ImageNet-C ResNet-18 is adapted from ImageNet to Tiny-ImageNet-C by training the last linear layer. G.2.2 T RAINING & OPTIMIZATION In this section, we describe the training configurations for both the source domain pre-training and test-time adaptation procedures. Source domain pre-training. For the PACS and VLCS datasets, models are fine-tuned on the selected source domains for 3,000 iterations. The Adam optimizer is utilized with a learning rate 37Published as a conference paper at ICLR 2024 of 10−4. In contrast, for the Office-Home dataset, the model is fine-tuned for a longer duration of 10,000 iterations with a slightly adjusted learning rate of 5 × 10−5. Test-time adaptation. For test-time adaptation across PACS and VLCS, the pre-trained source model is further fine-tuned using the SGD optimizer with a learning rate of 10−3. While on Office-Home and Tiny-ImageNet-C, a learning rate of 10−4 is adopted. For all TTA baselines, barring specific exceptions, we faithfully adhere to the original implementation settings. A noteworthy exception is the EATA method, which requires a cosine similarity threshold. The default threshold of the original EATA implementation was not suitable for the three datasets used in our study, necessitating an adjustment. We empirically set this threshold to 0.5 for training. Unlike Tent and SAR, which only require the optimization of batch normalization layers (Santurkar et al., 2018), SimATTA allows the training of all parameters in the networks. In experiments, we use a tolerance count (tol) to control the training process. SimATTA will stop updating once the loss does not descrease for more than 5 steps. However, for Tiny-ImageNet-C, SimATTA uses ‘steps=10‘ for time comparisons since other methods apply at most 10 steps. G.2.3 M ETHOD SETTINGS Tent. In our experiments, we apply the official implementation of Tent1. Specifically, we evaluate Tent with 1 test-time training step and 10 steps, respectively. EATA.Our EATA implementation follows its official code2. In our experiments, EATA has 2000 fisher training samples, E0 = 0.4 × log(# class), ϵ <0.5. CoTTA. For CoTTA, we strictly follow all the code and settings from its official implementation3. SAR. With SAR’s official implementation4, we set E0 = 0 .4 × log(# class) and e0 = 0 .1 in our experiments. ADA baselines. For ADA baselines, we follow the architecture of the official implementation of CLUE (Prabhu et al., 2021)5. SimATTA Implementation. Our implementation largely involves straightforward hyperparameter settings. The higher entropy bound eh = 10−2 should exceed the lower entropy bound el, but equal values are acceptable. Empirically, the lower entropy bound el can be set to 10−3 for VLCS and Office-Home, or 10−4 for PACS. The choice of el is largely dependent on the number of source-like samples obtained. A lower el may yield higher-accuracy low-entropy samples, but this could lead to unstable training due to sample scarcity. Though experimentation with different hyperparameters is encouraged, our findings suggest that maintaining a non-trivial number of low-entropy samples and setting an appropriateλ0 are of primary importance. If λ0 < 0.5, CF may ensue, which may negate any potential improvement. Regarding the management of budgets, numerous strategies can be adopted. In our experiments, we utilized a simple hyperparameter k, varying from 1 to 3, to regulate the increasing rate of budget consumption. This strategy is fairly elementary and can be substituted by any adaptive techniques. G.3 S OFTWARE AND HARDWARE We conduct our experiments with PyTorch (Paszke et al., 2019) and scikit-learn (Pedregosa et al., 2011) on Ubuntu 20.04. The Ubuntu server includes 112 Intel(R) Xeon(R) Gold 6258R CPU @2.70GHz, 1.47TB memory, and NVIDIA A100 80GB PCIe graphics cards. The training process costs graphics memory less than 10GB, and it requires CPU computational resources for scikit-learn K-Means clustering calculations. Our implementation also includes a GPU-based PyTorch K-Means method for transferring calculation loads from CPUs to GPUs. However, for consistency, the results of our experiments are obtained with the original scikit-learn K-Means implementation. 1https://github.com/DequanWang/tent 2https://github.com/mr-eggplant/EATA 3https://github.com/qinenergy/cotta 4https://github.com/mr-eggplant/SAR 5https://github.com/virajprabhu/CLUE 38Published as a conference paper at ICLR 2024 Figure 8: Target loss surface on 2000 samples without source pre-training. The red points denote the loss minimum for a fixed λ0. The orange line denote the place where w0 = λ0. Figure 9: Target loss surface on 2000 samples with source pre-training. H E MPIRICAL VALIDATIONS FOR THEORETICAL ANALYSIS In this section, we undertake empirical validation of our learning theory, which encompasses multiple facets awaiting verification. In contemporary computer vision fields, pre-trained models play a pivotal role, and performance would significantly decline without the use of pre-trained features. The learning theory suggests that given the vast VC-dimension of complete ResNets, without substantial data samples, the training error cannot be theoretically tight-bounded. However, we show empirically in the following experiments that fine-tuning pre-trained models is behaviorally akin to training a model with a low VC-dimension. Training on 2000 Samples Without Source Domain Pre-training. For an ImageNet pre-trained ResNet-18 model, we trained it using 2000 samples from the PACS dataset. To ascertain the optimal value w∗ 0 in Equation 4, we trained multiple models for different w0 and λ0 pairings. For each pair, we derived the target domain loss (from art, cartoons, and sketches) post-training and plotted this loss on the z-axis. With w0 and λ0 serving as the xy-axes, we drafted the target domain loss ϵT surface in Figure 8. As the results show, given a λ0, the optimal w∗ 0 typically aligns with the line λ0 = w0, with a slight downward shift, which aligns with Equation 4. 39Published as a conference paper at ICLR 2024 Figure 10: Target loss surface on 500 samples with source pre-training. Figure 11: Source loss surface on 500 samples with source pre-training. 40Published as a conference paper at ICLR 2024 Figure 12: Target and source loss surface on 500 samples with source pre-training. Table 6: TTA comparisons on Office-Home. This table includes the two data stream settings mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. Office-Home Domain-wise data stream Post-adaptation Random data stream Post-adaptation R →A→ →C→ →P R A C P 1 2 3 4 R A C P BN w/o adapt 93.78 42.93 37.62 59.90 93.78 42.93 37.62 59.90 46.82 46.82 46.82 46.82 93.78 42.93 37.62 59.90BN w/ adapt 92.38 49.69 39.43 63.53 92.38 49.69 39.43 63.53 50.88 50.88 50.88 50.88 92.38 49.69 39.43 63.53 Tent (steps=1) N/A 49.61 39.31 63.87 92.47 49.57 39.89 63.89 49.95 50.27 50.23 52.06 92.40 49.24 39.68 63.98Tent (steps=10) N/A 49.61 39.04 61.41 87.08 44.79 38.37 60.49 50.05 49.31 48.74 47.79 85.31 42.85 37.89 58.71EATA N/A 49.65 39.04 63.53 91.60 49.61 38.65 63.48 49.73 50.27 49.45 51.07 91.05 49.11 38.26 62.99CoTTA N/A 49.61 38.76 61.84 87.81 44.95 35.92 59.04 49.84 49.84 48.95 50.43 86.99 43.68 34.73 57.56SAR (steps=1) N/A 49.65 39.24 63.53 92.45 49.73 39.36 63.69 49.84 50.05 49.91 51.67 92.38 49.57 39.50 63.87SAR (steps=10) N/A 49.53 38.81 61.50 88.94 46.15 37.04 59.41 50.09 50.30 49.77 49.22 89.14 46.23 36.31 59.45 SimATTA (B ≤300) N/A 56.20 48.38 71.66 95.75 60.07 52.62 74.70 58.57 60.88 62.91 63.67 95.89 62.01 54.98 74.70SimATTA (B ≤500) N/A 58.71 51.11 74.36 96.03 62.05 57.41 76.98 58.85 62.63 63.41 64.31 95.91 63.78 57.87 77.09 Training on 2000 Samples with Source Domain Pre-training. To further assess the effects of source pre-training, we repeated the same experiment on a source pre-trained ResNet-18. The results are depicted in Figure 9. This experiment provides empirical guidance on selecting w0 in source domain pre-trained situations. The findings suggest that the optimal w∗ 0 non-trivially shifts away from the line λ0 = w0 towards lower-value regions. Considering the source pre-training process as using a greater quantity of source domain samples, it implies that when the number of source samples greatly exceeds target samples, a lower w0 can enhance target domain results. Training on 500 Samples with Source Domain Pre-training. We proceed to fine-tune the source domain pre-trained ResNet-18 using only 500 samples, thereby simulating active TTA settings. We train models with various w0 and λ0 pairings, then graph the target domain losses, source domain losses, and the combined losses. As shown in Figure 10, the target losses still comply with our theoretical deductions where the local minima are close to the line λ0 = w0 and marginally shift towards lower values. Considering the challenge of CF, the source domain results in Figure 11 suggest a reverse trend compared to the target domain, where lower λ0 and w0 values yield superior target domain results but inferior source domain results. Thus, to curb CF, the primary strategy is to maintain a relatively higher λ0. When considering both target and source domains, a balance emerges as depicted in Figure 12. The global minimum is located in the middle region, demonstrating the trade-off between the target domain and source domain performance. I A DDITIONAL EXPERIMENT RESULTS In this section, we provide additional experiment results. The Office-Home results and ablation studies will be presented in a similar way as the main paper. In the full results Sec. I.3, we will post more detailed experimental results with specific budget numbers and intermediate performance during the test-time adaptation. 41Published as a conference paper at ICLR 2024 Table 7: Comparisons to ADA baselines on Office-Home. The source domain is denoted as \"(S)\" in the table. Results are average accuracies with standard deviations). Office-Home R (S) A C P Random (B = 300) 95.04 (0.20) 57.54 (1.16) 53.43 (1.17) 73.46 (0.97) Entropy (B = 300) 94.39 (0.49) 61.21 (0.71) 56.53 (0.71) 72.31 (0.28) Kmeans (B = 300) 95.09 (0.14) 57.37 (0.90) 51.74 (1.34) 71.81 (0.39) CLUE (B = 300) 95.20 (0.23) 60.18 (0.98) 58.05 (0.43) 73.72 (0.70) Ours (B ≤300) 95.82 (0.07) 61.04 (0.97) 53.80 (1.18) 74.70 (0.00) I.1 R ESULTS ON OFFICE -HOME We conduct experiments on Office-Home and get the test-time performances and post-adaptation performances for two data streams. As shown in Tab. 6, SimATTA can outperform all TTA baselines with huge margins. Compared to ADA baselines under the source-free settings, as shown in Tab. 7, SimATTA obtains comparable results. I.2 A BLATION STUDIES Figure 13: Ablation study on PACS and VLCS.\"IC=0\" denotes removing incremental clustering (IC) selection. \"LE=0\" denotes removing the low-entropy (LE) sample training. Domain-wise stream and random stream are applied on first and second rows, respectively. The accuracy values are averaged across all splits/domains. In this section, we explore three variations of our method to examine the individual impacts of its components. The first variant replaces the incremental clustering selection with entropy selection, 42Published as a conference paper at ICLR 2024 where only the samples with the highest entropy are chosen. The second variant eliminates low- entropy sample training. The third variation combines the first and second variants. We perform this ablation study on the PACS and VLCS as outlined in Fig. 13. We denote the use of incremental clustering (IC) and low-entropy training (LE) respectively as IC=1 and LE=1. The experiments essentially reveals the effectiveness of incremental clustering and low-entropy- sample training. As we have detailed in Sec. 3.2, these techniques are designed to to select informative samples, increase distribution coverage, and mitigate catastrophic forgetting. These designs appositely serve the ATTA setting where the oracle has costs and the budget is limited. Therefore, their effectiveness is prominent particularly when the budget is small. As the results show, when the budget B ≤100 or B ≤300, removing the components observably impairs performances. When B gets large, more active samples cover a larger distribution; thus the performance gap from random selection and informative selection gets smaller. In the extreme case where B → ∞, all samples are selected and thus the superiority of our meticulously-designed techniques are not manifested. Specifically, our analysis yields several insights. First, SimATTA (LE=1, IC=1) comprehensively outperforms other variants on both datasets, different streams, and different budgets. Second, variants without low-entropy training (LE=0, IC=0/1) easily fail to produce stable results (e.g., domain-wise stream in VLCS). Third, SimATTA’s performance surpasses this variant on PACS’s domain-wise stream clearly especially when the budgets are low. This indicates these variants fail to retrieve the most informative style shift (PACS’s shifts) samples, which implies the advantage of incremental clustering when the budget is tight. In addition, these results show that IC has its unique advantage on domain-wise streams where distributions change abruptly instead of random streams. Therefore, compared to PACS’s domain- wise stream results, the reason for the smaller performance improvement of SimATTA over the variant (LE=1, IC=0) on VLCS’s domain-wise stream is that images in VLCS are all photos that do not include those severe style shifts in PACS (i.e., art, cartoons, and sketches). That is, when the shift is not severe, we don’t need IC to cover very different distributions, and selecting samples using entropy can produce good results. In brief, IC is extraordinary for severe distribution shifts and quick adaptation. It is worth mentioning that low budget comparison is essential to show the informative sample retrieval ability, since as the budget increases, all AL techniques will tend to perform closely. I.3 C OMPLETE EXPERIMENT RESULTS We provide complete experimental results in this section. As shown in Tab. 8, we present the full results for two data streams. The test-time adaptation accuracies are shown in the \"Current domain\" row, while the \"Budgets\" row denotes the used budget by the end of the domain. The rest four rows denote the four domain test results by the end of the real-time adaptation of the current domain, where the first column results are the test accuracy before the test-time adaptation phase. N/A represents \"do not apply\". Table 8: Tent (steps=1) on PACS. Tent (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.29 64.59 44.67 56.35 54.09 51.83 48.58 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.38 97.60 98.56 98.08 97.72 97.19 A 59.38 69.09 68.95 66.85 68.07 67.33 65.58 63.53 C 28.03 64.04 65.19 64.08 64.85 65.19 62.97 60.75 S 42.91 53.65 47.39 42.58 54.57 49.83 44.13 41.56 J C HALLENGES AND PERSPECTIVES Despite advancements, test-time adaptation continues to pose considerable challenges. As previously discussed, without supplementary information and assumptions, the ability to guarantee model generalization capabilities is limited. However, this is not unexpected given that recent progress 43Published as a conference paper at ICLR 2024 Table 9: Tent (steps=10) on PACS. Tent (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.38 57.85 20.23 47.36 31.01 22.84 20.33 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 95.45 87.43 62.63 93.83 81.32 65.39 50.78 A 59.38 64.94 55.03 34.52 55.32 40.28 28.27 23.68 C 28.03 55.89 56.70 40.57 54.52 39.68 27.22 20.95 S 42.91 36.96 26.27 13.59 32.25 23.16 20.95 19.62 Table 10: EATA on PACS. EATA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.04 64.72 50.27 57.31 56.06 58.17 59.78 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.62 98.50 98.62 98.68 98.62 98.50 98.62 A 59.38 68.90 68.16 66.50 68.65 68.95 69.34 69.63 C 28.03 63.74 65.36 62.46 65.19 66.00 65.57 65.70 S 42.91 54.01 52.89 48.18 55.71 55.64 54.09 54.26 Table 11: CoTTA on PACS. CoTTA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 65.48 62.12 53.17 56.06 54.33 57.16 57.42 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.62 98.62 98.62 98.62 98.56 98.62 A 59.38 65.82 65.87 65.48 66.02 65.87 66.31 65.97 C 28.03 62.63 63.05 63.10 63.01 62.88 63.01 62.97 S 42.91 53.88 54.03 53.78 54.67 55.31 55.10 54.62 Table 12: SAR (steps=1) on PACS. SAR (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 66.75 63.82 49.58 56.78 56.35 56.68 56.70 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.50 98.32 98.74 98.56 98.50 98.44 A 59.38 68.02 68.07 66.94 67.87 68.65 68.55 68.16 C 28.03 62.84 64.97 62.93 63.82 64.89 64.46 64.38 S 42.91 53.47 52.07 45.74 54.92 55.46 53.68 52.53 Table 13: SAR (steps=10) on PACS. SAR (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 69.38 68.26 49.02 53.51 51.15 51.78 45.60 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.20 95.39 96.47 97.13 97.78 97.72 94.13 A 59.38 72.36 66.60 62.16 62.74 64.94 66.11 56.64 C 28.03 63.44 68.30 56.19 59.77 61.73 62.03 56.02 S 42.91 53.37 44.59 54.62 41.00 49.66 48.79 36.37 44Published as a conference paper at ICLR 2024 Table 14: SimATTA (B ≤300) on PACS. SimATTA (B ≤300) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 76.86 70.90 75.39 69.47 76.49 82.45 82.22 Budgets N/A 75 145 223 66 142 203 267 P 99.70 98.44 98.86 98.80 97.96 98.68 99.04 98.98 A 59.38 80.71 82.32 84.47 73.97 80.52 81.10 84.91 C 28.03 48.12 82.00 82.25 72.35 81.06 83.36 83.92 S 42.91 32.78 56.25 81.52 79.49 83.10 84.78 86.00 Table 15: SimATTA (B ≤500) on PACS. SimATTA (B ≤500) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 77.93 76.02 76.30 68.46 78.22 80.91 85.49 Budgets N/A 121 230 358 102 221 343 425 P 99.70 98.92 98.86 98.62 98.20 99.46 99.10 99.16 A 59.38 87.01 87.60 88.33 73.39 79.20 84.91 86.67 C 28.03 54.78 83.96 83.49 68.43 74.40 84.22 84.77 S 42.91 46.37 63.53 83.74 81.34 81.04 86.66 87.71 Table 16: Tent (steps=1) on VLCS. Tent (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 38.55 34.40 53.88 44.85 44.29 47.38 44.98 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.81 85.44 84.73 84.95 85.16 85.80 85.30 L 33.55 40.02 43.11 43.86 39.68 41.98 43.11 43.49 S 41.10 33.39 35.41 33.61 36.29 37.90 38.27 37.81 V 49.08 53.20 54.06 53.11 53.76 54.18 53.76 53.35 Table 17: Tent (steps=10) on VLCS. Tent (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 45.41 31.44 32.32 46.13 42.31 43.51 39.48 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 73.07 48.34 42.54 74.13 62.19 56.54 52.01 L 33.55 46.61 38.44 37.65 44.88 45.93 43.41 40.32 S 41.10 31.75 28.82 27.79 35.37 36.14 35.28 33.64 V 49.08 48.05 40.14 33.12 50.50 44.49 42.48 40.37 Table 18: EATA on VLCS. EATA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.24 33.15 52.58 43.77 42.48 43.34 41.55 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 85.16 85.02 84.10 84.73 84.52 84.10 83.32 L 33.55 37.16 37.24 37.69 37.09 36.78 36.90 36.67 S 41.10 33.39 33.49 32.39 33.33 32.54 31.84 31.47 V 49.08 51.87 52.16 52.49 52.07 52.43 52.64 52.55 45Published as a conference paper at ICLR 2024 Table 19: CoTTA on VLCS. CoTTA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.39 32.54 52.25 43.69 42.14 43.21 42.32 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 81.55 81.98 82.12 82.61 82.47 82.12 81.98 L 33.55 37.20 37.91 37.65 38.48 38.22 38.40 37.99 S 41.10 30.71 32.78 33.12 34.00 33.70 33.97 33.52 V 49.08 52.01 52.64 52.90 53.64 53.14 53.08 53.23 Table 20: SAR (steps=1) on VLCS. SAR (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 36.18 34.43 52.46 43.64 43.04 44.20 41.93 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.31 84.17 83.96 85.09 85.23 85.23 85.09 L 33.55 35.62 38.29 39.72 38.55 39.34 40.21 40.70 S 41.10 33.24 36.41 36.53 34.37 35.62 36.29 36.44 V 49.08 51.75 52.61 52.37 52.90 52.75 53.05 53.02 Table 21: SAR (steps=10) on VLCS. SAR (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 35.32 34.10 51.66 43.56 42.05 42.53 41.16 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 83.96 83.04 82.12 84.03 84.24 85.23 85.09 L 33.55 34.07 35.92 41.49 39.53 38.37 37.65 37.58 S 41.10 31.93 34.89 33.94 35.19 32.94 33.88 33.12 V 49.08 51.33 51.51 53.08 52.78 52.34 51.78 52.01 Table 22: SimATTA (B ≤300) on VLCS. SimATTA (B ≤300) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 62.61 65.08 74.38 62.33 69.33 73.20 71.93 Budgets N/A 79 175 272 71 135 208 262 C 100.00 99.51 98.52 99.93 99.86 99.79 100.00 99.93 L 33.55 68.11 69.92 69.50 62.61 66.64 68.45 69.43 S 41.10 55.24 68.89 66.67 65.54 69.29 71.79 72.46 V 49.08 66.08 70.94 77.34 73.79 76.87 78.82 80.39 Table 23: SimATTA (B ≤500) on VLCS. SimATTA (B ≤500) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 63.52 68.01 76.13 62.29 70.45 73.50 72.02 Budgets N/A 113 266 446 107 203 283 356 C 100.00 99.29 98.59 99.51 99.93 99.86 99.86 99.43 L 33.55 62.95 70.63 70.56 66.57 67.09 67.24 70.29 S 41.10 51.31 73.83 73.10 65.33 71.79 72.91 72.55 V 49.08 59.36 71.65 78.35 73.58 77.84 80.01 80.18 46Published as a conference paper at ICLR 2024 Table 24: Tent (steps=1) on Office-Home. Tent (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.31 63.87 49.95 50.27 50.23 52.06 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.33 92.36 92.47 92.38 92.45 92.45 92.40 A 57.07 49.73 49.73 49.57 49.69 49.73 49.57 49.24 C 44.97 39.27 39.54 39.89 39.45 39.68 39.73 39.68 P 73.15 63.60 63.66 63.89 63.60 63.82 63.93 63.98 Table 25: Tent (steps=10) on Office-Home. Tent (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.04 61.41 50.05 49.31 48.74 47.79 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 91.99 89.14 87.08 92.08 90.80 88.59 85.31 A 57.07 49.94 46.77 44.79 49.44 48.21 45.69 42.85 C 44.97 38.58 39.11 38.37 40.18 40.02 38.63 37.89 P 73.15 63.28 61.03 60.49 64.36 63.64 61.12 58.71 Table 26: EATA on Office-Home. EATA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.04 63.53 49.73 50.27 49.45 51.07 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.36 92.17 91.60 92.38 92.22 91.71 91.05 A 57.07 49.57 49.53 49.61 49.69 49.40 49.36 49.11 C 44.97 39.08 39.01 38.65 39.27 39.01 38.42 38.26 P 73.15 63.42 63.42 63.48 63.51 63.37 63.33 62.99 Table 27: CoTTA on Office-Home. CoTTA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 38.76 61.84 49.84 49.84 48.95 50.43 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 90.38 88.02 87.81 90.48 89.37 88.00 86.99 A 57.07 48.58 45.53 44.95 47.34 46.35 44.62 43.68 C 44.97 36.66 35.58 35.92 37.55 36.40 35.44 34.73 P 73.15 60.40 57.74 59.04 61.12 59.63 58.35 57.56 Table 28: SAR (steps=1) on Office-Home. SAR (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.24 63.53 49.84 50.05 49.91 51.67 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.38 92.31 92.45 92.40 92.36 92.36 92.38 A 57.07 49.65 49.57 49.73 49.69 49.61 49.57 49.57 C 44.97 39.34 39.22 39.36 39.34 39.56 39.47 39.50 P 73.15 63.51 63.51 63.69 63.60 63.71 63.71 63.87 47Published as a conference paper at ICLR 2024 Table 29: SAR (steps=10) on Office-Home. SAR (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.53 38.81 61.50 50.09 50.30 49.77 49.22 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.20 92.06 88.94 92.40 92.47 91.53 89.14 A 57.07 49.40 49.77 46.15 49.81 50.02 48.91 46.23 C 44.97 39.20 38.63 37.04 39.50 39.29 38.65 36.31 P 73.15 63.53 62.69 59.41 64.18 64.18 62.83 59.45 Table 30: SimATTA (B ≤300) on Office-Home. SimATTA (B ≤300) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 56.20 48.38 71.66 58.57 60.88 62.91 63.67 Budgets N/A 75 187 277 79 147 216 278 R 96.44 95.43 95.43 95.75 95.91 95.96 96.01 95.89 A 57.07 57.56 59.50 60.07 58.34 59.91 61.15 62.01 C 44.97 42.25 52.46 52.62 51.66 52.30 54.75 54.98 P 73.15 68.84 70.13 74.70 72.45 73.10 74.50 74.70 Table 31: SimATTA (B ≤500) on Office-Home. SimATTA (B ≤500) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 58.71 51.11 74.36 58.85 62.63 63.41 64.31 Budgets N/A 107 284 440 126 248 361 467 R 96.44 95.69 95.71 96.03 96.26 96.19 95.87 95.91 A 57.07 61.43 61.43 62.05 58.18 61.15 61.52 63.78 C 44.97 46.41 57.73 57.41 53.17 55.14 56.79 57.87 P 73.15 70.74 71.98 76.98 73.51 74.18 75.78 77.09 48Published as a conference paper at ICLR 2024 in deep learning heavily relies on large-scale data. Consequently, two promising paths emerge: establishing credible assumptions and leveraging additional information. Firstly, developing credible assumptions can lead to comprehensive comparisons across various stud- ies. Given that theoretical guarantees highlight the inherent differences between methods primarily based on the application limits of their assumptions, comparing these assumptions becomes critical. Without such comparative studies, empirical evaluations may lack precise guidance and explanation. Secondly, while we acknowledge the value of real-world data (observations), discussions surrounding the use of extra information remain pertinent. Considerations include the strategies to acquire this supplementary information and the nature of the additional data needed. Despite the myriad of works on domain generalization, domain adaptation, and test-time adaptation, a comprehensive survey or benchmark encapsulating the aforementioned comparisons remains an unmet need. Moreover, potential future directions for out-of-distribution generalization extend beyond domain generalization and test-time adaptation. One promising avenue is bridging the gap between causal inference and deep learning, for instance, through causal representation learning. In conclusion, our hope is that this work not only offers a novel practical setting and algorithm but also illuminates meaningful future directions and research methodologies that can benefit the broader scientific community. 49",
      "meta_data": {
        "arxiv_id": "2404.05094v1",
        "authors": [
          "Shurui Gui",
          "Xiner Li",
          "Shuiwang Ji"
        ],
        "published_date": "2024-04-07T22:31:34Z",
        "pdf_url": "https://arxiv.org/pdf/2404.05094v1.pdf",
        "github_url": "https://github.com/divelab/ATTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Active Test-Time Adaptation (ATTA), a novel problem setting that integrates active learning within fully Test-Time Adaptation (FTTA) to address significant distribution shifts and catastrophic forgetting (CF) in streaming test data. It provides a learning theory analysis demonstrating that incorporating limited labeled test instances enhances overall performance with theoretical guarantees. The authors propose SimATTA, a simple yet effective algorithm that utilizes sample entropy balancing and incremental clustering to mitigate CF and perform real-time sample selection. Extensive experiments show that ATTA substantially improves performance over existing TTA methods, maintains efficiency, and achieves effectiveness comparable to more demanding Active Domain Adaptation (ADA) methods.",
        "methodology": "The ATTA framework formalizes the continuous optimization of a pre-trained model to streaming, continuously changing test data by actively selecting informative instances for labeling. It minimizes a combination of cross-entropy loss (for labeled samples) and an unsupervised loss (for unlabeled samples) under a defined budget for labeled instances. To mitigate catastrophic forgetting (CF), SimATTA employs selective entropy minimization: low-entropy samples are pseudo-labeled by a frozen source-pretrained model and included in training as 'source-like' data. High-entropy samples are chosen for active labeling using an incremental clustering technique. This clustering method (based on weighted K-means) selects representative 'anchors' to reduce redundancy and increase distribution coverage, adapting to newly seen distributions and merging old clusters within a budget. Training weights are balanced to approximate the ratio of low-entropy and actively labeled high-entropy samples.",
        "experimental_setup": "The effectiveness of ATTA was validated against three settings: TTA, Enhanced TTA, and Active Domain Adaptation (ADA). Datasets included PACS, VLCS, Office-Home (from DomainBed), and Tiny-ImageNet-C. Source domains were 'photos' for PACS, 'Caltech101' for VLCS, 'real' for Office-Home, and 'brightness corruption' for Tiny-ImageNet-C. Two data stream orders were used: domain-wise (processing one domain completely before the next) and random (shuffling samples from all target domains into splits). Baselines included source-only models (BN w/o adapt, BN w/ adapt), state-of-the-art TTA methods (Tent, EATA, CoTTA, SAR), and ADA methods (Random, Entropy, k-means, CLUE). Models used were ResNet-18 and ResNet-50, pre-trained on ImageNet. Evaluation metrics were accuracy and time cost. SimATTA used labeling budgets typically under 300 or 500 samples. Ablation studies investigated the impact of incremental clustering and low-entropy sample training.",
        "limitations": "Out-of-distribution generalization remains extremely challenging without certain information and assumptions. The theoretical bounds for ATTA can be loose with small batches of unlabeled test samples, although fine-tuning pre-trained models empirically reduces the effective VC-dimension. The current framework does not address class-incremental problems where the support of labels (Y) changes. The proposed selective entropy minimization technique relies on the quality of the pre-trained model, and training on incorrectly pseudo-labeled low-entropy samples might reinforce errors. It is also noted that expending annotation budgets on low-entropy samples might not always be cost-effective. Furthermore, the paper focuses on establishing the foundational framework for ATTA and does not extensively cover scaling the setting up for large models and datasets or task-specific practical applications.",
        "future_research_directions": "Future research directions include developing alternative methods to prevent catastrophic forgetting in ATTA scenarios, particularly those that are less reliant on the pre-trained model's quality and actively correct incorrectly predicted low-entropy samples. Investigating the cost-effectiveness of annotating low-entropy samples is also suggested. A promising avenue involves designing ATTA methods specifically for large language models (LLMs), given the high cost of retraining and potential inaccessibility of source data. Additionally, bridging the gap between causal inference and deep learning, for example, through causal representation learning, is highlighted as a potential direction for out-of-distribution generalization. The authors also call for a comprehensive survey or benchmark that systematically compares different OOD generalization approaches based on their underlying assumptions and application limits.",
        "experimental_code": "import copy\nimport pathlib\nimport time\nfrom typing import Union\n\nimport numpy as np\nfrom sklearn.metrics import pairwise_distances_argmin_min\nfrom typing import Literal\n\nfrom torch import nn\nimport torch\n\nfrom munch import Munch\nfrom ATTA import register\nfrom ATTA.utils.config_reader import Conf\nfrom ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader\nfrom torch.utils.data import TensorDataset\nfrom tqdm import tqdm\nfrom .Base import AlgBase\nimport pandas as pd\nfrom ATTA.definitions import STORAGE_DIR\n\n\n\n@register.alg_register\nclass SimATTA(AlgBase):\n    def __init__(self, config: Conf):\n        super(SimATTA, self).__init__(config)\n\n        self.teacher = copy.deepcopy(self.model.to('cpu'))\n\n        self.model.to(config.device)\n        self.teacher.to(config.device)\n        self.update_teacher(0)  # copy student to teacher\n\n        self.budgets = 0\n        self.anchors = None\n        self.source_anchors = None\n        self.buffer = []\n        self.n_clusters = 10\n        self.nc_increase = self.config.atta.SimATTA.nc_increase\n        self.source_n_clusters = 100\n\n        self.cold_start = self.config.atta.SimATTA.cold_start\n\n        self.consistency_weight = 0\n        self.alpha_teacher = 0\n        self.accumulate_weight = True\n        self.weighted_entropy: Union[Literal['low', 'high', 'both'], None] = 'both'\n        self.aggressive = True\n        self.beta = self.config.atta.SimATTA.beta\n        self.alpha = 0.2\n\n        self.target_cluster = True if self.config.atta.SimATTA.target_cluster else False\n        self.LE = True if self.config.atta.SimATTA.LE else False\n        self.vis_round = 0\n\n\n    def __call__(self, *args, **kwargs):\n        # super(SimATTA, self).__call__()\n        self.continue_result_df = pd.DataFrame(\n            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)\n        self.random_result_df = pd.DataFrame(\n            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)\n\n        self.enable_bn(self.model)\n        if 'ImageNet' not in self.config.dataset.name:\n            for env_id in self.config.dataset.test_envs:\n                acc = self.test_on_env(env_id)[1]\n                self.continue_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n                self.random_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n\n        for adapt_id in self.config.dataset.test_envs[1:]:\n            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)\n            self.continue_result_df.loc['Budgets', adapt_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]\n\n        self.__init__(self.config)\n        for target_split_id in range(4):\n            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader, target_split_id)\n            self.random_result_df.loc['Budgets', target_split_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]\n\n        print(f'#IM#\\n{self.continue_result_df.round(4).to_markdown()}\\n'\n              f'{self.random_result_df.round(4).to_markdown()}')\n        self.continue_result_df.round(4).to_csv(f'{self.config.log_file}.csv')\n        self.random_result_df.round(4).to_csv(f'{self.config.log_file}.csv', mode='a')\n\n\n    @torch.no_grad()\n    def val_anchor(self, loader):\n        self.model.eval()\n        val_loss = 0\n        val_acc = 0\n        for data, target in loader:\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            output = self.fc(self.encoder(data))\n            val_loss += self.config.metric.loss_func(output, target, reduction='sum').item()\n            val_acc += self.config.metric.score_func(target, output) * len(data)\n        val_loss /= len(loader.sampler)\n        val_acc /= len(loader.sampler)\n        return val_loss, val_acc\n\n    def update_teacher(self, alpha_teacher):  # , iteration):\n        for t_param, s_param in zip(self.teacher.parameters(), self.model.parameters()):\n            t_param.data[:] = alpha_teacher * t_param[:].data[:] + (1 - alpha_teacher) * s_param[:].data[:]\n        if not self.config.model.freeze_bn:\n            for tm, m in zip(self.teacher.modules(), self.model.modules()):\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    tm.running_mean = alpha_teacher * tm.running_mean + (1 - alpha_teacher) * m.running_mean\n                    tm.running_var = alpha_teacher * tm.running_var + (1 - alpha_teacher) * m.running_var\n\n    @torch.enable_grad()\n    def cluster_train(self, target_anchors, source_anchors):\n        self.model.train()\n\n        source_loader = InfiniteDataLoader(TensorDataset(source_anchors.data, source_anchors.target), weights=None,\n                                           batch_size=self.config.train.train_bs,\n                                           num_workers=self.config.num_workers)\n        target_loader = InfiniteDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                             batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        alpha = target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())\n        if source_anchors.num_elem() < self.cold_start:\n            alpha = min(0.2, alpha)\n\n        ST_loader = iter(zip(source_loader, target_loader))\n        val_loader = FastDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                    batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)\n        delay_break = False\n        loss_window = []\n        tol = 0\n        lowest_loss = float('inf')\n        for i, ((S_data, S_targets), (T_data, T_targets)) in enumerate(ST_loader):\n            S_data, S_targets = S_data.to(self.config.device), S_targets.to(self.config.device)\n            T_data, T_targets = T_data.to(self.config.device), T_targets.to(self.config.device)\n            L_T = self.one_step_train(S_data, S_targets, T_data, T_targets, alpha, optimizer)\n            if len(loss_window) < self.config.atta.SimATTA.stop_tol:\n                loss_window.append(L_T.item())\n            else:\n                mean_loss = np.mean(loss_window)\n                tol += 1\n                if mean_loss < lowest_loss:\n                    lowest_loss = mean_loss\n                    tol = 0\n                if tol > 5:\n                    break\n                loss_window = []\n            if 'ImageNet' in self.config.dataset.name or 'CIFAR' in self.config.dataset.name:\n                if i > self.config.atta.SimATTA.steps:\n                    break\n\n\n    def one_step_train(self, S_data, S_targets, T_data, T_targets, alpha, optimizer):\n        L_S = self.config.metric.loss_func(self.model(S_data), S_targets)\n        L_T = self.config.metric.loss_func(self.model(T_data), T_targets)\n        loss = (1 - alpha) * L_S + alpha * L_T\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        return L_T\n\n    def softmax_entropy(self, x: torch.Tensor, y: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"Entropy of softmax distribution from logits.\"\"\"\n        if y is None:\n            if x.shape[1] == 1:\n                x = torch.cat([x, -x], dim=1)\n            return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n        else:\n            return - 0.5 * (x.softmax(1) * y.log_softmax(1)).sum(1) - 0.5 * (y.softmax(1) * x.log_softmax(1)).sum(1)\n\n    def update_anchors(self, anchors, data, target, feats, weight):\n        if anchors is None:\n            anchors = Munch()\n            anchors.data = data\n            anchors.target = target\n            anchors.feats = feats\n            anchors.weight = weight\n            anchors.num_elem = lambda: len(anchors.data)\n        else:\n            anchors.data = torch.cat([anchors.data, data])\n            anchors.target = torch.cat([anchors.target, target])\n            anchors.feats = torch.cat([anchors.feats, feats])\n            anchors.weight = torch.cat([anchors.weight, weight])\n        return anchors\n\n    def update_anchors_feats(self, anchors):\n        anchors_loader = FastDataLoader(TensorDataset(anchors.data), weights=None,\n                                        batch_size=32, num_workers=self.config.num_workers, sequential=True)\n\n        anchors.feats = None\n        self.model.eval()\n        for data in anchors_loader:\n            data = data[0].to(self.config.device)\n            if anchors.feats is None:\n                anchors.feats = self.model[0](data).cpu().detach()\n            else:\n                anchors.feats = torch.cat([anchors.feats, self.model[0](data).cpu().detach()])\n\n        return anchors\n\n    @torch.no_grad()\n    def adapt_on_env(self, loader, env_id):\n        acc = 0\n        for data, target in tqdm(loader[env_id]):\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            outputs, closest, self.anchors = self.sample_select(self.model, data, target, self.anchors, int(self.n_clusters), 1, ent_bound=self.config.atta.SimATTA.eh, incremental_cluster=self.target_cluster)\n            acc += self.config.metric.score_func(target, outputs).item() * data.shape[0]\n            if self.LE:\n                _, _, self.source_anchors = self.sample_select(self.teacher, data, target, self.source_anchors, self.source_n_clusters, 0,\n                                                               use_pseudo_label=True, ent_bound=self.config.atta.SimATTA.el, incremental_cluster=False)\n            else:\n                self.source_anchors = self.update_anchors(None, torch.tensor([]), None, None, None)\n            if not self.target_cluster:\n                self.n_clusters = 0\n            self.source_n_clusters = 100\n\n            self.budgets += len(closest)\n            self.n_clusters += self.nc_increase\n            self.source_n_clusters += 1\n\n            print(self.anchors.num_elem(), self.source_anchors.num_elem())\n            if self.source_anchors.num_elem() > 0:\n                self.cluster_train(self.anchors, self.source_anchors)\n            else:\n                self.cluster_train(self.anchors, self.anchors)\n            self.anchors = self.update_anchors_feats(self.anchors)\n        acc /= len(loader[env_id].sampler)\n        print(f'#IN#Env {env_id} real-time Acc.: {acc:.4f}')\n        return acc\n\n    @torch.no_grad()\n    def sample_select(self, model, data, target, anchors, n_clusters, ent_beta, use_pseudo_label=False, ent_bound=1e-2, incremental_cluster=False):\n        model.eval()\n        feats = model[0](data)\n        outputs = model[1](feats)\n        pseudo_label = outputs.argmax(1).cpu().detach()\n        data = data.cpu().detach()\n        feats = feats.cpu().detach()\n        target = target.cpu().detach()\n        entropy = self.softmax_entropy(outputs).cpu()\n        if not incremental_cluster:\n            entropy = entropy.numpy()\n            if ent_beta == 0:\n                closest = np.argsort(entropy)[: n_clusters]\n                closest = closest[entropy[closest] < ent_bound]\n            elif ent_beta == 1:\n                closest = np.argsort(entropy)[- n_clusters:]\n                closest = closest[entropy[closest] >= ent_bound]\n            else:\n                raise NotImplementedError\n            weights = torch.zeros(len(closest), dtype=torch.float)\n        else:\n            if ent_beta == 0:\n                sample_choice = entropy < ent_bound\n            elif ent_beta == 1:\n                sample_choice = entropy >= ent_bound\n            else:\n                raise NotImplementedError\n\n            data = data[sample_choice]\n            target = target[sample_choice]\n            feats = feats[sample_choice]\n            pseudo_label = pseudo_label[sample_choice]\n\n            if anchors:\n                feats4cluster = torch.cat([anchors.feats, feats])\n                sample_weight = torch.cat([anchors.weight, torch.ones(len(feats), dtype=torch.float)])\n            else:\n                feats4cluster = feats\n                sample_weight = torch.ones(len(feats), dtype=torch.float)\n\n            if self.config.atta.gpu_clustering:\n                from ATTA.utils.fast_pytorch_kmeans import KMeans\n                from joblib import parallel_backend\n                kmeans = KMeans(n_clusters=n_clusters, n_init=10, device=self.config.device).fit(\n                    feats4cluster.to(self.config.device),\n                    sample_weight=sample_weight.to(self.config.device))\n                with parallel_backend('threading', n_jobs=8):\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n            else:\n                from joblib import parallel_backend\n                from sklearn.cluster import KMeans\n                with parallel_backend('threading', n_jobs=8):\n                    kmeans = KMeans(n_clusters=n_clusters, n_init=10, algorithm='elkan').fit(feats4cluster,\n                                                                                                  sample_weight=sample_weight)\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n\n\n\n            if anchors:\n                num_anchors = anchors.num_elem()\n                prev_anchor_cluster = torch.tensor(kmeans_labels[:num_anchors], dtype=torch.long)\n\n                if self.accumulate_weight:\n                    num_prev_anchors_per_cluster = prev_anchor_cluster.unique(return_counts=True)\n                    num_prev_anchors_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_prev_anchors_per_cluster_dict[num_prev_anchors_per_cluster[0].long()] = \\\n                    num_prev_anchors_per_cluster[1]\n\n                    num_newsample_per_cluster = torch.tensor(kmeans_labels).unique(return_counts=True)\n                    num_newsample_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_newsample_per_cluster_dict[num_newsample_per_cluster[0].long()] = num_newsample_per_cluster[1]\n                    assert (num_prev_anchors_per_cluster_dict[prev_anchor_cluster] == 0).sum() == 0\n                    anchors.weight = anchors.weight + num_newsample_per_cluster_dict[prev_anchor_cluster] / \\\n                                          num_prev_anchors_per_cluster_dict[prev_anchor_cluster].float()\n\n                anchored_cluster_mask = torch.zeros(len(raw_closest), dtype=torch.bool).index_fill_(0,\n                                                                                                    prev_anchor_cluster.unique().long(),\n                                                                                                    True)\n                new_cluster_mask = ~ anchored_cluster_mask\n\n                closest = raw_closest[new_cluster_mask] - num_anchors\n                if (closest < 0).sum() != 0:\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    new_cluster_mask = torch.where(new_cluster_mask)[0]\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    print(closest)\n                    print(closest >= 0)\n                    new_cluster_mask = new_cluster_mask[closest >= 0]\n                    closest = closest[closest >= 0]\n\n\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1][new_cluster_mask]\n            else:\n                num_anchors = 0\n                closest = raw_closest\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1]\n\n        if use_pseudo_label:\n            anchors = self.update_anchors(anchors, data[closest], pseudo_label[closest], feats[closest], weights)\n        else:\n            anchors = self.update_anchors(anchors, data[closest], target[closest], feats[closest], weights)\n\n        return outputs, closest, anchors\n\n    def enable_bn(self, model):\n        if not self.config.model.freeze_bn:\n            for m in model.modules():\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    m.momentum = 0.1\n\nimport os\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import ConcatDataset\n\nfrom ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader\nfrom ATTA.utils.config_reader import Conf\nfrom ATTA.utils.initial import reset_random_seed\nfrom ATTA.utils.register import register\n\n@register.alg_register\nclass AlgBase:\n    def __init__(self, config: Conf):\n        super(AlgBase, self).__init__()\n\n        if not os.path.exists(config.ckpt_dir):\n            os.makedirs(config.ckpt_dir)\n\n        reset_random_seed(config)\n        self.dataset = register.datasets[config.dataset.name](config.dataset.dataset_root, config.dataset.test_envs,\n                                                              config)\n        config.dataset.dataset_type = 'image'\n        config.dataset.input_shape = self.dataset.input_shape\n        config.dataset.num_classes = 1 if self.dataset.num_classes == 2 else self.dataset.num_classes\n        config.model.model_level = 'image'\n        config.metric.set_score_func(self.dataset.metric)\n        config.metric.set_loss_func(self.dataset.task)\n\n        self.config = config\n\n        self.inf_loader = [InfiniteDataLoader(env, weights=None, batch_size=self.config.train.train_bs,\n                                              num_workers=self.config.num_workers) for env in self.dataset]\n        reset_random_seed(config)\n        self.train_split = [np.random.choice(len(env), size=int(len(env) * 0.8), replace=False) for env in self.dataset]\n        print(self.train_split)\n        self.val_split = [np.setdiff1d(np.arange(len(env)), self.train_split[i]) for i, env in enumerate(self.dataset)]\n        self.train_loader = [InfiniteDataLoader(env, weights=None, batch_size=self.config.train.train_bs,\n                                                num_workers=self.config.num_workers, subset=self.train_split[i]) for\n                             i, env in enumerate(self.dataset)]\n        self.val_loader = [FastDataLoader(env, weights=None, batch_size=self.config.train.train_bs,\n                                          num_workers=self.config.num_workers,\n                                          subset=self.val_split[i], sequential=True) for i, env in\n                           enumerate(self.dataset)]\n        reset_random_seed(config)\n        fast_random = [np.random.permutation(len(env)) for env in self.dataset]\n        self.fast_loader = [FastDataLoader(env, weights=None,\n                                           batch_size=self.config.atta.batch_size,\n                                           num_workers=self.config.num_workers,\n                                           subset=fast_random[i], sequential=True) for i, env in\n                            enumerate(self.dataset)]\n\n        reset_random_seed(config)\n        self.target_dataset = ConcatDataset(\n            [env for i, env in enumerate(self.dataset) if i in config.dataset.test_envs[1:]])\n        len_target = len(self.target_dataset)\n        target_choices = np.random.permutation(len_target)\n        len_split = len_target // 4\n        self.target_splits = [target_choices[i * len_split: (i + 1) * len_split] for i in range(4)]\n        self.target_splits[-1] = target_choices[3 * len_split:]\n        self.target_loader = [FastDataLoader(self.target_dataset, weights=None,\n                                             batch_size=self.config.atta.batch_size,\n                                             num_workers=self.config.num_workers, subset=self.target_splits[i],\n                                             sequential=True) for i in range(4)]\n\n        self.encoder = register.models[config.model.name](config).to(self.config.device)\n\n        self.fc = nn.Linear(self.encoder.n_outputs, config.dataset.num_classes).to(self.config.device)\n        self.model = nn.Sequential(self.encoder, self.fc).to(self.config.device)\n\n        if 'ImageNet' in config.dataset.name or 'CIFAR' in config.dataset.name:\n            self.train_on_env(self.config.dataset.test_envs[0], train_only_fc=True, train_or_load='load')\n        else:\n            self.train_on_env(self.config.dataset.test_envs[0], train_only_fc=False, train_or_load='load')\n\n    def __call__(self, *args, **kwargs):\n        for env_id in self.config.dataset.test_envs:\n            self.test_on_env(env_id)\n\n    @torch.no_grad()\n    def test_on_env(self, env_id):\n        self.model.eval()\n        test_loss = 0\n        test_acc = 0\n        for data, target in self.fast_loader[env_id]:\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            output = self.fc(self.encoder(data))\n            test_loss += self.config.metric.loss_func(output, target, reduction='sum').item()\n            test_acc += self.config.metric.score_func(target, output) * len(data)\n        test_loss /= len(self.fast_loader[env_id].dataset)\n        test_acc /= len(self.fast_loader[env_id].dataset)\n        print(f'#I#Env {env_id} Test set: Average loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}')\n        return test_loss, test_acc\n\n    @torch.no_grad()\n    def val_on_env(self, env_id):\n        self.model.eval()\n        val_loss = 0\n        val_acc = 0\n        for data, target in self.val_loader[env_id]:\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            output = self.fc(self.encoder(data))\n            val_loss += self.config.metric.loss_func(output, target, reduction='sum').item()\n            val_acc += self.config.metric.score_func(target, output) * len(data)\n        val_loss /= len(self.val_split[env_id])\n        val_acc /= len(self.val_split[env_id])\n        return val_loss, val_acc\n\n    @torch.enable_grad()\n    def train_on_env(self, env_id, train_only_fc=True, train_or_load='train'):\n        if train_or_load == 'train' or not os.path.exists(self.config.ckpt_dir + f'/encoder_{env_id}.pth'):\n            best_val_acc = 0\n            if train_only_fc:\n                self.model.eval()\n                self.fc.train()\n                optimizer = torch.optim.Adam(self.fc.parameters(), lr=self.config.train.lr)\n            else:\n                self.model.train()\n                optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.train.lr)\n            for batch_idx, (data, target) in enumerate(self.train_loader[env_id]):\n                optimizer.zero_grad()\n                data, target = data.to(self.config.device), target.to(self.config.device)\n                output = self.fc(self.encoder(data))\n                loss = self.config.metric.loss_func(output, target)\n                acc = self.config.metric.score_func(target, output)\n                loss.backward()\n                optimizer.step()\n                if batch_idx % self.config.train.log_interval == 0:\n                    print(f'Iteration: {batch_idx} Loss: {loss.item():.4f} Acc: {acc:.4f}')\n                    val_loss, val_acc = self.val_on_env(env_id)\n                    if val_acc > best_val_acc:\n                        print(f'New best val acc: {val_acc:.4f}')\n                        best_val_acc = val_acc\n                        torch.save(self.encoder.state_dict(), self.config.ckpt_dir + f'/encoder_{env_id}.pth')\n                        torch.save(self.fc.state_dict(), self.config.ckpt_dir + f'/fc_{env_id}.pth')\n                    self.model.train()\n                if batch_idx > self.config.train.max_iters:\n                    break\n        else:\n            self.encoder.load_state_dict(\n                torch.load(self.config.ckpt_dir + f'/encoder_{env_id}.pth', map_location=self.config.device), strict=False)\n            self.fc.load_state_dict(\n                torch.load(self.config.ckpt_dir + f'/fc_{env_id}.pth', map_location=self.config.device))\n\nimport warnings\n\nimport math\nimport torch\nfrom time import time\nimport numpy as np\nimport pynvml\n\n\n\nclass KMeans:\n\n    def __init__(self, n_clusters, max_iter=300, tol=0.0001, verbose=0, mode=\"euclidean\", init_method=\"kmeans++\",\n                 minibatch=None, n_init=None, algorithm=None, device=None):\n        self.n_clusters = n_clusters\n        self.max_iter = max_iter\n        self.tol = tol\n        self.verbose = verbose\n        self.mode = mode\n        self.init_method = init_method\n        self.minibatch = minibatch\n        self._loop = False\n        self._show = False\n\n        self.n_init = n_init\n\n        if algorithm is not None:\n            warnings.warn(\"The parameter algorithm is not valid in this implementation of KMeans. Default: 'lloyd'\")\n\n        try:\n            import pynvml\n            self._pynvml_exist = True\n        except ModuleNotFoundError:\n            self._pynvml_exist = False\n\n        self.device = device\n        self.cluster_centers_ = None\n        self.labels_ = None\n\n    @staticmethod\n    def cos_sim(a, b):\n        a_norm = a.norm(dim=-1, keepdim=True)\n        b_norm = b.norm(dim=-1, keepdim=True)\n        a = a / (a_norm + 1e-8)\n        b = b / (b_norm + 1e-8)\n        return a @ b.transpose(-2, -1)\n\n    @staticmethod\n    def euc_sim(a, b):\n        return 2 * a @ b.transpose(-2, -1) - (a ** 2).sum(dim=1)[..., :, None] - (b ** 2).sum(dim=1)[..., None, :]\n\n    def remaining_memory(self):\n        with torch.cuda.device(self.device):\n            torch.cuda.synchronize()\n            torch.cuda.empty_cache()\n        if self._pynvml_exist:\n            pynvml.nvmlInit()\n            gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(self.device.index)\n            info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n            remaining = info.free\n        else:\n            remaining = torch.cuda.memory_allocated()\n        return remaining\n\n    def max_sim(self, a, b):\n        batch_size = a.shape[0]\n        if self.mode == 'cosine':\n            sim_func = self.cos_sim\n        elif self.mode == 'euclidean':\n            sim_func = self.euc_sim\n\n        if self.device == 'cpu':\n            sim = sim_func(a, b)\n            max_sim_v, max_sim_i = sim.max(dim=-1)\n            return max_sim_v, max_sim_i\n        else:\n            if a.dtype == torch.double:\n                expected = a.shape[0] * a.shape[1] * b.shape[0] * 8\n            if a.dtype == torch.float:\n                expected = a.shape[0] * a.shape[1] * b.shape[0] * 4\n            elif a.dtype == torch.half:\n                expected = a.shape[0] * a.shape[1] * b.shape[0] * 2\n            ratio = math.ceil(expected / self.remaining_memory())\n            subbatch_size = math.ceil(batch_size / ratio)\n            msv, msi = [], []\n            for i in range(ratio):\n                if i * subbatch_size >= batch_size:\n                    continue\n                sub_x = a[i * subbatch_size: (i + 1) * subbatch_size]\n                sub_sim = sim_func(sub_x, b)\n                sub_max_sim_v, sub_max_sim_i = sub_sim.max(dim=-1)\n                del sub_sim\n                msv.append(sub_max_sim_v)\n                msi.append(sub_max_sim_i)\n            if ratio == 1:\n                max_sim_v, max_sim_i = msv[0], msi[0]\n            else:\n                max_sim_v = torch.cat(msv, dim=0)\n                max_sim_i = torch.cat(msi, dim=0)\n            return max_sim_v, max_sim_i\n\n    def fit_predict(self, X, sample_weight=None, centroids=None):\n        assert isinstance(X, torch.Tensor), \"input must be torch.Tensor\"\n        assert X.dtype in [torch.half, torch.float, torch.double], \"input must be floating point\"\n        assert X.ndim == 2, \"input must be a 2d tensor with shape: [n_samples, n_features] \"\n\n        batch_size, emb_dim = X.shape\n        X = X.to(self.device)\n        if sample_weight is None:\n            sample_weight = torch.ones(batch_size, device=self.device, dtype=X.dtype)\n        else:\n            sample_weight = sample_weight.to(self.device)\n        start_time = time()\n        cluster_centers_ = centroids\n        num_points_in_clusters = torch.ones(self.n_clusters, device=self.device, dtype=X.dtype)\n        closest = None\n        closest, cluster_centers_, i, sample_weight, sim_score = self.fit_loop(X, batch_size, closest, cluster_centers_,\n                                                                               num_points_in_clusters, sample_weight)\n\n        if self.verbose >= 1:\n            print(\n                f'used {i + 1} iterations ({round(time() - start_time, 4)}s) to cluster {batch_size} items into {self.n_clusters} clusters')\n\n        inertia = (sim_score * sample_weight).sum().neg()\n        return cluster_centers_.detach(), closest.detach(), inertia.detach()\n\n    def fit_loop(self, X, batch_size, closest, cluster_centers_, num_points_in_clusters, sample_weight):\n        for i in range(self.max_iter):\n            iter_time = time()\n            if self.minibatch is not None:\n                minibatch_idx = np.random.choice(batch_size, size=[self.minibatch], replace=False)\n                x = X[minibatch_idx]\n                sample_weight = sample_weight[minibatch_idx]\n            else:\n                x = X\n\n            sim_score, closest = self.max_sim(a=x, b=cluster_centers_)\n            matched_clusters, counts = closest.unique(return_counts=True)\n            unmatched_clusters = torch.where(\n                torch.ones(len(cluster_centers_), dtype=torch.bool, device=self.device).index_fill_(0,\n                                                                                                    matched_clusters.long(),\n                                                                                                    False) == True)[0]\n            while unmatched_clusters.shape[0] > 0:\n                worst_x = x[sim_score.argmin(dim=0)]\n                cluster_centers_[unmatched_clusters[0]] = worst_x\n                sim_score, closest = self.max_sim(a=x, b=cluster_centers_)\n                matched_clusters, counts = closest.unique(return_counts=True)\n                unmatched_clusters = torch.where(\n                    torch.ones(len(cluster_centers_), dtype=torch.bool, device=self.device).index_fill_(0,\n                                                                                                        matched_clusters.long(),\n                                                                                                        False) == True)[\n                    0]\n\n            c_grad = torch.zeros_like(cluster_centers_)\n            expanded_closest = closest[None].expand(self.n_clusters, -1)\n            mask = (expanded_closest == torch.arange(self.n_clusters, device=self.device)[:, None]).to(\n                X.dtype) \n            mask = mask * sample_weight[None, :]\n            c_grad = mask @ x / mask.sum(-1)[..., :, None]\n            c_grad[c_grad != c_grad] = 0  # remove NaNs\n\n            error = (c_grad - cluster_centers_).pow(2).sum()\n            if self.minibatch is not None:\n                lr = 1 / num_points_in_clusters[:, None] * 0.9 + 0.1\n            else:\n                lr = 1\n            num_points_in_clusters[matched_clusters] += counts\n            cluster_centers_ = cluster_centers_ * (1 - lr) + c_grad * lr\n            if self.verbose >= 2:\n                print('iter:', i, 'error:', error.item(), 'time spent:', round(time() - iter_time, 4))\n            if error <= self.tol:\n                break\n        return closest, cluster_centers_, i, sample_weight, sim_score\n\n    def predict(self, X):\n        assert isinstance(X, torch.Tensor), \"input must be torch.Tensor\"\n        assert X.dtype in [torch.half, torch.float, torch.double], \"input must be floating point\"\n        assert X.ndim == 2, \"input must be a 2d tensor with shape: [n_samples, n_features] \"\n\n        return self.max_sim(a=X, b=self.cluster_centers_)[1]\n\n    def fit(self, X, sample_weight=None, centroids=None):\n        assert isinstance(X, torch.Tensor), \"input must be torch.Tensor\"\n        assert X.dtype in [torch.half, torch.float, torch.double], \"input must be floating point\"\n        assert X.ndim == 2, \"input must be a 2d tensor with shape: [n_samples, n_features] \"\n\n        self.cluster_centers_, self.labels_, self.inertia_ = [], [], []\n\n        if centroids is None:\n            cluster_centers_ = [init_methods[self.init_method](X, self.n_clusters, self.minibatch) for _ in range(self.n_init)]\n        else:\n            cluster_centers_ = centroids\n        cluster_centers_ = torch.stack(cluster_centers_)\n\n        for i in range(self.n_init):\n            cluster_centers, labels, inertia = self.fit_predict(X, sample_weight, cluster_centers_[i])\n            self.cluster_centers_.append(cluster_centers)\n            self.labels_.append(labels)\n            self.inertia_.append(inertia)\n        best_cluster_idx = torch.argmin(torch.stack(self.inertia_))\n        self.cluster_centers_, self.labels_, self.inertia_ = self.cluster_centers_[best_cluster_idx].cpu().numpy(), self.labels_[best_cluster_idx].cpu().numpy(), self.inertia_[best_cluster_idx].cpu().numpy()\n        return self\n\nclass _InfiniteSampler(torch.utils.data.Sampler):\n    def __init__(self, sampler):\n        self.sampler = sampler\n\n    def __iter__(self):\n        while True:\n            for batch in self.sampler:\n                yield batch\n\nclass InfiniteDataLoader:\n    def __init__(self, dataset, weights, batch_size, num_workers, sequential=False, subset=None):\n        super().__init__()\n        self.dataset = dataset\n        if weights is not None:\n            sampler = torch.utils.data.WeightedRandomSampler(weights,\n                replacement=True,\n                num_samples=batch_size)\n        elif sequential:\n            if subset is None:\n                sampler = torch.utils.data.SequentialSampler(dataset)\n            else:\n                sampler = ActualSequentialSampler(subset)\n        elif subset is not None:\n            sampler = torch.utils.data.SubsetRandomSampler(subset)\n        else:\n            sampler = torch.utils.data.RandomSampler(dataset,\n                replacement=True)\n        self.sampler = sampler\n\n        if weights == None:\n            weights = torch.ones(len(dataset))\n\n        batch_sampler = torch.utils.data.BatchSampler(\n            sampler,\n            batch_size=batch_size,\n            drop_last=False)\n\n        self._infinite_iterator = iter(torch.utils.data.DataLoader(\n            dataset,\n            num_workers=num_workers,\n            batch_sampler=_InfiniteSampler(batch_sampler),\n            pin_memory=False,\n            persistent_workers=True if num_workers > 0 else False\n        ))\n\n    def __iter__(self):\n        while True:\n            yield next(self._infinite_iterator)\n\n    def __len__(self):\n        raise ValueError\n\nclass FastDataLoader:\n    \"\"\"DataLoader wrapper with slightly improved speed by not respawning worker\n    processes at every epoch.\"\"\"\n    def __init__(self, dataset, weights, batch_size, num_workers, sequential=False, subset=None):\n        super().__init__()\n        self.dataset = dataset\n        if weights is not None:\n            sampler = torch.utils.data.WeightedRandomSampler(weights,\n                replacement=False,\n                num_samples=batch_size)\n        elif sequential:\n            if subset is None:\n                sampler = torch.utils.data.SequentialSampler(dataset)\n            else:\n                sampler = ActualSequentialSampler(subset)\n        elif subset is not None:\n            sampler = torch.utils.data.SubsetRandomSampler(subset)\n        else:\n            sampler = torch.utils.data.RandomSampler(dataset,\n                replacement=False)\n        self.sampler = sampler\n\n        batch_sampler = torch.utils.data.BatchSampler(\n            sampler,\n            batch_size=batch_size,\n            drop_last=False\n        )\n\n        self._infinite_iterator = iter(torch.utils.data.DataLoader(\n            dataset,\n            num_workers=num_workers,\n            batch_sampler=_InfiniteSampler(batch_sampler),\n            pin_memory=False,\n            persistent_workers=True if num_workers > 0 else False\n        ))\n\n        self._length = len(batch_sampler)\n\n    def __iter__(self):\n        for _ in range(len(self)):\n            yield next(self._infinite_iterator)\n\n    def __len__(self):\n        return self._length\n\n\nclass ActualSequentialSampler(torch.utils.data.Sampler):\n    r\"\"\"Samples elements sequentially, always in the same order.\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    \"\"\"\n\n    def __init__(self, data_source):\n        self.data_source = data_source\n\n    def __iter__(self):\n        return iter(self.data_source)\n\n    def __len__(self):\n        return len(self.data_source)",
        "experimental_info": "Method: SimATTA\nObjective: Continuous optimization of a pre-trained model to streaming, continuously changing test data by actively selecting informative instances for labeling. Minimizes a combination of cross-entropy loss (for labeled samples) and an unsupervised loss (for unlabeled samples) under a defined budget.\n\nCatastrophic Forgetting Mitigation: Selective entropy minimization is employed.\n- Low-entropy samples: Pseudo-labeled by a frozen source-pretrained model (self.teacher) and included in training as 'source-like' data. The low-entropy threshold is controlled by `config.atta.SimATTA.el`.\n- High-entropy samples: Chosen for active labeling using an incremental clustering technique. The high-entropy threshold is controlled by `config.atta.SimATTA.eh`.\n\nActive Labeling Mechanism: Incremental clustering based on weighted K-means (`sample_select` method).\n- This clustering method selects representative 'anchors' to reduce redundancy and increase distribution coverage.\n- It adapts to newly seen distributions and merges old clusters within a budget (`n_clusters`).\n- The clustering can be GPU-accelerated (`config.atta.gpu_clustering`).\n- `KMeans` is used for clustering, where `sample_weight` for existing anchors is dynamically updated based on cluster counts, and new samples have a weight of 1.\n\nTraining Weights: Training weights are balanced in `cluster_train` to approximate the ratio of low-entropy (source-like) and actively labeled high-entropy samples. The `alpha` parameter is calculated as `target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())`.\n\nModel Initialization: The base model is pre-trained on a source environment using `train_on_env` from `AlgBase`.\n\nKey Parameters:\n- `eh`: High entropy threshold for selecting active learning candidates.\n- `el`: Low entropy threshold for pseudo-labeling source-like data.\n- `cold_start`: Number of steps/samples for an initial cold start phase where `alpha` (weight for target loss) is capped at 0.2.\n- `nc_increase`: The rate at which the number of clusters (`n_clusters`) increases per adaptation step (e.g., values swept from 0.25 to 3).\n- `LE`: A boolean flag (`0` or `1`) indicating whether to use low-entropy samples (`source_anchors`).\n- `target_cluster`: A boolean flag (`0` or `1`) indicating whether to use incremental clustering for target samples.\n- `lr`: Learning rate for the SGD optimizer during cluster training (`config.atta.SimATTA.lr`).\n- `steps`: Number of optimization steps within each adaptation round (`config.atta.SimATTA.steps`).\n- `stop_tol`: Stopping tolerance for the cluster training loop based on a moving average of the loss.\n- `gpu_clustering`: Boolean flag to enable GPU for KMeans clustering.\n\nDatasets for experiments (as seen in `ATTA/kernel/launch.py`):\n- VLCS (swept `el` for this dataset).\n- PACS (mentioned as commented out example).\n\nModel Architecture: Inherits from `AlgBase`, typically uses a ResNet backbone (e.g., `resnet18` or `resnet50`) with a linear classifier."
      }
    },
    {
      "title": "Persistent Test-time Adaptation in Recurring Testing Scenarios",
      "abstract": "Current test-time adaptation (TTA) approaches aim to adapt a machine learning\nmodel to environments that change continuously. Yet, it is unclear whether TTA\nmethods can maintain their adaptability over prolonged periods. To answer this\nquestion, we introduce a diagnostic setting - recurring TTA where environments\nnot only change but also recur over time, creating an extensive data stream.\nThis setting allows us to examine the error accumulation of TTA models, in the\nmost basic scenario, when they are regularly exposed to previous testing\nenvironments. Furthermore, we simulate a TTA process on a simple yet\nrepresentative $\\epsilon$-perturbed Gaussian Mixture Model Classifier, deriving\ntheoretical insights into the dataset- and algorithm-dependent factors\ncontributing to gradual performance degradation. Our investigation leads us to\npropose persistent TTA (PeTTA), which senses when the model is diverging\ntowards collapse and adjusts the adaptation strategy, striking a balance\nbetween the dual objectives of adaptation and model collapse prevention. The\nsupreme stability of PeTTA over existing approaches, in the face of lifelong\nTTA scenarios, has been demonstrated over comprehensive experiments on various\nbenchmarks. Our project page is available at https://hthieu166.github.io/petta.",
      "full_text": "Persistent Test-time Adaptation in Recurring Testing Scenarios Trung-Hieu Hoang1 Duc Minh Vo2 Minh N. Do1,3 1Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign 2The University of Tokyo 3VinUni-Illinois Smart Health Center, VinUniversity {hthieu, minhdo}@illinois.edu vmduc@nlab.ci.i.u-tokyo.ac.jp Abstract Current test-time adaptation (TTA) approaches aim to adapt a machine learn- ing model to environments that change continuously. Yet, it is unclear whether TTA methods can maintain their adaptability over prolonged periods. To answer this question, we introduce a diagnostic setting - recurring TTA where envi- ronments not only change but also recur over time, creating an extensive data stream. This setting allows us to examine the error accumulation of TTA models, in the most basic scenario, when they are regularly exposed to previous testing environments. Furthermore, we simulate a TTA process on a simple yet repre- sentative ϵ-perturbed Gaussian Mixture Model Classifier, deriving theoretical insights into the dataset- and algorithm-dependent factors contributing to gradual performance degradation. Our investigation leads us to propose persistent TTA (PeTTA), which senses when the model is diverging towards collapse and adjusts the adaptation strategy, striking a balance between the dual objectives of adaptation and model collapse prevention. The supreme stability of PeTTA over existing approaches, in the face of lifelong TTA scenarios, has been demonstrated over comprehensive experiments on various benchmarks. Our project page is available at https://hthieu166.github.io/petta. 1 Introduction Machine learning (ML) models have demonstrated significant achievements in various areas [18, 38, 47, 23]. Still, they are inherently susceptible to distribution-shift [46, 13, 48, 21, 6] (also known as the divergence between the training and testing environments), leading to a significant degradation in model performance. The ability to deviate from the conventional testing setting appears as a crucial aspect in boosting ML models’ adaptability when confronted with a new testing environment that has been investigated [ 30, 53, 14]. Among common domain generalization methods [ 58, 24, 1], test-time adaptation (TTA) takes the most challenging yet rewarding path that leverages unlabeled data available at test time for self-supervised adaptation prior to the final inference [57, 39, 8, 41, 59]. Early TTA studies have concentrated on a simply ideal adaptation scenario where the test samples come from a fixed single domain [57, 39, 41]. As a result, such an assumption is far from the ever- changing and complex testing environments. To confront continually changing environments [59, 12], Yuan et al. [61] proposed a practical TTA scenario where distribution changing and correlative sampling occur [15] simultaneously. Though practical TTA is more realistic than what the previous assumptions have made, it still assumes that any environment only appears once in the data stream, a condition which does not hold true. Taking a surveillance camera as an example, it might accom- modate varying lighting conditions recurringly day after day (Fig. 1-left). Based on this reality, we hypothesize that the recurring of those conditions may reveal the error accumulation phenomenon in TTA, resulting in performance degradation over a long period. To verify our hypothesis, we simulate a 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2311.18193v4  [cs.CV]  2 Nov 2024Testing Error Time Day 1 Illumination Condition Day 2 Day 3 0 50 100 150 200 250 300 0 0.2 0.4 0.6 0.8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 201 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Test-time adaptation step Testing Error No TTA RoTTA PeTTA (ours) Figure 1: Recurring Test-time Adaption (TTA). (left) Testing environments may change recurringly and preserving adaptability when visiting the same testing condition is not guaranteed. (right) The testing error of RoTTA [61] progressively raises (performance degradation) and exceeds the error of the source model (no TTA) while our PeTTA demonstrates its stability when adapting to the test set of CIFAR-10-C [19] 20 times. The bold lines denote the running mean and the shaded lines in the background represent the testing error on each domain (excluding the source model, for clarity). recurring testing environment and observe the increasing error rate by recurringly adapting to the test set of CIFAR-10-C [19] multiple times. We showcase the testing error of RoTTA [61] after 20 cycles of adaptation in Fig. 1-right. As expected, RoTTA can successfully adapt and deliver encouraging outcomes within the first few passes. However, this advantage is short-lived as our study uncovers a significant issue: TTA approaches in this setting may experience severe and persistent degradation in performance. Consequently, the testing error of RoTTA gradually escalates over time and quickly surpasses the model without adaptation. This result confirms the risk of TTA deployment in our illustrative scenario, as an algorithm might work well in the first place and gradually degenerate. Therefore, ensuring sustainable quality is crucial for real-world applications, especially given the recurring nature of testing environments. This study examines whether the adaptability of a TTA algorithm persists over an extended testing stream. Specifically, in the most basic scenario, where the model returns to a previously encountered testing environment after undergoing various adjustments. We thus propose a more general testing scenario than the practical TTA [61], namely recurring TTA, where the environments not only change gradually but also recur in a correlated manner over time. We first analyze a simulation using the ϵ−perturbed Gaussian Mixture Model Classifier (ϵ−GMMC) on a synthesized dataset and derive a theoretical analysis to confirm our findings, offering insights to tackle similar issues in deep neural networks. The analysis provides hints for reasoning the success of many recent robust continual TTA approaches [61, 12, 59, 15] and leading us to propose a simple yet effective baseline to avoid performance degradation, namely Persistent TTA (PeTTA). PeTTA continuously monitors the chance of collapsing and adjusts the adaptation strategy on the fly, striking a balance between the two objectives: adaptation and collapse prevention. Our contributions can be summarized as follows: • First, this work proposes a testing scenario - recurring TTA, a simple yet sufficient setup for diagnosing the overlooked gradual performance degradation phenomenon of TTA. • Second, we formally define the phenomenon of TTA collapsing and undertake a theoretical analysis on an ϵ-GMMC, shedding light on dataset-dependent and algorithm-dependent factors that contribute to the error accumulation during TTA processes. • Third, we introduce persistent TTA (PeTTA)- a simple yet effective adaptation scheme that surpasses all baseline models and demonstrates a persisting performance. For more context on related work, readers are directed to visit our discussions in Appdx. A. 2 Background Test-time Adaptation (TTA). A TTA algorithm operates on an ML classifier ft : X → Ywith parameter θt ∈ Θ (parameter space) gradually changing over time (t ∈ T) that maps an input image x ∈ Xto a category (label) y ∈ Y. Let the capital letters (Xt, Yt) ∈ X × Ydenote a pair of random variables with the joint distribution Pt(x, y) ∈ Pd, t∈ T. Here, Pd belongs to collection of D sets of testing scenarios (domains) {Pd}D d=1. The covariate shift [46] is assumed: Pt(x) and Pt′(x) 2could be different but Pt(y|x) = Pt′(y|x) holds ∀t ̸= t′. At t = 0, θ0 is initialized by a supervised model trained on P0 ∈ P0 (source dataset). The model then explores an online stream of testing data. For each t >0, it receives Xt (typically in form of a batch of Nt testing samples) for adapting itself ft−1 → ft before making the final prediction ft (Xt). TTA with Mean Teacher Update. To achieve a stable optimization process, the main (teacher) model ft are updated indirectly through a student model with parameters θ′ t [57, 61, 12, 15, 55]. At first, the teacher model in the previous step introduces a pseudo label [28] ˆYt for each Xt: ˆYt = ft−1(Xt). (1) With a classification loss LCLS (e.g., cross-entropy [16]), and a model parameters regularizer R, the student model is first updated with a generic optimization operatorOptim, followed by an exponential moving average (EMA) update of the teacher model parameter θt−1: θ′ t = Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011i + λR(θ′), (2) θt = (1 − α)θt−1 + αθ′ t, (3) with α ∈ (0, 1) - the update rate of EMA, andλ ∈ R+ - the weighting coefficient of the regularization term, are the two hyper-parameters. Practical TTA. In practical TTA [61], two characteristics of the aforementioned distribution of data stream are noticeable. Firstly, Pt’s can be partitioned by td’s in which {Pt}td t=td−1 ⊂ Pd. Here, each partition of consecutive steps follows the same underlying distribution which will change continually through D domains [59] (P1 → P2 ··· → PD). Secondly, the category distribution in each testing batch is temporally correlated [15]. This means within a batch, a small subset of categories is dominant over others, making the marginal distribution Pt(y) = 0, ∀y ̸∈ Yt ⊂ Yeven though the category distribution over all batches are balanced. Optimizing under this low intra-batch diversity (|Yt| ≪ |Y|) situation can slowly degenerate the model [7]. 3 Recurring TTA and Theoretical Analysis This section conducts a theoretical analysis on a concrete failure case of a simple TTA model. The results presented at the end of Sec. 3.2 will elucidate the factors contributing to the collapse (Sec. 3.1), explaining existing good practices (Sec. 3.3) and give insights into potential solutions (Sec. 4). 3.1 Recurring TTA and Model Collapse Recurring TTA.To study the gradual performance degradation (or model collapse), we propose anew testing scenario based on practical TTA [61]. Conducting a single pass through D distributions, as done in earlier studies [61, 59], may not effectively identify the degradation. To promote consistency, our recurring TTA performs revisiting the previous distributions K times to compare the incremental error versus the previous visits. For example, a sequence with K = 2 could be P1 → P2 → ··· → PD → P1 → P2 → ··· → PD. Appdx. D extends our justifications on constructing recurring TTA. Definition 1 (Model Collapse). A model is said to be collapsed from step τ ∈ T, τ <∞ if there exists a non-empty subset of categories ˜Y ⊂ Ysuch that Pr{Yt ∈ ˜Y} > 0 but the marginal Pr{ˆYt ∈ ˜Y} converges to zero in probability: lim t→τ Pr{ˆYt ∈ ˜Y} = 0. Here, upon collapsing, a model tends to ignore almost categories in ˜Y. As it is irrecoverable once collapsed, the only remedy would be resetting all parameters back to θ0. 3.2 Simulation of Failure and Theoretical Analysis Collapsing behavior varies across datasets and the adaptation processes. Formally studying this phenomenon on a particular real dataset and a TTA algorithm is challenging. Therefore, we propose a theoretical analysis on ϵ-perturbed binary Gaussian Mixture Model Classifier (ϵ-GMMC) that shares the typical characteristics by construction and demonstrates the same collapsing pattern in action (Sec. 5.1) as observed on real continual TTA processes (Sec. 5.3). 3Pseudo-label Predictor ˆYt = argmax y∈Y Pr(Xt|y;θt−1) Xt Mean-teacher Update θ′ t = Optim θ′∈Θ EPt h LCLS \u0010ˆYt, Xt;θ′\u0011i θt = (1−α)θt−1 +αθ′ t ϵt ··· θt−1 θt ··· Figure 2: ϵ-perturbed binary Gaussian Mix- ture Model Classifier, imitating a continual TTA algorithm for theoretical analysis. Two main components include a pseudo-label predictor (Eq. 1), and a mean teacher up- date (Eqs. 2, 3). The predictor is perturbed for retaining a false negative rate of ϵt to simulate an undesirable TTA testing stream. Simulated Testing Stream. Observing a testing stream with (Xt, Yt) ∈ X × Y= R × {0, 1} and the underlying joint distribution Pt(x, y) = py,t · N(x; µy, σ2 y). The main task is predicting Xt was sampled from cluster 0 or 1 (negative or positive). Conveniently, let py,t ∆ = Pt(y) = Pr(Yt = y) and ˆpy,t ∆ = Pr( ˆYt = y) be the marginal distribution of the true label Yt and pseudo label ˆYt. GMMC and TTA. GMMC first implies an equal prior distribution by construction which is desirable for the actual TTA algorithms (e.g., category-balanced sampling strategies in [ 61, 15]). Thus, it simplifies ft into a maximum likelihood estimation ft(x) = argmaxy∈Y Pr(x|y; θt) with Pr(x|y; θt) = N(x; ˆµy,t, ˆσ2 y,t). The goal is estimating a set of parameters θt = {ˆµy,t, ˆσ2 y,t}y∈Y. A perfect classifier θ0 = {µy, σ2 y}y∈Y is initialized at t = 0. For the consecutive steps, the simplicity of GMMC allows solving the Optim (for finding θ′ t, Eq. 2) perfectly by computing the empirical mean and variance of new samples, approximating EPt. The mean teacher update (Eq. 3) for GMMC is: ˆµy,t = ( (1 − α)ˆµy,t−1 + αEPt h Xt|ˆYt i if ˆYt = y ˆµy,t−1 otherwise . (4) The update of ˆσ2 y,t is similar. ˆYt = ft−1(Xt) can be interpreted as a pseudo label (Eq. 1). ϵ-GMMC. Severe distribution shifts or low intra-batch category diversity of recurring TTA/practical TTA both result in an increase in the error rate of the predictor . Instead of directly modeling the dynamic changes of py,t (which can be complicated depending on the dataset), we study an ϵ−pertubed GMMC (ϵ−GMMC), where py,t is assumed to be static (defined below) and the pseudo- label predictor of this model is perturbed to simulate undesirable effects of the testing stream on the predictor. Two kinds of errors appear in a binary classifier [4]. Let ϵt = Pr{Yt = 1|ˆYt = 0} (5) be the false negative rate (FNR) of the model at step t. Without loss of generality, we study the increasing type II collapse of ϵ-GMMC. By intentionally flipping the true positive pseudo labels in simulation, an FNR of ϵt is maintained (Fig. 2). Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution Ber(p0): p0,t = p0, (p1,t = p1 = 1 − p0), ∀t ∈ T. Lemma 1 (Increasing FNR). Under Assumption 1, a binary ϵ-GMMC would collapsed (Def. 1) with lim t→τ ˆp1,t = 0 (or lim t→τ ˆp0,t = 1, equivalently) if and only if lim t→τ ϵt = p1. Lemma 1 states the negative correlation between ˆp1,t and ϵt. Unsurprisingly, towards the collapsing point where all predictions are zeros, the FNR also increases at every step and eventually reaches the highest possible FNR of p1. Lemma 2 (ϵ-GMMC After Collapsing ). For a binary ϵ-GMMC model, with Assumption 1, if lim t→τ ˆp1,t = 0 (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters: N(ˆµ0,t, ˆσ2 0,t) d. → N(p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2). Lemma 2 states the resulting ϵ−GMMC after collapsing. Cluster 0 now covers the whole data distribution (and assigning label 0 for all samples). Furthermore, collapsing happens when ˆµ0,t moves toward µ1. We next investigate the factors and conditions for this undesirable convergence. 4Theorem 1 (Convergence of ϵ−GMMC). For a binary ϵ-GMMC model, with Assumption 1, let the distance from ˆµ0,t toward µ1 is d0→1 t = |EPt [ˆµ0,t] − µ1|, then: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . From Thm. 1, we observe that the distance d0→1 t ’s converges (also indicating the convergence to the distribution in Lemma 2) if d0→1 t < d0→1 t−1 . The model collapse happens when this condition holds for a sufficiently long period. Corollary 1 (A Condition forϵ−GMMC Collapse). With fixedp0, α, µ0, µ1, ϵ−GMMC is collapsed if there exists a sequence of {ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) such that: p1 ≥ ϵt > 1 − d0→1 t−1 |µ0 − µ1|, t ∈ [τ − ∆τ , τ]. Corollary 1 introduces a condition ϵ-GMMC collapse. Here, ϵt’s are non-decreasing, lim t→τ ϵt = p1. Remarks. Thm. 1 concludes two sets of factors contributing to collapse: (i) data-dependent factors: the prior data distribution (p0), the nature difference between two categories (|µ0 − µ1|); and (ii) algorithm-dependent factors: the update rate (α), the FNR at each step (ϵt). ϵ-GMMC analysis sheds light on explaining model collapse on real datasets (Sec. 5.3), reasons the existing approaches (Sec. 3.3) and motivates the development of our baseline (Sec. 4). 3.3 Connection to Existing Solutions Prior TTA algorithms have already incorporated implicit mechanisms to mitigate model collapse. The theoretical results in the previous section explain the rationale behind these effective strategies. Regularization Term for θt. Knowing that f0 is always well-behaved, an attempt is restricting the divergence of θt from θ0, e.g. using R(θt) ∆ = ∥θ0 − θt∥2 2 regularization [40]. The key idea is introducing a penalty term to avoid an extreme divergence as happening in Thm. 1. Memory Bank for Harmonizing Pt(x). Upon receiving Xt, samples in this batch are selectively updated to a memory bank M (which already contains a subset of some instances ofXt′, t′ < tin the previous steps). By keeping a balanced number of samples from each category, distribution PM t (y) of samples in M is expected to have less zero entries than Pt(y), making the optimization step over PM t more desirable. From Thm. 1, M moderates the extreme value of the category distribution (p0 term) which typically appears on batches with low intra-batch category diversity. 4 Persistent Test-time Adaptation (PeTTA) Now we introduce our Persistent TTA (PeTTA) approach. Further inspecting Thm. 1, while ϵt (Eq. 5) is not computable without knowing the true labels, the measure of divergence from the initial distribution (analogously to d0→1 t−1 term) can provide hints to fine-tune the adaptation process. Key Idea. A proper adjustment toward the TTA algorithm can break the chain of monotonically increasing ϵt’s in Corollary 1 to prevent the model collapse. In the mean teacher update, the larger value of λ (Eq. 2) prioritizes the task of preventing collapse on one hand but also limits its adaptability to the new testing environment. Meanwhile, α (Eq. 3) controls the weight on preserving versus changing the model from the previous step. Drawing inspiration from the exploration-exploitation tradeoff [49, 25] encountered in reinforcement learning [54], we introduce a mechanism for adjusting λ and α on the fly, balancing between the two primary objectives: adaptation and preventing model collapse. Our strategy is prioritizing collapse prevention (increasing λ) and preserving the model from previous steps (decreasing α) when there is a significant deviation from θ0. In [40, 61, 59], λ and α were fixed through hyper-parameter tuning. This is suboptimal due to varying TTA environments and the lack of validation set [62]. Furthermore, Thm. 1 suggests the convergence rate quickly escalates when ϵt increases, making constant λ, αinsufficient to prevent collapse. Sensing the Divergence of θt. We first equip PeTTA with a mechanism for measuring its divergence from θ0. Since ft(x) = argmax y∈Y Pr(y|x; θt), we can decompose Pr(y|x; θt) = [h (ϕθt(x))]y, with ϕθt(·) is a θt-parameterized deep feature extractor followed by a fixed classification head (a linear and softmax layer) h(·). The operator [·]y extracts the yth component of a vector. 5Since h(·) remains unchanged, instead of comparing the divergence in the parameter space (Θ) or between the output probability Pr(y|x; θt) and Pr(y|x; θ0), we suggest an inspection over the feature embedding space that preserves a maximum amount of information in our case (data processing inequality [9]). Inspired by [31] and under Gaussian assumption, the Mahalanobis distance of the first moment of the feature embedding vectors is compared. Let z = ϕθt(x), we keep track of a collection of the running mean of feature vector z: {ˆµy t }y∈Y in which ˆµy t is EMA updated with vector z if ft(x) = y. The divergence of θt at step t, evaluated on class y is defined as: γy t = 1 − exp \u0010 −(ˆµy t − µy 0)T (Σy 0)−1 (ˆµy t − µy 0) \u0011 , (6) where µy 0 and Σy 0 are the pre-computed empirical mean and covariant matrix of feature vectors in the source dataset (P0). The covariant matrix here is diagonal for simplicity. In practice, without directly accessing the training set, we assume a small set of unlabeled samples can be drawn from the source distribution for empirically computing these values (visit Appdx. E.4 for further details). Here, we implicitly expect the independence of each entry in z and TTA approaches learn to align feature vectors of new domains back to the source domain (P0). Therefore, the accumulated statistics of these feature vectors at each step should be concentrated near the vectors of the initial model. The value of γy t ∈ [0, 1] is close to 0 when θt = θ0 and increases exponentially as ˆµy t diverging from µy 0. Adaptive Regularization and Model Update. With α0, λ0 are initial values, utilizing γy t derived in Eq. 6, a pair of (λt, αt) is adaptively chosen at each step: ¯γt = 1 | ˆYt| X y∈ ˆYt γy t , ˆYt = n ˆY (i) t |i = 1, ··· , Nt o ; λt = ¯γt · λ0, α t = (1 − ¯γt) · α0, (7) ˆYt is a set of unique pseudo labels in a testing batch ( ˆY (i) t is the ith realization of ˆYt). Anchor Loss. Penalizing the divergence with regular vector norms in high-dimensional space (Θ) is insufficient (curse of dimensionality [5, 51]), especially with a large model and limited samples. Anchor loss LAL can nail down the similarity between ft and f0 in the probability space [32, 12]: LAL(Xt; θ) = − X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ), (8) which is equivalent to minimizing the KL divergence DKL (Pr(y|Xt; θ0)∥Pr(y|Xt; θ)). Persistent TTA.Having all the ingredients, we design our approach, PeTTA, following the convention setup of the mean teacher update, with the category-balanced memory bank and the robust batch normalization layer from [61]. Appdx. E.1 introduces the pseudo code of PeTTA. ForLCLS, either the self-training scheme [12] or the regular cross-entropy [16] is adopted. With R(θ), cosine similarity or L2 distance are both valid metrics for measuring the distance between θ and θ0 in the parameter space. Fisher regularizer coefficient [ 40, 27] can also be used, optionally. To sum up, the teacher model update of PeTTA is an elaborated version of EMA with λt, αt (Eq. 7) and LAL (Eq. 8): θ′ t = Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011 + LAL (Xt; θ′) i + λtR(θ′), θt = (1 − αt)θt−1 + αtθ′ t. 5 Experimental Results 5.1 ϵ−MMC Simulation Result Simulation Setup. A total of 6000 samples from two Gaussian distributions: N(µ0 = 0, σ2 0 = 1) and N(µ1 = 2, σ2 1 = 1) with p0 = p1 = 1 2 are synthesized and gradually released in a batch of B = 10 samples. For evaluation, an independent set of 2000 samples following the same distribution is used for computing the prediction frequency, and the false negative rate (FNR). ϵ−GMMC update follows Eq. 4 with α = 5e−2. To simulate model collapse, the predictor is intercepted and 10% of the true-postive pseudo labels at each testing step are randomly flipped (Corollary 1). Simulation Result. In action, both the likelihood of predicting class 0 (Fig. 3a-left) and theϵt (Eq. 5) (Fig. 3c-right, solid line) gradually increases over time as expected (Lemma 1). After collapsing, 60 120 240 360 480 6000 0.2 0.4 0.6 0.8 1 Testing Step (t) 0 120 240 360 480 6000 0.2 0.4 0.6 0.8 1 Testing Step (t) −4 −2 0 2 4x−4 −2 0 2 40 0.2 0.4 0.6 0.8 1 x Probability density N(µ0, σ0) N(µ1, σ1) N(ˆµ0, ˆσ0) N(ˆµ1, ˆσ1) 0 100 200 300 400 500 600 0.8 1.2 1.6 2.0 Testing step (t) |ˆµ0,t −µ1| Numerical Simulation Theoretical Result 0 100 200 300 400 500 600 0.1 0.2 0.3 0.4 0.5 Testing step (t) ϵt Prediction Frequency GMMCϵ-GMMC ϵ-GMMC GMMC (a) (b) (c) Figure 3: Simulation result on ϵ-perturbed Gaussian Mixture Model Classifier ( ϵ-GMMC) and GMMC (perturbed-free). (a) Histogram of model predictions through time. A similar prediction frequency pattern is observed on CIFAR-10-C (Fig. 5a-left). (b) The probability density function of the two clusters after convergence versus the true data distribution. The initial two clusters of ϵ-GMMC collapsed into a single cluster with parameters stated in Lemma 2. In the perturbed-free, GMMC converges to the true data distribution. (c) Distance toward µ1 (|EPt [ˆµ0,t] − µ1|) and false- negative rate (ϵt) in simulation coincides with the result in Thm. 1 (with ϵt following Corollary 1). ϵ-GMMC merges the two initial clusters, resulting in a single one (Fig. 3b-left) with parameters that match Lemma 2. The distance from ˆµ0,t (initialized at µ0) towards µ1 converges (Fig. 3c-left, solid line), coincided with the analysis in Thm. 1 when ϵt is chosen following Corollary 1 (Fig. 3c, dashed line). GMMC (perturbed-free) stably produces accurate predictions (Fig. 3a-right) and approximates the true data distribution (Fig. 3b-right). The simulation empirically validates our analysis (Sec. 3.2), confirming the vulnerability of TTA models when the pseudo labels are inaccurately estimated. 5.2 Setup - Benchmark Datasets Datasets. We benchmark the performance on four TTA classification tasks. Specifically, CIFAR10 → CIFAR10-C, CIFAR100→ CIFAR100-C, and ImageNet → ImageNet-C [19] are three corrupted images classification tasks (corruption level 5, the most severe). Additionally, we incorporate DomainNet [44] with 126 categories from four domains for the task real → clipart, painting, sketch. Compared Methods. Besides PeTTA, the following algorithms are investigated: CoTTA [ 59], EATA [40], RMT [12], MECTA [22], RoTTA [61], ROID [37] and TRIBE [52]. Noteworthy, only RoTTA is specifically designed for the practical TTA setting while others fit the continual TTA setting in general. A parameter-free approach: LAME [ 7] and a reset-based approach (i.e., reverting the model to the source model after adapting to every 1, 000 images): RDumb [45] are also included. Recurring TTA. Following the practical TTA setup, multiple testing scenarios from each testing set will gradually change from one to another while the Dirichlet distribution (Dir(0.1) for CIFAR10- C, DomainNet, and ImageNet-C, and Dir(0.01) for CIFAR100-C) generates category temporally correlated batches of data. For all experiments, we set the number of revisits K = 20 (times) as this number is sufficient to fully observe the gradual degradation on existing TTA baselines. Implementation Details. We use PyTorch [43] for implementation. RobustBench [10] and torchvision [35] provide pre-trained source models. Hyper-parameter choices are kept as close as possible to the original selections of authors. Visit Sec. G for more implementation details. Unless otherwise noted, for all PeTTA experiments, the EMA update rate for robust batch normalization [61] and feature embedding statistics is set to 5e−2; α0 = 1e−3 and cosine similarity regularizer is used. On CIFAR10/100-C and ImageNet-C we use the self-training loss in [ 12] for LCLS and λ0 = 10 while the regular cross-entropy loss [ 13] and λ0 = 1 (severe domain shift requires prioritizing 7Table 1: Average classification error of the task CIFAR-10→ CIFAR-10-C in recurring TTA. The lowest error is in bold,(∗)average value across 5 runs (different random seeds) is reported for PeTTA. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 43.5 43.5 LAME [7] 31.1 31.1 CoTTA [59]82.2 85.6 87.2 87.8 88.2 88.5 88.7 88.7 88.9 88.9 88.9 89.2 89.2 89.2 89.1 89.2 89.2 89.1 89.3 89.388.3EATA [40]81.6 87.0 88.7 88.7 88.9 88.7 88.6 89.0 89.3 89.6 89.5 89.6 89.7 89.7 89.3 89.6 89.6 89.8 89.9 89.488.8RMT [12]77.5 76.9 76.5 75.8 75.5 75.5 75.4 75.4 75.5 75.3 75.5 75.6 75.5 75.5 75.7 75.6 75.7 75.6 75.7 75.875.8MECTA [22]72.2 82.0 85.2 86.3 87.0 87.3 87.3 87.5 88.1 88.8 88.9 88.9 88.6 89.1 88.7 88.8 88.5 88.6 88.3 88.886.9RoTTA [61]24.6 25.5 29.6 33.6 38.2 42.8 46.2 50.6 52.2 54.1 56.5 57.5 59.4 60.2 61.7 63.0 64.8 66.1 68.2 70.351.3RDumb [45]31.1 32.1 32.3 31.6 31.9 31.8 31.8 31.9 31.9 32.1 31.7 32.0 32.5 32.0 31.9 31.6 31.9 31.4 32.3 32.431.9ROID [37]72.7 72.6 73.1 72.4 72.7 72.8 72.7 72.7 72.9 72.8 72.9 72.9 72.8 72.5 73.0 72.8 72.5 72.5 72.7 72.772.7TRIBE [52]15.3 16.6 16.6 16.3 16.7 17.0 17.3 17.4 17.4 18.0 17.9 18.0 17.9 18.6 18.2 18.8 18.0 18.2 18.4 18.017.5PeTTA(ours)(∗) 24.323.022.622.422.422.522.322.522.822.822.622.722.722.922.622.722.622.822.923.022.8 adaptability) are applied in DomainNet experiments. In Appdx. F.5, we provide a sensitivity analysis on the choice of hyper-parameter λ0 in PeTTA. 5.3 Result - Benchmark Datasets Recurring TTA Performance. Fig. 1-right presents the testing error on CIFAR-10-C in recurring TTA setting. RoTTA [61] exhibits promising performance in the first several visits but soon raises and eventually exceeds the source model (no TTA). The classification error of compared methods on CIFAR-10→CIFAR-10-C, and ImageNet → ImageNet-C [19] tasks are shown in Tab. 1, and Tab. 2. Appdx. F.1 provides the results on the other two datasets. The observed performance degradation of CoTTA [59], EATA [40], RoTTA [61], and TRIBE [52] confirms the risk of error accumulation for an extensive period. While RMT [12], MECTA [22], and ROID [37] remain stable, they failed to adapt to the temporally correlated test stream at the beginning, with a higher error rate than the source model. LAME [7] (parameter-free TTA) and RDumb [45] (reset-based TTA) do not suffer from collapsing. However, their performance is lagging behind, and knowledge accumulation is limited in these approaches that could potentially favor a higher performance as achieved by PeTTA. Furthermore, LAME [7] is highly constrained by the source model, and selecting a precise reset frequency in RDumb [45] is challenging in practice (see Appdx. F.3 for a further discussion). 0 10 20 30 40 16 18 20 22 24 Recurring TTA Visit Classification Error PeTTA (ours) TRIBE [52] Figure 4: Classification error of TRIBE [ 52] and PeTTA (ours) of the task CIFAR-10→CIFAR10-C task in recurring TTA with 40 visits. In average, PeTTA outperforms almost every baseline approaches and persists across 20 vis- its over the three datasets. The only exception is at the case of TRIBE [ 52] on CIFAR-10- C. While this state-of-the-art model provides stronger adaptability, outweighing the PeTTA, and baseline RoTTA [61] in several recurrences, the risk of the model collapsing still presents in TRIBE [52]. This can be clearly observed when we increase the observation period to 40 recur- ring visits in Fig. 4. As the degree of freedom for adaptation in PeTTA is more constrained, it takes a bit longer for adaptation but remains sta- ble afterward. Fig. 5b-bottom exhibits the con- fusion matrix at the last visit with satisfactory accuracy. The same results are also observed when shuffling the order of domain shifts within each recurrence (Appdx. D.3), or extending the number of recurrences to 40 visits (Appdx. F.4). Continuously Changing Corruption (CCC) [45] Performance. Under CCC [45], Tab. 3 reveals the supreme performance of PeTTA over RoTTA [61] and RDumb [45]. Here, we report the average classification error between two consecutive adaptation step intervals. An adaptation step in this table corresponds to a mini-batch of data with 64 images. The model is adapted to 80, 000 steps in total with more than 5.1M images, significantly longer than 20 recurring TTA visits. Undoubtedly, PeTTA still achieves good performance where the corruptions are algorithmically generated, non-cyclic with two or more corruption types can happen simultaneously. This experiment also empirically justifies the construction of our recurring TTA as a diagnostic tool (Appdx. D.2) where similar observations are concluded on the two settings. Obviously, our recurring TTA is notably simpler than CCC [45]. 8Table 2: Average classification error of the task ImageNet → ImageNet-C in recurring TTA scenario. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 82.0 82.0 LAME [7] 80.9 80.9 CoTTA [59]98.6 99.1 99.4 99.4 99.5 99.5 99.5 99.5 99.6 99.7 99.6 99.6 99.6 99.6 99.6 99.6 99.6 99.6 99.7 99.799.5EATA [40]60.4 59.3 65.4 72.6 79.1 84.2 88.7 92.7 95.2 96.9 97.7 98.1 98.4 98.6 98.7 98.8 98.8 98.9 98.9 99.089.0RMT [12]72.3 71.0 69.9 69.1 68.8 68.5 68.4 68.3 70.0 70.2 70.1 70.2 72.8 76.8 75.6 75.1 75.1 75.2 74.8 74.771.8MECTA [22]77.2 82.8 86.1 87.9 88.9 89.4 89.8 89.9 90.0 90.4 90.6 90.7 90.7 90.8 90.8 90.9 90.8 90.8 90.7 90.889.0RoTTA [61]68.3 62.1 61.8 64.5 68.4 75.4 82.7 95.1 95.8 96.6 97.1 97.9 98.3 98.7 99.0 99.1 99.3 99.4 99.5 99.687.9RDumb [45]72.2 73.0 73.2 72.8 72.2 72.8 73.3 72.7 71.9 73.0 73.2 73.1 72.0 72.7 73.3 73.1 72.1 72.6 73.3 73.172.8ROID [37]62.7 62.3 62.3 62.3 62.5 62.3 62.4 62.4 62.3 62.6 62.5 62.3 62.5 62.4 62.5 62.4 62.4 62.5 62.4 62.562.4TRIBE [52]63.664.0 64.9 67.8 69.6 71.7 73.5 75.5 77.4 79.8 85.0 96.5 99.4 99.8 99.9 99.8 99.8 99.9 99.9 99.984.4PeTTA(ours)(∗) 65.361.759.859.159.459.659.859.359.460.060.361.060.760.460.660.760.860.760.460.260.5 Table 3: Average classification error on CCC [45] setting. Each column presents the average error within an adaptation interval (e.g., the second column provides the average error between the 6701 and 13400 adaptation steps). Each adaptation step here is performed on a mini-batch of 64 images. CCC [45] Adaptation Step− − − − − − − − − − − − − − − − − − − − − − − − − → Method6700 13400 20100 26800 33500 40200 46900 53600 60200 66800 73400 80000Avg Source 0.83 0.83 0.83 0.83 0.83 0.84 0.84 0.83 0.84 0.83 0.83 0.83 0.83 RoTTA [61]0.70 0.85 0.92 0.96 0.98 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.95 RDumb [45]0.78 0.74 0.75 0.77 0.75 0.72 0.75 0.77 0.75 0.74 0.75 0.75 0.75 PeTTA(ours) 0.67 0.63 0.62 0.65 0.65 0.64 0.64 0.68 0.63 0.63 0.65 0.65 0.64 0.46 0.44 0.4 0.43 0.46 0.47 0.44 0.43 0.48 0.4 0.43 0.43 0.41 airplane bird cat dog frog ship auto deer horse truck 0.13 0.34 0.44 0.32 0.14 0.44 0.51 0.46 0.46 0.34 airplane bird catdog frog ship auto deer horse truck Inter-category cosine similarity (source model)Misclassification rate of collapsed RoTTA 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.69 0 0 0 0.08 0 0.04 0 0.13 0.05 0.34 0.17 0 0 0.1 0 0.03 0 0.23 0.13 0.24 0 0.1 0.03 0.44 0 0.11 0 0.07 0.01 0.21 0 0 0.11 0.32 0 0.21 0 0.14 0.01 0.14 0 0 0.01 0.71 0 0.06 0.01 0.06 0.01 0.21 0 0 0.07 0.44 0 0.16 0 0.1 0.01 0.05 0 0 0.08 0.51 0 0.25 0 0.1 0.01 0.17 0 0 0.03 0.46 0 0.04 0.21 0.06 0.04 0.46 0 0 0.01 0.06 0 0.03 0 0.41 0.03 0.34 0 0 0 0.12 0 0.03 0 0.18 0.32 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.05 0.02 0.01 0 0 0 0.07 0.19 0.05 0.59 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.84 0 0.01 0.01 0.01 0 0.06 0.06 0.08 0.02 0.02 0.74 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.05 0.02 0.03 0.01 0 0.02 0.01 0.04 0.82 1 5 10 15 200 0.2 0.4 0.6 0.8 1 Visits 1 5 10 15 200 0.2 0.4 0.6 0.8 1 Visits RoTTA [61] PeTTA (ours) PeTTA (ours) - 20th visit RoTTA [61] - 20th visit Predicted label (a) (b)(c) True labelTrue label Prediction Frequency Figure 5: Recurring TTA (20 visits) on CIFAR-10 →CIFAR10-C task. (a) Histogram of model predictions (10 labels are color-coded). PeTTA achieves a persisting performance while RoTTA [61] degrades. (b) Confusion matrix at the last visit, RoTTA classifies all samples into a few categories (e.g., 0: airplane, 4: deer). (c) Force-directed graphs showing (left) the most prone to misclassification pairs (arrows indicating the portion and pointing from the true to the misclassified category); (right) similar categories tend to be easily collapsed. Edges denote the average cosine similarity of feature vectors (source model), only the highest similar pairs are shown. Best viewed in color. Collapsing Pattern. The rise in classification error (Fig. 1-right) can be reasoned by the prediction frequency of RoTTA [ 61] in an recurring TTA setting (Fig. 5a-left). Similar to ϵ-GMMC, the likelihood of receiving predictions on certain categories gradually increases and dominates the others. Further inspecting the confusion matrix of a collapsed model (Fig. 5b-top) reveals two major groups of categories are formed and a single category within each group represents all members, thereby becoming dominant. To see this, Fig. 5c-left simplifies the confusion matrix by only visualizing the 9Table 4: Average (across 20 visits) error of multiple variations of PeTTA: without (w/o) R(θ), LAL; LAL only; fixed regularization coefficient λ; adaptive coef- ficient λt, update rate αt; using anchor loss LAL. Method CF-10-CCF-100-CDN IN-C Baseline w/oR(θ),LAL 42.6 63.0 77.9 93.4 R(θ)fixedλ= 0.1λ0 43.3 65.0 80.0 92.5R(θ)fixedλ=λ0 42.0 64.6 66.6 92.9 LALonly 25.4 56.5 47.5 68.1 PeTTA -λt 27.1 55.0 59.7 92.7PeTTA -λt +αt 23.9 41.4 44.5 75.7PeTTA -λt +LAL 26.2 36.3 43.2 62.0 PeTTA -λt +αt +LAL 22.8 35.1 42.9 60.5 Table 5: Average (across 20 visits) error of PeTTA. PeTTA favors various choices of reg- ularizers R(θ): L2 and cosine similarity in conjunction with Fisher [27, 40] coefficient. Method CF-10-CCF-100-CDN IN-CR(θ) Fisher L2 ✗ 23.0 35.6 43.1 70.8✓ 22.7 36.0 43.9 70.0 Cosine ✗ 22.8 35.1 42.9 60.5✓ 22.6 35.9 43.3 63.8 CF: CIFAR, DN: DomainNet, IN: ImageNet top prone-to-misclassified pair of categories. Here, label deer is used for almost every living animal while airplane represents transport vehicles. The similarity between categories in the feature space of the source model (Fig. 5c-right) is correlated with the likelihood of being merged upon collapsing. As distance in feature space is analogous to |µ0 − µ1| (Thm. 1), closer clusters are at a higher risk of collapsing. This explains and showcases that the collapsing behavior is predictable up to some extent. 5.4 Ablation Study Effect of Each Component. Tab. 4 gives an ablation study on PeTTA, highlighting the use of a regularization term (R(θ)) with a fixed choice of λ, αnot only fails to mitigate model collapse but may also introduce a negative effect (rows 2-3). Trivially applying the anchor loss (LAL) alone is also incapable of eliminating the lifelong performance degradation in continual TTA (row 4). Within PeTTA, adopting the adaptiveλt scheme alone (row 5) or in conjunction with either αt or anchor loss LAL (rows 6-7) partially stabilizes the performance. Under the drastic domain shifts with a larger size of categories or model parameters (e.g., on CIFAR-100-C, DomainNet, ImageNet-C), restricting αt adjustment limits the ability of PeTTA to stop undesirable updates while a common regularization term without LAL is insufficient to guide the adaptation. Thus, leveraging all elements secures the persistence of PeTTA (row 8). Various Choices of Regularizers. The design of PeTTA is not coupled with any specific regu- larization term. Demonstrated in Tab. 5, PeTTA works well for the two common choices: L2 and cosine similarity. The conjunction use of Fisher coefficent [27, 40] for weighting the model parameter importance is also studied. While the benefit (in terms of improving accuracy) varies across datasets, PeTTA accommodates all choices, as the model collapse is not observed in any of the options. 6 Discussions and Conclusion On a Potential Risk of TTA in Practice. We provide empirical and theoretical evidence on the risk of deploying continual TTA algorithms. Existing studies fail to detect this issue with a single pass per test set. The recurring TTA could be conveniently adopted as astraightforward evaluation, where its challenging test stream magnifies the error accumulation that a model might encounter in practice. Limitations. PeTTA takes one step toward mitigating the gradual performance degradation of TTA. Nevertheless, a complete elimination of error accumulation cannot be guaranteed rigorously through regularization. Future research could delve deeper into expanding our efforts to develop an algorithm that achieves error accumulation-free by construction. Furthermore, as tackling the challenge of the temporally correlated testing stream is not the focus of PeTTA, using a small memory bank as in [61, 15] is necessary. It also assumes the features statistics from the source distribution are available (Appdx. E.3, E.4). These constraints potentially limit its scalability in real-world scenarios. Conclusion. Towards trustworthy and reliable TTA applications, we rigorously study theperformance degradation problem of TTA. The proposed recurring TTAsetting highlights the limitations of modern TTA methods, which struggle to prevent the error accumulation when continuously adapting to demanding test streams. Theoretically inspecting a failure case of ϵ−GMMC paves the road for designing PeTTA- a simple yet efficient solution that continuously assesses the model divergence for harmonizing the TTA process, balancing adaptation, and collapse prevention. 10Acknowledgements This work was supported by the Jump ARCHES Endowment through the Health Care Engineering Systems Center, JSPS/MEXT KAKENHI JP24K20830, ROIS NII Open Collaborative Research 2024-24S1201, in part by the National Institute of Health (NIH) under Grant R01 AI139401, and in part by the Vingroup Innovation Foundation under Grant VINIF.2021.DA00128. References [1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution gener- alization. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan, editors,Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=jlchsFOLfeF. [2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf. [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample se- lection for online continual learning. In Advances in Neural Information Processing Systems , volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf. [4] Amitav Banerjee, U. B. Chitnis, S. L. Jadhav, J. S. Bhawalkar, and S. Chaudhury. Hypothesis testing, type I and type II errors. Industrial Psychiatry Journal, 18(2):127–131, 2009. ISSN 0972-6748. doi: 10.4103/0972-6748.62274. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/. [5] Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1957. [6] Arno Blaas, Andrew Miller, Luca Zappella, Joern-Henrik Jacobsen, and Christina Heinze-Deml. Con- siderations for distribution shift robustness in health. In ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare, 2023. URL https://openreview.net/forum?id=y7XveyWYzIB. [7] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8334–8343, 2022. doi: 10.1109/CVPR52688.2022.00816. [8] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE International Conference on Computer Vision, 2022. [9] Thomas M. Cover and Joy A. Thomas.Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954. [10] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. URL https://openreview.net/forum?id=SSKZPJCt7B. [11] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3366–3385, 2022. doi: 10.1109/ TPAMI.2021.3057446. [12] Mario Döbler, Robert A. Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7704–7714, June 2022. [13] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of the 32nd International Conference on Machine Learning , volume 37 of Proceedings of Machine Learning Research , pages 1180–1189, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ganin15.html. [14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-Adversarial Training of Neural Networks, pages 189– 209. Springer International Publishing, 2017. doi: 10.1007/978-3-319-58347-1_10. URL https: //doi.org/10.1007/978-3-319-58347-1_10 . [15] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. NOTE: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems, 2022. 11[16] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In L. Saul, Y . Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems , volume 17, 2004. URL https://proceedings.neurips.cc/paper_files/paper/2004/file/ 96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf. [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015. [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1026–1034, 2015. [19] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019. [20] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. Proceedings of the International Conference on Learning Representations (ICLR), 2020. [21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 8320–8329, 2021. doi: 10.1109/ICCV48922.2021.00823. [22] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. MECTA: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=N92hjSf5NNh. [23] Fabian Isensee, Paul F. Jaeger, Simon A. A. Kohl, Jens Petersen, and Klaus H. Maier-Hein. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods, 18(2):203–211, February 2021. ISSN 1548-7105. doi: 10.1038/s41592-020-01008-z. URL https: //www.nature.com/articles/s41592-020-01008-z . [24] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wort- man Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 2427–2440, 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 1415fe9fea0fa1e45dddcff5682239a0-Paper.pdf. [25] Michael N. Katehakis and Arthur F. Veinott. The multi-armed bandit problem: Decomposition and compu- tation. Mathematics Operations Research, 12:262–268, 1987. URL https://api.semanticscholar. org/CorpusID:656323. [26] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412. 6980. [27] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks.Pro- ceedings of the National Academy of Sciences, 114(13):3521–3526, 2017. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/doi/abs/10.1073/pnas.1611835114. [28] Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks. ICML 2013 Workshop : Challenges in Representation Learning (WREPL), 07 2013. [29] T. Lee, S. Chottananurak, T. Gong, and S. Lee. Aetta: Label-free accuracy estimation for test-time adaptation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 28643–28652, Los Alamitos, CA, USA, jun 2024. IEEE Computer Society. doi: 10.1109/CVPR52733. 2024.02706. URL https://doi.ieeecomputersociety.org/10.1109/CVPR52733.2024.02706. [30] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [31] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In International Conference on Learning Representations Workshop, 2017. URL https://openreview.net/forum?id=BJuysoFeg. [32] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935–2947, 2018. doi: 10.1109/TPAMI.2017.2773081. [33] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML), pages 6028–6039, 2020. 12[34] Sen Lin, Peizhong Ju, Yingbin Liang, and Ness Shroff. Theory on forgetting and generalization of continual learning. In Proceedings of the 40th International Conference on Machine Learning, ICML’23, 2023. [35] TorchVision maintainers and contributors. Torchvision: Pytorch’s computer vision library. https: //github.com/pytorch/vision, 2016. [36] Robert A Marsden, Mario Döbler, and Bin Yang. Gradual test-time adaptation by self-training and style transfer. arXiv preprint arXiv:2208.07736, 2022. [37] Robert A Marsden, Mario Döbler, and Bin Yang. Universal test-time adaptation through weight ensem- bling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2555–2565, 2024. [38] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. [39] A. Tuan Nguyen, Thanh Nguyen-Tang, Ser-Nam Lim, and Philip Torr. TIPI: Test time adaptation with transformation invariance. In Conference on Computer Vision and Pattern Recognition 2023, 2023. URL https://openreview.net/forum?id=NVh1cy37Ge. [40] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In The Internetional Conference on Machine Learning, 2022. [41] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. [42] K. R. Parthasarathy. Introduction to Probability and Measure , volume 33 of Texts and Readings in Mathematics. Hindustan Book Agency, Gurgaon, 2005. ISBN 978-81-85931-55-5 978-93-86279-27-9. doi: 10.1007/978-93-86279-27-9. URL http://link.springer.com/10.1007/978-93-86279-27-9 . [43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. [44] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1406–1415, 2019. [45] Ori Press, Steffen Schneider, Matthias Kuemmerer, and Matthias Bethge. RDumb: A simple approach that questions our progress in continual test-time adaptation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=VfP6VTVsHc. [46] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset Shift in Machine Learning. The MIT Press, 2009. ISBN 0262170051. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748–8763. PMLR, 18–24 Jul 2021. URL https://proceedings. mlr.press/v139/radford21a.html. [48] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research, pages 5389–5400. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ recht19a.html. [49] Mooweon Rhee and Tohyun Kim. Exploration and Exploitation, pages 543–546. Palgrave Macmillan UK, London, 2018. ISBN 978-1-137-00772-8. doi: 10.1057/978-1-137-00772-8_388. URL https: //doi.org/10.1057/978-1-137-00772-8_388 . [50] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In Interna- tional Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= B1gTShAct7. [51] Tanin Sirimongkolkasem and Reza Drikvandi. On Regularisation Methods for Analysis of High Di- mensional Data. Annals of Data Science , 6(4):737–763, December 2019. ISSN 2198-5812. doi: 10.1007/s40745-019-00209-4. URL https://doi.org/10.1007/s40745-019-00209-4 . 13[52] Yongyi Su, Xun Xu, and Kui Jia. Towards real-world test-time adaptation: Tri-net self-training with balanced normalization. Proceedings of the AAAI Conference on Artificial Intelligence, 38(13):15126– 15135, 2024. [53] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9229–9248. PMLR, 13–18 Jul 2020. URL https://proceedings. mlr.press/v119/sun20b.html. [54] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 2018. [55] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 1195–1204, 2017. ISBN 9781510860964. [56] Daniel Vela, Andrew Sharp, Richard Zhang, Trang Nguyen, An Hoang, and Oleg S. Pianykh. Temporal quality degradation in AI models. Scientific Reports, 12(1):11654, July 2022. ISSN 2045-2322. doi: 10.1038/s41598-022-15245-z. URL https://www.nature.com/articles/s41598-022-15245-z . [57] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. [58] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin. Generalizing to unseen domains: A survey on domain generalization. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4627–4635. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/628. URL https://doi.org/10. 24963/ijcai.2021/628. Survey Track. [59] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7201–7211, June 2022. [60] Zachary Young and Robert Steele. Empirical evaluation of performance degradation of machine learning-based predictive models – a case study in healthcare information systems. International Journal of Information Management Data Insights , 2(1):100070, 2022. ISSN 2667-0968. doi: https: //doi.org/10.1016/j.jjimei.2022.100070. URL https://www.sciencedirect.com/science/article/ pii/S2667096822000143. [61] Longhui Yuan, Binhui Xie, and Shuang Li. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15922– 15932, 2023. [62] Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML , 2023. URL https: //openreview.net/forum?id=0Go_RsG_dYn. 14Persistent Test-time Adaptation in Recurring Testing Scenarios Technical Appendices Table of Contents A Related Work 16 B Proof of Lemmas and Theorems 16 B.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2 Proof of Lemma 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.3 Proof of Theorem 1 and Corollary 1. . . . . . . . . . . . . . . . . . . . . . . . 18 C Further Justifications on Gaussian Mixture Model Classifier 19 D Further Justifications on the Recurring Testing Scenario 20 D.1 Recurring TTA Follows the Design of a Practical TTA Stream . . . . . . . . . . 20 D.2 Recurring TTA as a Diagnostic Tool . . . . . . . . . . . . . . . . . . . . . . . . 20 D.3 Recurring TTA with Random Orders . . . . . . . . . . . . . . . . . . . . . . . 20 E Further Justifications on Persistent TTA (PeTTA) 21 E.1 Pseudo Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 E.2 Anchor Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 E.3 The Use of the Memory Bank . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset . 23 E.5 Novelty of PeTTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F Additional Experimental Results of PeTTA 24 F.1 Performance of PeTTA Versus Compared Methods . . . . . . . . . . . . . . . . 24 F.2 An Inspection of PeTTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.3 Does Model Reset Help? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.4 PeTTA with 40 Recurring Visits . . . . . . . . . . . . . . . . . . . . . . . . . . 27 F.5 The Sensitivity of Hyper-parameter Choices in PeTTA . . . . . . . . . . . . . . 27 F.6 More Details on the Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . 27 F.7 More Confusion Matrices in Recurring TTA Setting . . . . . . . . . . . . . . . 29 G Experimental Details 29 G.1 Computing Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.2 Experiments on CCC Testing Stream . . . . . . . . . . . . . . . . . . . . . . . 29 G.3 Test-time Adaptation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.4 The Use of Existing Assets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 15A Related Work Towards Robust and Practical TTA. While forming the basis, early single-target TTA ap- proaches [53, 57, 39, 41, 33] is far from practice. Observing the dynamic of many testing envi- ronments, a continual TTA setting is proposed where an ML model continuously adapts to a sequence of multiple shifts [36, 59]. Meanwhile, recent studies [15, 7] point out that the category distribution realistic streams is highly temporally correlated. Towards real-world TTA setting, Yuanet al. [61] launch the practical TTA which considers the simultaneous occurrence of the two aforementioned challenges. For a robust and gradual adaptation, an update via the mean teacher [55] mechanism is exploited in many continual TTA algorithms [59, 61, 12, 22]. To moderate the temporally correlated test stream, common approaches utilize a small memory bank for saving a category-balanced subset of testing samples [15, 61], inspired by the replay methods [50, 2] to avoid forgetting in the task of continual learning [34, 3, 11]. Our study emphasizes another perspective: beyond a supreme performance, a desirable TTA should also sustain it for an extended duration. Temporal Performance Degradation.By studying the quality of various ML models across multiple industry applications [56, 60] the issue of AI “aging\" with the temporal model degradation progress, even with data coming from a stable process has been confirmed. In TTA, the continuous changes of model parameters through gradient descent aggravate the situation, as also recently noticed in [45]. Apart from observation, we attempt to investigate and provide theoretical insights towards the mechanism of this phenomenon. Accumulated Errors in TTA. In TTA, the issue of accumulated error has been briefly acknowledged. Previous works strive to avoid drastic changes to model parameters as a good practice. Up to some degree, it helps to avoid performance degradation. Nevertheless, it is still unclear whether their effectiveness truly eliminates the risk. To preserve in-distribution performance, regularization [27, 40] or replaying of training samples at test-time [ 12] have been used. Other studies explore reset (recovering the initial model parameters) strategies [59, 45], periodically or upon the running entropy loss approaches a threshold [ 41]. Unfortunately, knowledge accumulated in the preceding steps will vanish, and a bad heuristic choice of threshold or period leads to highly frequent model resets. Noteworthy, tuning those hyper-parameters is exceedingly difficult due to the unavailability of the validation set [62]. LAME [ 7] suggests a post-processing step for adaptation (without updating the parameters). This approach, however, still limits the knowledge accumulation. Our PeTTA is reset-free by achieving an adaptable continual test-time training. B Proof of Lemmas and Theorems In this section, we prove the theoretical results regarding the ϵ−perturbed Gaussian Mixture Model Classifier (ϵ−GMMC) introduced in Sec. 3.2. We first briefly summarize the definition of model collapse and the static data stream assumption: Definition 1 (Model Collapse). A model is said to be collapsed from step τ ∈ T, τ <∞ if there exists a non-empty subset of categories ˜Y ⊂ Ysuch that Pr{Yt ∈ ˜Y} > 0 but the marginal Pr{ˆYt ∈ ˜Y} converges to zero in probability: lim t→τ Pr{ˆYt ∈ ˜Y} = 0. Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution Ber(p0): p0,t = p0, (p1,t = p1 = 1 − p0), ∀t ∈ T. Preliminary. Following the same set of notations introduced in the main text, recall that we denoted py,t ∆ = Pr{Yt = y}, ˆpy,t ∆ = Pr{ˆYt = y} (marginal distribution of the true label Yt and pseudo label ˆYt receiving label y, respectively) and ϵt = Pr{Yt = 1|ˆYt = 0} (the false negative rate (FNR) of 16ϵ−GMMC). At testing step t, we obtain the following relations: EPt h Xt|ˆYt = 0 i = (1 − ϵt)µ0 + ϵtµ1, (9) EPt h Xt|ˆYt = 1 i = µ1, (10) VarPt \u0010 Xt|ˆYt = 0 \u0011 = (1 − ϵt)σ2 0 + ϵtσ2 1 + ϵt(1 − ϵt)(µ0 − µ1)2, (11) VarPt \u0010 Xt|ˆYt = 1 \u0011 = σ2 1. (12) In addition, under Assumption 1, the marginal distribution Pt(x) (also referred as data distribution in our setup) is: Pt(x) = N(x; p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2) ∀t ∈ T. (13) B.1 Proof of Lemma 1 Lemma 1 (Increasing FNR). Under Assumption 1, a binary ϵ-GMMC would collapsed (Def. 1) with lim t→τ ˆp1,t = 0 (or lim t→τ ˆp0,t = 1, equivalently) if and only if lim t→τ ϵt = p1. Proof. Under Assumption 1, we have EPt [Xt] = p0µ0 + (1 − p0)µ1. Also note that: EPt [Xt] = EPt h EPt h Xt|ˆYt ii = EPt h Xt|ˆYt = 0 i ˆp0,t + EPt h Xt|ˆYt = 1 i ˆp1,t (14) = [(1 − ϵt)µ0 + ϵtµ1] ˆp0,t + µ1(1 − ˆp0,t) = [(1 − ϵt)ˆp0,t] µ0 + [1 − ˆp0,t(1 − ϵt)] µ1 = p0µ0 + (1 − p0)µ1, where the second equality follows Eqs. 9-10. Therefore: ˆp0,t = p0 1 − ϵt . (15) Eq. 15 shows positive correlation between ˆp0,t and ϵt. Given lim t→τ ϵt = p1, taking the limit introduces: lim t→τ ˆp0,t = lim t→τ p0 1 − ϵt = p0 1 − p1 = 1. Similarly, having lim t→τ ˆp0,t = 1, the false negative rate ϵt when t → τ is: lim t→τ ϵt = 1 − p0 = p1. Since ˆp0,t + ˆp1,t = 1, lim t→τ ˆp1,t = 0, equivalently. Towards the collapsing point, the model tends to predict a single label (class 0 in the current setup). In addition, the FNR of the model ϵt also raises correspondingly. B.2 Proof of Lemma 2. Lemma 2 (ϵ-GMMC After Collapsing ). For a binary ϵ-GMMC model, with Assumption 1, if lim t→τ ˆp1,t = 0 (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters: N(ˆµ0,t, ˆσ2 0,t) d. → N(p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2). Proof. From Eqs. 9-10, under the increasing type II collapse of ϵ−GMMC setting, the perturbation does not affect the approximation of µ1. Meanwhile, when ϵt increases, one can expect that ˆµ0,t 17moves further away from µ0 toward µ1. Frist, the mean teacher model of GMMC (Eq. 4, main text) gives: EPt h ˆµ0,t|ˆYt = 1 i = EPt−1 [ˆµ0,t−1] , EPt h ˆµ0,t|ˆYt = 0 i = (1 − α)EPt−1 h ˆµ0,t−1|ˆYt = 0 i + αEPt h Xt|ˆYt = 0 i = (1 − α)EPt−1 [ˆµ0,t−1] + α \u0010 EPt h Xi|ˆYt = 0 i\u0011 , EPt h ˆµ1,t|ˆYt = 1 i = (1 − α)EPt−1 h ˆµ1,t−1|ˆYt = 1 i + αEPt h Xt|ˆYt = 1 i = (1 − α)EPt−1 [ˆµ1,t−1] + α \u0010 EPt h Xi|ˆYt = 1 i\u0011 , EPt h ˆµ1,t|ˆYt = 0 i = EPt−1 [ˆµ1,t−1] . By defining uy,t = EPt [ˆµy,t], we obtain the following recurrence relation between u0,t and u0,t−1: u0,t = EPt h ˆµ0,t|ˆYt = 0 i ˆp0,t + EPt h ˆµ0,t|ˆYt = 1 i ˆp1,t = \u0010 (1 − α)u0,t−1 + αEPt h Xt|ˆYt = 0 i\u0011 ˆp0,t + u0,t−1 ˆp1,t = [(1 − α)ˆp0,t + ˆp1,t] u0,t−1 + αˆp0,tEPt h Xt|ˆYt = 0 i = (1 − αˆp0,t)u0,t−1 + αˆp0,tEPt h Xt|ˆYt = 0 i = (1 − αˆp0,t)u0,t−1 + αˆp0,t [(1 − ϵt)µ0 + ϵtµ1] . (16) Given lim t→τ ˆp0,t = 1, it follows that lim t→τ ϵ0,t = p1 by Lemma 1. From this point: u0,t = (1 − α)u0,t−1 + α (p0µ0 + p1µ1) ∀t > τ. Taking the limit t → ∞: lim t→∞ u0,t = lim t→∞ (1 − α)u0,t−1 + α (p0µ0 + p1µ1) = lim t→∞ (1 − α)t ˆµ0,0 + α tX i=1 (1 − α)i−1 (p0µ0 + p1µ1) = lim t→∞ (1 − α)t ˆµ0,0 + (1 − (1 − α)t)(p0µ0 + p1µ1) = p0µ0 + p1µ1. The second equation is obtained by solving the recurrence relation. When lim t→τ ˆp0,t = 1, {ˆµy,t}y∈{0,1} becomes a deterministic values. Hence, giving uy,t = EPt [ˆµy,t] = ˆµ0,t(∀t > τ) and lim t→∞ ˆµ0,t = lim t→∞ u0,t = p0µ0 + p1µ1. (17) Repeating the steps above with Eqs. 11-12 in place of Eqs. 9-10, we obtain a similar result for σ2 0,t: lim t→∞ ˆσ2 0,t = p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2. (18) By Lévy’s continuity theorem (p. 302, [ 42]), from Eqs. 17-18, when t → ∞, the estimated distribution of the first cluster N(x; ˆµ0,tˆσ2 0,t) converges to the whole data distribution Pt(x) (Eq. 13) when collapsing. B.3 Proof of Theorem 1 and Corollary 1. Theorem 1 (Convergence of ϵ−GMMC). For a binary ϵ-GMMC model, with Assumption 1, let the distance from ˆµ0,t toward µ1 is d0→1 t = |EPt [ˆµ0,t] − µ1|, then: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . 18Proof. Substituting Eq. 15 into ˆp0,t of Eq. 16 gives: u0,t = \u0012 1 − αp0 1 − ϵt \u0013 u0,t−1 + αp0 1 − ϵt [(1 − ϵt)µ0 + ϵtµ1] . Hence, we have the distance from u0,t toward µ1: |u0,t − µ1| = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 u0,t−1 + αp0µ0 + αp0ϵtµ1 1 − ϵt − µ1 \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0µ0 + αp0ϵtµ1 1 − ϵt − αp0µ1 1 − ϵt \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0µ0 − αp0µ1(1 − ϵt) 1 − ϵt \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0(µ0 − µ1) \f\f\f\f ≤ \u0012 1 − αp0 1 − ϵt \u0013 |u0,t−1 − µ1| + αp0|µ0 − µ1|. The last inequality holds due to the triangle inequality. Equivalently, |u0,t − µ1| − |u0,t−1 − µ1| ≤α · p0 · \u0012 |µ0 − µ1| −|u0,t−1 − µ1| 1 − ϵt \u0013 . Let d0→1 t = |EPt [ˆµ0,t] − µ1|, we conclude that: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . Corollary 1 (A Condition forϵ−GMMC Collapse). With fixedp0, α, µ0, µ1, ϵ−GMMC is collapsed if there exists a sequence of {ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) such that: p1 ≥ ϵt > 1 − d0→1 t−1 |µ0 − µ1|, t ∈ [τ − ∆τ , τ]. Proof. Initialized at µ0, ϵ-GMMC is collapsing when ˆµ0,t converges to the mid-point p0µ0 + p1µ1 (Lemma 2), i.e., moving closer to µ1. From Thm. 1, the distance towards µ1 d0→1 t < d0→1 t−1 if |µ0 − µ1| −|u0,t−1 − µ1| 1 − ϵt < 0 ⇔ |µ0 − µ1| < |u0,t−1 − µ1| 1 − ϵt ⇔ ϵt > 1 − |u0,t−1 − µ1| |µ0 − µ1| . When there exists this sequence{ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) it follows that d0→1 t < d0→1 t−1 and ϵt > ϵt−1 is guaranteed ∀t ∈ [τ − ∆τ , τ]. Hence, lim t→τ ϵt = p1 (model collapsed, by Lemma 1). C Further Justifications on Gaussian Mixture Model Classifier One may notice that in ϵ-GMMC (Sec. 4.2), the classifier is defined ft(x) = argmaxy∈Y Pr(x|y; θt) (maximum likelihood estimation) while in general, ft(x) = argmaxy∈Y Pr(y|x; θt) (maximum a posterior estimation), parameterized by a neural network. In this case, since the equal prior (i.e., Pr(y; θt) = Pr(y′; θt), ∀y, y′ ∈ C) is enforced in ϵ-GMMC, the two definitions are equivalent. Proof. Having: argmaxy∈Y Pr(y|x; θt) = argmaxy∈Y Pr(x|y; θt) Pr(y; θt)P y′∈Y Pr(x|y′; θt) Pr(y′; θt) = argmaxy∈Y Pr(x|y; θt). We conclude that the two definitions are equivalent. In fact, it is well-known that maximum likelihood estimation is a special case of maximum a posterior estimation when the prior is uniform. 19D Further Justifications on the Recurring Testing Scenario D.1 Recurring TTA Follows the Design of a Practical TTA Stream Note that in recurring TTA, besides the recurrence of environments (or corruptions) as in [59, 40], the distribution of class labels is also temporally correlated (non-i.i.d.) as suggested by [15, 61] to reflect the practical testing stream better. In short, recurring TTA is formed by recurring the environments of practical TTA scenario introduced in [61] multiple times (readers are encouraged to visit the original paper for additional motivations on this scenario). D.2 Recurring TTA as a Diagnostic Tool Noticeably, CoTTA [59] also performed 10-round repetition across multiple domain shifts to simulate a lifelong TTA testing stream just like our recurring TTA. However, the key difference is CoTTA assumes the distribution of class labels is i.i.d., which does not hold in many real-life testing scenarios as argued in [ 15, 61]. Our recurring TTA lifts this assumption and allows temporally correlated (non-i.i.d.) label distribution (more challenging, more practical). This extension allows recurring TTA to spot the risk of model collapse on CoTTA [59] and other methods. The over-simplicity of the repeating scheme in CoTTA for spotting performance degradation is also suggested in [45]. Clearly, it seems not to be a problem at first glance in Tab. 5 of [59] (CoTTA’s 10-round repetition), but in fact, the risk in CoTTA remains, as explored in our scenario and also on CCC [45]. The construction of our recurring TTA is notably simple - a technical effort to extend the testing stream. However, this simplicity is on purpose, serving as a diagnostic tool for lifelong continual TTA. Counterintuitively, our experiments on four different tasks with the latest methods verify that even if the model is exposed to the same environment(the most basic case), their adaptability and performance are still consistently reduced (demonstrated visually in Fig. 1, quantitatively in Sec. 5.3). We believe that the extensive testing stream by recurrence in our setup is a simple yet sufficient scenario to demonstrate the vulnerability of existing continual TTA methods when facing the issue of model collapse (compared to CCC [45], a notably more complicated scenario than our recurring TTA). Indeed, recurring shifts are sufficient to show this failure mode and any lifelong TTA method should necessarily be able to handle recurring conditions. D.3 Recurring TTA with Random Orders Recall that in Sec. 3.1,recurring TTAis constructed by repeatingthe same sequence of D distributions K times. For example, a sequence with K = 2 could be P1 → P2 → ··· → PD → P1 → P2 → ··· → PD. For simplicity and consistency that promote reproducibility, the same order of image corruptions (following [61]) is used for all recurrences. This section presents supplementary experimental findings indicating that the order of image corruptions within each recurrence, indeed, does not affect the demonstration of TTA model collapse and the performance of our PeTTA. Experiment Setup. We refer to the setting same-order as using one order of image corruptions in [61] for all recurrences (specifically, on CIFAR-10/100-C and ImageNet-C:motion → snow → fog → shot → defocus → contrast → zoom → brightness → frost → elastic → glass → gaussian → pixelated → jpeg → impulse). Conversely, in random-order, the order of image corruptions is randomly shuffled at the beginning of each recurrence. Hence, the corruption orders across K recurrences are now entirely different. We redo the experiment of the second setting three times (with different random seeds = 0, 1, 2). Nevertheless, different TTA methods are ensured to be evaluated on the same testing stream, since it is fixed after generation. Without updating its parameters, the performance of the source model is trivially independent of the order of corruptions. Experimental Result. The experimental results are visualized in Fig. 6. The first column plots the experiments under the same-order, while the remaining three columns plot the experiments in the random-order setting, with varying random seeds. Note that the message conveyed by each sub-figure entirely matches that of Fig. 1-right. Discussions. Clearly, a similar collapsing pattern is observed in all three TTA tasks, with three combinations of 20 image corruption orders. This pattern also matches the easiest setting using the same order of image corruptions we promoted in recurring TTA. 201 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (a) CIFAR-10 → CIFAR-10-C task. 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (b) CIFAR-100 → CIFAR-100-C task. 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (c) ImageNet → ImageNet-C task. Figure 6: Recurring TTA with different order of corruptions. This figure plots the testing error of two TTA approaches: RoTTA - - [61], and, PeTTA- - (ours), and source model-×- as a reference performance under our recurring TTA (with 20 visits) across three TTA tasks. On the same-order experiments (column 1), the same order of image corruptions is applied for all 20 visits. Meanwhile, in random-order, this order is reshuffled at the beginning of each visit (columns 2-4). Random-order experiments are redone three times with different random seeds. Here, we empirically validate that using the same order of domain shifts (image corruptions) in our recurring TTA is sufficient to showcase the model collapse and evaluate the persistence of our PeTTA. Best viewed in color. E Further Justifications on Persistent TTA (PeTTA) E.1 Pseudo Code We summarize the key steps of our proposed PeTTA in Alg. 1, with the key part (lines 4-13) highlighted in blue. Our approach fits well in the general workflow of a TTA algorithm, enhancing the regular mean-teacher update step. Appdx. E.5 elaborates more on our contributions in PeTTA, distinguishing them from other components proposed in previous work. The notations and definitions of all components follow the main text (described in detail in Sec. 4). On line 8 of Alg. 1, as a 21Algorithm 1 Persistent TTA (PeTTA) Input: Classification model ft and its deep feature extractor ϕθt, both parameterized by θt ∈ Θ. Testing stream {Xt}T t=0, initial model parameter (θ0), initial update rate (α0), regularization term coefficient (λ0), empirical mean ({µy 0}y∈Y) and covariant matrix ({Σy 0}y∈Y) of feature vectors in the training set, ˆµy t EMA update rate (ν). 1 ˆµy 0 ← µy 0, ∀y ∈ Y; // Initialization 2 for t ∈ [1, ··· , T] do 3 ˆYt ← ft−1(Xt) ; // Obtaining pseudo-labels for all samples in Xt 4 // Persistent TTA (PeTTA) 5 ˆYt ← n ˆY (i) t |i = 1, ··· , Nt o ; // Set of (unique) pseudo-labels in Xt 6 ¯γt ← 0 ; 7 for y ∈ ˆYt do 8 γy t ← 1 − exp \u0010 −(ˆµy t − µy 0)T (Σy 0)−1 (ˆµy t − µy 0) \u0011 ; // Divergence sensing term on category y 9 ¯γt ← ¯γt + γy t | ˆYt| ; // Average divergence sensing term for step t 10 ˆµy t ← (1 − ν)ˆµy t−1 + νϕθt−1 (Xt|ˆYt = y) ; // EMA update of ˆµy t for samples with ˆYt = y 11 end 12 λt ← ¯γt · λ0 ; // Computing adaptive regularization term coefficient 13 αt ← (1 − ¯γt) · α0 ; // Computing adaptive update rate 14 // Regular Mean-teacher Update 15 θ′ t ← Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011 + LAL (Xt; θ′) i + λtR(θ′) ; // Student model update 16 θt ← (1 − αt)θt−1 + αtθ′ t. ; // Teacher model update 17 // Final prediction 18 yeild ft(Xt) ; // Returning the final inference with updated model ft 19 end shorthand notation, ϕθt−1 (Xt|ˆYt = y) denotes the empirical mean of all feature vectors of X(i) t (extracted by ϕθt−1 \u0010 X(i) t \u0011 ) if ˆY (i) t = y, i= 1, ··· , Nt in the current testing batch. E.2 Anchor Loss KL Divergence Minimization-based Interpretation of Anchor Loss. In Sec. 4, we claimed that minimizing the anchor loss LAL is equivalent to minimizing the relative entropy (or KL divergence) between the output probability of two models parameterized by θ0 and θ. Proof. Having: DKL (Pr(y|Xt; θ0)||Pr(y|Xt; θ)) = X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ0) Pr(y|Xt; θ) = − X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ) | {z } LAL(Xt;θ) −H(Pr(y|Xt; θ0))| {z } constant . Hence, argmin θ∈Θ LAL(Xt; θ) = argmin θ∈Θ DKL (Pr(y|Xt; θ0)||Pr(y|Xt; θ)) . 22Intuitively, a desirable TTA solution should be able to adapt to novel testing distributions on the one hand, but it should not significantly diverge from the initial model. LAL fits this purpose, constraining the KL divergence between two models at each step. Connections between Anchor Loss and Regularizer Term. While supporting the same objective (collapse prevention by avoiding the model significantly diverging from the source model), the major difference between Anchor loss ( LAL) and the Regularizer term ( R(θ)) is that the anchor loss operates on the probability space of model prediction while the regularizer term works on the model parameter spaces. Tab. 4 (lines 1 and 5) summarizes the ablation study when each of them is eliminated. We see the role of the regularization term is crucial for avoiding model collapse, while the anchor loss guides the adaptation under the drastic domain shift. Nevertheless, fully utilizing all components is suggested for maintaining TTA persistence. E.3 The Use of the Memory Bank The size of Memory Bank. The size of the memory bank in PeTTA is relatively small, equal to the size of one mini-batch for update (64 images, specifically). The Use of the Memory Bank in PeTTA is Fair with Respect To the Compared Methods.Our directly comparable method - RoTTA [61] also takes this advantage (referred to as category-balanced sampling, Sec. 3.2 of [ 61]). Hence, the comparison between PeTTA and RoTTA is fair in terms of additional memory usage. Noteworthy, the use of a memory bank is a common practice in TTA literature (e.g., [15, 8, 61]), especially in situations where the class labels are temporally correlated or non-i.i.d. distributed (as we briefly summarized in Appdx. A - Related Work section). CoTTA [59], EATA [40] and MECTA [ 22] (compared method) assume labels are i.i.d. distributed. Hence, a memory bank is unnecessary, but their performance under temporally correlated label distribution has dropped significantly as a trade-off. The RMT [12] (compared method) does not require a memory bank but it needs to cache a portion of the source training set for replaying (Sec. 3.3 in [12]) which even requires more resources than the memory bank. Eliminating the Need for a Memory Bank. As addressing the challenge of temporally correlated label distribution on the testing stream is not the focus of PeTTA, we have conveniently adopted the use of the memory bank proposed in [61]. Since this small additional memory requirement is not universally applied in every real-world scenario, we believe that this is a reasonable assumption, and commonly adopted in TTA practices. Nevertheless, exploring alternative ways for reducing the memory size (e.g., storing the embedded features instead of the original image) would be an interesting future direction. E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset Two Ways of Computing µy 0 and Σy 0 in Practice. One may notice that in PeTTA, computing γy t requires the pre-computed empirical mean (µy 0) and covariance (Σy 0) of the source dataset . This requirement may not be met in real-world situations where the source data is unavailable. In practice, the empirical mean and covariance matrix computed on the source distribution can be provided in the following two ways: 1. Most ideally, these values are computed directly by inference on the entire training set once the model is fully trained. They will be provided alongside the source-distribution pre-trained model as a pair for running TTA. 2. With only the source pre-trained model available, assume we can sample a set of unlabeled data from the source distribution. The (pseudo) labels for them are obtained by inferring from the source model. Since the source model is well-performed in this case, using pseudo is approximately as good as the true label. Accessing the Source Distribution Assumption in TTA. In fact, the second way is typically assumed to be possible in previous TTA methods such as EATA [40], and MECTA [22] (a compared method) to estimate a Fisher matrix (for anti-forgetting regularization purposes). Our work - PeTTA follows the same second setup as the previous approaches mentioned above. A variation of RMT [12] (a compared method) approach even requires having the fully labeled source data available at test-time for source replaying (Sec. 3.3 of [12]). This variation is used for comparison in our experiments. 23We believe that having the empirical mean and covariant matrix pre-computed on a portion of the source distribution in PeTTA is a reasonable assumption . Even in the ideal way, revealing the statistics might not severely violate the risk of data privacy leakage or require notable additional computing resources. Number of Samples Needed for Computation. To elaborate more on the feasibility of setting (2) mentioned above, we perform a small additional experiment on the performance of PeTTA while varying the number of samples used for computing the empirical mean and covariant matrix on the source distribution. In this setting, we use the test set of CIFAR-10, CIFAR-100, DomainNet validation set of ImageNet (original images, without corruption, or the real domain test set of DomainNet), representing samples from the source distribution. The total number of images is 10, 000 in CIFAR-10/A00, 50, 000 in ImageNet, and 69, 622 in DomainNet. We randomly sample 25%, 50%, 75%, and 100% of the images in this set to run PeTTA for 20 rounds of recurring. The result is provided in Tab. 6 below. Table 6: Average classification error of PeTTA (across 20 visits) with varying sizes of source samples used for computing feature empirical mean (µy 0) and covariant matrix (Σy 0). TTA Task 25% 50% 75% 100% CIFAR-10→CIFAR-10-C 22.96 22.99 23.03 22.75 CIFAR-100→CIFAR-100-C 35.01 35.11 35.09 35.15 DomainNet:real→clip→paint→sketch 43.18 43.12 43.15 42.89 ImageNet→ImageNet-C 61.37 59.68 61.05 60.46 The default choice of PeTTA is using 100% samples of the validation set of the source dataset. However, we showcase that it is possible to reduce the number of unlabeled samples from the source distribution to compute the empirical mean and covariant matrix for PeTTA, without significantly impacting its performance. E.5 Novelty of PeTTA PeTTA is composed of multiple components. Among them, the anchor loss is an existing idea (examples of previous work utilizing this idea are [ 32, 12]). Similarly, the mean-teacher update; and regularization are well-established techniques and very useful for the continual or gradual TTA scenario. Hence, we do not aim to improve or alternate these components. Nevertheless, the novelty of our contribution is the sensing of the divergence and adaptive model update, in which the importance of minimizing the loss (adaptation) and regularization (collapse prevention) is changed adaptively. In short, we propose a harmonic way of combining those elements adaptively to achieve a persistent TTA process. The design of PeTTA draws inspiration from a theoretical analysis (Sec. 3.2), empirically surpassing both the conventional reset-based approach [45] (Appdx. F.3) and other continual TTA approaches [61, 12, 59, 22, 7] on our proposed recurring TTA (Sec. 3.1, Appdx. F.1), as well as the previously established CCC [45] benchmark. F Additional Experimental Results of PeTTA F.1 Performance of PeTTA Versus Compared Methods Performance on CIFAR-100-C and Domainnet Datasets. Due to the length constraint, the classification errors on the tasks CIFAR-100→CIFAR-100-C, and real → clipart, painting, sketch of DomainNet are provided in Tab. 7 and Tab. 8. To prevent model collapse, the adaptability of PeTTA is more constrained. As a result, it requires more time for adaptation initially (e.g., in the first visit) but remains stable thereafter. Generally, consistent trends and observations are identified across all four TTA tasks. Standard Deviation of PeTTA Performance Across Multiple Runs. For PeTTA experiments marked with (*) in Tab. 1, Tab. 2, Tab. 7, and Tab. 8, the average performance across five independent runs with different random seeds is reported. Due to the space constraint, the corresponding standard deviation values are now reported in Tab. 9. Generally, the average standard deviation across runs 24Table 7: Average classification error of the task CIFAR-100 → CIFAR-100-C in recurring TTA scenario. The lowest error is highlighted in bold, (∗)average value across 5 runs (different random seeds) is reported for PeTTA. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 46.5 46.5 LAME [7] 40.5 40.5 CoTTA [59]53.4 58.4 63.4 67.6 71.4 74.9 78.2 81.1 84.0 86.7 88.8 90.7 92.3 93.5 94.7 95.6 96.3 97.0 97.3 97.683.1EATA [40]88.5 95.0 96.8 97.3 97.4 97.2 97.2 97.3 97.4 97.5 97.5 97.5 97.6 97.7 97.7 97.7 97.8 97.8 97.7 97.796.9RMT [12]50.5 48.6 47.9 47.4 47.3 47.1 46.9 46.9 46.6 46.8 46.7 46.5 46.5 46.6 46.5 46.5 46.5 46.5 46.5 46.547.1MECTA [22]44.8 44.3 44.6 43.1 44.8 44.2 44.4 43.8 43.8 43.9 44.6 43.8 44.4 44.6 43.9 44.2 43.8 44.4 44.9 44.244.2RoTTA [61]35.5 35.2 38.5 41.9 45.3 49.2 52.0 55.2 58.1 61.5 64.6 67.5 70.7 73.2 75.4 77.1 79.2 81.5 82.8 84.561.4RDumb [45]36.7 36.7 36.6 36.6 36.7 36.8 36.7 36.5 36.6 36.5 36.7 36.6 36.5 36.7 36.5 36.6 36.6 36.7 36.6 36.536.6ROID [37]76.4 76.4 76.2 76.2 76.3 76.1 75.9 76.1 76.3 76.3 76.6 76.3 76.8 76.7 76.6 76.3 76.2 76.0 75.9 76.076.3TRIBE [52]33.8 33.335.334.935.335.137.1 37.2 37.2 39.1 39.2 41.1 41.0 43.1 45.1 45.1 45.0 44.9 44.9 44.939.6PeTTA(ours)(∗) 35.834.434.735.035.135.135.235.335.335.335.235.335.235.235.135.235.235.235.235.235.1 Table 8: Average classification error of the task real → clipart → painting → sketch on DomainNet dataset in recurring TTA scenario. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 45.3 45.3 LAME [7] 45.6 45.6 CoTTA [59]96.2 97.1 97.4 97.8 98.1 98.2 98.4 98.4 98.4 98.5 98.6 98.6 98.6 98.6 98.6 98.7 98.7 98.7 98.7 98.798.3RMT [12]76.2 77.1 77.3 77.3 77.2 77.1 76.8 76.9 76.5 76.4 76.4 76.3 76.4 76.2 76.2 76.1 76.4 76.1 76.0 75.876.5MECTA [22]94.6 98.4 98.6 98.8 99.1 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.098.7RoTTA [61]44.3 43.8 44.7 46.7 48.7 50.8 52.7 55.0 57.1 59.7 62.7 65.1 68.0 70.3 72.7 75.2 77.2 79.6 82.6 85.362.1RDumb [45]44.3 44.4 44.3 44.5 44.2 44.2 44.3 44.5 44.4 44.2 44.3 44.3 44.3 44.3 44.5 44.3 44.2 44.3 44.4 44.344.3PeTTA(ours)(∗) 43.842.642.342.342.642.842.843.042.942.943.143.042.943.043.043.143.042.842.942.942.9 stays within ±0.1% for small datasets (CIFAR-10-C, CIFAR-100-C) and±0.5% for larger datasets (ImageNet-C, DomainNet). Table 9: Mean and standard deviation classification error of PeTTA on the four datasets: CIFAR-10-C (CF-10-C), CIFAR-100-C (CF-100-C), DomainNet (DN), and ImageNet-C (IN-C) with recurring TTA scenario. Each experiment is run 5 times with different random seeds. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Dataset1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg CF-10-C24.3 23.0 22.6 22.4 22.4 22.5 22.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8±0.4±0.3±0.4±0.3±0.3±0.3±0.4±0.2±0.3±0.4±0.4±0.2±0.1±0.3±0.5±0.2±0.2±0.3±0.4±0.5 ±0.1 CF-100-C35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1±0.4±0.4±0.2±0.2±0.1±0.1±0.2±0.2±0.1±0.2±0.1±0.2±0.2±0.1±0.1±0.1±0.1±0.1±0.2±0.2 ±0.1 DN43.8 42.6 42.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9±0.1±0.1±0.2±0.2±0.3±0.3±0.3±0.4±0.4±0.4±0.4±0.4±0.4±0.3±0.3±0.2±0.4±0.3±0.3±0.3 ±0.3 IN-C65.3 61.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5±0.6±0.5±0.5±0.5±1.4±1.1±1.0±0.5±0.8±0.9±0.4±0.8±0.9±0.8±0.9±0.8±1.0±0.6±0.6±0.7 ±0.5 F.2 An Inspection of PeTTA In Fig. 7, we showcase an inspection of our PeTTA on the task CIFAR-10→ CIFAR-10-C [19] in a typical recurring TTA with 20 visits. Specifically, the visualizations of PeTTA parameters ( ¯γt, λt, and αt), adaptation losses (LCLS, LAL) and regularization term (R(θ)) are provided. Here, we observe the values of adaptive parameters λt and αt continuously changing through time, as the testing scenarios evolve during recurring TTA. This proposed mechanismstabilizes the value of the loss functions, and regularization term, balancing between the two primary objectives: adaptation and preventing model collapse. Thus, the error rate persists as a result. A similar pattern is observed on other datasets (CIFAR-100-C [19] and DomainNet [44]). F.3 Does Model Reset Help? Experiment Setup. We use the term “model reset” to represent the action of “reverting the current TTA model to the source model” . This straightforward approach is named RDumb [ 45]. We thoroughly conducted experiments to compare the performance of RDumb with PeTTA. The implementation of RDumb in this setting is as follows. We employ RoTTA [61] as the base test-time adaptor due to the characteristics of the practical TTA [ 61] stream. The model (including model 25parameters, the optimizer state, and the memory bank) is reset after adapting itself to T images.1 For each dataset, three values of this hyper-parameter T are selected: • T = 1, 000: This is the value selected by the RDumb’s authors [ 45]. Unless specifically stated, we use this value when reporting the performance of RDumb [45] in all other tables. • T = 10, 000 (CIFAR-10/100-C), T = 5, 000 (ImageNet-C) and T = 24, 237 (Domain- Net).2 This value is equal to the number of samples in the test set of a single corruption type, i.e., the model is reset exactly after visiting each Pi’s (see Sec. 3.1 for notations). For DomainNet [44], since the number of images within each domain is unequal, the average number of images is used instead. • T = 150, 000 (CIFAR-10/100-C), T = 75, 000 (ImageNet-C) and T = 72, 712 (Domain- Net). This number is equal to the number of samples in one recurrence of our recurring TTA, i.e., the model is reset exactly after visitingP1 → ··· → PD. Here, D = 15 - types of corruptions [19] for CIFAR-10/100-C and ImageNet-C and D = 3 for DomainNet (clipart, painting, sketch). For example, the model is reset 20 times within a recurring TTA setting with 20 recurrences under this choice of T. The second and the last reset scheme could be interpreted as assuming the model has access to an oracle model with a capability of signaling the transitions between domains, or recurrences. Typically, this is an unrealistic capability in real-world scenarios, and a desirable continual TTA algorithm should be able to operate independently without knowing when the domain shift happening. Experimental Results. An empirical comparison between RDumb [45] and our PeTTA are reported in Tab. 10, Tab. 11, Tab. 12 and Tab. 13 for all four tasks. Table 10: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-10→ CIFAR-10-C task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100031.1 32.1 32.3 31.6 31.9 31.8 31.8 31.9 31.9 32.1 31.7 32.0 32.5 32.0 31.9 31.6 31.9 31.4 32.3 32.431.9T= 1000025.8 25.9 26.5 26.1 26.4 25.4 25.8 25.8 26.1 26.2 26.1 26.1 26.1 26.1 26.1 25.9 25.5 25.5 25.7 26.226.0T= 15000024.8 25.3 24.3 24.1 25.3 25.4 25.4 24.5 25.0 24.9 25.0 24.8 25.0 24.5 24.9 24.1 24.0 24.7 24.9 24.424.8 PeTTA(ours)(∗) 24.323.022.622.422.422.522.322.522.822.822.622.722.722.922.622.722.622.822.923.022.8 Table 11: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-100-C dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100036.7 36.7 36.6 36.6 36.7 36.8 36.7 36.5 36.6 36.5 36.7 36.6 36.5 36.7 36.5 36.6 36.6 36.7 36.6 36.536.6T= 1000043.5 43.6 43.7 43.7 43.4 43.5 43.6 43.4 43.5 43.6 43.8 43.5 43.5 43.6 43.4 43.6 43.5 43.8 43.7 43.643.6T= 15000035.435.4 35.4 35.3 35.4 35.4 35.5 35.6 35.4 35.4 35.535.3 35.235.435.135.835.135.6 35.3 35.835.4 PeTTA(ours)(∗) 35.834.434.735.035.135.135.235.335.335.335.235.335.235.235.135.235.235.235.235.235.1 Table 12: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on DomainNet dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100044.3 44.4 44.3 44.5 44.2 44.2 44.3 44.5 44.4 44.2 44.3 44.3 44.3 44.3 44.5 44.3 44.2 44.3 44.4 44.344.3T= 2423744.1 44.3 43.9 44.2 44.1 44.3 44.2 44.4 44.1 44.1 44.0 44.3 44.1 44.0 44.0 44.2 44.1 44.1 44.1 44.444.1T= 7271244.3 44.3 44.0 44.3 44.1 44.3 44.2 44.4 44.2 44.1 44.0 44.1 44.2 44.1 44.1 44.1 44.1 44.0 44.0 44.344.2 PeTTA(ours)(∗) 43.842.642.342.342.642.842.843.042.942.943.143.042.943.043.043.143.042.842.942.942.9 Discussions. Across datasets and reset frequencies, our PeTTA approach is always better than RDumb [45]. The supreme performance holds even when RDumb has access to the oracle information that can reset the model exactly at the transition between each domain shift or recurrence. Importantly, this oracle information is typically unavailable in practice. 1A slight abuse of notation. T here is the number of images between two consecutive resets, following the notation on Sec. 3 of [45], not the sample indices in our notations. 2A subset of 5, 000 samples from ImageNet-C are selected following RobustBench [10] for a consistent evaluation with other benchmarks. 26Table 13: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on ImageNet-C dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100072.2 73.0 73.2 72.8 72.2 72.8 73.3 72.7 71.9 73.0 73.2 73.1 72.0 72.7 73.3 73.1 72.1 72.6 73.3 73.172.8T= 500070.2 70.8 71.6 72.1 72.4 72.6 72.9 73.1 73.2 73.6 73.7 73.9 74.0 74.0 74.3 74.1 74.1 73.8 73.5 71.973.0T= 7500067.0 67.1 67.2 67.5 67.5 67.6 67.8 67.6 67.6 67.6 67.5 67.7 67.6 67.9 68.1 67.9 67.4 67.5 67.7 67.567.6 PeTTA(ours)(∗) 65.361.759.859.159.459.659.859.359.460.060.361.060.760.460.660.760.860.760.460.260.5 Noteworthy, it is clear that the performance of RDumb varies when changing the choice of the reset frequency. For a given choice of T, the better performance on one dataset does not guarantee the same performance on other datasets. For example, T = 1, 000 - the best empirical value found by RDumb authors [45] on CCC, does not give the best performance on our recurring TTA scenario; the second choice of T negatively impact the performance on many tasks; the third choice gives the best results, but knowing this exact recurrence frequency of the testing stream is unrealistic. The result highlights the challenge in practice when tuning this parameter (too slow/frequent), especially in the TTA setting where a validation set is unavailable. Our PeTTA, in contrast, is reset-free. F.4 PeTTA with 40 Recurring Visits To demonstrate the persistence of PeTTA over an even longer testing stream, in Tab. 14 and Fig. 8, we provide the evaluation results of PeTTA on recurring with 40 recurrences. F.5 The Sensitivity of Hyper-parameter Choices in PeTTA Table 15: Sensitivity of PeTTA with different choices ofλ0. Dataset λ0 = 1e0 λ0 = 5e0 λ0 = 1e1 λ0 = 5e1 λ0 = 1e2 CIFAR-10-C 22.9 22.7 22.8 23.2 24.1 CIFAR-100-C 35.7 35.3 35.1 35.6 36.1 ImageNet-C 61.2 61.0 60.5 61.3 62.4 There are two hyper-parameters in PeTTA: α0 and λ0. The initial learning rate of α0 = 1e−3 is used for all experiments. We do not tune this hyper-parameter, and the choice of α0 is universal across all datasets, following the previous works/compared methods (e.g., RoTTA [61], CoTTA [59]). Since λ0 is more specific to PeTTA, we included a sensitive analysis with different choices of λ0 on PeTTA, evaluated with images from CIFAR-10/100-C and ImageNet-C in Tab. 15. Overall, the choice of λ0 is not extremely sensitive, and while the best value is1e1 on most datasets, other choices such as 5e0 or 5e1 also produce roughly similar performance. Selecting λ0 is intuitive, the larger value of λ0 stronger prevents the model from collapsing but also limits its adaptability as a trade-off. In action, λ0 is an initial value and will be adaptively scaled with the sensing model divergence mechanism in PeTTA, meaning it does not require careful tuning. More generally, this hyper- parameter can be tuned similarly to the hyper-parameters of other TTA approaches, via an additional validation set, or some accuracy prediction algorithm [29] when labeled data is unavailable. F.6 More Details on the Ablation Study We provide the detailed classification error for each visit in the recurring TTA setting of each row entry in Tab. 4 (PeTTA Ablation Study): Tab. 16, Tab. 17, Tab. 18, Tab. 19; and Tab. 5 (PeTTA with various choices of regularizers): Tab. 20, Tab. 21, Tab. 22, Tab. 23. Fig. 9 presents an additional examination of the ablation study conducted on the task CIFAR-100 → CIFAR-100-C [19] for our PeTTA approach. We plot the classification error (top) and the value of ¯γt (bottom) for various PeTTA variations. As the model diverges from the initial state, the value of ¯γt increases. Unable to adjust αt or constraint the probability space via LAL limits the ability of PeTTA to prevent model collapse. In all variations with the model collapse in ablation studies, the rapid saturation of ¯γt is all observed. Therefore, incorporating all components in PeTTA is necessary. 27Table 16: Average classification error of multiple variations of PeTTA. Experiments on CIFAR10→ CIFAR10-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 23.5 24.0 27.4 29.9 33.4 35.6 38.0 40.7 43.1 45.0 46.0 48.6 50.0 49.7 50.8 51.5 52.3 53.3 54.3 55.542.6 R(θ)fixedλ= 0.1λ0 23.5 24.0 27.2 29.8 33.4 35.3 37.9 40.5 43.3 45.3 46.8 49.3 50.9 51.0 52.1 53.2 54.0 54.8 56.0 57.643.3R(θ)fixedλ=λ0 23.5 23.6 26.2 28.4 31.6 33.5 36.4 38.7 41.1 43.1 44.8 47.6 49.3 49.5 50.9 52.1 53.1 54.2 55.6 57.042.0 PeTTA-λt 24.9 25.3 26.0 26.4 27.2 26.5 27.2 27.1 27.4 27.7 27.8 28.0 27.5 28.0 27.7 27.4 27.0 27.6 27.8 27.827.1PeTTA-λt+αt 25.5 24.5 23.7 23.1 23.222.423.3 23.2 23.7 24.1 23.9 24.5 24.3 24.0 23.8 23.9 23.8 24.1 24.6 24.723.9PeTTA-λt+LAL 23.323.9 24.6 25.3 26.2 25.9 26.4 26.6 26.9 26.6 26.7 26.7 26.7 26.8 26.8 27.2 26.9 26.9 26.8 27.026.2 PeTTAαt+LAL 24.323.0 22.6 22.4 22.422.522.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8 Table 17: Average classification error of multiple variations of PeTTA. Experiments on CIFAR-100 → CIFAR100-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 40.2 46.3 51.2 54.4 57.3 59.4 61.3 62.6 63.9 65.1 66.3 67.1 68.1 68.9 69.6 70.3 71.1 71.6 72.4 72.963.0 R(θ)fixedλ= 0.1λ0 40.5 46.1 51.5 55.1 58.2 60.5 62.6 64.2 65.7 67.3 68.6 69.5 70.6 71.6 72.5 73.4 74.2 74.9 75.8 76.565.0R(θ)fixedλ=λ0 41.8 47.6 52.6 56.1 58.9 60.7 62.5 63.9 65.0 66.2 67.1 68.3 69.5 70.3 71.4 72.4 73.4 74.1 75.0 75.664.6 PeTTA-λt 39.4 43.4 46.6 49.1 51.0 52.6 53.8 54.7 55.7 56.5 57.1 57.7 58.3 58.8 59.3 59.9 60.6 61.0 61.6 62.155.0PeTTA-λt+αt 39.4 40.1 40.8 40.7 41.2 41.5 41.4 41.6 41.5 41.5 41.7 41.6 41.8 41.7 41.8 42.0 41.9 41.9 42.0 41.841.4PeTTA-λt+LAL 36.2 35.6 35.7 36.1 36.2 36.4 36.4 36.5 36.2 36.2 36.6 36.5 36.5 36.6 36.5 36.6 36.5 36.5 36.3 36.536.3 PeTTAλt+αt+LAL 35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1 Table 18: Average classification error of multiple variations of PeTTA. Experiments onreal → clipart, painting, sketch task from DomainNet [44] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 52.3 69.0 68.6 68.6 69.4 70.5 71.8 73.4 75.6 77.6 78.8 81.0 82.8 84.3 85.9 87.4 88.5 89.9 90.8 92.177.9 R(θ)fixedλ= 0.1λ0 52.5 70.0 69.8 70.0 71.1 72.5 74.6 76.1 77.8 80.4 81.9 83.5 85.2 87.2 89.1 90.2 91.5 93.2 94.1 94.980.0R(θ)fixedλ=λ0 54.6 69.8 63.7 56.0 61.7 76.4 70.4 62.5 58.2 76.0 73.6 66.8 58.6 62.3 80.8 75.5 67.0 59.9 59.3 78.366.6 PeTTA-λt 49.2 64.5 62.4 60.9 59.6 58.6 57.7 57.8 57.6 57.7 58.0 58.5 59.0 59.5 59.8 61.1 62.0 62.6 63.6 64.959.7PeTTA-λt+αt 43.942.5 42.3 42.3 42.6 42.843.1 43.7 43.9 44.3 44.6 45.1 45.4 45.7 45.7 46.1 46.1 46.2 46.5 46.444.5PeTTA-λt+LAL 43.6 42.542.6 42.6 42.9 43.0 43.3 43.4 43.1 43.243.143.3 43.3 43.2 43.2 43.9 43.7 43.0 43.2 43.543.2 PeTTAλt+αt+LAL 43.8 42.642.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9 Table 19: Average classification error of multiple variations of PeTTA. Experiments on ImageNet→ ImageNet-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 66.9 61.9 72.7 93.6 97.4 97.8 98.0 98.2 98.3 98.3 98.4 98.4 98.5 98.5 98.6 98.6 98.6 98.6 98.7 98.793.4 R(θ)fixedλ= 0.1λ0 65.5 70.9 79.1 85.2 90.3 92.6 95.8 95.8 95.4 97.3 96.9 97.7 97.9 98.2 98.0 98.7 98.6 98.4 98.4 98.792.5R(θ)fixedλ=λ0 66.5 62.1 73.0 93.5 97.0 97.2 97.5 97.5 97.6 97.5 97.7 97.7 97.7 97.8 97.9 97.9 98.0 98.0 98.0 97.992.9 PeTTA-λt 65.9 62.1 76.3 96.7 97.0 96.9 96.9 96.9 97.0 97.1 97.0 97.2 97.0 97.1 97.1 97.0 97.0 97.0 97.0 97.092.7PeTTA-λt+αt 64.870.5 74.6 75.8 75.5 75.8 76.1 76.2 76.2 76.5 76.7 77.0 76.9 77.4 77.1 77.3 77.2 77.4 77.6 77.475.7PeTTA-λt+LAL 64.8 61.160.0 59.8 60.4 60.4 61.2 61.2 61.8 61.9 62.1 62.2 62.1 62.9 62.1 62.8 62.7 62.1 62.8 66.662.0 PeTTA(ours)(∗) 65.3 61.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5 Table 20: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR-10 → CIFAR-10-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 25.6 24.8 23.8 23.1 23.2 22.7 23.0 22.7 22.7 22.7 22.8 22.7 22.8 22.7 22.522.3 22.2 22.4 22.7 22.823.0L2+Fisher25.2 23.7 22.5 21.8 22.3 21.5 22.3 22.1 22.5 22.8 22.6 22.622.622.8 22.6 22.9 22.6 22.9 23.0 23.322.7 Cosine 24.3 23.022.6 22.4 22.4 22.5 22.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8Cosine+Fisher25.1 23.822.2 21.6 22.0 21.4 22.0 21.8 22.1 22.3 22.5 22.4 22.6 22.6 22.422.7 22.6 22.8 22.8 23.322.6 Table 21: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR-100 → CIFAR-100-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 36.9 35.5 35.5 35.5 35.7 35.6 35.6 35.5 35.5 35.4 35.6 35.5 35.7 35.7 35.7 35.7 35.8 35.5 35.4 35.535.6L2+Fisher36.8 35.4 35.4 35.8 35.9 36.0 35.9 35.9 35.9 35.8 36.1 36.1 36.1 36.1 36.1 36.1 36.2 36.0 36.0 35.936.0 Cosine 35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1Cosine+Fisher36.7 35.2 35.5 35.6 35.9 35.9 36.1 36.0 36.0 35.9 36.0 36.0 36.0 36.1 36.0 36.0 35.9 35.9 35.9 36.035.9 28Table 22: Average classification error of PeTTA with various choices of regularizers. Experiments on real → clipart, painting, sketch task from DomainNet [44] dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 43.8 42.7 42.5 42.4 42.8 42.9 43.0 43.1 43.1 43.2 43.4 43.3 43.2 43.3 43.2 43.2 43.4 43.0 43.1 43.143.1L2+Fisher43.9 42.8 42.7 43.0 43.2 43.4 43.6 43.8 43.9 44.1 44.0 44.2 44.2 44.2 44.4 44.4 44.5 44.5 44.5 44.543.9 Cosine 43.8 42.642.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9Cosine+Fisher43.7 42.542.5 42.6 42.9 43.2 43.2 43.5 43.4 43.5 43.4 43.5 43.4 43.6 43.5 43.5 43.4 43.5 43.3 43.443.3 Table 23: Average classification error of PeTTA with various choices of regularizers. Experiments on ImageNet → ImageNet-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 70.8 72.2 71.5 69.8 72.3 69.3 70.3 70.5 70.0 70.8 70.2 72.1 71.4 70.8 70.9 70.9 69.7 71.0 71.1 70.470.8L2+Fisher70.5 70.0 69.5 69.4 69.6 69.9 69.2 69.3 72.2 70.4 71.0 70.5 71.7 71.5 71.3 68.4 68.6 68.8 68.7 68.770.0 Cosine 65.361.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5Cosine+Fisher65.1 61.760.9 61.2 61.9 62.6 62.8 63.2 64.2 63.4 64.3 64.4 63.9 64.3 65.8 65.5 64.9 65.0 65.2 65.263.8 F.7 More Confusion Matrices in Recurring TTA Setting For the task CIFAR-10→ CIFAR-10-C [19] in recurring TTA setting (with 20 visits), we additionally showcase the confusion matrix of RoTTA [61] (Fig. 10) and our proposed PeTTA (Fig. 11) at each visit. Our PeTTA persistently achieves competitive performance across 20 visits while RoTTA [61] gradually degrades. G Experimental Details G.1 Computing Resources A computer cluster equipped with an Intel(R) Core(TM) 3.80GHz i7-10700K CPU, 64 GB RAM, and one NVIDIA GeForce RTX 3090 GPU (24 GB VRAM) is used for our experiments. G.2 Experiments on CCC Testing Stream In this section, we further evaluate the performance of our PeTTA on the testing data stream of Continuous Changing Corruption (CCC) [ 45] setting. Here we use the baseline accuracy 20%, transition speed 1000, and random seed 44.3 The compared methods are source model (ResNet 50), PeTTA, RoTTA [61], and RDumb [45]. Noteworthy, different from recurring TTA, the class labels here are i.i.d. distributed. The adaptation configuration of PeTTA follows the same settings as used on ImageNet-C, while the same setting introduced in Sec. F.3, with T = 1000 is used for RDumb [45]. G.3 Test-time Adaptation Methods Pre-trained Model on Source Distribution. Following previous studies [57, 61, 12, 59], only the batch norm layers are updated. As stated in Sec. 5.2, RobustBench [10] and torchvision [35] provide pre-trained models trained on source distributions. Specifically, for ImageNet-C and Do- mainNet experiments, a ResNet50 model [17] pre-trained on ImageNet V2 (specifically, checkpoint ResNet50_Weights.IMAGENET1K_V2 of torchvision) is used. From RobustBench, the model with checkpoint Standard and Hendrycks2020AugMix_ResNeXt [20] are adopted for CIFAR10-C and CIFAR-100-C experiments, respectively. Lastly, experiments on DomainNet dataset utilize the checkpoint (best_real_2020) provided in AdaContrast [8] study.4 Optimizer. Without specifically stated, Adam [26] optimizer with learning rate equal 1e−3, and β = (0.9, 0.999) is selected as a universal choice for all experiments. More Details on PeTTA. Since designing the batch normalization layers, and the memory bank is not the key focus of PeTTA, we conveniently adopt the implementation of the Robust Batch Norm layer and the Category-balanced Sampling strategy using a memory bank introduced in RoTTA [61]. 3https://github.com/oripress/CCC 4https://github.com/DianCh/AdaContrast 29G.4 The Use of Existing Assets Many components of PeTTA is utilized from the official repository of RoTTA [61] 5 and RMT [12]. 6 These two assets are released under MIT license. All the datasets, including CIFAR-10-C, CIFAR- 100-C and ImageNet-C [ 19] are publicly available online, released under Apache-2.0 license. 7 DomainNet dataset [44] (cleaned version) is also released for research purposes.8 5https://github.com/BIT-DA/RoTTA 6https://github.com/mariodoebler/test-time-adaptation 7https://github.com/hendrycks/robustness 8https://ai.bu.edu/M3SDA/ 300 10000 20000 30000 40000 Test-time adaptation step (t) 0.40 0.60 0.80 1.00 ¯γt 0 10000 20000 30000 40000 Test-time adaptation step (t) 4.00 6.00 8.00 10.00λt 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.25 0.50 0.75 1.00αt 1e 3 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 1.00 2.00 3.00 4.00LCLS 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 2.50 5.00 7.50 10.00LAL 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.50 1.00 1.50 2.00 R(θ) 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.25 0.50 0.75 1.00Testing error Figure 7: An inspection of PeTTA on the task CIFAR-10 → CIFAR-10-C [19] in a recurring with 20 visits (visits are separated by the vertical dashed lines). Here, we visualize (rows 1-3) the dynamic of PeTTA adaptive parameters (¯γt, λt, αt), (rows 4-5) the value of the loss functions (LCLS, LAL) and (row 6) the value of the regularization term (R(θ)) and (row 7) the classification error rate at each step. The solid line in the foreground of each plot denotes the running mean. The plots show an adaptive change of λt, αt through time in PeTTA, which stabilizes TTA performance, making PeTTA achieve a persisting adaptation process in all observed values across 20 visits. 31Figure 8: Testing error of PeTTA with 40 recurring TTA visits. Total Visits CF-10-C CF-100-C IN-C 20 visits 22.8 35.1 60.5 40 visits 22.9 35.1 61.0 Table 14: Average testing error of PeTTA in recurring TTA with 20 and 40 visits. PeTTA demonstrates its persistence over an extended testing time horizon beyond the 20 th visit milestone (Fig. 8’s horizontal dashed line). 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.20 0.30 0.40 0.50 0.60 0.70 0.80Testing Error PeTTA - λt Baseline w/o R(θ) PeTTA - λt + αt R(θ) fixed λ= 0.1λ0 PeTTA - λt + LAL R(θ) fixed λ= λ0 PeTTA - λt  + αt  + LAL 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.20 0.40 0.60 0.80 1.00 ¯γt PeTTA - λt PeTTA - λt + αt PeTTA - λt + LAL PeTTA - λt  + αt  + LAL Figure 9: An inspection on the ablation study of multiple variations of PeTTA on the task CIFAR-100 → CIFAR-100-C [19] in an episodic TTA with 20 visits (visits are separated by the vertical dashed lines). (top): testing error of multiple variations of PeTTA. The performance of PeTTA without (w/o) R(θ), or fixed regularization coefficient ( λ = λ0/0.1λ0) degrades through time (the top 3 lines). The degradation of PeTTA -λt is still happening but at a slower rate (justification below). The performance of the other three variations persists through time with PeTTA -λt + αt + LAL achieves the best performance. (bottom): changes of ¯γt in multiple variations of PeTTA. When limiting the degree of freedom in adjusting αt or lacking of supervision from LAL (e.g., PeTTA -λt + αt, PeTTA -λt + LAL, and especially PeTTA -λt), the value of γt, unfortunately, escalates and eventually saturated. After this point, PeTTA has the same effect as using a fixed regularization coefficient. Therefore, fully utilizing all components is necessary to preserve the persistence of PeTTA. Best viewed in color. 320: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.79 0.01 0.04 0.03 0.02 0.01 0.01 0.02 0.05 0.02 0.02 0.82 0.01 0.01 0 0.01 0.01 0.01 0.01 0.09 0.06 0 0.68 0.07 0.04 0.03 0.06 0.03 0.01 0.01 0.02 0.01 0.04 0.66 0.04 0.08 0.07 0.05 0.01 0.02 0.03 0 0.04 0.06 0.68 0.02 0.06 0.09 0.01 0.01 0.03 0 0.05 0.15 0.03 0.61 0.03 0.07 0.01 0.01 0.02 0.01 0.03 0.07 0.02 0.02 0.8 0.02 0 0.01 0.01 0 0.02 0.03 0.03 0.02 0.01 0.87 0 0.01 0.09 0.02 0.02 0.02 0.01 0 0.02 0.01 0.77 0.04 0.03 0.03 0.01 0.01 0 0 0.01 0.01 0.03 0.85 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.03 0.03 0.01 0 0.03 0.02 0.07 0.03 0.02 0.76 0 0.01 0 0 0.03 0.01 0.02 0.16 0.07 0 0.63 0.08 0.06 0.02 0.08 0.04 0.01 0.01 0.02 0 0.04 0.7 0.04 0.04 0.09 0.05 0.01 0.02 0.03 0 0.03 0.05 0.73 0.01 0.06 0.08 0.01 0.01 0.01 0 0.03 0.23 0.04 0.53 0.06 0.08 0.01 0.01 0.02 0 0.02 0.1 0.02 0.01 0.81 0.01 0 0.01 0.01 0 0.01 0.05 0.03 0.01 0.01 0.87 0 0.01 0.08 0.01 0.01 0.02 0.01 0 0.02 0.01 0.8 0.04 0.03 0.02 0.01 0.02 0 0 0.02 0.01 0.02 0.87 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.7 0.01 0.03 0.04 0.02 0 0.03 0.03 0.09 0.06 0.01 0.72 0 0.01 0 0 0.04 0 0.01 0.2 0.07 0 0.56 0.1 0.08 0.02 0.09 0.04 0.01 0.02 0.01 0 0.03 0.7 0.05 0.02 0.13 0.04 0 0.02 0.04 0 0.03 0.07 0.69 0 0.08 0.07 0.01 0.01 0.01 0 0.04 0.26 0.05 0.42 0.13 0.07 0 0.01 0.01 0 0.02 0.11 0.03 0 0.8 0.01 0 0.01 0.01 0 0.02 0.06 0.05 0.01 0.04 0.8 0 0.01 0.07 0.01 0.01 0.03 0.01 0 0.03 0.01 0.78 0.05 0.02 0.01 0.01 0.02 0.01 0 0.04 0.01 0.02 0.86 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.62 0.01 0.03 0.06 0.03 0 0.05 0.04 0.09 0.08 0.01 0.66 0 0.02 0.01 0 0.04 0 0.02 0.25 0.07 0 0.48 0.13 0.1 0.02 0.13 0.03 0.01 0.02 0.01 0 0.02 0.68 0.05 0.02 0.17 0.03 0 0.02 0.03 0 0.02 0.07 0.67 0 0.12 0.07 0.01 0.01 0.01 0 0.02 0.29 0.07 0.39 0.14 0.06 0 0.01 0.01 0 0.01 0.11 0.04 0 0.8 0.01 0 0.01 0.01 0 0.02 0.08 0.06 0.01 0.06 0.75 0 0.01 0.05 0.01 0.01 0.04 0.02 0 0.05 0.01 0.74 0.07 0.01 0.01 0 0.03 0.01 0 0.05 0.01 0.02 0.86 True label 1st visit 2nd visit 3rd visit 4th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.56 0 0.03 0.07 0.04 0 0.07 0.04 0.1 0.1 0.01 0.61 0 0.01 0.01 0 0.07 0 0.02 0.26 0.08 0 0.42 0.13 0.13 0.02 0.15 0.03 0.01 0.02 0.02 0 0.01 0.62 0.06 0.02 0.21 0.03 0 0.02 0.03 0 0.02 0.06 0.66 0 0.16 0.06 0.01 0.01 0.01 0 0.02 0.3 0.08 0.34 0.17 0.06 0 0.02 0.01 0 0.01 0.12 0.07 0 0.76 0.01 0 0.02 0.01 0 0.02 0.1 0.08 0.01 0.08 0.69 0 0.02 0.05 0.01 0.01 0.05 0.02 0 0.09 0.01 0.68 0.09 0.01 0.01 0 0.03 0.02 0 0.09 0.01 0.02 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.51 0 0.02 0.07 0.04 0 0.09 0.03 0.1 0.14 0.01 0.56 0 0.01 0.02 0 0.09 0 0.02 0.29 0.08 0 0.35 0.15 0.16 0.02 0.18 0.03 0.01 0.03 0.02 0 0.01 0.57 0.07 0.02 0.27 0.02 0 0.03 0.04 0 0.01 0.08 0.62 0 0.18 0.05 0.01 0.01 0.01 0 0.01 0.29 0.09 0.3 0.21 0.05 0 0.02 0.01 0 0.01 0.12 0.09 0 0.75 0 0 0.01 0.02 0 0.01 0.11 0.12 0.01 0.1 0.6 0 0.03 0.06 0.01 0 0.04 0.02 0 0.09 0 0.66 0.11 0.01 0.01 0 0.02 0.03 0 0.11 0 0.02 0.8 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.48 0 0.02 0.08 0.04 0 0.11 0.03 0.11 0.13 0.01 0.54 0 0.01 0.02 0 0.11 0 0.02 0.28 0.09 0 0.3 0.16 0.16 0.02 0.21 0.02 0.01 0.03 0.02 0 0.01 0.51 0.08 0.01 0.33 0.01 0.01 0.02 0.03 0 0.01 0.05 0.65 0 0.21 0.03 0.01 0.01 0.02 0 0.01 0.27 0.11 0.25 0.28 0.03 0 0.02 0.01 0 0.01 0.12 0.1 0 0.75 0 0 0.01 0.02 0 0.01 0.11 0.13 0.01 0.13 0.56 0 0.03 0.06 0 0 0.06 0.03 0 0.13 0 0.6 0.11 0.02 0.01 0 0.03 0.04 0 0.15 0 0.02 0.73 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.46 0 0.01 0.07 0.06 0 0.13 0.01 0.09 0.15 0.01 0.48 0 0.01 0.04 0 0.16 0 0.01 0.28 0.09 0 0.27 0.15 0.19 0.01 0.23 0.01 0.01 0.03 0.02 0 0.01 0.44 0.12 0.01 0.37 0.01 0.01 0.02 0.04 0 0.01 0.05 0.63 0 0.23 0.02 0.01 0.01 0.02 0 0.01 0.25 0.13 0.22 0.33 0.02 0 0.01 0.01 0 0 0.11 0.15 0 0.71 0 0 0.01 0.02 0 0.01 0.09 0.22 0 0.15 0.47 0 0.02 0.08 0 0 0.06 0.05 0 0.15 0 0.55 0.1 0.02 0.01 0 0.04 0.05 0 0.16 0 0.02 0.7 True label 5th visit 6th visit 7th visit 8th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.47 0 0.01 0.06 0.06 0 0.13 0.01 0.1 0.16 0.02 0.47 0 0.01 0.04 0 0.13 0 0.03 0.29 0.1 0 0.24 0.12 0.22 0.01 0.24 0.01 0.01 0.03 0.03 0 0 0.4 0.12 0.01 0.39 0 0.01 0.02 0.05 0 0.01 0.06 0.61 0 0.23 0.02 0.01 0.01 0.03 0 0.01 0.22 0.15 0.2 0.35 0.02 0 0.02 0.01 0 0 0.11 0.15 0 0.7 0 0.01 0.01 0.03 0 0.01 0.08 0.25 0 0.15 0.44 0.01 0.03 0.09 0 0 0.04 0.07 0 0.14 0 0.55 0.1 0.02 0.01 0 0.03 0.05 0 0.16 0 0.02 0.7 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.46 0 0.01 0.05 0.07 0 0.14 0.01 0.1 0.16 0.04 0.43 0 0.02 0.07 0 0.14 0 0.03 0.27 0.11 0 0.22 0.11 0.23 0.01 0.26 0.01 0.02 0.03 0.04 0 0 0.33 0.16 0.01 0.43 0 0.01 0.02 0.05 0 0 0.03 0.66 0 0.23 0.01 0.01 0.01 0.04 0 0.01 0.22 0.15 0.18 0.37 0.01 0.01 0.02 0.01 0 0 0.1 0.19 0 0.68 0 0.01 0.01 0.03 0 0.01 0.08 0.28 0.01 0.16 0.41 0.01 0.03 0.11 0 0 0.04 0.05 0 0.14 0 0.56 0.09 0.04 0.01 0 0.02 0.08 0 0.18 0 0.02 0.65 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.47 0 0.01 0.04 0.07 0 0.14 0 0.1 0.16 0.04 0.42 0 0.01 0.07 0 0.15 0 0.05 0.26 0.11 0 0.21 0.1 0.26 0.01 0.26 0 0.02 0.03 0.05 0 0 0.31 0.18 0.01 0.42 0 0.01 0.02 0.06 0 0 0.04 0.65 0 0.21 0.01 0.01 0.02 0.04 0 0.01 0.17 0.21 0.15 0.39 0.01 0.01 0.02 0.01 0 0 0.1 0.24 0 0.64 0 0.01 0.01 0.04 0 0.01 0.09 0.28 0 0.16 0.39 0 0.03 0.14 0 0 0.03 0.07 0 0.14 0 0.52 0.09 0.05 0.01 0 0.03 0.1 0 0.18 0 0.03 0.61 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.49 0 0.01 0.03 0.06 0 0.14 0 0.11 0.17 0.07 0.4 0 0.01 0.07 0 0.12 0 0.07 0.27 0.13 0 0.19 0.08 0.27 0.01 0.25 0 0.02 0.03 0.07 0 0 0.27 0.19 0 0.43 0 0.02 0.03 0.07 0 0 0.02 0.64 0 0.23 0.01 0.01 0.01 0.06 0 0.01 0.19 0.18 0.13 0.39 0.01 0.01 0.02 0.02 0 0 0.09 0.22 0 0.65 0 0.01 0.01 0.05 0 0 0.07 0.32 0 0.15 0.36 0.01 0.04 0.17 0 0 0.03 0.07 0 0.12 0 0.53 0.08 0.06 0.01 0 0.01 0.13 0 0.17 0 0.03 0.59 True label 9th visit 10th visit 11th visit 12th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.5 0 0 0.02 0.08 0 0.13 0 0.1 0.15 0.09 0.37 0 0.01 0.11 0 0.11 0 0.08 0.24 0.15 0 0.18 0.07 0.31 0.01 0.24 0 0.03 0.02 0.09 0 0 0.24 0.17 0 0.44 0 0.02 0.03 0.08 0 0 0.02 0.66 0 0.19 0.01 0.02 0.02 0.08 0 0.01 0.15 0.23 0.11 0.38 0.01 0.01 0.02 0.02 0 0 0.08 0.31 0 0.55 0 0.02 0.01 0.05 0 0 0.05 0.37 0 0.14 0.34 0.01 0.04 0.2 0 0 0.03 0.06 0 0.12 0 0.52 0.08 0.08 0.01 0 0.01 0.11 0 0.15 0 0.04 0.59 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.54 0 0 0.02 0.06 0 0.11 0 0.12 0.15 0.13 0.35 0 0.01 0.1 0 0.09 0 0.12 0.21 0.16 0 0.18 0.07 0.29 0.01 0.24 0 0.03 0.02 0.11 0 0 0.22 0.19 0 0.42 0 0.03 0.03 0.08 0 0 0.03 0.65 0 0.2 0.01 0.02 0.01 0.09 0 0.01 0.12 0.29 0.08 0.37 0 0.02 0.02 0.02 0 0 0.09 0.29 0 0.56 0 0.02 0.01 0.06 0 0 0.05 0.39 0 0.13 0.32 0.01 0.04 0.23 0 0 0.02 0.07 0 0.1 0 0.51 0.07 0.12 0.01 0 0.01 0.11 0 0.13 0 0.05 0.57 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.56 0 0 0.02 0.08 0 0.1 0 0.12 0.12 0.18 0.32 0 0 0.11 0 0.08 0 0.13 0.19 0.18 0 0.15 0.05 0.34 0 0.2 0 0.04 0.02 0.12 0 0 0.19 0.27 0 0.36 0 0.04 0.02 0.09 0 0 0.02 0.69 0 0.15 0.01 0.02 0.02 0.11 0 0 0.1 0.33 0.07 0.33 0 0.03 0.01 0.03 0 0 0.09 0.35 0 0.5 0 0.02 0.01 0.08 0 0 0.04 0.43 0 0.1 0.29 0.01 0.04 0.26 0 0 0.02 0.08 0 0.08 0 0.51 0.06 0.15 0.01 0 0.01 0.12 0 0.1 0 0.07 0.55 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.58 0 0 0.01 0.07 0 0.09 0 0.13 0.11 0.16 0.32 0 0 0.11 0 0.07 0 0.16 0.18 0.18 0 0.15 0.05 0.36 0 0.19 0 0.04 0.02 0.14 0 0 0.18 0.26 0 0.35 0 0.05 0.02 0.1 0 0 0.01 0.69 0 0.15 0.01 0.03 0.01 0.11 0 0 0.1 0.36 0.05 0.32 0 0.04 0.01 0.03 0 0 0.08 0.38 0 0.46 0 0.03 0.01 0.09 0 0 0.04 0.43 0 0.09 0.29 0.02 0.04 0.29 0 0 0.02 0.09 0 0.08 0 0.47 0.06 0.18 0.01 0 0.01 0.11 0 0.08 0 0.1 0.5 True label 13th visit 14th visit 15th visit 16th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.6 0 0 0.01 0.08 0 0.08 0 0.13 0.1 0.2 0.28 0 0 0.1 0 0.06 0 0.19 0.17 0.2 0 0.14 0.05 0.36 0 0.18 0 0.05 0.02 0.17 0 0 0.16 0.28 0 0.29 0 0.08 0.02 0.1 0 0 0.01 0.71 0 0.11 0.01 0.04 0.02 0.13 0 0 0.1 0.4 0.04 0.27 0 0.05 0.01 0.04 0 0 0.09 0.4 0 0.41 0 0.04 0.01 0.1 0 0 0.04 0.45 0 0.07 0.27 0.03 0.04 0.34 0 0 0.01 0.08 0 0.05 0 0.47 0.05 0.22 0.01 0 0.01 0.13 0 0.06 0 0.12 0.44 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.62 0 0 0.01 0.09 0 0.08 0 0.13 0.08 0.24 0.26 0 0 0.1 0 0.05 0 0.19 0.15 0.2 0 0.13 0.04 0.41 0 0.16 0 0.05 0.02 0.16 0 0 0.14 0.3 0 0.29 0 0.09 0.02 0.11 0 0 0.01 0.7 0 0.1 0.01 0.05 0.02 0.14 0 0 0.09 0.42 0.02 0.27 0 0.05 0.01 0.03 0 0 0.09 0.44 0 0.39 0 0.04 0.01 0.12 0 0 0.04 0.43 0 0.06 0.28 0.03 0.04 0.35 0 0 0.01 0.07 0 0.06 0 0.46 0.04 0.26 0.01 0 0.01 0.13 0 0.06 0 0.13 0.41 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.67 0 0 0 0.1 0 0.05 0 0.11 0.06 0.3 0.21 0 0 0.1 0 0.03 0 0.21 0.13 0.26 0 0.11 0.04 0.4 0 0.11 0 0.06 0.02 0.2 0 0 0.13 0.32 0 0.21 0 0.12 0.02 0.13 0 0 0.01 0.72 0 0.07 0.01 0.05 0.01 0.2 0 0 0.09 0.42 0.01 0.19 0 0.08 0.01 0.04 0 0 0.08 0.49 0 0.3 0 0.07 0.01 0.16 0 0 0.03 0.45 0 0.04 0.24 0.05 0.03 0.42 0 0 0.01 0.06 0 0.04 0 0.43 0.04 0.33 0.01 0 0 0.11 0 0.03 0 0.15 0.36 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.69 0 0 0 0.08 0 0.04 0 0.13 0.05 0.34 0.17 0 0 0.1 0 0.03 0 0.23 0.13 0.24 0 0.1 0.03 0.44 0 0.11 0 0.07 0.01 0.21 0 0 0.11 0.32 0 0.21 0 0.14 0.01 0.14 0 0 0.01 0.71 0 0.06 0.01 0.06 0.01 0.21 0 0 0.07 0.44 0 0.16 0 0.1 0.01 0.05 0 0 0.08 0.51 0 0.25 0 0.1 0.01 0.17 0 0 0.03 0.46 0 0.04 0.21 0.06 0.04 0.46 0 0 0.01 0.06 0 0.03 0 0.41 0.03 0.34 0 0 0 0.12 0 0.03 0 0.18 0.32 Predicted label Predicted label Predicted label Predicted label True label 17th visit 18th visit 19th visit 20th visit Figure 10: The dynamic of the confusion matrix of RoTTA [61] in episodic TTA with 20 visits. 330: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.77 0.01 0.04 0.03 0.03 0.01 0.02 0.02 0.05 0.02 0.02 0.84 0.01 0.02 0 0.01 0.02 0.01 0.02 0.06 0.04 0 0.69 0.07 0.05 0.05 0.05 0.02 0.01 0.01 0.04 0.01 0.05 0.62 0.05 0.1 0.06 0.04 0.01 0.02 0.03 0 0.06 0.07 0.68 0.05 0.04 0.05 0.01 0.01 0.01 0 0.04 0.14 0.03 0.7 0.03 0.04 0.01 0.01 0.01 0.01 0.04 0.06 0.03 0.03 0.78 0.01 0.01 0.01 0.03 0 0.03 0.04 0.04 0.04 0.01 0.79 0.01 0.01 0.08 0.02 0.02 0.02 0.01 0.01 0.02 0.01 0.8 0.03 0.03 0.05 0.02 0.02 0.01 0.01 0.01 0.01 0.03 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.77 0.01 0.04 0.03 0.02 0.01 0.03 0.01 0.06 0.02 0.01 0.87 0.01 0.01 0 0.01 0.01 0 0.02 0.05 0.04 0 0.7 0.09 0.05 0.03 0.06 0.02 0.01 0.01 0.03 0.01 0.06 0.64 0.05 0.08 0.06 0.04 0.01 0.02 0.02 0 0.05 0.06 0.74 0.03 0.05 0.04 0.01 0.01 0.01 0 0.05 0.15 0.04 0.66 0.04 0.04 0.01 0.01 0.02 0.01 0.04 0.06 0.02 0.02 0.78 0.01 0.01 0.03 0.02 0 0.03 0.05 0.05 0.03 0.01 0.81 0 0.01 0.05 0.02 0.01 0.02 0.01 0 0.02 0.01 0.83 0.03 0.02 0.05 0.01 0.02 0.01 0.01 0.01 0.01 0.03 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.03 0.02 0 0.03 0.01 0.06 0.02 0.02 0.87 0.01 0.02 0 0 0.01 0 0.02 0.05 0.05 0 0.7 0.07 0.05 0.03 0.06 0.02 0.01 0.01 0.02 0.01 0.05 0.68 0.05 0.07 0.07 0.03 0.01 0.02 0.02 0 0.05 0.06 0.77 0.02 0.04 0.03 0 0 0.01 0 0.07 0.15 0.04 0.65 0.04 0.03 0.01 0.01 0.01 0 0.03 0.07 0.03 0.02 0.83 0.01 0 0.01 0.01 0 0.03 0.04 0.04 0.02 0.01 0.82 0 0.01 0.06 0.02 0.01 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.01 0.02 0.01 0 0.01 0.01 0.03 0.85 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.05 0.04 0.02 0 0.02 0.01 0.07 0.03 0.01 0.87 0.01 0.01 0 0 0.01 0 0.02 0.05 0.04 0 0.73 0.06 0.05 0.03 0.06 0.01 0.01 0.01 0.01 0.01 0.05 0.71 0.05 0.06 0.06 0.03 0.01 0.01 0.02 0 0.04 0.05 0.78 0.02 0.04 0.03 0.01 0 0.01 0 0.06 0.17 0.04 0.64 0.04 0.03 0.01 0.01 0.01 0 0.03 0.06 0.03 0.01 0.85 0.01 0 0.01 0.01 0 0.04 0.04 0.05 0.02 0.01 0.81 0.01 0.01 0.05 0.02 0.01 0.02 0.01 0 0.02 0 0.84 0.02 0.02 0.05 0.01 0.02 0.01 0 0.02 0.01 0.04 0.83 True label 1st visit 2nd visit 3rd visit 4th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.02 0.04 0.04 0.02 0 0.02 0.01 0.08 0.02 0.02 0.86 0.01 0.02 0 0 0.01 0 0.02 0.05 0.04 0 0.73 0.07 0.05 0.03 0.05 0.01 0.01 0.01 0.01 0.01 0.06 0.69 0.05 0.06 0.07 0.02 0.01 0.01 0.02 0 0.05 0.07 0.76 0.02 0.04 0.03 0.01 0 0.01 0 0.07 0.17 0.04 0.64 0.03 0.03 0.01 0.01 0.01 0 0.03 0.07 0.02 0.01 0.84 0.01 0.01 0.01 0.01 0 0.04 0.04 0.06 0.02 0.01 0.81 0.01 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.86 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.03 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.01 0 0.02 0.05 0.05 0 0.74 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.05 0.7 0.06 0.06 0.07 0.02 0.01 0.01 0.01 0 0.04 0.06 0.79 0.02 0.04 0.02 0.01 0 0.01 0 0.06 0.17 0.04 0.65 0.04 0.03 0.01 0.01 0.01 0 0.03 0.07 0.02 0.01 0.85 0 0 0.01 0.01 0 0.04 0.04 0.06 0.02 0.01 0.8 0.01 0.01 0.04 0.02 0.01 0.02 0.01 0 0.02 0 0.87 0.02 0.02 0.05 0.02 0.02 0 0 0.02 0.01 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.04 0.04 0.02 0 0.03 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.01 0 0.02 0.05 0.04 0 0.74 0.06 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0.01 0.06 0.68 0.05 0.06 0.08 0.02 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.04 0.02 0 0 0.01 0 0.07 0.18 0.05 0.61 0.04 0.03 0.01 0.01 0 0 0.03 0.06 0.02 0.01 0.86 0 0 0 0.01 0 0.04 0.04 0.06 0.02 0.01 0.8 0 0.01 0.06 0.02 0.02 0.02 0.01 0 0.02 0 0.84 0.02 0.02 0.05 0.01 0.03 0.01 0 0.02 0.01 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.04 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0 0.05 0.73 0.05 0.05 0.07 0.02 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.06 0.18 0.04 0.63 0.04 0.02 0.01 0.01 0 0 0.03 0.08 0.02 0.01 0.83 0 0 0 0.01 0 0.04 0.05 0.07 0.02 0.01 0.79 0 0.01 0.04 0.02 0.01 0.02 0.01 0 0.02 0 0.86 0.02 0.02 0.04 0.01 0.03 0.01 0 0.03 0.01 0.04 0.81 True label 5th visit 6th visit 7th visit 8th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.74 0.07 0.05 0.02 0.06 0.01 0.01 0.01 0.01 0 0.06 0.71 0.05 0.05 0.07 0.02 0.01 0.01 0.01 0 0.04 0.07 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.62 0.04 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.84 0 0 0.01 0.01 0 0.05 0.05 0.08 0.02 0.02 0.77 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.07 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0.01 0.05 0.7 0.06 0.05 0.08 0.02 0.02 0.02 0.02 0 0.05 0.07 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.04 0.03 0.01 0.01 0 0 0.04 0.07 0.02 0.01 0.84 0 0 0.01 0.01 0 0.04 0.05 0.08 0.02 0.01 0.78 0 0 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.02 0.06 0.05 0.02 0 0.04 0.01 0.07 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.74 0.08 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0 0.06 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.04 0.02 0.01 0 0 0 0.06 0.19 0.05 0.61 0.05 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.04 0.05 0.08 0.02 0.01 0.77 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.04 0.02 0.03 0.01 0 0.02 0 0.03 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.04 0.02 0 0.04 0.01 0.08 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.04 0.05 0 0.72 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.73 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.05 0.06 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.06 0.19 0.05 0.61 0.04 0.03 0.01 0 0 0 0.03 0.09 0.02 0.01 0.83 0 0 0 0.01 0 0.05 0.05 0.07 0.02 0.01 0.78 0 0.01 0.03 0.02 0.02 0.02 0.01 0 0.03 0 0.85 0.02 0.01 0.05 0.01 0.03 0.01 0 0.02 0 0.04 0.83 True label 9th visit 10th visit 11th visit 12th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.05 0.04 0.02 0 0.03 0.01 0.09 0.02 0.01 0.86 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0 0.02 0 0.06 0.73 0.05 0.04 0.06 0.02 0.01 0.01 0.01 0 0.05 0.06 0.8 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.04 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.07 0.02 0.01 0.77 0 0 0.03 0.02 0.02 0.02 0.01 0 0.04 0 0.83 0.02 0.01 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.87 0.01 0.02 0 0 0.02 0 0.02 0.05 0.05 0 0.72 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0.01 0.05 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.57 0.05 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.02 0.76 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.03 0.01 0 0.02 0 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.05 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.07 0.01 0.01 0.01 0.02 0 0.04 0.06 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.2 0.05 0.6 0.04 0.02 0.01 0.01 0 0 0.04 0.07 0.02 0.01 0.85 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.01 0.78 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.05 0.04 0.02 0 0.02 0.01 0.09 0.02 0.01 0.86 0.01 0.02 0 0 0.02 0 0.02 0.05 0.04 0 0.74 0.07 0.05 0.02 0.05 0.01 0.01 0.01 0.02 0 0.06 0.73 0.05 0.04 0.07 0.01 0.01 0.01 0.02 0 0.05 0.06 0.78 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.85 0 0 0.01 0.01 0 0.04 0.06 0.08 0.02 0.01 0.77 0 0.01 0.04 0.02 0.03 0.03 0.01 0 0.04 0 0.8 0.02 0.01 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.83 True label 13th visit 14th visit 15th visit 16th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.04 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.74 0.05 0.05 0.06 0.01 0.01 0.01 0.01 0 0.05 0.06 0.8 0.01 0.04 0.02 0 0 0.01 0 0.07 0.2 0.05 0.59 0.06 0.02 0.01 0.01 0 0 0.04 0.08 0.02 0.01 0.84 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.02 0.76 0 0.01 0.05 0.01 0.01 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.03 0.01 0 0.03 0 0.03 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.04 0.04 0 0.73 0.07 0.06 0.02 0.06 0.01 0.01 0 0.01 0 0.06 0.73 0.05 0.04 0.07 0.01 0.01 0.01 0.01 0 0.06 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.59 0.04 0.02 0.01 0 0 0 0.04 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.01 0.76 0.01 0.01 0.05 0.02 0.02 0.03 0.01 0 0.02 0 0.85 0.01 0.02 0.05 0.02 0.03 0.01 0 0.03 0.01 0.04 0.8 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.73 0.06 0.05 0.02 0.06 0.01 0.01 0.01 0.01 0 0.06 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.01 0 0.05 0.06 0.78 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.58 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.85 0 0.01 0.01 0.01 0 0.06 0.05 0.08 0.02 0.02 0.75 0 0.01 0.03 0.02 0.02 0.02 0.01 0 0.03 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.05 0.02 0.01 0 0 0 0.07 0.19 0.05 0.59 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.84 0 0.01 0.01 0.01 0 0.06 0.06 0.08 0.02 0.02 0.74 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.05 0.02 0.03 0.01 0 0.02 0.01 0.04 0.82 Predicted label Predicted label Predicted label Predicted label True label 17th visit 18th visit 19th visit 20th visit Figure 11: The dynamic of the confusion matrix of PeTTA (ours) in episodic TTA with 20 visits. 34NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: We have highlighted the three main claims and contributions of our work in both the abstract (highlighted in bold font) and the introduction section (listed as bullet points). Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations and potential future work of our study in Sec. 6. Specifically, three main limitations are included: (1) Collapse prevention can not be guaranteed through regularization, PeTTA requires (2) the use of a relatively small memory bank is available and (3) the empirical mean and covariant matrix of feature vectors on the source dataset is computable. We also include discussions in Appdx. E.3 and Appdx. E.4 to further elaborate (2), and (3) respectively. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 353. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have provided the full proof of all lemmas and theorem in Appdx. B. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: This study propose a new TTA approach - PeTTA. A full description of this approach is given in Sec. 4 with its pseudo-code provided in Appdx. E.1. The implementation of PeTTA in Python is also attached as supplemental material. Additionally, Sec. 5.2 and Appdx. G are dedicated to providing further implementation details for reproducing the main experimental results. Lastly, the construction of recurring TTA is notably simple, and can be easily extended to other TTA streams. Its configuration on each tasks is described in the Recurring TTA paragraph of Sec. 5.2. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 36(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: This study does not involve any private datasets. All datasets used in our exper- iments are publicly available online from previous works (more information in Appdx. G.4). The source code of PeTTA is also attached as supplemental material. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental settings of the key results in the paper have been provided in Sec. 5.1 (Simulation Setup) and Sec. 5.2 (Setup - Benchmark Datasets). In the supplementary material, any additional experimental results beyond the main paper, such as those in Appdx. D.3, and Appdx. F.3, are consistently preceded by a subsection titledExperiment Setup summarizing the experimental details before presenting the results. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? 37Answer: [Yes] Justification: Due to the limited computing resources, we only extensively evaluate the performance of our proposed method (PeTTA) across 5 independent runs, with different random seeds. Specifically, the mean values in 5 runs are reported in Tab. 1, Tab. 2, Tab. 7, and Tab. 8. The corresponding standard deviation values are provided in Appdx. F.1. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided the information on the computing resources used in our experiments in Appdx. G.1. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed and to the best of our judgment, this study has conformed to the NeurIPS Code of Ethics. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. 38• The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This study advances the research in test-time adaptation area in general, and not tied to particular applications. Hence, there are no significant potential societal consequences of our work which we feel must be specifically highlighted here. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: To the best of our judgment, this study poses no risks for misuse. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? 39Answer: [Yes] Justification: The original papers that produced the code package or dataset have been properly cited throughout the paper. Further information on the licenses of used assets are provided in Appdx. G.4. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This study does not release new assets. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This study does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects 40Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This study does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 41",
      "meta_data": {
        "arxiv_id": "2311.18193v4",
        "authors": [
          "Trung-Hieu Hoang",
          "Duc Minh Vo",
          "Minh N. Do"
        ],
        "published_date": "2023-11-30T02:24:44Z",
        "pdf_url": "https://arxiv.org/pdf/2311.18193v4.pdf",
        "github_url": "https://github.com/BIT-DA/RoTTA"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the overlooked problem of gradual performance degradation (model collapse) in Test-Time Adaptation (TTA) methods when models are exposed to recurring testing environments over prolonged periods. The main contributions are: 1. Proposing a new testing scenario called 'recurring TTA' to diagnose this degradation, where environments not only change but also recur over time. 2. Formally defining model collapse and providing a theoretical analysis using an \\u03b5-perturbed Gaussian Mixture Model Classifier (\\u03b5-GMMC) to explain dataset- and algorithm-dependent factors causing error accumulation. 3. Introducing Persistent TTA (PeTTA), a simple yet effective adaptation scheme that continuously senses model divergence and adaptively adjusts the adaptation strategy to balance adaptation and collapse prevention, demonstrating superior stability over existing approaches in lifelong TTA scenarios.",
        "methodology": "The methodology involves several key components: 1. **Recurring TTA Scenario**: An extension of practical TTA where test environments recur multiple times, simulating prolonged exposure and highlighting error accumulation. 2. **Theoretical Analysis on \\u03b5-GMMC**: A simplified yet representative \\u03b5-perturbed binary Gaussian Mixture Model Classifier is used to derive theoretical insights into factors contributing to model collapse (data-dependent factors like prior distribution and category difference, and algorithm-dependent factors like update rate and false negative rate). 3. **Persistent TTA (PeTTA)**: An adaptation scheme built upon the mean teacher update mechanism. It introduces a divergence sensing mechanism that measures the Mahalanobis distance of feature embedding vectors from the source distribution's pre-computed empirical mean and covariance matrix. Based on this divergence (\\u03b3y t), PeTTA adaptively adjusts the regularization coefficient (\\u03bbt = \\u03b3t \\u00b7 \\u03bb0) and the Exponential Moving Average (EMA) update rate (\\u03b1t = (1 - \\u03b3t) \\u00b7 \\u03b10). It also incorporates an Anchor Loss (LAL) equivalent to minimizing KL divergence from the source model's predictions, a category-balanced memory bank, and robust batch normalization layers (adopted from prior work).",
        "experimental_setup": "The experimental setup includes: 1. **Simulation**: A synthesized dataset with 6000 samples from two Gaussian distributions N(\\u00b50=0, \\u03c30^2=1) and N(\\u00b51=2, \\u03c31^2=1) with equal priors (p0=p1=0.5), released in batches of 10. An independent set of 2000 samples is used for evaluation. Model collapse is simulated by randomly flipping 10% of true-positive pseudo labels. 2. **Benchmark Datasets**: CIFAR-10 \\u2192 CIFAR-10-C, CIFAR-100 \\u2192 CIFAR-100-C, and ImageNet \\u2192 ImageNet-C (corruption level 5) for corrupted image classification, and DomainNet (real \\u2192 clipart, painting, sketch) with 126 categories. 3. **Recurring TTA Setting**: Environments (corruptions) recur K=20 times. Category temporally correlated batches are generated using Dirichlet distribution (Dir(0.1) for CIFAR-10-C, DomainNet, ImageNet-C; Dir(0.01) for CIFAR-100-C). 4. **Compared Methods**: CoTTA, EATA, RMT, MECTA, RoTTA, ROID, TRIBE, LAME (parameter-free), and RDumb (reset-based). 5. **Implementation Details**: PyTorch is used. Pre-trained source models are from RobustBench and torchvision. Adam optimizer is used (learning rate 1e-3). PeTTA uses EMA update rate 5e-2 for BN and feature stats, \\u03b10=1e-3, cosine similarity regularizer, and \\u03bb0=10 (for CIFAR/ImageNet-C using self-training loss) or \\u03bb0=1 (for DomainNet using cross-entropy loss). Experiments also include Continuously Changing Corruption (CCC) [45] setting with 80,000 adaptation steps.",
        "limitations": "The paper acknowledges several limitations: 1. A complete elimination of error accumulation cannot be rigorously guaranteed solely through regularization. 2. PeTTA relies on a small memory bank to handle temporally correlated testing streams, which might not be universally applicable in all real-world scenarios. 3. The empirical mean and covariance matrix of feature vectors from the source distribution are assumed to be available, which might be a constraint if source data access is strictly limited (though alternatives are discussed). 4. Tackling the challenge of the temporally correlated testing stream is not the primary focus of PeTTA.",
        "future_research_directions": "Future research could explore: 1. Developing algorithms that achieve error accumulation-free by construction, rigorously guaranteeing the complete elimination of performance degradation. 2. Investigating alternative ways to reduce or eliminate the need for a memory bank, such as storing embedded features instead of original images. 3. Addressing the scalability of TTA in real-world scenarios where assumptions about memory banks or source feature statistics might limit deployment. 4. Exploring how to reduce the dependency on source distribution statistics, or improving methods to estimate them under stricter privacy or access constraints. 5. Further research into how different regularization techniques or adaptive parameter adjustments could be theoretically guaranteed to prevent collapse.",
        "experimental_code": "from copy import deepcopy\nimport torch\nimport torch.nn as nn\nimport logging\n\n\nclass BaseAdapter(nn.Module):\n    def __init__(self, cfg, model, optimizer):\n        super().__init__()\n        self.logger = logging.getLogger(\"TTA.adapter\")\n        self.cfg = cfg\n        self.model = self.configure_model(model)\n\n        params, param_names = self.collect_params(self.model)\n        if len(param_names) == 0:\n            self.optimizer = None\n        else:\n            self.optimizer = optimizer(params)\n\n        self.steps = self.cfg.OPTIM.STEPS\n        assert self.steps > 0, \"requires >= 1 step(s) to forward and update\"\n\n    def forward(self, x):\n        for _ in range(self.steps):\n            outputs = self.forward_and_adapt(x, self.model, self.optimizer)\n\n        return outputs\n\n    def forward_and_adapt(self, *args):\n        raise NotImplementedError(\"implement forward_and_adapt by yourself!\")\n\n    def configure_model(self, model):\n        raise NotImplementedError(\"implement configure_model by yourself!\")\n\n    def collect_params(self, model: nn.Module):\n        names = []\n        params = []\n\n        for n, p in model.named_parameters():\n            if p.requires_grad:\n                names.append(n)\n                params.append(p)\n\n        return params, names\n\n    def check_model(self, model):\n        pass\n\n    def before_tta(self, *args, **kwargs):\n        pass\n\n    @staticmethod\n    def build_ema(model):\n        ema_model = deepcopy(model)\n        for param in ema_model.parameters():\n            param.detach_()\n        return ema_model\n\n\n\n@torch.jit.script\ndef softmax_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\n\nimport torch\nimport torch.nn as nn\nfrom ..utils import memory\nfrom .base_adapter import BaseAdapter\nfrom copy import deepcopy\nfrom .base_adapter import softmax_entropy\nfrom ..utils.bn_layers import RobustBN1d, RobustBN2d\nfrom ..utils.utils import set_named_submodule, get_named_submodule\nfrom ..utils.custom_transforms import get_tta_transforms\n\n\nclass RoTTA(BaseAdapter):\n    def __init__(self, cfg, model, optimizer):\n        super(RoTTA, self).__init__(cfg, model, optimizer)\n        self.mem = memory.CSTU(capacity=self.cfg.ADAPTER.RoTTA.MEMORY_SIZE, num_class=cfg.CORRUPTION.NUM_CLASS, lambda_t=cfg.ADAPTER.RoTTA.LAMBDA_T, lambda_u=cfg.ADAPTER.RoTTA.LAMBDA_U)\n        self.model_ema = self.build_ema(self.model)\n        self.transform = get_tta_transforms(cfg)\n        self.nu = cfg.ADAPTER.RoTTA.NU\n        self.update_frequency = cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY  # actually the same as the size of memory bank\n        self.current_instance = 0\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, batch_data, model, optimizer):\n        # batch data\n        with torch.no_grad():\n            model.eval()\n            self.model_ema.eval()\n            ema_out = self.model_ema(batch_data)\n            predict = torch.softmax(ema_out, dim=1)\n            pseudo_label = torch.argmax(predict, dim=1)\n            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)\n\n        # add into memory\n        for i, data in enumerate(batch_data):\n            p_l = pseudo_label[i].item()\n            uncertainty = entropy[i].item()\n            current_instance = (data, p_l, uncertainty)\n            self.mem.add_instance(current_instance)\n            self.current_instance += 1\n\n            if self.current_instance % self.update_frequency == 0:\n                self.update_model(model, optimizer)\n\n        return ema_out\n\n    def update_model(self, model, optimizer):\n        model.train()\n        self.model_ema.train()\n        # get memory data\n        sup_data, ages = self.mem.get_memory()\n        l_sup = None\n        if len(sup_data) > 0:\n            sup_data = torch.stack(sup_data)\n            strong_sup_aug = self.transform(sup_data)\n            ema_sup_out = self.model_ema(sup_data)\n            stu_sup_out = model(strong_sup_aug)\n            instance_weight = timeliness_reweighting(ages)\n            l_sup = (softmax_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()\n\n        l = l_sup\n        if l is not None:\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n\n        self.update_ema_variables(self.model_ema, self.model, self.nu)\n\n    @staticmethod\n    def update_ema_variables(ema_model, model, nu):\n        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n            ema_param.data[:] = (1 - nu) * ema_param[:].data[:] + nu * param[:].data[:]\n        return ema_model\n\n    def configure_model(self, model: nn.Module):\n\n        model.requires_grad_(False)\n        normlayer_names = []\n\n        for name, sub_module in model.named_modules():\n            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):\n                normlayer_names.append(name)\n\n        for name in normlayer_names:\n            bn_layer = get_named_submodule(model, name)\n            if isinstance(bn_layer, nn.BatchNorm1d):\n                NewBN = RobustBN1d\n            elif isinstance(bn_layer, nn.BatchNorm2d):\n                NewBN = RobustBN2d\n            else:\n                raise RuntimeError()\n\n            momentum_bn = NewBN(bn_layer,\n                                self.cfg.ADAPTER.RoTTA.ALPHA)\n            momentum_bn.requires_grad_(True)\n            set_named_submodule(model, name, momentum_bn)\n        return model\n\n\ndef timeliness_reweighting(ages):\n    if isinstance(ages, list):\n        ages = torch.tensor(ages).float().cuda()\n    return torch.exp(-ages) / (1 + torch.exp(-ages))\n\nimport torch\nimport torch.nn as nn\nfrom copy import deepcopy\n\n\nclass MomentumBN(nn.Module):\n    def __init__(self, bn_layer: nn.BatchNorm2d, momentum):\n        super().__init__()\n        self.num_features = bn_layer.num_features\n        self.momentum = momentum\n        if bn_layer.track_running_stats and bn_layer.running_var is not None and bn_layer.running_mean is not None:\n            self.register_buffer(\"source_mean\", deepcopy(bn_layer.running_mean))\n            self.register_buffer(\"source_var\", deepcopy(bn_layer.running_var))\n            self.source_num = bn_layer.num_batches_tracked\n        self.weight = deepcopy(bn_layer.weight)\n        self.bias = deepcopy(bn_layer.bias)\n\n        self.register_buffer(\"target_mean\", torch.zeros_like(self.source_mean))\n        self.register_buffer(\"target_var\", torch.ones_like(self.source_var))\n        self.eps = bn_layer.eps\n\n        self.current_mu = None\n        self.current_sigma = None\n\n    def forward(self, x):\n        raise NotImplementedError\n\n\nclass RobustBN1d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=0, unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1), var.view(1, -1)\n        else:\n            mean, var = self.source_mean.view(1, -1), self.source_var.view(1, -1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1)\n        bias = self.bias.view(1, -1)\n\n        return x * weight + bias\n\n\nclass RobustBN2d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=[0, 2, 3], unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1, 1, 1), var.view(1, -1, 1, 1)\n        else:\n            mean, var = self.source_mean.view(1, -1, 1, 1), self.source_var.view(1, -1, 1, 1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1, 1, 1)\n        bias = self.bias.view(1, -1, 1, 1)\n\n        return x * weight + bias\n\nimport random\nimport copy\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport math\n\n\nclass MemoryItem:\n    def __init__(self, data=None, uncertainty=0, age=0):\n        self.data = data\n        self.uncertainty = uncertainty\n        self.age = age\n\n    def increase_age(self):\n        if not self.empty():\n            self.age += 1\n\n    def get_data(self):\n        return self.data, self.uncertainty, self.age\n\n    def empty(self):\n        return self.data == \"empty\"\n\n\nclass CSTU:\n    def __init__(self, capacity, num_class, lambda_t=1.0, lambda_u=1.0):\n        self.capacity = capacity\n        self.num_class = num_class\n        self.per_class = self.capacity / self.num_class\n        self.lambda_t = lambda_t\n        self.lambda_u = lambda_u\n\n        self.data: list[list[MemoryItem]] = [[] for _ in range(self.num_class)]\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls)\n        return occupancy\n\n    def per_class_dist(self):\n        per_class_occupied = [0] * self.num_class\n        for cls, class_list in enumerate(self.data):\n            per_class_occupied[cls] = len(class_list)\n\n        return per_class_occupied\n\n    def add_instance(self, instance):\n        assert (len(instance) == 3)\n        x, prediction, uncertainty = instance\n        new_item = MemoryItem(data=x, uncertainty=uncertainty, age=0)\n        new_score = self.heuristic_score(0, uncertainty)\n        if self.remove_instance(prediction, new_score):\n            self.data[prediction].append(new_item)\n        self.add_age()\n\n    def remove_instance(self, cls, score):\n        class_list = self.data[cls]\n        class_occupied = len(class_list)\n        all_occupancy = self.get_occupancy()\n        if class_occupied < self.per_class:\n            if all_occupancy < self.capacity:\n                return True\n            else:\n                majority_classes = self.get_majority_classes()\n                return self.remove_from_classes(majority_classes, score)\n        else:\n            return self.remove_from_classes([cls], score)\n\n    def remove_from_classes(self, classes: list[int], score_base):\n        max_class = None\n        max_index = None\n        max_score = None\n        for cls in classes:\n            for idx, item in enumerate(self.data[cls]):\n                uncertainty = item.uncertainty\n                age = item.age\n                score = self.heuristic_score(age=age, uncertainty=uncertainty)\n                if max_score is None or score >= max_score:\n                    max_score = score\n                    max_index = idx\n                    max_class = cls\n\n        if max_class is not None:\n            if max_score > score_base:\n                self.data[max_class].pop(max_index)\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_majority_classes(self):\n        per_class_dist = self.per_class_dist()\n        max_occupied = max(per_class_dist)\n        classes = []\n        for i, occupied in enumerate(per_class_dist):\n            if occupied == max_occupied:\n                classes.append(i)\n\n        return classes\n\n    def heuristic_score(self, age, uncertainty):\n        return self.lambda_t * 1 / (1 + math.exp(-age / self.capacity)) + self.lambda_u * uncertainty / math.log(self.num_class)\n\n    def add_age(self):\n        for class_list in self.data:\n            for item in class_list:\n                item.increase_age()\n        return\n\n    def get_memory(self):\n        tmp_data = []\n        tmp_age = []\n\n        for class_list in self.data:\n            for item in class_list:\n                tmp_data.append(item.data)\n                tmp_age.append(item.age)\n\n        tmp_age = [x / self.capacity for x in tmp_age]\n\n        return tmp_data, tmp_age\n\nimport torch\nimport torchvision.transforms.functional as F\nfrom torchvision.transforms import ColorJitter, Compose, Lambda\nfrom numpy import random\nimport PIL\nimport torchvision.transforms as transforms\n\n\ndef get_tta_transforms(cfg, gaussian_std: float=0.005, soft=False):\n    img_shape = (*cfg.INPUT.SIZE, 3)\n    n_pixels = img_shape[0]\n\n    clip_min, clip_max = 0.0, 1.0\n\n    p_hflip = 0.5\n\n    tta_transforms = transforms.Compose([\n        Clip(0.0, 1.0),\n        ColorJitterPro(\n            brightness=[0.8, 1.2] if soft else [0.6, 1.4],\n            contrast=[0.85, 1.15] if soft else [0.7, 1.3],\n            saturation=[0.75, 1.25] if soft else [0.5, 1.5],\n            hue=[-0.03, 0.03] if soft else [-0.06, 0.06],\n            gamma=[0.85, 1.15] if soft else [0.7, 1.3]\n        ),\n        transforms.Pad(padding=int(n_pixels / 2), padding_mode='edge'),\n        transforms.RandomAffine(\n            degrees=[-8, 8] if soft else [-15, 15],\n            translate=(1/16, 1/16),\n            scale=(0.95, 1.05) if soft else (0.9, 1.1),\n            shear=None,\n            resample=PIL.Image.BILINEAR,\n            fillcolor=None\n        ),\n        transforms.GaussianBlur(kernel_size=5, sigma=[0.001, 0.25] if soft else [0.001, 0.5]),\n        transforms.CenterCrop(size=n_pixels),\n        transforms.RandomHorizontalFlip(p=p_hflip),\n        GaussianNoise(0, gaussian_std),\n        Clip(clip_min, clip_max)\n    ])\n    return tta_transforms\n\n\nclass GaussianNoise(torch.nn.Module):\n    def __init__(self, mean=0., std=1.):\n        super().__init__()\n        self.std = std\n        self.mean = mean\n\n    def forward(self, img):\n        noise = torch.randn(img.size()) * self.std + self.mean\n        noise = noise.to(img.device)\n        return img + noise\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\n\nclass Clip(torch.nn.Module):\n    def __init__(self, min_val=0., max_val=1.):\n        super().__init__()\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def forward(self, img):\n        return torch.clip(img, self.min_val, self.max_val)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(min_val={0}, max_val={1})'.format(self.min_val, self.max_val)\n\n\nclass ColorJitterPro(ColorJitter):\n    \"\"\"Randomly change the brightness, contrast, saturation, and gamma correction of an image.\"\"\"\n\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, gamma=0):\n        super().__init__(brightness, contrast, saturation, hue)\n        self.gamma = self._check_input(gamma, 'gamma')\n\n    @staticmethod\n    @torch.jit.unused\n    def get_params(brightness, contrast, saturation, hue, gamma):\n        \"\"\"Get a randomized transform to be applied on image.\n\n        Arguments are same as that of __init__.\n\n        Returns:\n            Transform which randomly adjusts brightness, contrast and\n            saturation in a random order.\n        \"\"\"\n        transforms = []\n\n        if brightness is not None:\n            brightness_factor = random.uniform(brightness[0], brightness[1])\n            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n\n        if contrast is not None:\n            contrast_factor = random.uniform(contrast[0], contrast[1])\n            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n\n        if saturation is not None:\n            saturation_factor = random.uniform(saturation[0], saturation[1])\n            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n\n        if hue is not None:\n            hue_factor = random.uniform(hue[0], hue[1])\n            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n\n        if gamma is not None:\n            gamma_factor = random.uniform(gamma[0], gamma[1])\n            transforms.append(Lambda(lambda img: F.adjust_gamma(img, gamma_factor)))\n\n        random.shuffle(transforms)\n        transform = Compose(transforms)\n\n        return transform\n\n    def forward(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image or Tensor): Input image.\n\n        Returns:\n            PIL Image or Tensor: Color jittered image.\n        \"\"\"\n        fn_idx = torch.randperm(5)\n        for fn_id in fn_idx:\n            if fn_id == 0 and self.brightness is not None:\n                brightness = self.brightness\n                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()\n                img = F.adjust_brightness(img, brightness_factor)\n\n            if fn_id == 1 and self.contrast is not None:\n                contrast = self.contrast\n                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()\n                img = F.adjust_contrast(img, contrast_factor)\n\n            if fn_id == 2 and self.saturation is not None:\n                saturation = self.saturation\n                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()\n                img = F.adjust_saturation(img, saturation_factor)\n\n            if fn_id == 3 and self.hue is not None:\n                hue = self.hue\n                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()\n                img = F.adjust_hue(img, hue_factor)\n\n            if fn_id == 4 and self.gamma is not None:\n                gamma = self.gamma\n                gamma_factor = torch.tensor(1.0).uniform_(gamma[0], gamma[1]).item()\n                img = img.clamp(1e-8, 1.0)  # to fix Nan values in gradients, which happens when applying gamma\n                                            # after contrast\n                img = F.adjust_gamma(img, gamma_factor)\n\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        format_string += 'brightness={0}'.format(self.brightness)\n        format_string += ', contrast={0}'.format(self.contrast)\n        format_string += ', saturation={0}'.format(self.saturation)\n        format_string += ', hue={0})'.format(self.hue)\n        format_string += ', gamma={0})'.format(self.gamma)\n        return format_string\n\nimport numpy as np\nfrom torch.utils.data.sampler import Sampler\nfrom .datasets.base_dataset import DatumBase\nfrom typing import List\nfrom collections import defaultdict\nfrom numpy.random import dirichlet\n\n\nclass LabelDirichletDomainSequence(Sampler):\n    def __init__(self, data_source: List[DatumBase], gamma, batch_size, slots=None):\n\n        self.domain_dict = defaultdict(list)\n        self.classes = set()\n        for i, item in enumerate(data_source):\n            self.domain_dict[item.domain].append(i)\n            self.classes.add(item.label)\n        self.domains = list(self.domain_dict.keys())\n        self.domains.sort()\n\n        self.data_source = data_source\n        self.gamma = gamma\n        self.batch_size = batch_size\n        self.num_class = len(self.classes)\n        if slots is not None:\n            self.num_slots = slots\n        else:\n            self.num_slots = self.num_class if self.num_class <= 100 else 100\n\n    def __len__(self):\n        return len(self.data_source)\n\n    def __iter__(self):\n        final_indices = []\n        for domain in self.domains:\n            indices = np.array(self.domain_dict[domain])\n            labels = np.array([self.data_source[i].label for i in indices])\n\n            class_indices = [np.argwhere(labels == y).flatten() for y in range(self.num_class)]\n            slot_indices = [[] for _ in range(self.num_slots)]\n\n            label_distribution = dirichlet([self.gamma] * self.num_slots, self.num_class)\n\n            for c_ids, partition in zip(class_indices, label_distribution):\n                for s, ids in enumerate(np.split(c_ids, (np.cumsum(partition)[:-1] * len(c_ids)).astype(int))):\n                    slot_indices[s].append(ids)\n\n            for s_ids in slot_indices:\n                permutation = np.random.permutation(range(len(s_ids)))\n                ids = []\n                for i in permutation:\n                    ids.extend(s_ids[i])\n                final_indices.extend(indices[ids])\n\n        return iter(final_indices)\n\nimport logging\nimport torch\nimport argparse\n\nfrom core.configs import cfg\nfrom core.utils import *\nfrom core.model import build_model\nfrom core.data import build_loader\nfrom core.optim import build_optimizer\nfrom core.adapter import build_adapter\nfrom tqdm import tqdm\nfrom setproctitle import setproctitle\n\n\ndef testTimeAdaptation(cfg):\n    logger = logging.getLogger(\"TTA.test_time\")\n    # model, optimizer\n    model = build_model(cfg)\n\n    optimizer = build_optimizer(cfg)\n\n    tta_adapter = build_adapter(cfg)\n\n    tta_model = tta_adapter(cfg, model, optimizer)\n    tta_model.cuda()\n\n    loader, processor = build_loader(cfg, cfg.CORRUPTION.DATASET, cfg.CORRUPTION.TYPE, cfg.CORRUPTION.SEVERITY)\n\n    tbar = tqdm(loader)\n    for batch_id, data_package in enumerate(tbar):\n        data, label, domain = data_package[\"image\"], data_package['label'], data_package['domain']\n        if len(label) == 1:\n            continue  # ignore the final single point\n        data, label = data.cuda(), label.cuda()\n        output = tta_model(data)\n        predict = torch.argmax(output, dim=1)\n        accurate = (predict == label)\n        processor.process(accurate, domain)\n        if batch_id % 10 == 0:\n            if hasattr(tta_model, \"mem\"):\n                tbar.set_postfix(acc=processor.cumulative_acc(), bank=tta_model.mem.get_occupancy())\n            else:\n                tbar.set_postfix(acc=processor.cumulative_acc())\n\n    processor.calculate()\n\n    logger.info(f\"All Results\\n{processor.info()}\")",
        "experimental_info": "The method, implemented as `RoTTA` (Robust Test-Time Adaptation), is built upon a mean teacher update mechanism. It utilizes an Exponential Moving Average (EMA) model (`model_ema`) as the teacher.\n\n**Adaptation Mechanism:**\n1.  **Mean Teacher Update:** The student model is updated via an optimizer, and the teacher model (`model_ema`) is updated using an EMA of the student model's parameters with a fixed rate `nu` (configured by `cfg.ADAPTER.RoTTA.NU`, default 0.001).\n2.  **Memory Bank:** A `CSTU` (Class-balanced Spatio-Temporal Uncertainty) memory bank is used to store past samples. The memory has a fixed `capacity` (`cfg.ADAPTER.RoTTA.MEMORY_SIZE`, default 64) and maintains class balance by trying to keep an equal number of samples `per_class`.\n3.  **Instance Selection for Memory:** New instances are added to the memory bank based on a `heuristic_score` that considers both the `age` of the instance (how long it has been in memory, weighted by `lambda_t`, default 1.0) and its `uncertainty` (measured as entropy of the EMA model's prediction, weighted by `lambda_u`, default 1.0). Older or more uncertain samples are prioritized for replacement if the memory is full or a class is over-represented.\n4.  **Anchor Loss (LAL):** When the memory bank is updated (every `cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY` instances, default 64), a loss is calculated for the stored memory samples. This loss is an `softmax_entropy` (equivalent to negative KL divergence) between the student model's predictions on strongly augmented memory samples (`strong_sup_aug`) and the teacher model's predictions on the original memory samples (`ema_sup_out`). These losses are weighted by `timeliness_reweighting`, which exponentially decays weights based on the age of the sample.\n5.  **Robust Batch Normalization:** Standard `BatchNorm` layers in the model are replaced with `RobustBN1d` or `RobustBN2d` modules. These robust BN layers update their running mean and variance using a momentum `cfg.ADAPTER.RoTTA.ALPHA` (default 0.05) from the current batch's statistics, allowing them to adapt to target domain shifts while maintaining robustness.\n6.  **Strong Augmentations:** `get_tta_transforms` applies strong data augmentations (including `ColorJitterPro`, `RandomAffine`, `GaussianBlur`, `RandomHorizontalFlip`, `GaussianNoise`, and `Clip`) to memory samples before feeding them to the student model for anchor loss calculation.\n\n**Experimental Scenario:**\n*   **Recurring TTA:** The system is evaluated in a recurring TTA scenario where test environments recur multiple times. This is facilitated by the `LabelDirichletDomainSequence` sampler, which organizes data into domains and processes them sequentially, simulating prolonged exposure.\n*   **Dataset and Corruption:** Experiments are conducted on `cfg.CORRUPTION.DATASET` (e.g., CIFAR-10/100) with various `cfg.CORRUPTION.TYPE` (e.g., gaussian_noise, defocus_blur) and `cfg.CORRUPTION.SEVERITY` levels.\n\n**Configuration Parameters:**\n*   `cfg.ADAPTER.NAME`: \"rotta\"\n*   `cfg.ADAPTER.RoTTA.MEMORY_SIZE`: Capacity of the memory bank (default: 64)\n*   `cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY`: How often the model is updated using memory samples (default: 64)\n*   `cfg.ADAPTER.RoTTA.NU`: EMA update rate for the teacher model (default: 0.001)\n*   `cfg.ADAPTER.RoTTA.ALPHA`: Momentum for updating RobustBN layers' statistics (default: 0.05)\n*   `cfg.ADAPTER.RoTTA.LAMBDA_T`: Weight for age in memory heuristic score (default: 1.0)\n*   `cfg.ADAPTER.RoTTA.LAMBDA_U`: Weight for uncertainty (entropy) in memory heuristic score (default: 1.0)\n*   `cfg.OPTIM.METHOD`: Optimizer type (e.g., 'Adam', 'SGD')\n*   `cfg.OPTIM.LR`: Learning rate for adaptation (default: 1e-3)\n*   `cfg.TEST.BATCH_SIZE`: Batch size for evaluation (default: 64)"
      }
    },
    {
      "title": "Test Time Adaptation via Conjugate Pseudo-labels",
      "abstract": "Test-time adaptation (TTA) refers to adapting neural networks to distribution\nshifts, with access to only the unlabeled test samples from the new domain at\ntest-time. Prior TTA methods optimize over unsupervised objectives such as the\nentropy of model predictions in TENT [Wang et al., 2021], but it is unclear\nwhat exactly makes a good TTA loss. In this paper, we start by presenting a\nsurprising phenomenon: if we attempt to meta-learn the best possible TTA loss\nover a wide class of functions, then we recover a function that is remarkably\nsimilar to (a temperature-scaled version of) the softmax-entropy employed by\nTENT. This only holds, however, if the classifier we are adapting is trained\nvia cross-entropy; if trained via squared loss, a different best TTA loss\nemerges. To explain this phenomenon, we analyze TTA through the lens of the\ntraining losses's convex conjugate. We show that under natural conditions, this\n(unsupervised) conjugate function can be viewed as a good local approximation\nto the original supervised loss and indeed, it recovers the best losses found\nby meta-learning. This leads to a generic recipe that can be used to find a\ngood TTA loss for any given supervised training loss function of a general\nclass. Empirically, our approach consistently dominates other baselines over a\nwide range of benchmarks. Our approach is particularly of interest when applied\nto classifiers trained with novel loss functions, e.g., the recently-proposed\nPolyLoss, where it differs substantially from (and outperforms) an\nentropy-based loss. Further, we show that our approach can also be interpreted\nas a kind of self-training using a very specific soft label, which we refer to\nas the conjugate pseudolabel. Overall, our method provides a broad framework\nfor better understanding and improving test-time adaptation. Code is available\nat https://github.com/locuslab/tta_conjugate.",
      "full_text": "Test-Time Adaptation via Conjugate Pseudo-labels Sachin Goyal⋆1 Mingjie Sun⋆1 Aditi Raghunathan1 Zico Kolter1,2 1Carnegie Mellon University, 2Bosch Center for AI {sachingo, mingjies, raditi, zkolter}@cs.cmu.edu Abstract Test-time adaptation (TTA) refers to adapting neural networks to distribution shifts, with access to only the unlabeled test samples from the new domain at test-time. Prior TTA methods optimize over unsupervised objectives such as the entropy of model predictions in TENT [50], but it is unclear what exactly makes a good TTA loss. In this paper, we start by presenting a surprising phenomenon: if we attempt to meta-learn the “best” possible TTA loss over a wide class of functions, then we recover a function that isremarkably similar to (a temperature-scaled version of) the softmax-entropy employed by TENT. This only holds, however, if the classiﬁer we are adapting is trained via cross-entropy loss; if the classiﬁer is trained via squared loss, a different “best” TTA loss emerges. To explain this phenomenon, we analyze test-time adaptation through the lens of the training losses’sconvex conjugate. We show that under natural conditions, this (unsupervised) conjugate function can be viewed as a good local approximation to the original supervised loss and indeed, it recovers the “best” losses found by meta-learning. This leads to a generic recipe that can be used to ﬁnd a good TTA loss for any given supervised training loss function of a general class. Empirically, our approach consistently dominates other TTA alternatives over a wide range of domain adaptation benchmarks. Our approach is particularly of interest when applied to classiﬁers trained with novel loss functions, e.g., the recently-proposed PolyLoss [25] function, where it differs substantially from (and outperforms) an entropy-based loss. Further, we show that our conjugate based approach can also be interpreted as a kind of self-training using a very speciﬁc soft label, which we refer to as the conjugate pseudo-label. Overall, our method provides a broad framework for better understanding and improving test-time adaptation. Code is available at https://github.com/locuslab/ tta_conjugate. 1 Introduction Modern deep networks perform exceeding well on new test inputs that are close to the training distribution. However, this performance dramatically decreases on test inputs drawn from a different distribution. While there is a large body of work on improving the robustness of models, most robust training methods are highly specialized to the setting they cater to. For e.g., they assume pre-speciﬁed perturbations, subpopulations, and spurious correlations, or access to unlabeled data from the target distribution, and most methods offer close to no improvement on general distribution shifts beyond what they were trained for [12, 21]. In practice, it is often cumbersome (or even impossible) to precisely characterize all possible distri- bution shifts a model could encounter and then train accordingly. Instead, a model already trained on some source data must be able to adapt at test-time to new inputs from a different domain. This setting of test-time adaptation (TTA) has gained interest in recent years [ 6, 47, 50, 54]. TTA is typically accomplished by updating the source model parameters via a few steps of optimization on an unsupervised objective involving the new test sample from the target distribution. The choice ⋆ Equal Contribution 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2207.09640v2  [cs.LG]  23 Nov 2022of this unsupervised objective, which we call the TTA loss, dictates the success of the adaptation procedure. [47] uses a self-supervised objective on the test sample, [50] uses the entropy of model predictions, and several follow-ups have proposed variants or alternatives [ 40, 54]. However, it remains unclear as to how to choose or guide the selection of this TTA loss, and thus far the choice of these losses has remained largely heuristic in nature. In this work, we begin by presenting a set of intriguing experiments where we attempt to learn the “best” TTA loss for a given source classiﬁer and distribution shift. We parameterize the TTA loss by another neural network whose parameters are learnt via meta-learning [ 3, 9] where we differentiate through the adaptation process to ﬁnd the TTA loss that achieves the best adaptation on distribution shifts. Surprisingly, we ultimately learn a TTA loss that looksremarkably similar to (a temperature-scaled version of) the softmax-entropy loss, which was already proposed by [50]. Why did we recover the commonly used softmax-entropy loss despite the fact that the procedure is capable of learning a very general class of losses and the meta-learning process could potentially specialize to both the source classiﬁer and the distribution shift of interest? Furthermore, we ﬁnd that this pattern only holds when the loss used to train the source classiﬁer is cross-entropy loss; when a different loss such as squared loss is used instead, the meta-learning procedure recovers a TTA loss that itself looks more like a negative squared error, and is very different from the softmax-entropy loss (Section 3). In order to explain this phenomenon, we propose to consider TTA through the lens of the convex conjugate function. Speciﬁcally, given a hypothesis function h(x) and label y, several common losses (cross-entropy and the squared loss amongst them, but not limited to these) can be written in the form L(h(x),y) = f(h(x)) −yTh(x) for some function f. In these cases, we show that “natural” TTA loss for such classiﬁers is precisely the (negation of) the convex conjugate evaluated at the gradient of h, LTTA(x) = −f∗(∇f(h(x)), where f∗is the convex conjugate of f. This framework not only recovers the results of our meta-learning experiments, but also justiﬁes why some speciﬁc choices of TTA loss in the previous literature work well (e.g., this framework recovers TENT’s choice of softmax-entropy for cross-entropy-trained classiﬁer). Moreover, it also provides a broad framework for what the TTA loss should be when the source model is trained using various different loss functions (for example the recently-proposed PolyLoss [25, 29]) as is becoming increasingly common in machine learning. Further, we show that our proposed conjugate adaptation loss is in fact a kind of self-training with pseudo-labels [42], a classic approach in machine learning. Various formulations of the pseudo-label have been proposed in the literature, and our conjugate analysis provides a general recipe for the “correct” choice of soft pseudo-labels given byˆy(x) = ∇f(h(x)). We thus refer to these as conjugate pseudo-labels (Conjugate PL’s), and believe our work provides a broad framework for understanding adaptation with unlabeled data in general. Finally, we empirically verify the effectiveness of our proposed conjugate adaptation loss across several datasets and training losses, such as cross-entropy and squared loss, along with the recently- proposed PolyLoss [ 25] (which itself has shown higher standard test accuracy on a wide range of vision tasks). Over all models, datasets and training losses, we ﬁnd our proposed conjugate pseudo-labeling consistently outperforms prior TTA losses and improves TTA performance over the current state of the art. 2 Background and preliminaries. Test-time adaptation. We are interested in mapping an input x∈Rd to a label y∈Y. We learn a model hθ : Rd ↦→R|Y|parameterized by θthat maps an input xto predictions hθ(x). We assume access to a trained source model and adapt at test-time over the test input, before making the ﬁnal prediction. This is the standard test-time adaptation (TTA) setting [47, 50]. During TTA, we update the model parameters on an unsupervised objective L(x,hθ). For example, in TENT [50], this loss is the entropy of the softmax-normalized predictions of the model. At each time step of adaptation, we observe a batch of test inputs and we take a gradient step towards optimizing the TTA loss on this test batch. As is standard, we measure the average online performance of models across all steps (number of test batch inputs seen) in the adaptation process. Meta learning the loss function. In order to explore the existence of different TTA losses, we employ the meta-learning procedure where we attempt to learn the TTA loss. We use a similar procedure as prior work on meta-learning loss functions [3, 37] and parameterize the loss function via a neural network mφ : R|Y| ↦→R that takes in the model predictions/logits and outputs a loss value. We want to learn parameter φsuch that when we update θvia the loss function mφ, our ﬁnal 2performance is optimal. In order to do so, let xbe the unlabeled test samples to adapt to, and ybe the corresponding labels. We update θand φalternatively as follows. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt , φt+1 ←φt −β∂L(hθt+1 (x′),y′) ∂φt , (1) where Lis some supervised surrogate loss function such as cross-entropy. Please refer to Appendix A3 for further details regarding meta-learning setup. Note that the meta-learning process above assumes access to labels yof test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We discuss our ﬁndings from this exploration in the next section. 3 Test-time Adaptation via Meta-Learnt Losses The objective used in TENT is the softmax-entropy of the model predictions which essentially makes the classiﬁer more conﬁdent in its current predictions. The same can be achieved by various other loss formulations such as those mentioned in [40]. With so many possible choices for the loss function, what should we use for TTA? In this section, we attempt to answer this empirically and present some intriguing observations. (a)  (b) Figure 1: Visualization of meta loss (blue) by varying one input prediction score. (a) For cross-entropy loss trained model, the learnt meta loss can be approximated with a scaled softmax-entropy function (dashed red). (b) When the source model is trained with a squared loss for classiﬁcation, the learnt meta loss (blue) can be ﬁtted closely with a quadratic function (dashed red), shown in Figure 1b. The range (max/min) of the prediction score (logit) in x-axis is chosen to cover the empirical range of the predicted logits. Experiment 1. We learn the TTA loss parameterized by a neural network via meta-learning as described in Section 2. Our source classiﬁer is a ResNet-26 trained on CIFAR-10 and we adapt to distribution shifts in CIFAR-10-C. We use the 4 labeled validation noises in CIFAR-10-C to learn the meta-loss network parameters and we denote the resulting learnt loss function by meta-TTA loss. We then adapt the source classiﬁer to the test set of 15 corruptions by optimizing the meta-TTA loss. Observations. First, we ﬁnd that TTA using meta-TTA loss performs better than TENT (12.35% vs 13.14%), suggesting that there are better TTA losses than previous losses based on softmax-entropy. However, on examining this meta-TTA loss, we ﬁnd a surprising observation. Figure 1a (blue curve) visualizes the learnt meta-loss over model predictions as we vary a single class prediction with the rest ﬁxed. Qualitatively, the learnt meta-loss looks very similar to softmax-entropy in one dimension. In fact, we can ﬁt it closely with a scaled softmax-entropy function (dashed red curve): α·H(softmax(hθ(x)/T)), where αis a magnitude parameter and T is a temperature scaler. We want to test if the meta-loss is basically learning the softmax-entropy function. Hence, we perform test-time adaptation with the ﬁtted softmax-entropy function instead (dashed red curve) and achieve an error of 12.32%, essentially recovering the performance of meta-TTA. 3Despite the ability to represent many different loss functions and potentially specialize to the CIFAR- 10-C setting, the meta-loss procedure gave back the standard entropy objective.Do we always recover a loss that looks like softmax-entropy? Experiment 2. In an attempt to isolate when we get back the entropy objective, we vary several things. We tried different architectures for the source classiﬁer, different lossesLduring the meta- learning process (1) and different training losses for the source classiﬁer. Results. We observed that we consistently recovered the temperature scaled softmax-entropy function in all cases except when we varied the training loss for the source classiﬁer (Appendix A.10). On using the squared loss function [18], a strikingly different meta-TTA loss emerges. Figure 1b (blue curve) shows the learnt meta-loss (13.48% error) for this network. Here again, the meta-TTA loss outperforms entropy (14.57%) but it is not simply due to a scaling factor. The loss now looks like the negative squared error (red curve). Like previously, we tried ﬁtting a quadratic loss directly to the meta loss in Figure 1b, and this time we even slightly outperformed the meta-TTA loss. To summarize, we used a meta-learning procedure to search for the “best” TTA loss, where the loss itself was parameterized by a neural network that could potentially represent arbitrarily complex loss functions. However, we ended up with loss functions displaying remarkable structure: across different architectures and different variants of meta-learning, for a classiﬁer trained with cross-entropy, the meta-TTA loss was temperature scaled softmax-entropy and for a classiﬁer trained with squared loss, the meta-TTA loss was a negative squared loss. This is interesting from both a practical and conceptual standpoint where the “best” TTA loss depends on the loss used to train the source classiﬁer in a clean fashion. We attempt to understand and explain this phenomenon in the next section. 4 Conjugate Pseudo Labels Results in the previous section raise an obvious question: why does softmax-entropy as used in TENT seem to be the “best” possible test time adaptation loss for classiﬁers trained via cross-entropy (at least, best in the sense that meta-learning consistently recovers something which essentially mimics softmax-entropy, even though meta-loss is parameterized by a neural network and hence could learn much more complex functions speciﬁc to the model and the particular shift)? And why, alternatively, does a quadratic TTA loss seem to perform best when the classiﬁer is trained via squared loss? In this section, we offer an explanation of this phenomenon via the construct of the convex conjugate function [1]. As we will see, our method recovers softmax-entropy and quadratic loss as the “natural” objectives for classiﬁers trained via cross-entropy and squared loss respectively. Furthermore, for classiﬁers trained via other loss functions, as is becoming increasingly common in deep learning, our approach naturally suggests corresponding test-time adaptation losses, which we show in the next section to comparatively outperform alternatives. Thus, we argue that our framework overall provides a compelling recipe for specifying the “correct” method for TTA for a large class of possible losses. 4.1 Losses and the convex conjugate We begin by formally considering loss functions between a hypothesis outputhθ(x) (e.g., the logit outputs of a classiﬁer, or the direct prediction of a regressor) and targetythat take the following form L(hθ(x),y) = f(hθ(x)) −yThθ(x) (2) for some function f; when there is no risk of confusion, we will use hin place of hθ(x) for simplicity of notation. While not every loss can be expressed in such a form, this captures a wide variety of common losses (possibly scaled by a constant value). For example, cross-entropy loss corresponds to the choice f(h) = log ∑ iexp(hi) and where y denotes a one-hot encoding of the class label; similarly, squared loss corresponds to the choice f(h) = 1 2 ∥h∥2 2. When training an over-parameterized classiﬁer, we can roughly view the training process as (approxi- mately) attaining the minimum over hypotheses hfor each training example min θ 1 t t∑ i=1 L(hθ(xi),yi) ≈1 t t∑ i=1 min h L(h,yi) (3) 4where t is the number of training samples. However, in the case of losses in the form (2), the minimization over hin this form represents a very speciﬁc and well-known optimization problem: it is known as the convex conjugate [1] of the function f min h L(h,y) = min h {f(h) −yTh}= −f⋆(y) (4) where f⋆ denotes the convex conjugate of f. f⋆ is a convex function in y(and indeed, is convex regardless of whether or not f is convex). Furthermore, for the case that f is convex differentiable, the optimality condition of this minimization problem is given by ∇f(hopt) = y, so we also have that f⋆(y) = f⋆(∇f(hopt)) (5) where hopt refers to the optimal classiﬁer (used interchangeably with hθopt ). Putting this all together, we can state (admittedly, in a rather informal manner) that under the assumption that θopt is chosen so as to approximately minimize the empirical loss on the source data in the over-parameterized setting, we have that for tinputs 1 t t∑ i=1 L(hθopt (xi),yi) ≈1 t t∑ i=1 −f⋆(∇f(hθopt (xi))) (6) i.e., the empirical loss can be approximated by the (negative) conjugate applied to the gradient of the f, at least in a region close to the optimal θopt that minimizes the empirical loss. But the later expression has the notable beneﬁt that it does not require any label yi in order to compute the loss, and thus can be used as a basis for TTA on target domain of the hypothesis function hθopt . Deﬁnition 1 (conjugate adaptation loss) Consider a loss function that takes the form given in 2, used for training a hypothesis hθ in the over-parameterized regime. We deﬁne the conjugate adaptation loss Lconj(hθ(x)) : R|Y|↦→R as follows. Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))⊤hθ(x). (7) 4.2 Recovery of existing test-time adaptation strategies Cross-entropy The interesting aspect to this formalism is that when applied to classiﬁers trained with cross-entropy, it recovers exactly the TENT approach to TTA : minimizing the softmax-entropy of hθ(x). And indeed, this loss was also recovered when using meta-learning to learn the “optimal” test-time adaptation loss. To see this, note that for cross-entropy, we have thatf(h) = log ∑ iexp(hi), giving the optimality condition y= ∇f(hopt) = exp(hopt)∑ iexp(hopt i ) and the conjugate function f⋆(y) = { ∑ iyilog yi if ∑ iyi = 1 ∞ otherwise . (8) In other words, Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (9) i.e. softmax-entropy of the model prediction, which is exactly the TTA loss that TENT uses. Squared loss For the squared loss, we have thatf(h) = 1 2 ∥h∥2 2, leading to the optimality condition y = hand conjugate function f⋆(y) = 1 2 ∥y∥2 2. Hence, the adaptation loss in this case would be simply given by Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = −1 2 ∥h∥2 2 which is also what we observed in the meta-learning experiments discussed in Section 3. 4.3 Conjugate pseudo-labels We now emphasize that by the nature of our approximations, there is an additional simple interpre- tation of the conjugate loss: it is also equal to the original loss (2) applied to the “psuedo-labels” ˜yCPL θ (x) = ∇f(hθ(x)), where CPL refers to conjugate pseudo-labels, i.e., Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))Thθ(x) = L(hθ(x),∇f(hθ(x))). (10) 5This property is known as the Fenchel-Young inequality, that isf(x) + f⋆(u) ≥xTuholding with equality when u = ∇f(x). In other words, our conjugate adaptation loss is precisely equivalent to self-training under the speciﬁc soft pseudo-labels given by ˜yCPL = ∇f(hθ(x)). And indeed, for many cases, this may be a more convenient form to compute than explicitly computing the conjugate function at all. For this reason, we refer to our method as that of conjugate pseudo-labels. In the case of cross-entropy loss, this approach then corresponds exactly to self-training using labels given by the softmax applied to the current hypothesis. We must emphasize, however, that while our conjugate formulation indeed has this “simple” form for the case of cross-entropy loss, the real advantage comes in that it provides the “correct”pseudo-label for use with other losses, which may result in pseudo-labels different from the “common” softmax operation. Example: conjugate pseudo-labels for PolyLoss. PolyLoss [25] is a recently-proposed simple alternative to cross-entropy loss than has been shown to improve performance across a wide variety of compute tasks. This loss is given by the form Lpoly(hθ(x),y) = Lce(hθ(x),y) + ϵ·yT(1 −softmax(hθ(x))) (11) We note that this can be put exactly into our conjugate form (equation 2) by writing the loss in a slightly more involved fashion, which we refer to as the expanded conjugate form Lpoly(hθ(x),y) = f(hθ(x)) −yTg(hθ(x)). (12) where f is the log-sum-exp function as before, and g(h) = h−ϵ(1 −softmax(h)). In order to formally put this into the form of the previous loss function (equation 2), we can simply deﬁne an alternative hypothesis as the function h′ θ(x) = g(hθ(x)), and then deﬁne PolyLoss in the conjugate form as Lpoly(h′ θ(x),y) = f(g−1(h′ θ(x))) −yTh′ θ(x). (13) Typically, however, it is easier to simply operate on the expanded conjugate form, which yields the optimality condition for the pseudo-label ∇f(hopt) = Dg(hopt)˜yCPL θ (x), where D is the Jacobian operator. For the case of PolyLoss, this leads to the conjugate pseudo-label of the following form: ˜yCPL θ (x) = (I+ ϵdiag(z) −ϵzzT)−1z, z ≡softmax(hθ(x)). Test-time adaptation. Finally, we note that the above discussion doesn’t actually address any topics related to test-time adaptation to OOD data, but merely provides a generic characterization of a self- training procedure for generic loss functions of the form(2). However, the application toTTA on OOD data is fairly straightforward: as long as the learnt source parameters θis a reasonable approximation to the true optimal θopt on the shifted domain, self-training with the conjugate pseudo-labels provides a reasonable proxy for ﬁne-tuning the network on the true OOD loss. We emphasize that, common to most approaches for TTA , there are still some amount of design decisions that must be put in place; these are detailed in Section 5.1. In practice, we observe OOD generalization typically beneﬁts (across all baselines) from an additional “temperature” scaling, i.e., applying the TTA loss to hθ(x)/T for some ﬁxed temperature T, although it requires a held-out validation dataset for tuningT. However, we should emphasize that truly unsupervisedTTA would require making an informed guess for the value of these hyper-parameters. The full procedure for test time adaptation via conjugate pseudo-labels is shown in Algorithm 1. Algorithm 1 Conjugate pseudo-labeling (Conjugate PL) Input: Source classiﬁer θ0 trained using loss L(hθ(x),y) = f(hθ(x)) −hθ(x)⊤y. N batches of test data Dtest = [x1,x2,...,x N] Hyperparams: learning rate ηand temperature T. Let ¯hθ(x) def = hθ(x)/T be the temperature scaled predictor. Let ˜yCPL θ (x) denote the conjugate pseudo-label function ˜yCPL θ (x) = ∇(f(¯hθ(x))). for n= 0,1,...N −1 do θn+1 = θn −η∇L ( ¯hθ(xn),˜yCPL θ (xn) ) [Self-training with conjugate pseudo-labels] 65 Experiments In this section, we empirically evaluate the effectiveness and generality of the proposed conjugate pseudo-labeling procedure (Algorithm 1) for test-time adaptation on a variety of datasets. 5.1 Setup Datasets. We evaluate on the three common corruption benchmarks: adapting a classiﬁer trained on CIFAR-10 to CIFAR-10-C, CIFAR-100 to CIFAR-100-C and ImageNet to ImageNet-C [ 15]. Following the previous works [47, 50], we report the error averaged across corruptions at the highest severity for CIFAR-10/100-C and averaged across corruptions and severity level for ImageNet-C. We also evaluate on three domain adaptation datasets: adapting a classiﬁer trained on SVHN to MNIST, an ImageNet classiﬁer to ImageNet-R [16] and adapting from synthetic to real data in VISDA-C [38]. Models and Training losses. Following previous works on TTA[47, 50], we use ResNet-26 [14] as the source classiﬁer architecture for CIFAR-10/100 experiments, ResNet-18 for SVHN to MNIST and a ResNet-50 for ImageNet and source synthetic data on VisDA-C. We consider source classiﬁers trained via the following loss functions: the de-facto cross-entropy, recently proposed polyloss [25] and squared loss [18]. Baselines. Our proposed conjugate pseudo-label is the classic approach of self-training with a speciﬁc form of pseudo-labels. In self-training, we replace the label ywith a pseudo-label ˜y(x) and adapt by optimizing the loss function L(hθ(x),˜y(x)). Note that we could either instantaneously update the pseudo-labels using the current classiﬁer, or generate pseudo-labels once with just the source classiﬁer. Instantaneous updates have been shown to work better for domain adaptation [7, 40], and we perform instantaneous updates for all methods. While we propose using ˜yCPL(x) = ∇f(hθ(x)) (See Section 4.3), we compare to the standard pseudo-labels used in the literature: • (i) the “hard” pseudo-label (hard PL) where ˜y(x) = arg maxi ( hθ(x) ) i is the most likely class as predicted by hθ. As is common in the self-training literature, we perform conﬁdence thresholding. • (ii) The “soft” pseudo-label (soft PL) where ˜y(x) is obtained by applying a softmax function to the model predictions hθ(x). We also compare with the following recently proposed test-time adaptation methods. • Entropy Minimization (ENT) [50] minimizes the entropy of model predictions. • Robust Pseudo-Label [40] where we minimize a robust classiﬁcation loss, Lrpl = q−1(1 −p(i|x)q) where i= argmaxjp(j|x) and q∈[0,1]. • MEMO [54] minimizes entropy of a model’s outputs across different augmentations of a test input. We implement a batch version, where we see multiple test points at once, for fair comparisons. TTA methodology. Following [ 50] and [40], we ﬁne-tune by updating the learnable scale and shift parameters of the batch normalization layers across all adaptation losses. For each batch, batch normalization statistics is also updated, as suggested in [41]. We report performance at the end of one round of test-time adaptation over the entire test set. We tune the learning rate (LR) and temperature (T) on the validation noises in the corruption benchmark by grid-search. LR is selected from {1e−1,1e−2,... 1e−4}and T from {1,2 ... 5}. All the experiments have been performed on A6000 GPU’s. On domain adaptation benchmarks, where there is no held-out target domain, we set T to be 1 and use the LR suggested by [ 6, 50]. We use the same hyperparameter tuning protocol across all methods. We single out temperature as a very important hyperparameter, as we discuss in the results below. 5.2 Results on classiﬁers trained with cross-entropy We study the effectiveness of our proposed conjugate pseudo-labels when the source classiﬁer is trained via cross-entropy loss. In this case, baselines Softmax PL and ENT are the same as Conjugate PL. Thus we omit them in our results. Table 1, reports the performance of various TTA methods. When the source classiﬁer is trained via cross-entropy, our conjugate pseudo-label algorithm exactly corresponds to entropy minimization with an additional temperature scaling. Entropy minimization as 7Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 13.95 (±0.06) 13.97 ( ±0.04) 12.60(±0.04) 13.07 (±0.05) \u0013 13.95 (±0.06) 12.85 ( ±0.04) 12.51(±0.01) 12.51(±0.03) CIFAR-100-C \u0017 45.22 (±0.4) 39.80 ( ±0.18) 38.52(±0.16) 41.15 (±0.25) \u0013 45.22 (±0.4) 36.37 ( ±0.10) 37.38 ( ±0.06) 36.10(±0.07) ImageNet-C \u0017 45.43(±0.05) 45.68 ( ±0.01) 48.91( ±0.03) 45.82(±0.01) \u0013 45.43 (±0.05) 45.61 ( ±0.01) 48.91( ±0.04) 45.36(±0.01) Table 1: Mean errors when adapting to corruptions using a source classiﬁer trained via cross- entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. With the right temperature scaling, softmax-entropy minimization matches or outperforms other approaches. Prior reported gains of other methods over softmax-entropy minimization disappear when we use temperature scaling. For additional context, the source classiﬁer errors without adaptation are: CIFAR-10-C (29.54%), CIFAR-100-C (62.26%), ImageNet-C (61.89%) proposed in prior work [50] does not tune the temperature parameter, and some newer objectives such as robust PL or MEMO outperform vanilla entropy minimization. For example, on CIFAR-100-C, vanilla ENT obtaines 41.15% average error, while robust PL improves this to39.80% and MEMO to 38.52%. However, with the right temperature scaling, entropy minimization obtains 36.10% error which outperforms the newer objectives (with and without temperature scaling). A similar observation holds for CIFAR-10-C and ImageNet-C as well. Essentially, the gains over vanilla entropy minimization vanish when we do temperature scaling, and entropy minimization (i.e. conjugate pseudo-labeling corresponding to cross-entropy) turns out to be the best objective after all. 5.3 Results on classiﬁers trained with polyloss and squared loss In the case of cross-entropy, conjugate pseudo-labeling reduces to the familiar notion of entropy minimization. We now explore the performance of our method on different loss functions where the conjugate pseudo-labels differ substantially from entropy minimization (section 4.3). Table 2 presents the results on the corruption benchmarks and Table 3 presents the results on the other domain adaptation datasets for source classiﬁers trained with PolyLoss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C \u0017 13.81(±0.12) 14.23(±0.02) 13.46(±0.06) 13.23(±0.07) 14.64(±0.11) 13.02(±0.09) \u0013 13.81(±0.12) 12.45(±0.05) 12.23(±0.06) 12.33(±0.04) 12.26(±0.04) 12.08(±0.05) CIFAR-100-C\u0017 40.47(±0.05) 42.86(±0.11) 40.12(±0.08) 39.90(±0.05) 41.00(±0.11) 38.17(±0.17) \u0013 40.47(±0.05) 39.80(±0.08) 38.23(±0.05) 39.23(±0.04) 37.04(±0.06) 36.83(±0.08) ImageNet-C \u0017 45.44(±0.21) 46.27(±0.03) 46.10(±0.03) 48.21(±0.05) 44.63(±0.03) 44.01(±0.01) \u0013 45.44(±0.21) 46.27(±0.03) 45.50(±0.02) 48.21(±0.04) 44.45(±0.03) 44.01(±0.01) Table 2: Mean errors when adapting to corruptions using a source classiﬁer trained via recently proposed Poly-1 Loss [ 25]. Conjugate pseudo-labeling consistently outperforms all previous ap- proaches. For additional context, source classiﬁer errors without adaptation : CIFAR-10-C (30.22%), CIFAR-100-C (63.91%) and ImageNet-C (62.18%). First, we note that, across all datasets in Table 2 and Table 3, our conjugate PL approach outperforms all other TTA losses. With polyloss classiﬁers, entropy minimization is no longer the best method—on CIFAR-100-C, entropy minimization achieves38.23% error while our conjugate PL achieves36.83%. We see similar consistent gains on CIFAR-10-C, ImageNet-C, ImageNet-R and VisDA-C. On digit adaptation tasks from SVHN to MNIST/USPS/MNISTM, where there is a larger shift between source and target, the gains are especially pronounced. Figure 2 compares how the task loss (polyloss ϵ= 6) on the test data decreases as we adapt the model through conjugate PL and other baselines. We use CIFAR-10-C as an example. Observe that our proposed conjugate PL indeed reduces the task loss the most among other baselines. 8Dataset Source Error Hard PL Robust PL EntropySoftmax PL Conjugate PL Ours SVHN→MNIST 28.33 20.21 19.73 14.28 16.54 10.73 SVHN→USPS 31.58 23.32 26.12 23.12 24.07 21.62 SVHN→MNISTM61.69 50.73 51.35 49.33 50.47 47.59 ImageNet-R 64.19 58.52 59.46 58.25 56.62 55.63 VisDA-C 58.13 40.43 45.44 44.11 39.63 38.42 Table 3: Target error when adapting models trained via polyloss on source domains across different domain adaptation bench- marks. Conjugate pseudo-labeling offers consistent and substan- tial gains over previous approaches across three datasets. Figure 2: Task Loss (PolyLoss ϵ= 6) evaluated on CIFAR-10-C test data during test-time adaptation. Furthermore, on CIFAR-10-C and ImageNet-C, we ﬁnd that adapting polyloss classiﬁers via conjugate PL improves the performance over all methods applied to cross-entropy trained source classiﬁers. For e.g., on ImageNet-C, the performance improves from 45.34% to 44.01%. However, this is only true when using the proposed conjugate PL. If we just did softmax-entropy minimization (even with temperature scaling), the ﬁnal adapted performance of a polyloss classiﬁer (45.5%) is in fact worse than that of a cross-entropy classiﬁer (45.34%). Our results suggest that as we develop new training losses that improve the source classiﬁers, it is important to adapt via conjugate pseudo-labeling to reap the maximum gains. Similarly, we experiment with the case when the source classiﬁer is trained using squared loss on the CIFAR-10 and CIFAR-100 datasets, and observe consistent gains using the proposed conjugate pseudo-labels over the baselines. For example, on CIFAR-10-C, TTA using conjugate PL gives and error of 12.87%, outperforming baselines like ENT (13.24%) and Softmax PL (31.81%). Table 5 in Appendix A.7 shows the detailed results. Comparing Table 1 and Table 2, we see that the relative ordering between the various baselines differs. This is further evidence that the adaptation loss has to depend on the training loss, and we believe our conjugate pseudo-label approach captures this appropriately by offering consistent gains across the various settings we experimented with. 6 Related Works Test-time adaptation methods. In recent years, the setting of test-time adaptation has gained a lot of interest with a host of different approaches proposed in the literature. One family of TTA approaches update the source classiﬁer by minimizing an unsupervised loss on the target distribution [4, 6, 20, 22, 35, 36, 40, 43, 44, 50, 51, 54]. TENT [ 50] proposes to minimize the entropy of model predictions at test time. Several follow ups like [ 6, 35, 40, 44, 54] propose alternative TTA objectives, e.g. robust pseudo-labelling [40], likelihood ratio loss [35], entropy of marginal probability averaged across augmentations [54] and self-supervised contrastive losses [6, 49]. However, most of these objectives are heuristically designed or chosen. In this paper, we provide a principled approach of designing unsupervised objectives for TTA . Another family of approaches for test-time adaptation such as [ 2, 8, 13, 31, 34, 47] leverage an auxiliary self-supervised task (e.g. rotation prediction [ 47], masked autoencoders [10]) to update model parameters on each test sample. Crucially, these methods require modifying the source model training by augmenting the supervised training objective with an auxiliary self-supervised loss. Hence it cannot be applied to typical standard classiﬁers that are trained by minimizing a supervised loss on the source data. Source-free domain adaptation. A very related setting to test-time adaptation is source-free domain adaptation, where a trained source classiﬁer must be adapted to a target distribution of interest, although the entire target unlabeled data is available at once. SHOT [28] proposes to optimize the source hypothesis (i.e. feature extractor) with a combination of entropy minimization, diversity and self-training on pseudo-labels on the unlabeled target data. [53] promotes feature clustering on features from target distributions. [24, 26] use generative modeling to estimate the underlying source distributions for enforcing feature invariance. Such approaches typically require multiple epochs over the target data and cannot be easily adopted to work in an online fashion. 9Unsupervised domain adaptation. The most canonical setting of domain adaptation involves access to labeled source data and unlabeled target data, all during training. The availability of source and target data during training lends itself to approaches that “align” the source and target representations in some way: [ 32, 33, 45, 48] match distribution statistics, [ 11] uses a discriminator, [ 46] uses self-supervised learning. However, such approaches require access to source data which might not always be feasible due to data privacy and efﬁciency issues. Pseudo-labels and self-training. Self-training is a classic idea for leveraging unlabeled data, devel- oped ﬁrst for the semi-supervised setting. Self-training generates pseudo-labels on the unlabeled data, allowing us to use any “supervised” loss on this pseudo-labeled data. Self-training has shown promising results in various settings like semi-supervised learning [ 19] and improving adversarial robustness [ 5]. Self-training has also been gaining attention in the setting of unsupervised domain adaptation [28, 39], where pseudo-labels generated on the unlabeled data from target domain is used to supervise the adaptation process. [ 7, 23, 52] provide theoretical insights into how self-training with pseudo-labels can help under distribution shift. TENT [50] (i.e entropy minimization) can be viewed as a form of self-training with instantaneous softmax pseudo-labels. Our work provides a general framework for the choice of soft pseudo-labels based on the conjugate analysis of the source training objective. Some prior works like [7, 17, 27, 30, 55, 56] have documented the improvement in performance when using instantaneous pseudo-labels over pre-computed pseudo-labels, and thus lend further support to the beneﬁts of our proposed conjugate pseudo-labeling approach. The ex- periment results presented in this work supporting conjugate pseudo-labels suggest that conjugate pseudo-labels is a promising direction of pseudo-labeling in a broader context. 7 Conclusion, Limitations and Future Directions In this work, we proposed a general test-time adaptation loss, based on the convex conjugate formulation which in turn was motivated by the intriguing meta learning experiments. The fact that meta-learning recovers the proposed loss hints at some kind of optimality of the loss. In Section 4, we prove that for a broad set of loss functions, the proposed (unsupervised) conjugate loss is close to the oracle supervised loss. However, this still does not completely answer what the optimal test-time adaptation loss is and why. The meta-learning framework in this work was constrained to learn functions over the logits of each individual input. It can be expanded to more involved setups, where we consider functions over the intermediate representations too and also consider learning functions over a batch of input while accounting for their interactions. Beyond the choice of the adaptation loss itself, achieving good test-time adaptation generally involves several heuristics like updating only the batch norm parameters [50]. While our work was motivated by the loss function, via the meta-learning experiments, we discovered that temperature scaling is another important hyper-parameter that improves the performance of all previous baselines as well. At a high level, test-time adaptation has to be appropriately regularized to prevent the updates over batches from taking the model too far: updating only a few batch norm parameters is one way to do that, and perhaps temperature scaling provides a similar beneﬁcial regularization effect by making the network predictions on unlabeled inputs less conﬁdent. Understanding the role of these heuristics more concretely is an interesting direction for future work. It also remains an open problem to understand under what sort of real-world distribution shifts would self-training based approaches would help. Finally, it is also worth extending and applying the conjugate pseudo-labeling to other settings like semi-supervised learning. 8 Acknowledgments We thank Shubhang Bhatnagar and Asher Trockman for helping with running the ImageNet experi- ments. We thank Zhili Feng for useful feedback. Sachin Goyal and Mingjie Sun were supported by funding from the Bosch Center for Artiﬁcial Intelligence. Aditi Raghunathan was supported by an Open Philanthropy AI Fellowship. 10References [1] https://en.wikipedia.org/wiki/Convex_conjugate. [2] Pratyay Banerjee, Tejas Gokhale, and Chitta Baral. Self-supervised test-time learning for reading comprehension. In Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2021. [3] Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, and Franziska Meier. Meta-learning via learned loss. arXiv preprint arXiv:1906.05374, 2019. [4] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [5] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Un- labeled data improves adversarial robustness. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf. [6] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [7] Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. In Advances in Neural Information Processing Systems, 2020. [8] Mohammad Zalbagi Darestani, Jiayu Liu, and Reinhard Heckel. Test-time training can close the natural distribution shift performance gap in deep learning based compressed sensing. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017. [10] Yossi Gandelsaman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. In Advances in Neural Information Processing Systems, 2022. [11] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, 2016. [12] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. InInternational Conference on Learning Representations, 2021. [13] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenya, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In International Conference on Learning Representations, 2021. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [15] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. [16] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [17] Yosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Advancing momentum pseudo-labeling with conformer and initialization strategy. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. 11[18] Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy in classiﬁcation tasks. In International Conference on Learning Representations, 2021. [19] Dong hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. [20] Yusuke Iwasawa and Yutaka Matsuo. Test-time classiﬁer adjustment module for model-agnostic domain generalization. In Advances in Neural Information Processing Systems, 2021. [21] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. [22] Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. Robustifying vision transformer without retraining from scratch by test-time class-conditional feature alignment. In International Joint Conference on Artiﬁcial Intelligence, 2022. [23] Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. In Proceedings of the 37 th International Conference on Machine Learning (ICML), 2020. [24] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2021. [25] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, and Dragomir Anguelov. Polyloss: A polynomial expansion perspective of classiﬁcation loss functions. In International Conference on Learning Representations, 2022. [26] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsuper- vised domain adaptation without source data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [27] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele. Learning to self-train for semi-supervised few-shot classiﬁcation. In Advances in Neural Information Processing Systems, 2019. [28] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. InProceedings of the 37th International Conference on Machine Learning (ICML), 2020. [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In IEEE/CVF International Conference on Computer Vision (ICCV), 2017. [30] Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation. In Advances in Neural Information Processing Systems, 2021. [31] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In Advances in Neural Information Processing Systems, 2021. [32] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer feature learning with joint distribution adaptation. In IEEE/CVF International Conference on Computer Vision (ICCV), 2013. [33] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32nd International Conference on Machine Learning, 2015. [34] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. In SIGGRAPH, 2020. 12[35] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-Time Adaptation to Distribution Shift by Conﬁdence Maximization and Input Transformation. arXiv preprint arXiv: 2106.14999, 2021. [36] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efﬁcient test-time model adaptation without forgetting. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [37] Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado P van Hasselt, Satinder Singh, and David Silver. Discovering reinforcement learning algorithms. In Advances in Neural Information Processing Systems, 2020. [38] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge, 2017. [39] Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [40] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. If your data distribution shifts, use self- learning, 2022. URL https://openreview.net/forum?id=1oEvY1a67c1. [41] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems, 2020. [42] H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transac- tions on Information Theory, 1965. [43] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. [44] Prabhu Teja Sivaprasad and François Fleuret. Test time adaptation through perturbation robust- ness. arXiv preprint arXiv: 2110.10232, 2021. [45] Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. arXiv preprint arXiv: 1612.01939, 2016. [46] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A. Efros. Unsupervised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. [47] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2019. [48] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014. [49] Dequan Wang, Shaoteng Liu, Sayna Ebrahimi, Evan Shelhamer, and Trevor Darrell. On-target adaptation. arXiv preprint arXiv: 2109.01087, 2021. [50] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. [51] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [52] Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations, 2021. 13[53] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [54] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. [55] Yang Zou, Zhiding Yu, B. V . K. Vijaya Kumar, and Jinsong Wang. Domain adaptation for semantic segmentation via class-balanced self-training. European Conference on Computer Vision, 2018. [56] Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V . K. Vijaya Kumar, and Jinsong Wang. Conﬁdence regularized self-training. In IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 14A Appendix A.1 Conjugate Derivations Cross-Entropy Loss : L(h,y) = − c∑ i=1 yilog exp(hi)∑c j=1 exp(hj) = − c∑ i=1 yi ∗hi + log c∑ j=1 exp(hj) = f(h) −y⊤h, (14) where f(h) is log ∑c j=1 exp(hj) and the constraint that ∑c i=1 yi = 1. Now, the conjugate f⋆(y) is given by : f⋆(y) = −min h {f(h) −yTh}= −min h {log c∑ j=1 exp(hj) −yTh} (15) with the constraint ∑c i=1 yi = 1. At the optimality, yi = (∇f(h))i = exp(hi)∑ jexp(hj) (16) Then, f⋆(y) = −log c∑ j=1 exp(hj) + c∑ i=1 hi exp(hi)∑ jexp(hj) = ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj), (17) if the constraint ∑c i=1 yi = 1 is satisﬁed, otherwise f⋆(y) = ∞by duality. This in turn gives, the conjugate loss for cross-entropy (when the constraint is satisﬁed) : Lconj(h) = −f⋆(y) = −f⋆(∇f(h)) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (18) Squared Loss : L(h,y) = 1 2||h−y||2 2 ≈1 2||h||2 2 −y⊤h [ignoring the constant term] = f(h) −y⊤h, (19) Now, the conjugate f⋆(y) is given by: f⋆(y) = −min h {f(h) −yTh}= −min h {1 2||h||2 2 −yTh} = −1 2||h||2 2 (20) A.2 Experiments on Binary Classiﬁcation with Exponential Loss Here we present the results on a binary classiﬁcation task over a synthetic dataset of 100 dimensional gaussian clusters. 15Dataset Creation For the binary classiﬁcation task, we create a synthetic dataset similar to [23]. Speciﬁcally, let the data X ∼ N(µ,Σ) ∈ R100 and labels Y ∈ {−1,+1}. We sample µ ∼ N(k,I100). For Σ, similar to [ 23], we sample a diagonal matrix D, where each entry is sampled uniformly from a speciﬁed range, and a rotation matrix U from a HAAR distribution, giving Σ = UDUT. For the source data, we sample µ−1 s ,µ+1 s ,Σ−1 s ,Σ+1 s as speciﬁed above with k= 0. Now to create a distribution shifted data of various severity, we sampleµ−1 t ,µ+1 t ,Σ−1 t ,Σ+1 t as speciﬁed above with k= 1, which are then used to sample the shifted data as follows : µ1 λ = λµ1 t + (1 −λ)µ1 s µ−1 λ = λµ−1 t + (1 −λ)µ−1 s Σ1 λ = λΣ1 t + (1 −λ)Σ1 s Σ−1 λ = λΣ−1 t + (1 −λ)Σ−1 s Xλ ∼N(µλ,Σλ) In the following experiments, easy shift refers to λ= 0.6, moderate shift to λ= 0.65 and hard shift to λ= 0.7. Exponential Loss for Binary Classiﬁcation Let zbe the classiﬁcation score hθ(x). For logistic training loss, conjugate adaptation loss would default to entropy with sigmoid probability. Thus, here we experiment with a different but also commonly used surrogate loss to 0/1 loss: exponential loss, which is deﬁned as: Lexp(z,y) = exp(−yz) (21) where y∈{−1,+1}. It can be rewritten in the expanded conjugate form of: Lexp(z,y) = 1 2 · ( ez + e−z) −1 2 ·y· ( ez −e−z) (22) For exponential loss, the conjugate pseudo-label function and the conjugate pseudo-label loss are: yCPL exp (z) = ez −e−z ez + e−z, LCPL exp (z) = 2 ez + e−z (23) The model is adapted on shifted gaussian clusters and we compare the conjugate loss with two baseline approaches: 1) Hard pseudo-labelling exp(−yhard pl ·z); 2) Entropy applied to sigmoid probability P(y= +1) = σ(z). The losses are compared on three degrees of shift (easy, moderate and hard), which is controlled by the drifted distance of Gaussian clusters. The results are shown in Figure 3, where we plot the accuracy curve with respect to adaptation iterations. With easy and moderate shift, conjugate loss (green) generalizes faster to shifted test data; with hard shift, only conjugate loss improves model accuracy on shifted test data while entropy (blue) deteriorates model performance. Figure 3: Test-time adaptation result on synthetic data with three shift levels ranging from easy, moderate and hard (detailed in section A.2). The source model is a linear classiﬁer trained with exponential loss Lexp = e−yhθ(x). Adaptation with the conjugate loss generalizes better compared to baseline losses. 16A.3 Meta Learning Experiment Details In section 3 we talked about learning the meta-loss function parameterized by a neural network mφ : R|Y|↦→R, that takes in the model predictions/logits and outputs a loss value. Here we discuss the architecture chosen and the implementation details. Further, in Appendix A.4 we empirically show that the learnt meta-loss is not affected by the choice of task loss / surrogate loss used in meta learning (Lin Equation 1). Note that the task loss / surrogate loss function is used to update the meta-loss mφ during meta-learning. The surrogate loss is calculated on updated source model’s predictions on labeled samples from test domain. The surrogate loss tries to update the meta-loss in the outer loop such that when meta-loss is later used to update the source model in the inner loop, the source model generalizes better to the test domain. Architecture and Implementation Details Figure 4 gives an overall schema for meta-learning the loss function and algorithm 2 gives the pseudo-code for meta-learning the loss function. Below we describe this in further detail. We use a transformer (denoted by T) with a MLP (denoted by P) over the output of transformer as the architecture for mφ, i.e. mφ(x) = P(T(x)). Speciﬁcally, for a given source trained model hθ and input x∼Dtest : 1. Let hθ(x) ∈R|Y|be the model predictions/logits, where |Y|denotes the number of classes. 2. Let hj θ(x) ∈R,∀j ∈|Y| be the prediction corresponding to class j. 3. The input to transformer is then given by z ∈R|Y|×(1+e), where zj ∈R1+e,∀j ∈|Y| is the concatenation of hj θ(x) and the learnable positional embedding pej ∈Re. 4. The transformer output is given by w= T(z) ∈Rd, where ddenotes the feed-forward dimension of the transformer. 5. The transformer output wis ﬁnally passed through a MLP to get the meta-loss valuemφ(hθ(x)) = P(w) ∈R 6. The source model is updated by optimizing over the meta-loss. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt (24) 7. The updated source model is then used to update the meta-loss by optimizing over some supervised loss function Ltask. φt+1 ←φt −β∂Ltask(hθt+1 (x′),y′) ∂φt , where (x′,y′) ∼Dtest (25) Note that the last step assumes access to labels of test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We select the trasformer input embedding dimension (1 + e) from {16,32,64}and transformer feed-forward dimension dfrom {32,64,128}. The number of transformer layers and the hidden layers in MLP are selected from {1,2}. We use Adam optimizer with a learning rate of 1e−3 for learning the meta-loss (i.e. the transformer + MLP). We train the meta-loss for 100 epochs with a batch size of 200. A.4 Effect of Task Loss in Meta Learning In section 3, we show that the meta losses learned on different source classiﬁers differ substantially if the source classiﬁers are trained using different source loss functions. Here we further empirically verify that the learnt meta loss is not affected by the task loss used in meta learning (Lin Equation 1). Thus the learnt meta loss is determined by the source model. In Figure 5, we show the meta loss learnt on a ResNet-26 trained with Cross Entropy loss for two meta task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. We plot the meta loss as a function over one of its input prediction scores, while keeping other ﬁxed. We can see that the task loss barely affects the learnt meta loss. Similar observations can be made for the classiﬁer trained with squared loss Figure 6. 17Meta-Loss  Backpropogate  Figure 4: Meta-Loss learning procedure : The model predictions hθt(x) are passed through the parameterized loss function mφt, which outputs a loss value. We optimize φ such that when optimizing the source model over the loss mφt(hθt(x)), the updated θt+1 has a better performance on the test domain. To do this, we take one gradient step over the meta-loss to get the update source model parameters θt+1, and then update φby evaluating θt+1 on the labeled validation data using some task loss Ltask. Algorithm 2 Learning the Meta-Loss Input: Source trained classiﬁer hθ0 . Randomly initialized meta-loss mφ0 . Task loss / Surrogate loss Ltask like cross-entropy or squared loss for meta learning N batches of test data Dtest = [(x1,y1),..., (xN,yN)] Hyperparams: learning rates αand β. for epoch= 0,1,2,... do for n= 0,1,...N −1 do θt+1 ←θt −α ∂mφt(hθt(xn)) ∂θt Sample (xr,yr) ∼Dtest. φt+1 ←φt −β∂Ltask(hθt+1 (xr),yr) ∂φt A.5 Test-Time Adaptation Detail For completeness, we also give the test-time adaptation setup in Algorithm 3. A.6 ImageNet results on each severity level In continuation with results shown in Table 2 in Section 5.3, Table 4 shows the mean errors averaged across the 15 corruption types for each of the severity level on ImageNet-C, for a source classiﬁer trained with PolyLoss (ϵ= 8). A.7 Square Loss Trained Source Classiﬁer In Section 5.3, we brieﬂy discussed that similar to the other source training losses like cross-entropy and polyloss, our proposed conjugate loss outperforms the baselines when the source classiﬁer is 18(a)  (b) Figure 5: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Cross Entropy. Here we show meta loss trained by two different task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. (a)  (b) Figure 6: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Squared Loss. Here we show meta loss trained by two different task losses: Cross Entropy Figure 6a and Squared Loss Figure 6b. Algorithm 3 Test-Time Adaptation Input: Source classiﬁer θ0 trained using loss L(hθ(x),y), An unsupervised loss function for test-time adaptation Ltta(x), N batches of test data Dtest = [x1,...,x N] Hyperparams: learning rate η. for n= 0,1,...N −1 do θn+1 = θn −η∇Ltta(xn) ˆyn = hθn+1 (xn) [Predictions for the nth batch] 19Corrution Severity Temperature Robust PL Entropy MEMO Softmax PL Conjugate 1 \u0017 34.27 33.17 34.39 32.49 32.26 \u0013 34.27 32.84 34.39 32.70 32.26 2 \u0017 41.25 39.04 40.38 37.78 37.40 \u0013 41.25 38.50 40.38 37.75 37.40 3 \u0017 47.37 44.04 45.67 42.30 41.72 \u0013 47.37 43.33 45.67 42.14 41.72 4 \u0017 56.63 51.88 54.49 49.61 48.84 \u0013 56.63 51.03 54.49 49.39 48.84 5 \u0017 67.11 62.53 66.13 60.94 59.90 \u0013 67.11 61.80 66.13 60.30 59.90 Mean \u0017 49.32 46.13 48.21 44.62 44.02 \u0013 49.32 45.50 48.21 44.45 44.02 Table 4: Mean Errors across the 15 noises for various severity level on the ImageNet-C dataset, with source model trained using Poly-1 Loss. Note that Temperature scaling helped only in the case of Entropy and Softmax PL. trained using a squared loss. Table 5 shows a detailed comparison with the baselines. We note that for the conjugate of squared loss, the temperature scaling can be wrapped into the learning rate as shown in Section 4.2. Further, on the CIFAR-10-C dataset we observe temperature scaling doesn’t help any of the other baselines too, hence we do not include the temperature row in CIFAR-10-C. Dataset Temperature Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL CIFAR-10-C \u0017 13.71 (±0.07) 13.06 (±0.05) 13.24 (±0.02) 13.22 (±0.04) 14.85 (±0.08)12.99(±0.04) CIFAR-100-C \u0017 50.82 (±0.31) 44.53 (±0.13) 43.55 (±0.12) 51.35 (±0.04) 51.99 (±0.03)43.39(±0.11) \u0013 50.82 (±0.31) 43.99 (±0.15)43.21(±0.08) 51.35 (±0.04) 51.99 (±0.03) 43.39 (±0.11) Table 5: Mean Errors on the common corruptions datasets for source classiﬁer trained using squared loss. We note that temperature scaling didn’t help on the CIFAR-10-C dataset. Source Classiﬁer Errors without adaptation : CIFAR-10-C (28.34%), CIFAR-100-C (68.79%) Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,1 e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1 e−2, 2 SGD,5 e−3, 3 Adam,1e−3, 2 CIFAR-100-C \u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,5 e−3, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD,1e−2, 2 ImageNet-C \u0017 SGD,1e−2, 1 SGD,2.5 e−3, 1 SGD,1 e−3, 1 SGD,2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1.5 SGD,1e−3, 1 SGD,2.5e−3, 1.5 Table 6: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 1, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using cross-entropy loss. A.8 Hyper-Parameters We share the exact hyper-parameters found using gridsearch over the 4 validation noises for the common corruptions dataset. 20Cross Entropy Classiﬁer Experiments In Section 5.2, Table 1 shows the results when adapting a cross entropy trained classiﬁer on various common corruptions dataset. Table 6 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. PolyLoss Classiﬁer Experiments In Section 5.3, Table 2 shows the results when adapting a polyloss trained classiﬁer on various common corruptions dataset. Table 7 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−3, 1 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,5 e−3, 1 SGD, 1e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1e−2, 3 SGD,1 e−2, 3 SGD,5 e−3, 3 SGD, 1e−3, 2 SGD, 1e−3, 1.5 CIFAR-100-C\u0017 SGD,1e−2, 1 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD, 1e−2, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 Adam,1e−3, 3 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD, 1e−2, 2.5 SGD, 1e−2, 1.5 ImageNet-C\u0017 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1 SGD,5e−3, 1 SGD, 2.5e−3, 1 SGD, 2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1.5 SGD,5e−3, 1 SGD, 2.5e−3, 2 SGD, 2.5e−3, 1 Table 7: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 2, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using poly-loss. Squared Loss Classiﬁer Experiments In Section 5.3, we brieﬂy discussed the results when adapt- ing a squared loss trained classiﬁer on various common corruptions dataset. Table 8 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss for the results in Table 5. Digit Adaptation Datasets For the experiments on digits adaptation tasks, we do not have any validation set. Hence, we don’t use temperature scaling here (T = 1) and ﬁx the optimizer and LR as Adam and 1e−2 respectively for all the baselines. A.9 Additional Experiments on Digit Adaptation Datasets Similar to the setting of Table 1, we perform additional experiments on digit adaptation datasets when the source classiﬁer is trained using the cross-entropy loss. Note that when the source classiﬁer is trained using cross-entropy loss, the conjugate loss is equal to the softmax-entropy. In the absence of validation dataset in digit adaptation benchmarks, we used a ﬁxed learning rate of 0.01 for all the baselines, optimizer as Adam and an informed temperature scaling guess of T=2. Table 9 compares softmax-entropy minimization with various baselines. Here, again we observe that on SVHN →MNIST benchmark, without temperature scaling, MEMO (10.67% error) outperforms softmax-entropy (14.41% error). However, similar to the observations in Table 1, with temperature scaling, softmax-entropy minimization (9.26% error) is able to match the performance of MEMO (9.36% error). Further, on the SVHN →USPS benchmark, softmax-entropy (conjugate) and MEMO perform similar even without temperature scaling. A.10 Additional Meta Learning the TTA Loss Experiments In Section 3, we tried to learn a test-time adaptation (TTA) loss via meta-learning for adapting a CIFAR10 trained ResNet26 to distribution shifts on CIFAR10 corruptions. Figure 1 showed that the learnt meta-loss looks like a temperature scaled softmax-entropy. In this section, we show the learnt meta loss across a range of settings as described below : 1. Digit Adaptation: Figure 7a and 7b show the learnt meta-loss when adapting a SVHN trained ResNet26 to MNIST dataset and USPS dataset respectively. We observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 2. Various Noise Types: In Figure 8, we show the learnt meta-loss when adapting a ResNet26 trained on CIFAR10 dataset using cross-entropy loss, to various noise types like speckle, gaussian, saturate and spatter. The severity level is kept ﬁxed at the maximum i.e. 5. 21Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD,1e−2, 1 SGD,1 e−4, 1 SGD,1e−2, 1 CIFAR-100-C\u0017 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam, 1e−4, 1 Adam, 1e−3, 1 \u0013 Adam,1e−3, 1 Adam,1e−3, 0.5 Adam,1e−3, 2 Adam,1e−3, 2 Adam, 1e−4, 2.5 Adam, 1e−3, 1 Table 8: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 5, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using squared loss. Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) SVHN→MNIST \u0017 21.54 27.44 10.67 14.41 \u0013 21.54 13.26 9.36 9.26 SVHN→USPS \u0017 26.06 26.81 22.72 22.57 \u0013 26.06 22.32 22.42 22.27 Table 9: Mean errors when adapting to digit adaptation benchmarks using a source classiﬁer trained via cross-entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. Again we observe that with the right temperature scaling, softmax-entropy minimization matches other approaches. For additional context, the source classiﬁer errors without adaptation are: SVHN →MNIST (34.17%), SVHN →USPS (31.84%). 20  10  0 10 20 prediction score 5 0 5 10loss value meta loss (error 10.44%) softmax entropy (error 14.41) fitted entropy (error 9.26) Meta Loss for SVHN -> MNIST (a) 20  10  0 10 20 prediction score 6 4 2 0 2 4 6 8 loss value meta loss (error 20.13%) softmax entropy (error 22.57) fitted entropy (error 22.22) Meta Loss for SVHN -> USPS adpatation (b) Figure 7: Visualizations of the learnt meta-loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with cross-entropy on the SVHN dataset. (a) The learnt meta-loss when adapting to the MNIST test dataset. (b) The learnt meta-loss when adapting to the USPS test dataset. 3. Various Severity Levels: In Figure 9, we vary the severity level of the noise, keeping the noise type ﬁxed. 4. Dataset and Architecture: In Figure 10, we compare the learnt meta-loss when adapting to speckle noise, for different source classiﬁer architectures (ResNet26 and ResNet50) and different source training dataset (CIFAR10 and CIFAR100). In all the cases, we again observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 5. Squared Loss : Finally, in Figure 11 we show the learnt meta-loss for classiﬁers trained with squared loss function instead of cross-entropy. We observe that in this case, the learnt meta loss mimics a quadratic function as expected from the conjugate formulation. 22For each of the learnt meta losses, we also show the values (α,T,C ) we use to ﬁt the meta loss with softmax entropy function: α·H(softmax(x/T)) −C. Note that although the learnt meta-loss can be approximated by the conjugate, the parameters α,T,C differ across the settings. In the case of classiﬁers trained with squared loss, we ﬁt the meta loss with a quadratic function∑K i=1(A·x2 i + C), where Kis the number of classes and xis the logit vector. Again, we also show the ﬁtted parameter value A,C. The meta loss follows the trend of a quadratic function. The ﬁtted quadratic function performs better or similar as the meta loss, while the parameters of the ﬁtted quadratic function remain different across the meta learning setup (base classiﬁer architectures and noise types). (a)  (b) (c)  (d) Figure 8: Visualization of meta loss (blue) learnt from various noise types in CIFAR-10-C validation set, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 23(a)  (b) (c)  (d) Figure 9: Visualization of meta loss (blue) learnt on speckle noise with different severity level for CIFAR-10-C, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 24(a)  (b) (c)  (d) Figure 10: Visualization of meta loss (blue) learnt across datasets (CIFAR-10-C/CIFAR-100-C) and base classiﬁer architectures (ResNet-26/ResNet-50), where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters ( α,T,C ) in the ﬁtted entropy. (a)  (b) Figure 11: Visualization of meta loss (blue), where base classiﬁer is trained with quadratic loss. We show the error of meta loss, softmax entropy and ﬁtted quadratic function for test-time adaptation on the corresponding noise types. We also show the parameters ( A,B,C ) in the ﬁtted quadratic function. 25",
      "meta_data": {
        "arxiv_id": "2207.09640v2",
        "authors": [
          "Sachin Goyal",
          "Mingjie Sun",
          "Aditi Raghunathan",
          "Zico Kolter"
        ],
        "published_date": "2022-07-20T04:02:19Z",
        "pdf_url": "https://arxiv.org/pdf/2207.09640v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the ambiguity of choosing effective test-time adaptation (TTA) losses for neural networks under distribution shifts. It proposes a novel approach based on the convex conjugate of the source training loss, revealing that the 'best' TTA loss is intrinsically linked to the original training objective. Key contributions include: 1) Demonstrating via meta-learning that the optimal TTA loss recovers softmax-entropy for cross-entropy trained classifiers and negative squared error for squared-loss trained classifiers. 2) Developing a generic, principled recipe for deriving TTA losses using the convex conjugate function, which approximates the supervised loss without labels. 3) Introducing 'conjugate pseudo-labels' (CPL) as a self-training mechanism derived from this conjugate analysis, offering a 'correct' choice of soft pseudo-labels for various loss functions, especially novel ones like PolyLoss. 4) Achieving state-of-the-art performance across diverse domain adaptation benchmarks, consistently outperforming prior TTA methods.",
        "methodology": "The research employs three main methodological components. First, a meta-learning procedure is used to empirically search for the 'best' TTA loss function. This involves parameterizing the TTA loss as a neural network and updating its parameters by differentiating through an adaptation process to optimize final performance on distribution shifts. Second, the core of the proposed method is the analysis of TTA through the lens of the convex conjugate function. For a class of loss functions L(h(x),y) = f(h(x)) −yTh(x), the conjugate adaptation loss is defined as Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))⊤hθ(x), where f⋆ is the convex conjugate of f. This framework recovers previously observed 'optimal' losses. Third, the conjugate adaptation loss is interpreted as a self-training approach utilizing 'conjugate pseudo-labels' (CPL), where ˜yCPLθ (x) = ∇f(hθ(x)). The adaptation process involves updating model parameters by optimizing L(¯hθ(xn),˜yCPLθ (xn)), where ¯hθ(x) = hθ(x)/T (temperature-scaled predictions). This method applies to various source training losses, including cross-entropy, squared loss, and PolyLoss, with specific CPL formulations derived for each.",
        "experimental_setup": "The effectiveness and generality of the conjugate pseudo-labeling procedure were evaluated across various datasets and models. **Datasets:** Common corruption benchmarks (CIFAR-10-C, CIFAR-100-C, ImageNet-C) and domain adaptation datasets (SVHN to MNIST, ImageNet to ImageNet-R, VISDA-C). **Models:** ResNet-26 for CIFAR experiments, ResNet-18 for SVHN to MNIST, and ResNet-50 for ImageNet and VisDA-C. **Source Training Losses:** Classifiers were trained using cross-entropy, PolyLoss, and squared loss. **Baselines:** Hard Pseudo-Labeling, Soft Pseudo-Labeling, Entropy Minimization (TENT), Robust Pseudo-Label, and MEMO. **TTA Methodology:** Fine-tuning involved updating only the learnable scale and shift parameters of batch normalization layers, with batch normalization statistics updated per batch. Instantaneous pseudo-label updates were used. Performance was reported as average online error. **Hyperparameter Tuning:** Learning rates (1e-1 to 1e-4) and temperature (1 to 5) were tuned via grid-search on validation noises for corruption benchmarks. For domain adaptation, temperature was fixed at 1, and learning rates from prior work were used. All experiments were performed on A6000 GPUs.",
        "limitations": "The current work has several limitations: 1) While the conjugate formulation hints at optimality, it does not fully explain why it is the optimal test-time adaptation loss. 2) The meta-learning framework used for exploration was constrained to learning functions over individual input logits, and could be expanded to include intermediate representations or batch interactions. 3) Achieving good TTA often relies on heuristics such as updating only batch normalization parameters or temperature scaling; a more concrete understanding of their regularization role is needed. 4) It remains an open problem to identify the specific real-world distribution shifts where self-training based approaches are most beneficial.",
        "future_research_directions": "Future research directions include: 1) Expanding the meta-learning framework to learn more complex TTA losses, potentially incorporating intermediate representations and interactions within a batch of inputs. 2) Gaining a more concrete understanding of the role and benefits of common TTA heuristics, such as updating only batch normalization parameters and temperature scaling, particularly how they provide regularization. 3) Investigating and characterizing the types of real-world distribution shifts where self-training based approaches, like conjugate pseudo-labeling, are most effective. 4) Extending the application of conjugate pseudo-labeling to other machine learning settings, such as semi-supervised learning."
      }
    },
    {
      "title": "TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive?"
    },
    {
      "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
      "abstract": "In this paper, we propose Test-Time Training, a general approach for\nimproving the performance of predictive models when training and test data come\nfrom different distributions. We turn a single unlabeled test sample into a\nself-supervised learning problem, on which we update the model parameters\nbefore making a prediction. This also extends naturally to data in an online\nstream. Our simple approach leads to improvements on diverse image\nclassification benchmarks aimed at evaluating robustness to distribution\nshifts.",
      "full_text": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Yu Sun1 Xiaolong Wang1 2 Zhuang Liu1 John Miller1 Alexei A. Efros1 Moritz Hardt1 Abstract In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a sin- gle unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on di- verse image classiﬁcation benchmarks aimed at evaluating robustness to distribution shifts. 1. Introduction Supervised learning remains notoriously weak at generaliza- tion under distribution shifts. Unless training and test data are drawn from the same distribution, even seemingly minor differences turn out to defeat state-of-the-art models (Recht et al., 2018). Adversarial robustness and domain adapta- tion are but a few existing paradigms that try to anticipate differences between the training and test distribution with either topological structure or data from the test distribution available during training. We explore a new take on gener- alization that does not anticipate the distribution shifts, but instead learns from them at test time. We start from a simple observation. The unlabeled test sample xpresented at test time gives us a hint about the distribution from which it was drawn. We propose to take advantage of this hint on the test distribution by allowing the model parameters θto depend on the test sample x, but not its unknown label y. The concept of a variable decision boundary θ(x) is powerful in theory since it breaks away from the limitation of ﬁxed model capacity (see additional discussion in Section A1), but the design of a feedback mechanism from xto θ(x) raises new challenges in practice that we only begin to address here. 1University of California, Berkeley 2University of California, San Diego. Correspondence to: Yu Sun <yusun@berkeley.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Our proposed test-time training method creates a self- supervised learning problem based on this single test sample x, updating θat test time before making a prediction. Self- supervised learning uses an auxiliary task that automatically creates labels from unlabeled inputs. In our experiments, we use the task of rotating each input image by a multiple of 90 degrees and predicting its angle (Gidaris et al., 2018). This approach can also be easily modiﬁed to work outside the standard supervised learning setting. If several test samples arrive in a batch, we can use the entire batch for test-time training. If samples arrive in an online stream, we obtain further improvements by keeping the state of the parameters. After all, prediction is rarely a single event. The online version can be the natural mode of deployment under the additional assumption that test samples are produced by the same or smoothly changing distribution shifts. We experimentally validate our method in the context of object recognition on several standard benchmarks. These include images with diverse types of corruption at various levels (Hendrycks & Dietterich, 2019), video frames of moving objects (Shankar et al., 2019), and a new test set of unknown shifts collected by (Recht et al., 2018). Our algorithm makes substantial improvements under distribu- tion shifts, while maintaining the same performance on the original distribution. In our experiments, we compare with a strong baseline (labeled joint training) that uses both supervised and self- supervised learning at training-time, but keeps the model ﬁxed at test time. Recent work shows that training-time self- supervision improves robustness (Hendrycks et al., 2019a); our joint training baseline corresponds to an improved imple- mentation of this work. A comprehensive review of related work follows in Section 5. We complement the empirical results with theoretical inves- tigations in Section 4, and establish an intuitive sufﬁcient condition on a convex model of when Test-Time Training helps; this condition, roughly speaking, is to have correlated gradients between the loss functions of the two tasks. Project website: https://test-time-training.github.io/. arXiv:1909.13231v3  [cs.LG]  1 Jul 2020Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 2. Method This section describes the algorithmic details of our method. To set up notation, consider a standard K-layer neural net- work with parameters θk for layer k. The stacked parameter vector θ = ( θ1,...,θ K) speciﬁes the entire model for a classiﬁcation task with loss function lm(x,y; θ) on the test sample (x,y). We call this the main task, as indicated by the subscript of the loss function. We assume to have training data (x1,y1),..., (xn,yn) drawn i.i.d. from a distribution P. Standard empirical risk minimization solves the optimization problem: min θ 1 n n∑ i=1 lm(xi,yi; θ). (1) Our method requires a self-supervised auxiliary task with loss function ls(x). In this paper, we choose the rotation prediction task (Gidaris et al., 2018), which has been demon- strated to be simple and effective at feature learning for convolutional neural networks. The task simply rotates x in the image plane by one of 0, 90, 180 and 270 degrees and have the model predict the angle of rotation as a four- way classiﬁcation problem. Other self-supervised tasks in Section 5 might also be used for our method. The auxiliary task shares some of the model parameters θe = ( θ1,...,θ κ) up to a certain κ ∈ {1,...,K }. We designate those κlayers as a shared feature extractor. The auxiliary task uses its own task-speciﬁc parameters θs = (θ′ κ+1,...,θ ′ K). We call the unshared parameters θs the self-supervised task branch, and θm = (θκ+1,...,θ K) the main task branch . Pictorially, the joint architecture is a Y-structure with a shared bottom and two branches. For our experiments, the self-supervised task branch has the same architecture as the main branch, except for the output dimensionality of the last layer due to the different number of classes in the two tasks. Training is done in the fashion of multi-task learning (Caru- ana, 1997); the model is trained on both tasks on the same data drawn fromP. Losses for both tasks are added together, and gradients are taken for the collection of all parameters. The joint training problem is therefore min θe,θm,θs 1 n n∑ i=1 lm(xi,yi; θm,θe) + ls(xi; θs,θe). (2) Now we describe the standard version of Test-Time Training on a single test sample x. Simply put, Test-Time Training ﬁne-tunes the shared feature extractor θe by minimizing the auxiliary task loss on x. This can be formulated as min θe ls(x; θs,θe). (3) Denote θ∗ e the (approximate) minimizer of Equation 3. The model then makes a prediction using the updated parameters θ(x) = (θ∗ e,θm). Empirically, the difference is negligible between minimizing Equation 3 over θe versus over both θe and θs. Theoretically, the difference exists only when optimization is done with more than one gradient step. Test-Time Training naturally beneﬁts from standard data augmentation techniques. On each test sample x, we per- form the exact same set of random transformations as for data augmentation during training, to form a batch only con- taining these augmented copies of xfor Test-Time Training. Online Test-Time Training. In the standard version of our method, the optimization problem in Equation 3 is al- ways initialized with parameters θ= (θe,θs) obtained by minimizing Equation 2. After making a prediction on x, θ∗ e is discarded. Outside of the standard supervised learning setting, when the test samples arrive online sequentially, the online version solves the same optimization problem as in Equation 3 to update the shared feature extractor θe. How- ever, on test sample xt, θis instead initialized with θ(xt−1) updated on the previous sample xt−1. This allows θ(xt) to take advantage of the distributional information available in x1,...,x t−1 as well as xt. 3. Empirical Results We experiment with both versions of our method (standard and online) on three kinds of benchmarks for distribution shifts, presented here in the order of visually low to high- level. Our code is available at the project website. Network details. Our architecture and hyper-parameters are consistent across all experiments. We use ResNets (He et al., 2016b), which are constructed differently for CIFAR-10 (Krizhevsky & Hinton, 2009) (26-layer) and Ima- geNet (Russakovsky et al., 2015) (18-layer). The CIFAR-10 dataset contains 50K images for training, and 10K images for testing. The ImageNet contains 1.2M images for train- ing and the 50K validation images are used as the test set. ResNets on CIFAR-10 have three groups, each containing convolutional layers with the same number of channels and size of feature maps; our splitting point is the end of the second group. ResNets on ImageNet have four groups; our splitting point is the end of the third group. We use Group Normalization (GN) instead of Batch Nor- malization (BN) in our architecture, since BN has been shown to be ineffective when training with small batches, for which the estimated batch statistics are not accurate (Ioffe & Szegedy, 2015). This technicality hurts Test-Time Training since each batch only contains (augmented) copies of a single image. Different from BN, GN is not dependent on batch size and achieves similar results on our baselines.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure 1.Test error (%) on CIFAR-10-C with level 5 corruptions.We compare our approaches, Test-Time Training (TTT) and its online version (TTT-Online), with two baselines: object recognition without self-supervision, and joint training with self-supervision but keeping the model ﬁxed at test time. TTT improves over the baselines and TTT-Online improves even further. We report results with BN in Section A4 of the appendix for completeness. We directly compare our architecture to that of Hendrycks et al. (2018) in subsection A4.5. Optimization details. For joint training (Equation 2), we use stochastic gradient descent with standard hyper- parameters as (Huang et al., 2016; He et al., 2016a). For Test-Time Training (Equation 3), we use stochastic gradient descent with the learning rate set to that of the last epoch during training, which is 0.001 in all our experiments. We set weight decay and momentum to zero during Test-Time Training, inspired by practice in (He et al., 2018; Liu et al., 2018). For the standard version of Test-Time Training, we take ten gradient steps, using batches independently gener- ated by the same image. For online version of Test-Time Training, we take only one gradient step given each new im- age. We use random crop and random horizontal ﬂip for data augmentation. See Section A2 of the appendix for computa- tional aspects of our method. In all the tables and ﬁgures, object recognition task onlyrefers to the plain ResNet model (using GN, unless otherwise speciﬁed); joint training refers to the model jointly trained on both the main task and the self-supervised task, ﬁxed at test time; this has been pro- posed as the method in Hendrycks et al. (2019a); Test-Time Training (TTT) refers to the standard version described sec- tion 2; and online Test-Time Training (TTT-Online)refers to the online version that does not discardθ(xt) for xt arriving sequentially from the same distribution. Performance for TTT-Online is calculated as the average over the entire test set; we always shufﬂe the test set before TTT-Online to avoid ordering artifacts. 3.1. Object Recognition on Corrupted Images Hendrycks & Dietterich (2019) propose to benchmark ro- bustness of object recognition with 15 types of corruptions from four broad categories: noise, blur, weather and digital. Each corruption type comes in ﬁve levels of severity, with level 5 the most severe (details and sample images in the ap- pendix). The corruptions are simulated to mimic real-world corruptions as much as possible on copies of the test set for both CIFAR-10 and ImageNet. The new test sets are named as CIFAR-10-C and ImageNet-C, respectively. In the pro- posed benchmark, training should be done on the original training set, and the diversity of corruption types should make it difﬁcult for any methods to work well across the board if it relies too much on corruption speciﬁc knowledge. For online Test-Time Training, we take the entire test set as a stream of incoming images, and update and test on each image in an online manner as it arrives. CIFAR-10-C. Our results on the level 5 corruptions (most severe) are shown in Figure 1. The results on levels 1-4 are shown in Section A4 in appendix. Across all ﬁve levels and 15 corruption types, both standard and online versions of Test-Time Training improve over the object recognition task only baseline by a large margin. The standard version always improves over joint training, and the online version often improves signiﬁcantly (>10%) over joint training and never hurts by more than 0.2%. Speciﬁcally, TTT-Online contributes >24% on the three noise types and 38% on pix- elation. For a learning problem with the seemingly unstable setup that abuses a single image, this kind of consistency is rather surprising. The baseline ResNet-26 with object recognition task only has error 8.9% on the original test set of CIFAR-10. The joint training baseline actually improves performance on the original to 8.1%. More surprisingly, unlike many other methods that trade off original performance for robustness, Test-Time Training further improves on the original test set by 0.2% consistently over multiple independent trials. This suggests that our method does not choose between speciﬁcity and generality.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Accuracy (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online 0 20000 40000 Number of samples 60 62 64 66 68 70 72 74 76Accuracy (%) Original Sliding window average 0 20000 40000 Number of samples 12 15 18 21 24 27 30 33Accuracy (%) Gaussian Noise Sliding window average 0 20000 40000 Number of samples 16 18 20 22 24 26 28 30 32Accuracy (%) Defocus Blur Sliding window average 0 20000 40000 Number of samples 28 30 32 34 36 38Accuracy (%) Zoom Blur Sliding window average 0 20000 40000 Number of samples 33 36 39 42 45 48 51 54Accuracy (%) Fog Sliding window average 0 20000 40000 Number of samples 30 33 36 39 42 45 48 51Accuracy (%) Elastic Transform Sliding window average Figure 2.Test accuracy (%) on ImageNet-C with level 5 corruptions.Upper panel: Our approaches, TTT and TTT-Online, show signiﬁcant improvements in all corruption types over the two baselines. Lower panel: We show the accuracy of TTT-Online as the average over a sliding window of 100 samples; TTT-Online generalizes better as more samples are evaluated (x-axis), without hurting on the original distribution. We use accuracy instead of error here because the baseline performance is very low for most corruptions. Separate from our method, it is interesting to note that joint training consistently improves over the single-task baseline, as discovered by Hendrycks et al. (2019a). Hendrycks & Dietterich (2019) have also experimented with various other training methods on this benchmark, and point to Adversar- ial Logit Pairing (ALP) (Kannan et al., 2018) as the most effective approach. Results of this additional baseline on all levels of CIFAR-10-C are shown in the appendix, along with its implementation details. While surprisingly robust under some of the most severe corruptions (especially the three noise types), ALP incurs a much larger error (by a factor of two) on the original distribution and some corruptions (e.g. all levels of contrast and fog), and hurts performance signiﬁcantly when the corruptions are not as severe (espe- cially on levels 1-3); this kind of tradeoff is to be expected for methods based on adversarial training. ImageNet-C. Our results on the level 5 corruptions (most severe) are shown in Figure 2. We use accuracy instead of error for this dataset because the baseline performance is very low for most corruptions. The general trend is roughly the same as on CIFAR-10-C. The standard version of TTT always improves over the baseline and joint training, while the online version only hurts on the original by 0.1% over the baseline, but signiﬁcantly improves (by a factor of more than three) on many of the corruption types. In the lower panel of Figure 2, we visualize how the accu- racy (averaged over a sliding window) of the online version changes as more images are tested. Due to space constraints, we show this plot on the original test set, as well as every third corruption type, following the same order as in the original paper. On the original test set, there is no visible trend in performance change after updating on the 50,000 samples. With corruptions, accuracy has already risen sig- niﬁcantly after 10,000 samples, but is still rising towards the end of the 50,000 samples, indicating room for additional improvements if more samples were available. Without seeing a single label, TTT-Online behaves as if we were training on the test set from the appearance of the plots.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 Table 1.Test error (%) on CIFAR-10-C with level 5 corruption.Comparison between online Test-Time Training (TTT-Online) and unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019) with access to the entire (unlabeled) test set during training. We highlight the lower error in bold. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. The reported numbers for TTT-Online are the same as in Figure 1. See complete table in Table A2. 0 2000 4000 6000 8000 Number of samples 12 16 20 24 28 32 36 40 44 48Error (%) Gaussian Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 9 12 15 18 21 24 27 30 33 36Error (%) Shot Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 15 20 25 30 35 40 45 50Error (%) Impulse Noise Joint training TTT TTT-Online UDA-SS Figure 3.Test error (%) on CIFAR-10-C, for the three noise types, with gradually changing distribution.The distribution shifts are created by increasing the standard deviation of each noise type from small to large, the further we go on the x-axis. As the samples get noisier, all methods suffer greater errors the more we evaluate into the test set, but online Test-Time Training (TTT-Online) achieves gentler slopes than joint training. For the ﬁrst two noise types, TTT-Online also achieves better results over unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019). Comparison with unsupervised domain adaptation. Table 1 empirically compares online Test-Time Training (TTT-Online) with unsupervised domain adaptation through self-supervision (UDA-SS) (Sun et al., 2019), which is sim- ilar to our method in spirit but is designed for the setting of unsupervised domain adaptation (Section 5 provides a sur- vey of other related work in this setting). Given labeled data from the training distribution and unlabeled data from the test distribution, UDA-SS hopes to ﬁnd an invariant repre- sentation that extracts useful features for both distributions by learning to perform a self-supervised task, speciﬁcally rotation prediction, simultaneously on data from both. It then learns a labeling function on top of the invariant rep- resentation using the labeled data. In our experiments, the unlabeled data given to UDA-SS is the entire test set itself without the labels. Because TTT-Online can only learn from the unlabeled test samples that have already been evaluated on, it is given less information than UDA-SS at all times. In this sense, UDA- SS should be regarded as an oracle rather than a baseline. Surprisingly, TTT-Online outperforms UDA-SS on 13 out of the 15 corruptions as well as the original distribution. Our explanation is that UDA-SS has to ﬁnd an invariant representation for both distributions, while TTT-Online only adapts the representation to be good for the current test distribution. That is, TTT-Online has the ﬂexibility to forget the training distribution representation, which is no longer relevant. This suggests that in our setting, forgetting is not harmful and perhaps should even be taken advantage of. Gradually changing distribution shifts.In our previous experiments, we have been evaluating the online version under the assumption that the test inputs xt for t= 1...nare all sampled from the same test distribution Q, which can be different from the training distribution P. This assumption is indeed satisﬁed for i.i.d. samples from a shufﬂed test set. But here we show that this assumption can in fact be relaxed to allow xt ∼Qt, where Qt is close to Qt+1 (in the sense of distributional distance). We call this the assumption of gradually changing distribution shifts. We perform experiments by simulating such distribution shifts on the three noise types of CIFAR-10-C. For each noise type, xt is corrupted with standard deviation σt, and σ1,...,σ n interpolate between the standard deviation of level 1 and level 5. So xt is more severely corrupted as we evaluate further into the test set and t grows larger. As shown in Figure 3, TTT-Online still improves upon joint training (and our standard version) with this relaxed assumption, and even upon UDA-SS for the ﬁrst two noise types.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Accuracy (%) Airplane Bird Car Dog Cat Horse Ship Average Object recognition task only 67.9 35.8 42.6 14.7 52.0 42.0 66.7 41.4 Joint training (Hendrycks et al., 2019a) 70.2 36.7 42.6 15.5 52.0 44.0 66.7 42.4 TTT (standard version) 70.2 39.2 42.6 21.6 54.7 46.0 77.8 45.2 TTT-Online 70.2 39.2 42.6 22.4 54.7 46.0 77.8 45.4 Table 2.Class-wise and average classiﬁcation accuracy (%) on CIFAR classes in VID-Robust, adapted from (Shankar et al., 2019). Test-Time Training (TTT) and online Test-Time Training (TTT-Online) improve over the two baselines on average, and by a large margin on “ship” and “dog” classes where the rotation task is more meaningful than in classes like “airplane” (sample images in Figure A7). 3.2. Object Recognition on Video Frames The Robust ImageNet Video Classiﬁcation (VID-Robust) dataset was developed by Shankar et al. (2019) from the Ima- geNet Video detection dataset (Russakovsky et al., 2015), to demonstrate how deep models for object recognition trained on ImageNet (still images) fail to adapt well to video frames. The VID-Robust dataset contains 1109 sets of video frames in 30 classes; each set is a short video clip of frames that are similar to an anchor frame. Our results are reported on the anchor frames. To map the 1000 ImageNet classes to the 30 VID-Robust classes, we use the max-conversion function in Shankar et al. (2019). Without any modiﬁcations for videos, we apply our method to VID-Robust on top of the same ImageNet model as in the previous subsection. Our classiﬁcation accuracy is reported in Table 3. In addition, we take the seven classes in VID-Robust that overlap with CIFAR-10, and re-scale those video frames to the size of CIFAR-10 images, as a new test set for the model trained on CIFAR-10 in the previous subsection. Again, we apply our method to this dataset without any modiﬁcations. Our results are shown in Table 2, with a breakdown for each class. Noticing that Test-Time Training does not improve on the airplane class, we inspect some airplane samples (Figure A7), and observe black margins on two sides of most images, which provide a trivial hint for rotation prediction. In addition, given an image of airplanes in the sky, it is often impossible even for humans to tell if it is rotated. This shows that our method requires the self-supervised task to be both well deﬁned and non-trivial. 3.3. CIFAR-10.1: Unknown Distribution Shifts CIFAR-10.1 (Recht et al., 2018) is a new test set of size 2000 modeled after CIFAR-10, with the exact same classes and image dimensionality, following the dataset creation process documented by the original CIFAR-10 paper as closely as possible. The purpose is to investigate the distribution shifts present between the two test sets, and the effect on object recognition. All models tested by the authors suffer a large performance drop on CIFAR-10.1 comparing to CIFAR-10, even though there is no human noticeable difference, and Method Accuracy (%) Object recognition task only 62.7 Joint training (Hendrycks et al., 2019a) 63.5 TTT (standard version) 63.8 TTT-Online 64.3 Table 3.Test accuracy (%) on VID-Robust dataset (Shankar et al., 2019). TTT and TTT-Online improve over the baselines. Method Error (%) Object recognition task only 17.4 Joint training (Hendrycks et al., 2019a) 16.7 TTT (standard version) 15.9 Table 4.Test error (%) on CIFAR-10.1 (Recht et al., 2018). TTT is the ﬁrst method to improve the performance of an existing model on this new test set. both have the same human accuracy. This demonstrates how insidious and ubiquitous distribution shifts are, even when researchers strive to minimize them. The distribution shifts from CIFAR-10 to CIFAR-10.1 pose an extremely difﬁcult problem, and no prior work has been able to improve the performance of an existing model on this new test set, probably because: 1) researchers cannot even identify the distribution shifts, let alone describe them mathematically; 2) the samples in CIFAR-10.1 are only revealed at test time; and even if they were revealed during training, the distribution shifts are too subtle, and the sample size is too small, for domain adaptation (Recht et al., 2018). On the original CIFAR-10 test set, the baseline with only object recognition has error 8.9%, and with joint training has 8.1%; comparing to the ﬁrst two rows of Table 4, both suffer the typical performance drop (by a factor of two). TTT yields an improvement of 0.8% (relative improvement of 4.8%) over joint training. We recognize that this improve- ment is small relative to the performance drop, but see it as an encouraging ﬁrst step for this very difﬁcult problem.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 0 10 20 30 40 50 60 Gradient inner product 0 1 2 3 4 5Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 0 10 20 30 40 50 60 Gradient inner product 0 5 10 15 20 25 30 35Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 Figure 4.Scatter plot of the inner product between the gradients (on the shared feature extractor θe) of the main task lm and the self- supervised task le, and the improvement in test error (%) from Test-Time Training, for the standard (left) and online (right) version. Each point is the average over a test set, and each scatter plot has 75 test sets, from all 15 types of corruptions over ﬁve levels as described in subsection 3.1. The blue lines and bands are the best linear ﬁts and the 99% conﬁdence intervals. The linear correlation coefﬁcients are 0.93 and 0.89 respectively, indicating strong positive correlation between the two quantities, as suggested by Theorem 1. 4. Theoretical Results This section contains our preliminary study of when and why Test-Time Training is expected to work. For convex models, we prove that positive gradient correlation between the loss functions leads to better performance on the main task after Test-Time Training. Equipped with this insight, we then empirically demonstrate that gradient correlation governs the success of Test-Time Training on the deep learning model discussed in Section 3. Before stating our main theoretical result, we ﬁrst illustrate the general intuition with a toy model. Consider a regression problem where x∈Rd denotes the input, y1 ∈R denotes the label, and the objective is the square loss (ˆy−y1)2/2 for a prediction ˆy. Consider a two layer linear network parametrized by A∈Rh×d and v ∈Rh (where hstands for the hidden dimension). The prediction according to this model is ˆy= v⊤Ax, and the main task loss is lm(x,y1; A,v) = 1 2 ( y1 −v⊤Ax )2 . (4) In addition, consider a self-supervised regression task that also uses the square loss and automatically generates a label ys for x. Let the self-supervised head be parametrized by w∈Rh. Then the self-supervised task loss is ls(x,y2; A,w) = 1 2 ( y2 −w⊤Ax )2 . (5) Now we apply Test-Time Training to update the shared feature extractor Aby one step of gradient descent on ls, which we can compute with y2 known. This gives us A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (6) where A′is the updated matrix and ηis the learning rate. If we set η= η∗where η∗= y1 −v⊤Ax (y2 −w⊤Ax) v⊤wx⊤x, (7) then with some simple algebra, it is easy to see that the main task loss lm(x,y1; A′,v) = 0. Concretely, Test-Time Training drives the main task loss down to zero with a single gradient step for a carefully chosen learning rate. In prac- tice, this learning rate is unknown since it depends on the unknown y1. However, since our model is convex, as long as η∗is positive, it sufﬁces to set η to be a small positive constant (see details in the appendix). If x̸= 0, one sufﬁ- cient condition for η∗to be positive (when neither loss is zero) is to have sign ( y1 −v⊤Ax ) = sign ( y2 −w⊤Ax ) (8) and v⊤w>0 . (9) For our toy model, both parts of the condition above have an intuition interpretation. The ﬁrst part says that the mistakes should be correlated, in the sense that predictions from both tasks are mistaken in the same direction. The second part, v⊤w>0, says that the decision boundaries on the feature space should be correlated. In fact, these two parts hold iff. ⟨∇lm(A),∇ls(A)⟩>0 (see a simple proof of this fact in the appendix). To summarize, if the gradients have positive correlation, Test-Time Training is guaranteed to reduce the main task loss. Our main theoretical result extends this to general smooth and convex loss functions.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Theorem 1. Let lm(x,y; θ) denote the main task loss on test instance x,y with parameters θ, and ls(x; θ) the self- supervised task loss that only depends onx. Assume that for all x,y, lm(x,y; θ) is differentiable, convex andβ-smooth in θ, and both ∥∇lm(x,y; θ)∥,∥∇ls(x,θ)∥≤ Gfor all θ. With a ﬁxed learning rate η= ϵ βG2 , for every x,y such that ⟨∇lm(x,y; θ),∇ls(x; θ)⟩>ϵ, (10) we have lm(x,y; θ) >lm(x,y; θ(x)), (11) where θ(x) = θ−η∇ls(x; θ) i.e. Test-Time Training with one step of gradient descent. The proof uses standard techniques in optimization, and is left for the appendix. Theorem 1 reveals gradient correlation as a determining factor of the success of Test-Time Training in the smooth and convex case. In Figure 4, we empirically show that our insight also holds for non-convex loss func- tions, on the deep learning model and across the diverse set of corruptions considered in Section 3; stronger gradient cor- relation clearly indicates more performance improvement over the baseline. 5. Related Work Learning on test instances. Shocher et al. (2018) pro- vide a key inspiration for our work by showing that image super-resolution could be learned at test time simply by try- ing to upsample a downsampled version of the input image. More recently, Bau et al. (2019) improve photo manipula- tion by adapting a pre-trained GAN to the statistics of the input image. One of the earlier examples of this idea comes from Jain & Learned-Miller (2011), who improve Viola- Jones face detection (Viola et al., 2001) by bootstrapping the more difﬁcult faces in an image from the more easily detected faces in that same image. The online version of our algorithm is inspired by the work of Mullapudi et al. (2018), which makes video segmentation more efﬁcient by using a student model that learns online from a teacher model. The idea of online updates has also been used in Kalal et al. (2011) for tracking and detection. A recent work in echocardiography (Zhu et al., 2019) improves the deep learning model that tracks myocardial motion and cardiac blood ﬂow with sequential updates. Lastly, we share the philosophy of transductive learning (Vapnik, 2013; Gam- merman et al., 1998), but have little in common with their classical algorithms; recent work by Tripuraneni & Mackey (2019) theoretically explores this for linear prediction, in the context of debiasing the LASSO estimator. Self-supervised learning studies how to create labels from the data, by designing various pretext tasks that can learn semantic information without human annotations, such as context prediction (Doersch et al., 2015), solving jig- saw puzzles (Noroozi & Favaro, 2016), colorization (Lars- son et al., 2017; Zhang et al., 2016), noise prediction (Bo- janowski & Joulin, 2017), feature clustering (Caron et al., 2018). Our paper uses rotation prediction (Gidaris et al., 2018). Asano et al. (2019) show that self-supervised learn- ing on only a single image, surprisingly, can produce low- level features that generalize well. Closely related to our work, Hendrycks et al. (2019a) propose that jointly training a main task and a self-supervised task (our joint training baseline in Section 3) can improve robustness on the main task. The same idea is used in few-shot learning (Su et al., 2019), domain generalization (Carlucci et al., 2019), and unsupervised domain adaptation (Sun et al., 2019). Adversarial robustness studies the robust risk RP,∆(θ) = Ex,y∼P maxδ∈∆ l(x + δ,y; θ), where l is some loss function, and ∆ is the set of perturbations; ∆ is often chosen as the Lp ball, for p ∈{1,2,∞}. Many popular algorithms formulate and solve this as a robust optimization problem (Goodfellow et al., 2014; Madry et al., 2017; Sinha et al., 2017; Raghunathan et al., 2018; Wong & Kolter, 2017; Croce et al., 2018), and the most well known technique is adversarial training. Another line of work is based on randomized smoothing (Cohen et al., 2019; Salman et al., 2019), while some other approaches, such as input transformations (Guo et al., 2017; Song et al., 2017), are shown to be less effective (Athalye et al., 2018). There are two main problems with the approaches above. First, all of them can be seen as smoothing the decision boundary. This establishes a theoretical tradeoff between accuracy and robustness (Tsipras et al., 2018; Zhang et al., 2019), which we also observe empirically with our adversarial training baseline in Section 3. Intuitively, the more diverse ∆ is, the less effective this one-boundary-ﬁts-all approach can be for a particular element of ∆. Second, adversarial methods rely heavily on the mathematical structure of ∆, which might not accurately model perturbations in the real world. Therefore, generalization remains hard outside of the ∆ we know in advance or can mathematically model, especially for non-adversarial distribution shifts. Empirically, Kang et al. (2019) shows that robustness for one ∆ might not transfer to another, and training on the L∞ball actually hurts robustness on the L1 ball. Non-adversarial robustness studies the effect of corrup- tions, perturbations, out-of-distribution examples, and real- world distribution shifts (Hendrycks et al., 2019b;a; 2018; Hendrycks & Gimpel, 2016). Geirhos et al. (2018) show that training on images corrupted by Gaussian noise makes deep learning models robust to this particular noise type, but does not improve performance on images corrupted by another noise type e.g. salt-and-pepper noise.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Unsupervised domain adaptation (a.k.a. transfer learn- ing) studies the problem of distribution shifts, when an unlabeled dataset from the test distribution (target domain) is available at training time, in addition to a labeled dataset from the training distribution (source domain) (Chen et al., 2011; Gong et al., 2012; Long et al., 2015; Ganin et al., 2016; Long et al., 2016; Tzeng et al., 2017; Hoffman et al., 2017; Csurka, 2017; Chen et al., 2018). The limitation of the problem setting, however, is that generalization might only be improved for this speciﬁc test distribution, which can be difﬁcult to anticipate in advance. Prior work try to anticipate broader distributions by using multiple and evolv- ing domains (Hoffman et al., 2018; 2012; 2014). Test-Time Training does not anticipate any test distribution, by chang- ing the setting of unsupervised domain adaptation, while taking inspiration from its algorithms. Our paper is a follow- up to Sun et al. (2019), which we explain and empirically compare with in Section 3. Our update rule can be viewed as performing one-sample unsupervised domain adaptation on the ﬂy, with the caveat that standard domain adaptation techniques might become ill-deﬁned when there is only one sample from the target domain. Domain generalization studies the setting where a meta distribution generates multiple environment distributions, some of which are available during training (source), while others are used for testing (target) (Li et al., 2018; Shankar et al., 2018; Muandet et al., 2013; Balaji et al., 2018; Ghifary et al., 2015; Motiian et al., 2017; Li et al., 2017a; Gan et al., 2016). With only a few environments, information on the meta distribution is often too scarce to be helpful, and with many environments, we are back to the i.i.d. setting where each environment can be seen as a sample, and a strong baseline is to simply train on all the environments (Li et al., 2019). The setting of domain generalization is limited by the inherent tradeoff between speciﬁcity and generality of a ﬁxed decision boundary, and the fact that generalization is again elusive outside of the meta distribution i.e. the actual P learned by the algorithm. One (few)-shot learning studies how to learn a new task or a new classiﬁcation category using only one (or a few) sample(s), on top of a general representation that has been learned on diverse samples (Snell et al., 2017; Vinyals et al., 2016; Fei-Fei et al., 2006; Ravi & Larochelle, 2016; Li et al., 2017b; Finn et al., 2017; Gidaris & Komodakis, 2018). Our update rule can be viewed as performing one-shot self- supervised learning and can potentially be improved by progress in one-shot learning. Continual learning (a.k.a. learning without forgetting) studies the setting where a model is made to learn a sequence of tasks, and not forget about the earlier ones while training for the later (Li & Hoiem, 2017; Lopez-Paz & Ranzato, 2017; Kirkpatrick et al., 2017; Santoro et al., 2016). In contrast, with Test-Time Training, we are not concerned about forgetting the past test samples since they have already been evaluated on; and if a past sample comes up by any chance, it would go through Test-Time Training again. In addition, the impact of forgetting the training set is minimal, because both tasks have already been jointly trained. Online learning (a.k.a. online optimization) is a well- studied area of learning theory (Shalev-Shwartz et al., 2012; Hazan et al., 2016). The basic setting repeats the following: receive xt, predict ˆyt, receive yt from a worst-case oracle, and learn. Final performance is evaluated using the regret, which colloquially translates to how much worse the online learning algorithm performs in comparison to the best ﬁxed model in hindsight. In contrast, our setting never reveals any yt during testing even for the online version, so we do not need to invoke the concept of the worst-case oracle or the regret. Also, due to the lack of feedback from the envi- ronment after predicting, our algorithm is motivated to learn (with self-supervision) before predicting ˆyt instead of after. Note that some of the previously covered papers (Hoffman et al., 2014; Jain & Learned-Miller, 2011; Mullapudi et al., 2018) use the term “online learning” outside of the learning theory setting, so the term can be overloaded. 6. Discussion The idea of test-time training also makes sense for other tasks, such as segmentation and detection, and in other ﬁelds, such as speech recognition and natural language process- ing. For machine learning practitioners with prior domain knowledge in their respective ﬁelds, their expertise can be leveraged to design better special-purpose self-supervised tasks for test-time training. Researchers for general-purpose self-supervised tasks can also use test-time training as an evaluation benchmark, in addition to the currently prevalent benchmark of pre-training and ﬁne-tuning. More generally, we hope this paper can encourage re- searchers to abandon the self-imposed constraint of a ﬁxed decision boundary for testing, or even the artiﬁcial division between training and testing altogether. Our work is but a small step toward a new paradigm where much of the learning happens after a model is deployed. Acknowledgements. This work is supported by NSF grant 1764033, DARPA and Berkeley DeepDrive. This paper took a long time to develop, and beneﬁted from con- versations with many of our colleagues, including Ben Recht and his students Ludwig Schmidt, Vaishaal Shanker and Becca Roelofs; Ravi Teja Mullapudi, Achal Dave and Deva Ramanan; and Armin Askari, Allan Jabri, Ashish Kumar, Angjoo Kanazawa and Jitendra Malik.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts References Asano, Y . M., Rupprecht, C., and Vedaldi, A. Surprising effectiveness of few-image unsupervised feature learning. arXiv preprint arXiv:1904.13132, 2019. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security: Circumvent- ing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. Balaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems, pp. 998–1008, 2018. Bau, D., Strobelt, H., Peebles, W., Wulff, J., Zhou, B., Zhu, J.-Y ., and Torralba, A. Semantic photo manipulation with a generative image prior. ACM Transactions on Graphics (TOG), 38(4):59, 2019. Bojanowski, P. and Joulin, A. Unsupervised learning by predicting noise. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 517– 526. JMLR. org, 2017. Carlucci, F. M., D’Innocente, A., Bucci, S., Caputo, B., and Tommasi, T. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pp. 2229–2238, 2019. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 132–149, 2018. Caruana, R. Multitask learning. Machine learning, 28(1): 41–75, 1997. Chen, M., Weinberger, K. Q., and Blitzer, J. Co-training for domain adaptation. In Advances in neural information processing systems, pp. 2456–2464, 2011. Chen, X., Sun, Y ., Athiwaratkun, B., Cardie, C., and Wein- berger, K. Adversarial deep averaging networks for cross- lingual sentiment classiﬁcation. Transactions of the Asso- ciation for Computational Linguistics, 6:557–570, 2018. Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed adversarial robustness via randomized smoothing. arXiv preprint arXiv:1902.02918, 2019. Croce, F., Andriushchenko, M., and Hein, M. Provable robustness of relu networks via maximization of linear regions. arXiv preprint arXiv:1810.07481, 2018. Csurka, G. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374, 2017. Ding, G. W., Wang, L., and Jin, X. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422–1430, 2015. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594–611, 2006. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–1135. JMLR. org, 2017. Gammerman, A., V ovk, V ., and Vapnik, V . Learning by transduction. In Proceedings of the Fourteenth conference on Uncertainty in artiﬁcial intelligence , pp. 148–155. Morgan Kaufmann Publishers Inc., 1998. Gan, C., Yang, T., and Gong, B. Learning attributes equals multi-source domain generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 87–97, 2016. Ganin, Y ., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V . Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. In Advances in Neural Information Processing Systems, pp. 7538–7550, 2018. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., and Balduzzi, D. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pp. 2551– 2559, 2015. Gidaris, S. and Komodakis, N. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, B., Shi, Y ., Sha, F., and Grauman, K. Geodesic ﬂow kernel for unsupervised domain adaptation. In2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2066–2073. IEEE, 2012.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun- tering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017. Hazan, E. et al. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2(3-4):157– 325, 2016. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a. He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016b. He, K., Girshick, R., and Doll ´ar, P. Rethinking imagenet pre-training. arXiv preprint arXiv:1811.08883, 2018. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. Using trusted data to train deep networks on labels cor- rupted by severe noise. InAdvances in neural information processing systems, pp. 10456–10465, 2018. Hendrycks, D., Lee, K., and Mazeika, M. Using pre-training can improve model robustness and uncertainty. arXiv preprint arXiv:1901.09960, 2019a. Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Improving model robustness and uncertainty estimates with self-supervised learning. arXiv preprint, 2019b. Hoffman, J., Kulis, B., Darrell, T., and Saenko, K. Discover- ing latent domains for multisource domain adaptation. In European Conference on Computer Vision, pp. 702–715. Springer, 2012. Hoffman, J., Darrell, T., and Saenko, K. Continuous man- ifold based adaptation for evolving visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 867–874, 2014. Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y ., Isola, P., Saenko, K., Efros, A. A., and Darrell, T. Cycada: Cycle- consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017. Hoffman, J., Mohri, M., and Zhang, N. Algorithms and theory for multiple-source adaptation. In Advances in Neural Information Processing Systems, pp. 8246–8256, 2018. Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep networks with stochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Jain, V . and Learned-Miller, E. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR 2011, pp. 577–584. IEEE, 2011. Kalal, Z., Mikolajczyk, K., and Matas, J. Tracking-learning- detection. IEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422, 2011. Kang, D., Sun, Y ., Brown, T., Hendrycks, D., and Steinhardt, J. Transfer of adversarial robustness between perturbation types. arXiv preprint arXiv:1905.01034, 2019. Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Larsson, G., Maire, M., and Shakhnarovich, G. Colorization as a proxy task for visual understanding. In CVPR, 2017. Li, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 5542–5550, 2017a. Li, D., Zhang, J., Yang, Y ., Liu, C., Song, Y .-Z., and Hospedales, T. M. Episodic training for domain gen- eralization. arXiv preprint arXiv:1902.00113, 2019. Li, Y ., Tian, X., Gong, M., Liu, Y ., Liu, T., Zhang, K., and Tao, D. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624–639, 2018.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Li, Z. and Hoiem, D. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Li, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017b. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. Long, M., Cao, Y ., Wang, J., and Jordan, M. I. Learn- ing transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015. Long, M., Zhu, H., Wang, J., and Jordan, M. I. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136–144, 2016. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017. Motiian, S., Piccirilli, M., Adjeroh, D. A., and Doretto, G. Uniﬁed deep supervised domain adaptation and gen- eralization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5715–5725, 2017. Muandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pp. 10– 18, 2013. Mullapudi, R. T., Chen, S., Zhang, K., Ramanan, D., and Fatahalian, K. Online model distillation for efﬁcient video inference. arXiv preprint arXiv:1812.02699, 2018. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision , pp. 69–84. Springer, 2016. Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018. Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. IEEE transactions on pattern analysis and machine intelligence, 2016. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen- shteyn, I., and Bubeck, S. Provably robust deep learn- ing via adversarially trained smoothed classiﬁers. arXiv preprint arXiv:1906.04584, 2019. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. Meta-learning with memory-augmented neu- ral networks. In International conference on machine learning, pp. 1842–1850, 2016. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Shankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S., Jyothi, P., and Sarawagi, S. Generalizing across domains via cross-gradient training. arXiv preprint arXiv:1804.10745, 2018. Shankar, V ., Dave, A., Roelofs, R., Ramanan, D., Recht, B., and Schmidt, L. Do image classiﬁers generalize across time? arXiv, 2019. Shocher, A., Cohen, N., and Irani, M. zero-shot super- resolution using deep internal learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3118–3126, 2018. Sinha, A., Namkoong, H., and Duchi, J. Certifying some dis- tributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077–4087, 2017. Song, Y ., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017. Su, J.-C., Maji, S., and Hariharan, B. Boosting supervi- sion with self-supervision for few-shot learning. arXiv preprint arXiv:1906.07079, 2019. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint, 2019.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Tripuraneni, N. and Mackey, L. Debiasing linear prediction. arXiv preprint arXiv:1908.02341, 2019. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7167–7176, 2017. Vapnik, V .The nature of statistical learning theory. Springer science & business media, 2013. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. Viola, P., Jones, M., et al. Rapid object detection using a boosted cascade of simple features. CVPR (1), 1(511- 518):3, 2001. Wong, E. and Kolter, J. Z. Provable defenses against adver- sarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017. Zhang, H., Yu, Y ., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jor- dan, M. I. Theoretically principled trade-off between ro- bustness and accuracy. arXiv preprint arXiv:1901.08573, 2019. Zhang, R., Isola, P., and Efros, A. A. Colorful image col- orization. In European conference on computer vision, pp. 649–666. Springer, 2016. Zhu, W., Huang, Y ., Vannan, M. A., Liu, S., Xu, D., Fan, W., Qian, Z., and Xie, X. Neural multi-scale self-supervised registration for echocardiogram dense tracking. arXiv preprint arXiv:1906.07357, 2019.Appendix: Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A1. Informal Discussion on Our Variable Decision Boundary In the introduction, we claim that in traditional supervised learning θgives a ﬁxed decision boundary, while ourθgives a variable decision boundary. Here we informally discuss this claim. Denote the input space Xand output space Y. A decision boundary is simply a mapping f : X →Y. Let Θ be a model class e.g Rd. Now consider a family of parametrized functions gθ : X→Y , where θ∈Θ. In the context of deep learning, gis the neural network architecture and θcontains the parameters. We say that f is a ﬁxed decision boundary w.r.t. g and Θ if there exists θ ∈Θ s.t. f(x) = gθ(x) for every x ∈X , and a variable decision boundary if for every x∈X, there exists θ∈Θ s.t. f(x) = gθ(x). Note how selection of θcan depend on xfor a variable decision boundary, and cannot for a ﬁxed one. It is then trivial to verify that our claim is true under those deﬁnitions. A critical reader might say that with an arbitrarily large model class, can’t every decision boundary be ﬁxed? Yes, but this is not the end of the story. Let d = dim( X) × dim(Y), and consider the enormous model class Θ′= Rd which is capable of representing all possible mappings be- tween Xand Y. Let g′ θ′ simply be the mapping represented by θ′ ∈Θ′. A variable decision boundary w.r.t. g and Θ then indeed must be a ﬁxed decision boundary w.r.t. g′and Θ′, but we would like to note two things. First, without any prior knowledge, generalization in Θ′is impossible with any ﬁnite amount of training data; reasoning about g′and Θ′is most likely not productive from an algorithmic point of view, and the concept of a variable decision boundary is to avoid such reasoning. Second, selecting θbased on xfor a variable decision boundary can be thought of as “training” on all points x ∈Rd; however, “training” only happens when necessary, for the xthat it actually encounters. Altogether, the concept of a variable decision boundary is different from what can be described by traditional learning theory. A formal discussion is beyond the scope of this paper and might be of interest to future work. A2. Computational Aspects of Our Method At test time, our method is 2 × batch size × number of iterations times slower than regular test- ing, which only performs a single forward pass for each sample. As the ﬁrst work on Test-Time Training, this paper is not as concerned about computational efﬁciency as improving robustness, but here we provide two poten- tial solutions that might be useful, but have not been thor- oughly veriﬁed. The ﬁrst is to use the thresholding trick on ls, introduced as a solution for the small batches prob- lem in the method section. For the models considered in our experiments, roughly 80% of the test instances fall below the threshold, so Test-Time Training can only be performed on the other 20% without much effect on per- formance, because those 20% contain most of the sam- ples with wrong predictions. The second is to reduce the number of iterations of test-time updates. For the online version, the number of iterations is al- ready 1, so there is nothing to do. For the standard ver- sion, we have done some preliminary experiments setting number of iterations to 1 (instead of 10) and learn- ing rate to 0.01 (instead of 0.001), and observing results almost as good as the standard hyper-parameter setting. A more in depth discussion on efﬁciency is left for future works, which might, during training, explicitly make the model amenable to fast updates. A3. Proofs Here we prove the theoretical results in the main paper. A3.1. The Toy Problem The following setting applies to the two lemmas; this is simply the setting of our toy problem, reproduced here for ease of reference.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Consider a two layer linear network parametrized by A∈ Rh×d (shared) and v,w ∈Rh (ﬁxed) for the two heads, respectively. Denote x∈Rd the input and y1,y2 ∈R the labels for the two tasks, respectively. For the main task loss lm(A; v) = 1 2 ( y1 −v⊤Ax )2 , (12) and the self-supervised task loss ls(A; w) = 1 2 ( y2 −w⊤Ax )2 , (13) Test-Time Training yields an updated matrix A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (14) where ηis the learning rate. Lemma 1. Following the exposition of the main paper, let η∗= (y1 −v⊤Ax) (y2 −w⊤Ax)v⊤wx⊤x. (15) Assume η∗∈[ϵ,∞) for some ϵ> 0. Then for any η∈(0,ϵ], we are guaranteed an improvement on the main loss i.e. lm(A′) <lm(A). Proof. From the exposition of the main paper, we know that lm(A−η∗∇lsA)) = 0, which can also be derived from simple algebra. Then by convexity, we have lm(A−η∇ls(A)) (16) = lm (( 1 − η η∗ ) A+ η η∗(A−η∗∇ls(A)) ) (17) ≤ ( 1 − η η∗ ) lm(A) + 0 (18) ≤ ( 1 −η ϵ ) lm(A) (19) <lm(A), (20) where the last inequality uses the assumption that lm(A) > 0, which holds because η∗>0. Lemma 2. Deﬁne ⟨U,V⟩= vec (U)⊤vec (V) i.e. the Frobenious inner product, then sign (η∗) = sign (⟨∇lm(A),∇ls(A)⟩) . (21) Proof. By simple algebra, ⟨∇lm(A),∇ls(A)⟩ = ⟨ ( y1 −v⊤Ax )( −vx⊤) , ( y2 −w⊤Ax )( −wx⊤) ⟩ = ( y1 −v⊤Ax )( y2 −w⊤Ax ) Tr ( xv⊤wx⊤) = ( y1 −v⊤Ax )( y2 −w⊤Ax ) v⊤wx⊤x, which has the same sign as η∗. A3.2. Proof of Theorem 1 For any η, by smoothness and convexity, lm(x,y; θ(x)) = lm(x,y; θ−η∇ls(x; θ)) ≤lm(x,y; θ) + η⟨∇lm(x,y; θ),∇ls(x,θ)⟩ + η2β 2 ∥∇ls(x; θ)∥2 . Denote η∗= ⟨∇lm(x,y; θ),∇ls(x,θ)⟩ β∥∇ls(x; θ)∥2 . Then Equation 22 becomes lm(x,y; θ−η∗∇ls(x; θ)) (22) ≤lm(x,y; θ) −⟨∇lm(x,y; θ),∇ls(x,θ)⟩2 2β∥∇ls(x; θ)∥2 . (23) And by our assumptions on the gradient norm and gradient inner product, lm(x,y; θ) −lm(x,y; θ−η∗∇ls(x; θ)) ≥ ϵ2 2βG2 . (24) Because we cannot observe η∗in practice, we instead use a ﬁxed learning rate η = ϵ βG2 , as stated in Theorem 1. Now we argue that this ﬁxed learning rate still improves performance on the main task. By our assumptions, η∗ ≥ ϵ βG2 , so η ∈(0,η∗]. Denote g= ∇ls(x; θ), then by convexity of lm, lm(x,y; θ(x)) = lm(x,y; θ−ηg) (25) = lm ( x,y; ( 1 − η η∗ ) θ+ η η∗(θ−η∗g) ) (26) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗lm(x,y; θ−η∗g) (27) Combining with Equation 24, we have lm(x,y; θ(x)) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗ ( lm(x,y; θ) − ϵ2 2βG2 ) = lm(x,y; θ) − η η∗ ϵ2 2βG2 Since η/η∗>0, we have shown that lm(x,y; θ) −lm(x,y; θ(x)) >0. (28)Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A4. Additional Results on the Common Corruptions Dataset For table aethetics, we use the following abbreviations: B for baseline, JT for joint training, TTT for Test-Time Train- ing standard version, and TTT-Online for online Test-Time Training i.e. the online version. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. A4.1. Results Using Batch Normalization As discussed in the results section, Batch Normalization (BN) is ineffective for small batches, which are the inputs for Test-Time Training (both standard and online version) since there is only one sample available when forming each batch; therefore, our main results are based on a ResNet using Group Normalization (GN). Figure A2 and Table A1 show results of our method on CIFAR-10-C level 5, with a ResNet using Batch Normalization (BN). These results are only meant to be a point of reference for the curious readers. In the early stage of this project, we have experimented with two potential solutions to the small batches problem with BN. The naive solution is to ﬁx the BN layers during Test-Time Training. but this diminishes the performance gains since there are fewer shared parameters. The better solution, adopted for the results below, is hard example mining: instead of updating on all inputs, we only update on inputs that incur large self-supervised task loss ls, where the large improvements might counter the negative effects of inaccurate statistics. Test-Time Training (standard version) is still very effective with BN. In fact, some of the improvements are quite dra- matic, such as on contrast (34%), defocus blue (18%) and Gaussian noise (22% comparing to joint-training, and 16% comparing to the baseline). Performance on the original distribution is still almost the same, and the original error with BN is in fact slightly lower than with GN, and takes half as many epochs to converge. We did not further experiment with BN because of two rea- sons: 1) The online version does not work with BN, because the problem with inaccurate batch statistics is exacerbated when training online for many (e.g. 10000) steps. 2) The baseline error for almost every corruption type is signiﬁ- cantly higher with BN than with GN. Although unrelated to the main idea of our paper, we make the interesting note that GN signiﬁcantly improves model robustness. A4.2. Additional Baseline: Adversarial Logit Pairing As discussed in the results section, Hendrycks & Dietterich (2019) point to Adversarial Logit Pairing (ALP) (Kannan et al., 2018) as an effective method for improving model robustness to corruptions and perturbations, even though it was designed to defend against adversarial attacks. We take ALP as an additional baseline on all benchmarks based on CIFAR-10 (using GN), following the training proce- dure in Kannan et al. (2018) and their recommended hyper- parameters. The implementation of the adversarial attack comes from the codebase of Ding et al. (2019). We did not run ALP on ImageNet because the two papers we reference for this method, Kannan et al. (2018) and Hendrycks & Di- etterich (2019), did not run on ImageNet or make any claim or recommendation. A4.3. Results on CIFAR-10-C and ImageNet-C, Level 5 Table A2 and Table A3 correspond to the bar plots in the results section. Two rows of Table A2 have been presented as Table 1 in the main text. A4.4. Results on CIFAR-10-C, Levels 1-4 The following bar plots and tables are on levels 1-4 of CIFAR-10-C. The original distribution is the same for all levels, so are our results on the original distribution. A4.5. Direct Comparison with Hendrycks et al. (2019a) The following comparison has been requested by an anony- mous reviewer for our ﬁnal version. Our joint training baseline is based on Hendrycks et al. (2019a), but also incor- porates some architectural changes (see below). We found these changes improved the robustness of our method, and felt that it was important to give the baseline the same ben- eﬁt. Note that our joint training baseline overall performs better than Hendrycks: Compare Table S2 to Figure 3 of Hendrycks et al. (2019a) (provided by the authors), our baseline has average error of 22.8% across all corruptions and levels, while their average error is 28.6%. Summary of architectural changes: 1) Group Normalization (GN) instead of Batch Normalization (BN). For complete- ness, the results with BN are provided in Table S1; c.f. GN results in Table S2 which signiﬁcantly improves robustness, with or without self-supervision. 2) We split after the sec- ond residual group, while they split after the third residual group right before the linear layer. This consistently gives about 0.5% - 1% improvement. 3) We use a ResNet-26, while they use a 40-2 Wide ResNet. But our baseline still performs better than their method even though our network is 4x smaller, due to the two tricks above.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure A1.Sample images from the Common Corruptions Benchmark, taken from the original paper by Hendrycks & Dietterich (2019). originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT Figure A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 7.9 63.9 58.8 64.3 46.3 54.6 41.6 45.9 31.9 44.0 37.5 13.0 69.2 33.8 61.4 31.7 JT 7.5 70.7 65.6 67.2 43.1 55.4 40.9 42.7 30.3 44.5 42.5 12.7 58.6 30.7 62.6 31.9 TTT 7.9 47.9 45.2 54.8 27.6 50.4 31.5 30.9 28.7 34.3 26.9 12.6 35.2 30.6 51.2 31.3 Table A1.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 50.5 47.2 56.1 23.7 51.7 24.3 26.3 25.6 34.4 28.1 13.5 25.0 27.4 55.8 29.8 JT 8.1 49.4 45.3 53.4 24.2 48.5 24.8 26.4 25.0 32.5 27.5 12.6 25.3 24.0 51.6 28.7 TTT 7.9 45.6 41.8 50.0 21.8 46.1 23.0 23.9 23.9 30.0 25.1 12.2 23.9 22.6 47.2 27.2 TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 ALP 16.5 22.7 22.9 28.3 25.0 25.6 27.4 23.1 25.2 27.2 64.8 21.7 73.6 23.0 20.2 18.9 Table A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 68.9 1.3 2.0 1.3 7.5 6.6 11.8 16.2 15.7 14.9 15.3 43.9 9.7 16.5 15.3 23.4 JT 69.1 2.1 3.1 2.1 8.7 6.7 12.3 16.0 15.3 15.8 17.0 45.3 11.0 18.4 19.7 22.9 TTT 69.0 3.1 4.5 3.5 10.1 6.8 13.5 18.5 17.1 17.9 20.0 47.0 14.4 20.9 22.8 25.3 TTT-Online 68.8 26.3 28.6 26.9 23.7 6.6 28.7 33.4 35.6 18.7 47.6 58.3 35.3 44.3 47.8 44.3 Table A3.Test accuracy (%) on ImageNet-C, level 5, ResNet-18.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A3.Test error (%) on CIFAR-10-C, level 4. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 46.4 39.2 44.8 15.3 52.5 19.1 20.5 21.3 26.9 13.3 10.5 13.7 20.8 35.3 26.9 JT 8.1 45.0 38.3 42.2 16.4 50.2 20.7 20.5 21.1 25.4 14.1 10.0 14.7 19.0 33.2 25.1 TTT 7.9 41.5 35.4 39.8 15.0 47.8 19.1 18.4 20.1 24.0 13.5 10.0 14.1 17.7 29.4 24.5 TTT-Online 8.2 22.9 20.0 23.9 11.2 35.1 15.6 13.8 18.6 15.9 12.3 9.7 11.9 16.7 13.6 19.8 ALP 16.5 21.3 20.5 24.5 20.7 25.9 23.7 21.4 24.2 23.9 42.2 17.5 53.7 22.1 19.1 18.5 Table A4.Test error (%) on CIFAR-10-C, level 4, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A4.Test error (%) on CIFAR-10-C, level 3. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 42.2 35.1 30.7 12.2 41.7 18.6 17.5 19.0 25.3 10.8 9.7 11.6 15.3 21.7 24.6 JT 8.1 40.2 34.4 29.9 12.2 37.9 20.8 17.3 18.4 25.0 11.4 9.2 12.0 15.2 20.8 22.8 TTT 7.9 37.2 31.6 28.6 11.5 35.8 19.1 15.8 17.8 23.3 11.0 9.1 11.6 14.3 18.9 22.3 TTT-Online 8.2 21.3 17.7 17.9 9.0 23.4 15.3 12.5 16.4 15.8 10.9 9.0 10.7 12.8 12.2 18.7 ALP 16.5 20.0 19.3 20.5 19.2 21.2 24.0 20.5 20.9 24.2 30.1 16.6 39.6 20.9 17.8 18.0 Table A5.Test error (%) on CIFAR-10-C, level 3, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A5.Test error (%) on CIFAR-10-C, level 2. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 31.7 22.6 24.3 9.9 42.6 14.9 14.7 21.7 18.4 9.8 9.1 10.0 13.1 17.1 22.4 JT 8.1 31.0 22.6 23.4 9.1 39.2 16.4 14.2 21.2 17.5 9.4 8.3 10.6 12.8 15.9 20.5 TTT 7.9 28.8 20.7 23.0 9.0 36.6 15.4 13.1 20.2 16.9 9.2 8.3 10.2 12.5 14.8 19.7 TTT-Online 8.2 16.8 13.8 15.5 8.5 23.4 13.3 11.5 16.8 12.7 9.4 8.4 9.7 12.4 11.5 17.0 ALP 16.5 18.0 17.2 19.0 17.8 20.7 21.2 19.3 19.0 20.1 22.4 16.3 29.2 20.3 17.4 17.8 Table A6.Test error (%) on CIFAR-10-C, level 2, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A6.Test error (%) on CIFAR-10-C, level 1. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 21.7 17.1 17.0 9.0 44.0 12.1 13.9 14.3 13.4 9.2 8.9 9.0 13.2 12.0 17.3 JT 8.1 20.4 16.6 16.9 8.2 40.5 12.2 13.0 13.1 12.3 8.4 8.1 8.5 12.9 11.3 15.9 TTT 7.9 19.1 15.8 16.5 8.0 37.9 11.7 12.2 12.8 11.9 8.2 8.0 8.3 12.6 11.1 15.5 TTT-Online 8.2 13.8 11.9 12.2 8.5 24.4 10.5 11.5 12.4 10.7 8.5 8.3 8.6 12.4 10.7 14.4 ALP 17.0 16.8 17.6 16.8 20.9 18.7 19.0 17.3 17.5 17.4 16.1 18.4 20.4 17.0 17.2 17.5 Table A7.Test error (%) on CIFAR-10-C, level 1, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Figure A7.Sample Images from the VID-Robust dataset (Shankar et al., 2019) in the results section adapted to CIFAR-10. Each row shows eight sample images from one class. The seven classes shown are, in order: airplane, bird, car, dog, cat, horse, ship.",
      "meta_data": {
        "arxiv_id": "1909.13231v3",
        "authors": [
          "Yu Sun",
          "Xiaolong Wang",
          "Zhuang Liu",
          "John Miller",
          "Alexei A. Efros",
          "Moritz Hardt"
        ],
        "published_date": "2019-09-29T08:09:15Z",
        "pdf_url": "https://arxiv.org/pdf/1909.13231v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The main research problem addressed is the lack of generalization in predictive models under distribution shifts, where training and test data come from different distributions. The paper introduces \"Test-Time Training\" (TTT), a novel approach that improves model performance by turning a single unlabeled test sample into a self-supervised learning problem. The model parameters are updated on this self-supervised task before making a prediction for the main task. Key contributions include demonstrating significant performance improvements and enhanced robustness on diverse image classification benchmarks (corrupted images, video frames, unknown shifts) without prior knowledge of the test-time distribution shifts. An online version of TTT is also proposed, which continually adapts the model across a stream of test samples. Furthermore, the paper provides theoretical insights, showing that for convex models, positive gradient correlation between the main and self-supervised tasks leads to performance gains, which is also empirically validated for deep learning models.",
        "methodology": "The core methodology is Test-Time Training (TTT), which fine-tunes a pre-trained model at test time. The model architecture is a K-layer neural network with a Y-structure, comprising a shared feature extractor (bottom layers) and two task-specific branches: one for the main classification task and another for a self-supervised auxiliary task. The model is initially trained using multi-task learning on the source distribution, combining the main classification loss with the self-supervised loss. For the self-supervised task, rotation prediction is primarily used, where the model predicts the rotation angle (0, 90, 180, 270 degrees) of augmented input images. At test time, for each unlabeled test sample, the shared feature extractor's parameters are fine-tuned by minimizing the self-supervised auxiliary task loss on augmented copies of that single test sample. The updated model then makes the main task prediction. The online version of TTT extends this by initializing the model parameters for the current test sample with the parameters updated from the previous test sample in a sequential stream, enabling continuous adaptation. Stochastic Gradient Descent (SGD) is used for optimization, and Group Normalization (GN) is employed instead of Batch Normalization (BN) due to the small batch sizes at test time.",
        "experimental_setup": "Experiments were conducted using ResNets (26-layer for CIFAR-10, 18-layer for ImageNet). The evaluation utilized several benchmarks for distribution shifts: CIFAR-10-C and ImageNet-C (datasets with 15 types of common corruptions at 5 severity levels), VID-Robust (video frames from ImageNet Video detection), and CIFAR-10.1 (a new test set with unknown, subtle distribution shifts). Baselines included a plain ResNet model (object recognition task only), a model jointly trained on both main and self-supervised tasks but fixed at test time (an improved implementation of Hendrycks et al., 2019a), Adversarial Logit Pairing (ALP) for CIFAR-10, and Unsupervised Domain Adaptation by Self-Supervision (UDA-SS) for specific comparisons on CIFAR-10-C. Performance was measured primarily by test error or accuracy. For online TTT, results were often presented as averages over the entire test set or sliding window averages to visualize adaptation dynamics. The theoretical insight regarding gradient correlation was empirically validated by plotting gradient inner products against performance improvements across different corruption types and severity levels.",
        "limitations": "The primary limitation identified is the computational cost, as Test-Time Training is significantly slower than regular inference (e.g., 2 times batch size times number of iterations for standard TTT). The effectiveness of the method depends on the quality of the self-supervised task, requiring it to be \"well defined and non-trivial\"; for instance, the rotation prediction task showed limited improvement for certain classes (e.g., 'airplane') where rotational cues were ambiguous or trivial. The online version of TTT relies on the assumption of gradually changing distribution shifts, where sequential test samples are drawn from distributions that are close to each other. Furthermore, the use of Batch Normalization (BN) proved ineffective during Test-Time Training due to the single-image batch size, necessitating the use of Group Normalization (GN). While theoretical results are provided for convex models and empirically supported for deep learning, a formal extension of the theory to non-convex deep neural networks is not fully developed.",
        "future_research_directions": "Future research directions include improving the computational efficiency of TTT, potentially through techniques like thresholding the self-supervised loss to update only 'hard' examples, reducing the number of update iterations, or designing models explicitly amenable to fast updates. The approach can be extended to other machine learning tasks such as segmentation and detection, and to other fields like speech recognition and natural language processing. Leveraging domain expertise to design more effective and specialized self-supervised tasks for TTT is another promising avenue. The paper also suggests that TTT can serve as a new evaluation benchmark for general-purpose self-supervised learning methods. From a theoretical standpoint, further formal discussion and development of the concept of a variable decision boundary and its implications for learning theory are proposed. Integrating TTT with advancements in one-shot learning could potentially improve its update rule. More broadly, the authors encourage a shift towards a new paradigm where learning is a continuous process, with significant learning happening after a model's deployment, moving beyond the traditional fixed decision boundary and the strict division between training and testing."
      }
    },
    {
      "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
      "abstract": "A model must adapt itself to generalize to new and different data during\ntesting. In this setting of fully test-time adaptation the model has only the\ntest data and its own parameters. We propose to adapt by test entropy\nminimization (tent): we optimize the model for confidence as measured by the\nentropy of its predictions. Our method estimates normalization statistics and\noptimizes channel-wise affine transformations to update online on each batch.\nTent reduces generalization error for image classification on corrupted\nImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on\nImageNet-C. Tent handles source-free domain adaptation on digit recognition\nfrom SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to\nCityscapes, and on the VisDA-C benchmark. These results are achieved in one\nepoch of test-time optimization without altering training.",
      "full_text": "Published as a conference paper at ICLR 2021 TENT : F ULLY TEST-TIME ADAPTATION BY ENTROPY MINIMIZATION Dequan Wang1∗, Evan Shelhamer2∗†, Shaoteng Liu1, Bruno Olshausen1, Trevor Darrell1 dqwang@cs.berkeley.edu, shelhamer@google.com UC Berkeley1 Adobe Research2 ABSTRACT A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent 1): we optimize the model for conﬁdence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise afﬁne transformations to update online on each batch. Tent reduces generalization error for image classiﬁcation on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adapta- tion on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training. 1 I NTRODUCTION Deep networks can achieve high accuracy on training and testing data from the same distribution, as evidenced by tremendous benchmark progress (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016). However, generalization to new and different data is limited (Hendrycks & Dietterich, 2019; Recht et al., 2019; Geirhos et al., 2018). Accuracy suffers when the training (source) data differ from the testing (target) data, a condition known as dataset shift(Quionero-Candela et al., 2009). Models can be sensitive to shifts during testing that were not known during training, whether natural variations or corruptions, such as unexpected weather or sensor degradation. Nevertheless, it can be necessary to deploy a model on different data distributions, so adaptation is needed. During testing, the model must adapt given only its parameters and the target data. Thisfully test-time adaptation setting cannot rely on source data or supervision. Neither is practical when the model ﬁrst encounters new testing data, before it can be collected and annotated, as inference must go on. Real-world usage motivates fully test-time adaptation by data, computation, and task needs: 1. Availability. A model might be distributed without source data for bandwidth, privacy, or proﬁt. 2. Efﬁciency. It might not be computationally practical to (re-)process source data during testing. 3. Accuracy. A model might be too inaccurate without adaptation to serve its purpose. To adapt during testing we minimize the entropy of model predictions. We call this objective the test entropy and name our method tent after it. We choose entropy for its connections to error and shift. Entropy is related to error, as more conﬁdent predictions are all-in-all more correct (Figure 1). Entropy is related to shifts due to corruption, as more corruption results in more entropy, with a strong rank correlation to the loss for image classiﬁcation as the level of corruption increases (Figure 2). To minimize entropy, tent normalizes and transforms inference on target data by estimating statistics and optimizing afﬁne parameters batch-by-batch. This choice of low-dimensional, channel-wise feature modulation is efﬁcient to adapt during testing, even for online updates. Tent does not restrict or alter model training: it is independent of the source data given the model parameters. If the model can be run, it can be adapted. Most importantly, tent effectively reduces not just entropy but error. ∗Equal contribution. †Work done at Adobe Research; the author is now at DeepMind. 1Please see the project page at https://github.com/DequanWang/tent for the code and more. 1 arXiv:2006.10726v3  [cs.LG]  18 Mar 2021Published as a conference paper at ICLR 2021 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Entropy 0 20 40 60 80Error (%) Figure 1: Predictions with lower entropy have lower error rates on corrupted CIFAR-100-C. Certainty can serve as supervision during testing. 0.2 0.3 0.4 0.5 0.6 Entropy 0.2 0.4 0.6 0.8 1.0 1.2Loss = 0.61 original noise blur digital weather  level level Figure 2: More corruption causes more loss and entropy on CIFAR-100-C. Entropy can estimate the degree of shift without training data or labels. Our results evaluate generalization to corruptions for image classiﬁcation, to domain shift for digit recognition, and to simulation-to-real shift for semantic segmentation. For context with more data and optimization, we evaluate methods for robust training, domain adaptation, and self-supervised learning given the labeled source data. Tent can achieve less error given only the target data, and it improves on the state-of-the-art for the ImageNet-C benchmark. Analysis experiments support our entropy objective, check sensitivity to the amount of data and the choice of parameters for adaptation, and back the generality of tent across architectures. Our contributions • We highlight the setting of fully test-time adaptation with only target data and no source data. To emphasize practical adaptation during inference we benchmark with ofﬂine and online updates. • We examine entropy as an adaptation objective and propose tent: a test-time entropy minimization scheme to reduce generalization error by reducing the entropy of model predictions on test data. • For robustness to corruptions, tent reaches 44.0% error on ImageNet-C, better than the state-of- the-art for robust training (50.2%) and the strong baseline of test-time normalization (49.9%). • For domain adaptation, tent is capable of online and source-free adaptation for digit classiﬁcation and semantic segmentation, and can even rival methods that use source data and more optimization. 2 S ETTING : F ULLY TEST-TIME ADAPTATION Adaptation addresses generalization from source to target. A model fθ(x) with parameters θtrained on source data and labels xs,ys may not generalize when tested on shifted target data xt. Table 1 summarizes adaptation settings, their required data, and types of losses. Our fully test-time adaptation setting uniquely requires only the model fθ and unlabeled target data xt for adaptation during inference. Existing adaptation settings extend training given more data and supervision. Transfer learning by ﬁne-tuning (Donahue et al., 2014; Yosinski et al., 2014) needs target labels to (re-)train with a supervised loss L(xt,yt). Without target labels, our setting denies this supervised training. Domain adaptation (DA) (Quionero-Candela et al., 2009; Saenko et al., 2010; Ganin & Lempitsky, 2015; Tzeng et al., 2015) needs both the source and target data to train with a cross-domain loss L(xs,xt). Test-time training (TTT) (Sun et al., 2019b) adapts during testing but ﬁrst alters training to jointly optimize its supervised loss L(xs,ys) and self-supervised loss L(xs). Without source, our setting denies joint training across domains (DA) or losses (TTT). Existing settings have their purposes, but do not cover all practical cases when source, target, or supervision are not simultaneously available. Unexpected target data during testing requires test-time adaptation. TTT and our setting adapt the model by optimizing an unsupervised loss during testing L(xt). During training, TTT jointly optimizes this same loss on source data L(xs) with a supervised loss L(xs,ys), to ensure the parameters θare shared across losses for compatibility with adaptation by L(xt). Fully test-time adaptation is independent of the training data and training loss given the parameters θ. By not changing training, our setting has the potential to require less data and computation for adaptation. 2Published as a conference paper at ICLR 2021 Table 1: Adaptation settings differ by their data and therefore losses during training and testing. Of the source s and target t data xand labels y, our fully test-time setting only needs the target data xt. setting source data target data train loss test loss ﬁne-tuning - xt,yt L(xt,yt) - domain adaptation xs, ys xt L(xs,ys) + L(xs,xt) - test-time training xs, ys xt L(xs,ys) + L(xs) L(xt) fully test-time adaptation - xt - L(xt)     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t Figure 3: Method overview. Tent does not alter training (a), but minimizes the entropy of predictions during testing (b) over a constrained modulation ∆, given the parameters θand target data xt. 3 M ETHOD : T EST ENTROPY MINIMIZATION VIA FEATURE MODULATION We optimize the model during testing to minimize the entropy of its predictions by modulating its features. We call our method tent for test entropy. Tent requires a compatible model, an objective to minimize (Section 3.1), and parameters to optimize over (Section 3.2) to fully deﬁne the algorithm (Section Section 3.3). Figure 3 outlines our method for fully test-time adaptation. The model to be adapted must be trained for the supervised task, probabilistic, and differentiable. No supervision is provided during testing, so the model must already be trained. Measuring the entropy of predictions requires a distribution over predictions, so the model must be probabilistic. Gradients are required for fast iterative optimization, so the model must be differentiable. Typical deep networks for supervised learning satisfy these model requirements. 3.1 E NTROPY OBJECTIVE Our test-time objective L(xt) is to minimize the entropy H(ˆy) of model predictions ˆy= fθ(xt). In particular, we measure the Shannon entropy (Shannon, 1948), H(ˆy) = −∑ cp(ˆyc) logp(ˆyc) for the probability ˆyc of class c. Note that optimizing a single prediction has a trivial solution: assign all probability to the most probable class. We prevent this by jointly optimizing batched predictions over parameters that are shared across the batch. Entropy is an unsupervised objective because it only depends on predictions and not annotations. However, as a measure of the predictions it is directly related to the supervised task and model. In contrast, proxy tasks for self-supervised learning are not directly related to the supervised task. Proxy tasks derive a self-supervised label y′from the input xt without the task label y. Examples of these proxies include rotation prediction (Gidaris et al., 2018), context prediction (Doersch et al., 2015), and cross-channel auto-encoding (Zhang et al., 2017). Too much progress on a proxy task could interfere with performance on the supervised task, and self-supervised adaptation methods have to limit or mix updates accordingly (Sun et al., 2019b;a). As such, care is needed to choose a proxy compatible with the domain and task, to design the architecture for the proxy model, and to balance optimization between the task and proxy objectives. Our entropy objective does not need such efforts. 3.2 M ODULATION PARAMETERS The model parameters θare a natural choice for test-time optimization, and these are the choice of prior work for train-time entropy minimization (Grandvalet & Bengio, 2005; Dhillon et al., 2020; Carlucci et al., 2017). However, θis the only representation of the training/source data in our setting, and altering θcould cause the model to diverge from its training. Furthermore, f can be nonlinear and θcan be high dimensional, making optimization too sensitive and inefﬁcient for test-time usage. 3Published as a conference paper at ICLR 2021 IN OUT+ <latexit sha1_base64=\"FGMSn1olAms3UkJ+mUM6lRBkJrw=\">AAAB6HicbVDLSgNBEOyNryS+oh69DAZBEMKuKHoMevGYgHlgsoTZSW8yZvbBzKwYlnyBFw+K5OoP+C/e/BqdJB40saChqOqmu8uLBVfatj+tzNLyyupaNpdf39jc2i7s7NZVlEiGNRaJSDY9qlDwEGuaa4HNWCINPIENb3A18Rv3KBWPwhs9jNENaC/kPmdUG6l63CkU7ZI9BVkkzg8plnPx+Pb94avSKXy0uxFLAgw1E1SplmPH2k2p1JwJHOXbicKYsgHtYcvQkAao3HR66IgcGqVL/EiaCjWZqr8nUhooNQw80xlQ3Vfz3kT8z2sl2r9wUx7GicaQzRb5iSA6IpOvSZdLZFoMDaFMcnMrYX0qKdMmm7wJwZl/eZHUT0rOaemsatK4hBmysA8HcAQOnEMZrqECNWCA8AjP8GLdWU/WqzWetWasn5k9+APr7RuTUJCF</latexit> \u0000 <latexit sha1_base64=\"8eHH7cr25vA7s0zJYYCDPQNSaT0=\">AAAB7XicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhMwDwgWcLsZDYZM7OzzMwKYcnRuxcPinj1F/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk3LRBFaJZJL1QiwppxFtGqY4bQRK4pFwGk96N+M/foDVZrJ6M4MYuoL3I1YyAg2Vqq1ulgI3M4X3KI7AVok3owUSoejyvfj0ajczn+2OpIkgkaGcKx103Nj46dYGUY4HeZaiaYxJn3cpU1LIyyo9tPJtUN0YpUOCqWyFRk0UX9PpFhoPRCB7RTY9PS8Nxb/85qJCS/9lEVxYmhEpovChCMj0fh11GGKEsMHlmCimL0VkR5WmBgbUM6G4M2/vEhqZ0XvvHhVsWlcwxRZOIBjOAUPLqAEt1CGKhC4hyd4gVdHOs/Om/M+bc04s5l9+APn4wd3ypLI</latexit> ⇥ <latexit sha1_base64=\"r9CoIRh1LwyAxszWUWZZpZEIYvU=\">AAAB7XicbVA9TwJBEJ3DL8Av1NLmIjGxIndGoyXRxhIT+YhwIXvLHqzs7V5254yE8B9sLDDG1tL/Yuev0QUsFHzJJC/vzWRmXpgIbtDzPp3M0vLK6lo2l1/f2NzaLuzs1oxKNWVVqoTSjZAYJrhkVeQoWCPRjMShYPWwfznx6/dMG67kDQ4SFsSkK3nEKUEr1VrIY2bahaJX8qZwF4n/Q4rlXDK+fX/4qrQLH62OomnMJFJBjGn6XoLBkGjkVLBRvpUalhDaJ13WtFQSuyQYTq8duYdW6biR0rYkulP198SQxMYM4tB2xgR7Zt6biP95zRSj82DIZZIik3S2KEqFi8qdvO52uGYUxcASQjW3t7q0RzShaAPK2xD8+ZcXSe245J+UTq9tGhcwQxb24QCOwIczKMMVVKAKFO7gEcbw7CjnyXlxXmetGednZg/+wHn7Btf2kwo=</latexit> \u0000 <latexit sha1_base64=\"icKTvSnYuWAwxCN4MXaVcPxJrUE=\">AAAB7HicbVBNS8NAEN34WetX1aMiwSJ4KokI6q3oxWMLpi20oWy2k3bpZhN2J0IJPXr24kERr/6G/g5v/gb/hNuPg7Y+GHi8N8PMvCARXKPjfFlLyyura+u5jfzm1vbObmFvv6bjVDHwWCxi1QioBsEleMhRQCNRQKNAQD3o3479+gMozWN5j4ME/Ih2JQ85o2gkrxUA0nah6JScCexF4s5IsXw0qn4/Ho8q7cJnqxOzNAKJTFCtm66ToJ9RhZwJGOZbqYaEsj7tQtNQSSPQfjY5dmifGqVjh7EyJdGeqL8nMhppPYgC0xlR7Ol5byz+5zVTDK/8jMskRZBsuihMhY2xPf7c7nAFDMXAEMoUN7farEcVZWjyyZsQ3PmXF0ntvORelK6rJo0bMkWOHJITckZccknK5I5UiEcY4eSJvJBXS1rP1pv1Pm1dsmYzB+QPrI8ftLWSVw==</latexit> \u0000 <latexit sha1_base64=\"6pSYsGji0D9Bm0vY9by0e43+pZo=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgxbArAfUW9OIxAfOAZAmzk95kzOzsMjMrhJAv8OJBEa9+kjf/xkmyB00saCiquunuChLBtXHdbye3tr6xuZXfLuzs7u0fFA+PmjpOFcMGi0Ws2gHVKLjEhuFGYDtRSKNAYCsY3c381hMqzWP5YMYJ+hEdSB5yRo2V6he9Ysktu3OQVeJlpAQZar3iV7cfszRCaZigWnc8NzH+hCrDmcBpoZtqTCgb0QF2LJU0Qu1P5odOyZlV+iSMlS1pyFz9PTGhkdbjKLCdETVDvezNxP+8TmrCa3/CZZIalGyxKEwFMTGZfU36XCEzYmwJZYrbWwkbUkWZsdkUbAje8surpHlZ9irlm3qlVL3N4sjDCZzCOXhwBVW4hxo0gAHCM7zCm/PovDjvzseiNedkM8fwB87nD3htjL0=</latexit> ÷ <latexit sha1_base64=\"KLNiQjydwC+UjsLtIanox9T+rq8=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjxWsB/QhrLZbNqlu5uwuymU0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmBQln2rjut1Pa2Nza3invVvb2Dw6PqscnHR2nitA2iXmsegHWlDNJ24YZTnuJolgEnHaDyX3ud6dUaRbLJzNLqC/wSLKIEWxyaRCy6bBac+vuAmideAWpQYHWsPo1CGOSCioN4Vjrvucmxs+wMoxwOq8MUk0TTCZ4RPuWSiyo9rPFrXN0YZUQRbGyJQ1aqL8nMiy0nonAdgpsxnrVy8X/vH5qohs/YzJJDZVkuShKOTIxyh9HIVOUGD6zBBPF7K2IjLHCxNh4KjYEb/XlddK5qnuN+u1jo9a8K+IowxmcwyV4cA1NeIAWtIHAGJ7hFd4c4bw4787HsrXkFDOn8AfO5w8aWY5N</latexit> µ <latexit sha1_base64=\"lbHwl5bkUbenc+Yo+u8yNzpxsy0=\">AAAB6nicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhM0DwgWcLsZDYZMjO7zMwKYcnRoxcPinj1I/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk1HiSK0SiIeqUaANeVM0qphhtNGrCgWAaf1oH8z9usPVGkWyXsziKkvcFeykBFsrHTXEkk7X3CL7gRokXgzUigdjirfj0ejcjv/2epEJBFUGsKx1k3PjY2fYmUY4XSYayWaxpj0cZc2LZVYUO2nk1OH6MQqHRRGypY0aKL+nkix0HogAtspsOnpeW8s/uc1ExNe+imTcWKoJNNFYcKRidD4b9RhihLDB5Zgopi9FZEeVpgYm07OhuDNv7xIamdF77x4VbFpXMMUWTiAYzgFDy6gBLdQhioQ6MITvMCrw51n5815n7ZmnNnMPvyB8/EDTj2RiQ==</latexit> \u0000 <latexit sha1_base64=\"xnrzB72KzfqBMQ17s1zlsxQWR+k=\">AAAB7XicbZDLSgMxFIbP1Fsdb1WXboJFcFVmRFAXYtGNywr2Au1QMmmmjU0yQ5IRytB3cONCETcufBT3bsS3Mb0stPWHwMf/n0POOWHCmTae9+3kFhaXllfyq+7a+sbmVmF7p6bjVBFaJTGPVSPEmnImadUww2kjURSLkNN62L8a5fV7qjSL5a0ZJDQQuCtZxAg21qq1NOsK3C4UvZI3FpoHfwrFiw/3PHn7civtwmerE5NUUGkIx1o3fS8xQYaVYYTTodtKNU0w6eMubVqUWFAdZONph+jAOh0Uxco+adDY/d2RYaH1QIS2UmDT07PZyPwva6YmOg0yJpPUUEkmH0UpRyZGo9VRhylKDB9YwEQxOysiPawwMfZArj2CP7vyPNSOSv5x6ezGK5YvYaI87ME+HIIPJ1CGa6hAFQjcwQM8wbMTO4/Oi/M6Kc05055d+CPn/Qf/xpJs</latexit> <latexit sha1_base64=\"9MzbukliF0G5U4WyINCTJmMNjA8=\">AAACNnicdVBNS8NAFNz4bf2KevSyWAQFLUlR9CiK4EWoYFuhiWWz3dSlu0nYfVFL6K/y4u/w1osHRbz6E9y0PWjVgYVhZh773gSJ4Bocp29NTE5Nz8zOzRcWFpeWV+zVtZqOU0VZlcYiVtcB0UzwiFWBg2DXiWJEBoLVg85p7tfvmNI8jq6gmzBfknbEQ04JGKlpX3gyxZ5gIRCl4nvsSQK3QZCd9RoPTfB3sad5W5Kb8j+h7Xx+D5vszk3Zb9pFp+QMgH8Td0SKaIRK0372WjFNJYuACqJ1w3US8DOigFPBegUv1SwhtEParGFoRCTTfjY4u4e3jNLCYazMiwAP1O8TGZFad2VgkvnCetzLxb+8RgrhkZ/xKEmBRXT4UZgKDDHOO8QtrhgF0TWEUMXNrpjeEkUomKYLpgR3/OTfpFYuuQcl53K/eHwyqmMObaBNtI1cdIiO0TmqoCqi6BH10St6s56sF+vd+hhGJ6zRzDr6AevzC4nRq7w=</latexit> µ  E [ x t ] , \u0000 2  E [( µ \u0000 x t ) 2 ] <latexit sha1_base64=\"5uCFLjsyhVlotMr43Rw1BdZFk0s=\">AAACYXicbZFLS+RAFIUrGZ/tK+Ms3RQ2gqC0iSgzy2bcuHTAVqHTNDfVN21hVRKqbmamCf0nZzcbN/4RKzH4vlBw+O659TiVFEpaCsP/nv9lYXFpeWW1s7a+sbkVfN2+snlpBA5ErnJzk4BFJTMckCSFN4VB0InC6+TurO5f/0ZjZZ5d0qzAkYZpJlMpgBwaB3/jKWgNPFaYEhiT/+EtOeBxAYYkqFgD3UqqzudHL6wxHfI4QXo73YBPh59RbRkH3bAXNsU/iqgVXdbWxTj4F09yUWrMSCiwdhiFBY2qekuhcN6JS4sFiDuY4tDJDDTaUdUkNOd7jkx4mhu3MuINfT1RgbZ2phPnrO9r3/dq+FlvWFL6Y1TJrCgJM/F0UFoqTjmv4+YTaVCQmjkBwkh3Vy5uwYAg9ykdF0L0/skfxdVxLzrthb9Ouv2fbRwrbIftsn0Wse+sz87ZBRswwe69BW/D2/Qe/FU/8LefrL7Xznxjb8rfeQSpH7dZ</latexit> \u0000  \u0000 + @ H / @\u0000 , \u0000  \u0000 + @ H / @\u0000 normalization transformation Figure 4: Tent modulates features during testing by estimating normalization statistics µ,σ and optimizing transformation parameters γ,β. Normalization and transformation apply channel-wise scales and shifts to the features. The statistics and parameters are updated on target data without use of source data. In practice, adapting γ,β is efﬁcient because they make up <1% of model parameters. For stability and efﬁciency, we instead only update feature modulations that are linear (scales and shifts), and low-dimensional (channel-wise). Figure 4 shows the two steps of our modulations: normalization by statistics and transformation by parameters. Normalization centers and standardizes the input xinto ¯x= (x−µ)/σby its mean µand standard deviation σ. Transformation turns ¯xinto the output x′= γ¯x+ βby afﬁne parameters for scale γand shift β. Note that the statistics µ,σ are estimated from the data while the parameters γ,β are optimized by the loss. For implementation, we simply repurpose the normalization layers of the source model. We update their normalization statistics and afﬁne parameters for all layers and channels during testing. 3.3 A LGORITHM Initialization The optimizer collects the afﬁne transformation parameters {γl,k,βl,k}for each normalization layer land channel kin the source model. The remaining parameters θ\\{γl,k,βl,k} are ﬁxed. The normalization statistics {µl,k,σl,k}from the source data are discarded. Iteration Each step updates the normalization statistics and transformation parameters on a batch of data. The normalization statistics are estimated for each layer in turn, during the forward pass. The transformation parameters γ,β are updated by the gradient of the prediction entropy ∇H(ˆy), during the backward pass. Note that the transformation update follows the prediction for the current batch, and so it only affects the next batch (unless forward is repeated). This needs just one gradient per point of additional computation, so we use this scheme by default for efﬁciency. Termination For online adaptation, no termination is necessary, and iteration continues as long as there is test data. For ofﬂine adaptation, the model is ﬁrst updated and then inference is repeated. Adaptation may of course continue by updating for multiple epochs. 4 E XPERIMENTS We evaluate tent for corruption robustness on CIFAR-10/CIFAR-100 and ImageNet, and for domain adaptation on digit adaptation from SVHN to MNIST/MNIST-M/USPS. Our implementation is in PyTorch (Paszke et al., 2019) with the pycls library (Radosavovic et al., 2019). Datasets We run on image classiﬁcation datasets for corruption and domain adaptation conditions. For large-scale experiments we choose ImageNet (Russakovsky et al., 2015), with 1,000 classes, a training set of 1.2 million, and a validation set of 50,000. For experiments at an accessible scale we choose CIFAR-10/CIFAR-100 (Krizhevsky, 2009), with 10/100 classes, a training set of 50,000, and a test set of 10,000. For domain adaptation we choose SVHN (Netzer et al., 2011) as source and MNIST (LeCun et al., 1998)/MNIST-M (Ganin & Lempitsky, 2015)/USPS (Hull, 1994) as targets, with ten classes for the digits 0–9. SVHN has color images of house numbers from street views with a training set of 73,257 and test set of 26,032. MNIST/MNIST-M/USPS have handwritten digits with a training sets of 60,000/60,000/7,291 and test sets of 10,000/10,000/2,007. Models For corruption we use residual networks (He et al., 2016) with 26 layers (R-26) on CIFAR- 10/100 and 50 layers (R-50) on ImageNet. For domain adaptation we use the R-26 architecture. For fair comparison, all methods in each experimental condition share the same architecture. Our networks are equipped with batch normalization (Ioffe & Szegedy, 2015). For the source model without adaptation, the normalization statistics are estimated during training on the source data. For all test-time adaptation methods, we estimate these statistics during testing on the target data, as done in concurrent work on adaptation by normalization (Schneider et al., 2020; Nado et al., 2020). 4Published as a conference paper at ICLR 2021 Table 2: Corruption benchmark on CIFAR-10-C and CIFAR-100-C for the highest severity. Tent has least error, with less optimization than domain adaptation (RG, UDA-SS) and test-time training (TTT), and improves on test-time norm (BN). Method Source Target Error (%) C10-C C100-C Source train 40.8 67.2 RG train train 18.3 38.9 UDA-SS train train 16.7 47.0 TTT train test 17.5 45.0 BN test 17.3 42.6 PL test 15.7 41.2 Tent (ours) test 14.3 37.3 originalgaussshot impulsedefocus glassmotionzoomsnowfrostfog bright contrastelasticpixeljpeg 0 25 50 75Error (%) source 59.5% norm 49.9% tent 44.0% ANT 50.2% Figure 5: Corruption benchmark on ImageNet-C: error for each type averaged over severity levels. Tent improves on the prior state-of-the-art, adver- sarial noise training (Rusak et al., 2020), by fully test-time adaptation without altering training. Optimization We optimize the modulation parameters γ,β following the training hyperparameters for the source model with few changes. On ImageNet we optimize by SGD with momentum; on other datasets we optimize by Adam (Kingma & Ba, 2015). We lower the batch size (BS) to reduce memory usage for inference, then lower the learning rate (LR) by the same factor to compensate (Goyal et al., 2017). On ImageNet, we set BS = 64 and LR = 0.00025, and on other datasets we set BS = 128 and LR = 0.001.We control for ordering by shufﬂing and sharing the order across methods. Baselines We compare to domain adaptation, self-supervision, normalization, and pseudo-labeling: • source applies the trained classiﬁer to the test data without adaptation, • adversarial domain adaptation (RG) reverses the gradients of a domain classiﬁer on source and target to optimize for a domain-invariant representation (Ganin & Lempitsky, 2015), • self-supervised domain adaptation (UDA-SS) jointly trains self-supervised rotation and position tasks on source and target to optimize for a shared representation (Sun et al., 2019a), • test-time training (TTT) jointly trains for supervised and self-supervised tasks on source, then keeps training the self-supervised task on target during testing (Sun et al., 2019b), • test-time normalization (BN) updates batch normalization statistics (Ioffe & Szegedy, 2015) on the target data during testing (Schneider et al., 2020; Nado et al., 2020), • pseudo-labeling (PL) tunes a conﬁdence threshold, assigns predictions over the threshold as labels, and then optimizes the model to these pseudo-labels before testing (Lee, 2013). Only test-time normalization (BN), pseudo-labeling (PL), and tent (ours) are fully test-time adaptation methods. See Section 2 for an explanation and contrast with domain adaptation and test-time training. 4.1 R OBUSTNESS TO CORRUPTIONS To benchmark robustness to corruption, we make use of common image corruptions (see Appendix A for examples). The CIFAR-10/100 and ImageNet datasets are turned into the CIFAR-10/100-C and ImageNet-C corruption benchmarks by duplicating their test/validation sets and applying 15 types of corruptions at ﬁve severity levels (Hendrycks & Dietterich, 2019). Tent improves more with less data and computation.Table 2 reports errors averaged over corrup- tion types at the severest level of corruption. On CIFAR-10/100-C we compare all methods, including those that require joint training across domains or losses, given the convenient sizes of these datasets. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent improves on the fully test-time adaptation baselines (BN, PL) but also the domain adaptation (RG, UDA-SS) and test-time training (TTT) methods that need several epochs of optimization on source and target. Tent consistently improves across corruption types.Figure 5 plots the error for each corruption type averaged over corruption levels on ImageNet-C. We compare the most efﬁcient methods—source, normalization, and tent—given the large scale of the source data (>1 million images) needed by other methods and the 75 target combinations of corruption types and levels. Tent and BN adapt online to rival the efﬁciency of inference without adaptation. Tent reaches the least error for most corruption types without increasing the error on the original data. 5Published as a conference paper at ICLR 2021 Table 3: Digit domain adaptation from SVHN to MNIST/MNIST-M/USPS. Source-free adaptation is not only feasible, but more efﬁcient. Tent always improves on normalization (BN), and in 2/3 cases achieves less error than domain adaptation (RG, UDA-SS) without joint training on source & target. Method Source Target Epochs Error (%) Source + Target MNIST MNIST-M USPS Source train - 18.2 39.7 19.3 RG train train 10 + 10 15.0 33.4 18.9 UDA-SS train train 10 + 10 11.1 22.2 18.4 BN test 0 + 1 15.7 39.7 18.0 Tent (ours) test 0 + 1 10.0 37.0 16.3 Tent (ours) test 0 + 10 8.2 36.8 14.4 Tent reaches a new state-of-the-art without altering training.The state-of-the-art methods for robustness extend training with adversarial noise (ANT) (Rusak et al., 2020) for 50.2% error or mixtures of data augmentations (AugMix) (Hendrycks et al., 2020) for 51.7% error. Combined with stylization from external images (SIN) (Geirhos et al., 2019), ANT+SIN reaches 47.4%. Tent reaches a new state-of-the-art of 44.0% by online adaptation and 42.3% by ofﬂine adaptation. It improves on ANT for all types except noise, on which ANT is trained. This requires just one gradient per test point, without more optimization on the training set (ANT, AugMix) or use of external images (SIN). Among fully test-time adaptation methods, tent reduces the error beyond test-time normalization for 18% relative improvement. In concurrent work, Schneider et al. (2020) report 49.3% error for test-time normalization, for which tent still gives 14% relative improvement. 4.2 S OURCE -FREE DOMAIN ADAPTATION We benchmark digit adaptation (Ganin & Lempitsky, 2015; Tzeng et al., 2015; 2017; Shu et al., 2018) for shifts from SVHN to MNIST/MNIST-M/USPS. Recall that unsupervised domain adaptation makes use the labeled source data and unlabeled target data, while our fully test-time adaptation setting denies use of source data. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent adapts to target without source.Table 3 reports the target errors for domain adaptation and fully test-time adaptation methods. Test-time normalization (BN) marginally improves, while adversarial domain adaptation (RG) and self-supervised domain adaptation (UDA-SS) improve more by joint training on source and target. Tent always has lower error than the source model and BN, and it achieves the lowest error in 2/3 cases, even in just one epoch and without use of source data. While encouraging for fully test-time adaptation, unsupervised domain adaptation remains necessary for the highest accuracy and harder shifts. For SVHN-to-MNIST, DIRT-T (Shu et al., 2018) achieves a remarkable 0.6% error 2. For MNIST-to-SVHN, a difﬁcult shift with source-only error of 71.3%, DIRT-T reaches45.5% and UDA-SS reaches 38.7%. Tent fails on this shift and increases error to 79.8%. In this case success presently requires joint optimization over source and target. Tent needs less computation, but still improves with more.Tent adapts efﬁciently on target data alone with just one gradient per point. RG & UDA-SS also use the source data (SVHN train), which is ∼7×the size of the target data (MNIST test), and optimize for 10 epochs. Tent adapts with ∼80× less computation. With more updates, tent reaches 8.2% error in 10 epochs and 6.5% in 100 epochs. With online updates, tent reaches 12.5% error in one epoch and 8.4% error in 10 epochs. Tent scales to semantic segmentation.To show scalability to large models and inputs, we evaluate semantic segmentation (pixel-wise classiﬁcation) on a domain shift from a simulated source to a real target. The source is GTA (Richter et al., 2017), a video game in an urban environment, and the target is Cityscapes (Cordts et al., 2016), an urban autonomous driving dataset. The model is HRNet-W18, a fully convolutional network (Shelhamer et al., 2017) with high-resolution architecture (Wang et al., 2020). The target intersection-over-union scores (higher is better) are source 28.8%, BN 31.4%, and tent 35.8% with ofﬂine optimization by Adam. For adaptation to a single image, tent reaches 36.4% in 10 iterations with episodic optimization. See the appendix for a qualitative example (Appendix B). 2We exclude DIRT-T from our experiments because of incomparable differences in architecture and model selection. DIRT-T tunes with labeled target data, but we do not. Please refer to Shu et al. (2018) for more detail. 6Published as a conference paper at ICLR 2021 Figure 6: Tent reduces the entropy and loss. We plot changes in entropy∆Hand loss ∆Lfor all of CIFAR-100-C. Change in entropy rank-correlates with change in loss: note the dark diagonal and the rank correlation coefﬁcient of 0.22. (a) Source (b) BN  (c) Tent (d) Oracle  Figure 7: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts fea- tures away from the reference, but BN reduces the shifts. Tent instead shifts features more, and closer to an oracle that optimizes on target labels. Tent scales to the VisDA-C challenge.To show adaptation on a more difﬁcult benchmark, we evaluate on the VisDA-C challenge (Peng et al., 2017). The task is object recognition for 12 classes where the source data is synthesized by rendering 3D models and the target data is collected from real scenes. The validation error for our source model (ResNet-50, pretrained on ImageNet) is 56.1%, while tent reaches 45.6%, and improves to 39.6% by updating all layers except for the ﬁnal classiﬁer as done by Liang et al. (2020). Although ofﬂine source-free adaptation by model adaptation (Li et al., 2020) or SHOT (Liang et al., 2020) can reach lower error with more computation and tuning, tent can adapt online during testing. 4.3 A NALYSIS Tent reduces entropy and error.Figure 6 veriﬁes tent does indeed reduce the entropy and the task loss (softmax cross-entropy). We plot changes in entropy and loss on CIFAR-100-C for all 75 corruption type/level combinations. Both axes are normalized by the maximum entropy of a prediction (log 100) and clipped to ±1. Most points have lower entropy and error after adaptation. Tent needs feature modulation.We ablate the normalization and transformation steps of feature modulation. Not updating normalization increases errors, and can fail to improve over BN and PL. Not updating transformation parameters reduces the method to test-time normalization. Updating only the last layer of the model can improve but then degrades with further optimization. Updating the full model parameters θnever improves over the unadapted source model. Tent generalizes across target data.Adaptation could be limited to the points used for updates. We check that adaptation generalizes across points by adapting on target train and not target test. Test errors drop: CIFAR-100-C error goes from 37.3% to 34.2% and SVHN-to-MNIST error goes from 8.2% to 6.5%. (Train is larger than test; when subsampling to the same size errors differ by <0.1%.) Therefore the adapted modulation is not point speciﬁc but general. Tent modulation differs from normalization.Modulation normalizes and transforms features. We examine the combined effect. Figure 7 contrasts adapted features on corrupted data against reference features on uncorrupted data. We plot features from the source model, normalization, tent, and an oracle that optimizes on the target labels. Normalization makes features more like the reference, but tent does not. Instead, tent makes features more like the oracle. This suggests a different and task-speciﬁc effect. See the appendix for visualizations of more layers (Appendix C). 7Published as a conference paper at ICLR 2021 Tent adapts alternative architectures.Tent is architecture agnostic in principle. To gauge its generality in practice, we evaluate new architectures based on self-attention (SAN) (Zhao et al., 2020) and equilibrium solving (MDEQ) (Bai et al., 2020) for corruption robustness on CIFAR-100-C. Table 4 shows that tent reduces error with the same settings as convolutional residual networks. Table 4: Tent adapts alternative architectures on CIFAR-100-C without tuning. Results are error (%). SAN-10 (pair) SAN-10 (patch) MDEQ (large) Source BN Tent Source BN Tent Source BN Tent 55.3 39.7 36.7 48.0 31.8 29.2 53.3 44.9 41.7 5 R ELATED WORK We relate tent to existing adaptation, entropy minimization, and feature modulation methods. Train-Time AdaptationDomain adaptation jointly optimizes on source and target by cross-domain losses L(xs,xt) to mitigate shift. These losses optimize feature alignment (Gretton et al., 2009; Sun et al., 2017), adversarial invariance (Ganin & Lempitsky, 2015; Tzeng et al., 2017), or shared proxy tasks (Sun et al., 2019a). Transduction (Gammerman et al., 1998; Joachims, 1999; Zhou et al., 2004) jointly optimizes on train and test to better ﬁt speciﬁc test instances. While effective in their settings, neither applies when joint use of source/train and target/test is denied. Tent adapts on target alone. Recent “source-free” methods (Li et al., 2020; Kundu et al., 2020; Liang et al., 2020) also adapt without source data. Li et al. (2020); Kundu et al. (2020) rely on generative modeling and optimize multiple models with multiple losses. Kundu et al. (2020); Liang et al. (2020) also alter training. Tent does not need generative modeling, nor does it alter training, and so it can deployed more generally to adapt online with much more computational efﬁciency. SHOT (Liang et al., 2020) adapts by informa- tion maximization (entropy minimization and diversity regularization), but differs in its other losses and its parameterization. These source-free methods optimize ofﬂine with multiple losses for multiple epochs, which requires more tuning and computation than tent, but may achieve more accuracy with more computation. Tent optimizes online with just one loss and an efﬁcient parameterization of modulation to emphasize fully test-time adaptation during inference. We encourage examination of each of these works on the frontier of adaptation without source data. Chidlovskii et al. (2016) are the ﬁrst to motivate adaptation without source data for legal, commercial, or technical concerns. They adapt predictions by applying denoising auto-encoders while we adapt models by entropy minimization. We share their motivations, but the methods and experiments differ. Test-Time AdaptationTent adapts by test-time optimization and normalization to update the model. Test-time adaptation of predictions, through which harder and uncertain cases are adjusted based on easier and certain cases (Jain & Learned-Miller, 2011), provides inspiration for certainty-based model adaptation schemes like our own. Test-time training (TTT) (Sun et al., 2019b) also optimizes during testing, but differs in its loss and must alter training. TTT relies on a proxy task, such as recognizing rotations of an image, and so its loss depends on the choice of proxy. (Indeed, its authors caution that the proxy must be “both well-deﬁned and non-trivial in the new domain”). TTT alters training to optimize this proxy loss on source before adapting to target. Tent adapts without proxy tasks and without altering training. Normalizing feature statistics is common for domain adaptation (Gretton et al., 2009; Sun et al., 2017). For batch normalization Li et al. (2017); Carlucci et al. (2017) separate source and target statistics during training. Schneider et al. (2020); Nado et al. (2020) estimate target statistics during testing to improve generalization. Tent builds on test-time normalization to further reduce generalization error. Entropy MinimizationEntropy minimization is a key regularizer for domain adaptation (Carlucci et al., 2017; Shu et al., 2018; Saito et al., 2019; Roy et al., 2019), semi-supervised learning (Grandvalet & Bengio, 2005; Lee, 2013; Berthelot et al., 2019), and few-shot learning (Dhillon et al., 2020). Regularizing entropy penalizes decisions at high densities in the data distribution to improve accuracy for distinct classes (Grandvalet & Bengio, 2005). These methods regularize entropy during training in concert with other supervised and unsupervised losses on additional data. Tent is the ﬁrst to minimize 8Published as a conference paper at ICLR 2021 entropy during testing, for adaptation to dataset shifts, without other losses or data. Entropic losses are common; our contribution is to exhibit entropy as the sole lossfor fully test-time adaptation. Feature ModulationModulation makes a model vary with its input. We optimize modulations that are simpler than the full model for stable and efﬁcient adaptation. We modulate channel-wise afﬁne transformations, for their effectiveness in tandem with normalization (Ioffe & Szegedy, 2015; Wu & He, 2018), and for their ﬂexibility in conditioning for different tasks (Perez et al., 2018). These normalization and conditioning methods optimize the modulation during training by a supervised loss, but keep it ﬁxed during testing. We optimize the modulation during testing by an unsupervised loss, so that it can adapt to different target data. 6 D ISCUSSION Tent reduces generalization error on shifted data by test-time entropy minimization. In minimizing entropy, the model adapts itself to feedback from its own predictions. This is truly self-supervised self-improvement. Self-supervision of this sort is totally deﬁned by the supervised task, unlike proxy tasks designed to extract more supervision from the data, and yet it remarkably still reduces error. Nevertheless, errors due to corruption and other shifts remain, and therefore more adaptation is needed. Next steps should pursue test-time adaptation on more and harder types of shift, over more general parameters, and by more effective and efﬁcient losses. Shifts Tent reduces error for a variety of shifts including image corruptions, simple changes in appearance for digits, and simulation-to-real discrepancies. These shifts are popular as standardized benchmarks, but other real-world shifts exist. For instance, the CIFAR 10.1 and ImageNetV2 test sets (Recht et al., 2018; 2019), made by reproducing the dataset collection procedures, entail natural but unknown shifts. Although error is higher on both sets, indicating the presence of shift, tent does not improve generalization. Adversarial shifts (Szegedy et al., 2014) also threaten real-world usage, and attackers keep adapting to defenses. While adversarial training (Madry et al., 2018) makes a difference, test-time adaptation could help counter such test-time attacks. Parameters Tent modulates the model by normalization and transformation, but much of the model stays ﬁxed. Test-time adaptation could update more of the model, but the issue is to identify parameters that are both expressive and reliable, and this may interact with the choice of loss. TTT adapts multiple layers of features shared by supervised and self-supervised models and SHOT adapts all but the last layer(s) of the model. These choices depend on the model architecture, the loss, and tuning. For tent modulation is reliable, but the larger shift on VisDA is better addressed by the SHOT parameterization. Jointly adapting the input could be a more general alternative. If a model can adapt itself on target, then perhaps its input gradients might optimize spatial transformations or image translations to reduce shift without source data. Losses Tent minimizes entropy. For more adaptation, is there an effective loss for general but episodic test-time optimization? Entropy is general across tasks but limited in scope. It needs batches for optimization, and cannot update episodically on one point at a time. TTT can do so, but only with the right proxy task. For less computation, is there an efﬁcient loss for more local optimization? Tent and TTT both require full (re-)computation of the model for updates because they depend on its predictions. If the loss were instead deﬁned on the representation, then updates would require less forward and backward computation. Returning to entropy speciﬁcally, this loss may interact with calibration (Guo et al., 2017), as better uncertainty estimation could drive better adaptation. We hope that the fully test-time adaptation setting can promote new methods for equipping a model to adapt itself, just as tent yields a new model with every update. ACKNOWLEDGMENTS We thank Eric Tzeng for discussions on domain adaptation, Bill Freeman for comments on the experiments, Yu Sun for consultations on test-time training, and Kelsey Allen for feedback on the exposition. We thank the anonymous reviewers of ICLR 2021 for their feedback, which certainly improved the latest adaptation of the paper. 9Published as a conference paper at ICLR 2021 REFERENCES Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXiv preprint arXiv:2006.08656, 2020. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In NeurIPS, 2019. Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, and Samuel Rota Bulo. Autodial: Automatic domain alignment layers. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5077–5085. IEEE, 2017. Boris Chidlovskii, Stephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In SIGKDD, pp. 451–460, 2016. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classiﬁcation. In ICLR, 2020. Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014. A Gammerman, V V ovk, and V Vapnik. Learning by transduction. InUAI, 1998. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015. Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. In NeurIPS, 2018. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, 2005. A. Gretton, AJ. Smola, J. Huang, M. Schmittfull, KM. Borgwardt, and B. Schölkopf. Covariate shift and local learning by distribution matching. In Dataset Shift in Machine Learning, pp. 131–160. MIT Press, Cambridge, MA, USA, 2009. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, June 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In ICLR, 2019. Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In ICLR, 2020. Jonathan J. Hull. A database for handwritten text recognition research. TPAMI, 1994. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 10Published as a conference paper at ICLR 2021 Vidit Jain and Erik Learned-Miller. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR, 2011. Thorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML, 1999. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. NeurIPS, 25, 2012. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain adaptation. In CVPR, pp. 4544–4553, 2020. Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In ICML Workshop on challenges in representation learning, 2013. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In CVPR, June 2020. Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In ICLRW, 2017. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In ICML, 2020. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. VisDA: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017. Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. MIT Press, Cambridge, MA, USA, 2009. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Dollár. On network design spaces for visual recognition. In ICCV, 2019. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classiﬁers generalize to ImageNet? In ICML, 2019. Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In ICCV, 2017. Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Samuel Rota Bulo, Nicu Sebe, and Elisa Ricci. Unsuper- vised domain adaptation using feature-whitening and consensus loss. In CVPR, 2019. 11Published as a conference paper at ICLR 2021 Evgenia Rusak, Lukas Schott, Roland S Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. A simple way to make neural networks robust against diverse image corruptions. In ECCV, 2020. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. IJCV, 2015. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European conference on computer vision, pp. 213–226. Springer, 2010. Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised domain adaptation via minimax entropy. In ICCV, 2019. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improv- ing robustness against common corruptions by covariate shift adaptation. arXiv preprint arXiv:2006.16971, 2020. C.E. Shannon. A mathematical theory of communication. Bell system technical journal, 27, 1948. Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional networks for semantic segmentation. PAMI, 2017. Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. In ICLR, 2018. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. In Domain Adaptation in Computer Vision Applications, pp. 153–171. Springer, 2017. Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through self- supervision. arXiv preprint arXiv:1909.11825, 2019a. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A Efros, and Moritz Hardt. Test-time training for out-of-distribution generalization. arXiv preprint arXiv:1909.13231, 2019b. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. 2014. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, 2017. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. PAMI, 2020. Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In NeurIPS, 2014. Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross- channel prediction. In CVPR, 2017. Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Schölkopf. Learning with local and global consistency. NeurIPS, 2004. 12Published as a conference paper at ICLR 2021 APPENDIX This supplement summarizes the image corruptions used in our experiments, highlights a qualitative example of instance-wise adaptation for semantic segmentation, and visualizes feature shifts across more layers. A R OBUSTNESS TO CORRUPTIONS In Section 4.1 we evaluate methods on a common image corruptions benchmark. Table 2 reports errors on the most severe level of corruption, level 5, and Figure 5 reports errors for each corruption type averaged across each of the levels 1–5. We summarize these corruptions types by example in Figure 8. Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure 8: Examples of each corruption type in the image corruptions benchmark. While synthetic, this set of corruptions aims to represent natural factors of variation like noise, blur, weather, and digital imaging effects. This ﬁgure is reproduced from Hendrycks & Dietterich (2019). B S OURCE -FREE ADAPTATION FOR SEMANTIC SEGMENTATION Figure 9 shows a qualitative result on source-free adaptation for semantic segmentation (pixel-wise classiﬁcation) with simulation-to-real (sim-to-real) shift. For this sim-to-real condition, the source data is simulated while the target data is real. Our source data is GTA Richter et al. (2017), a visually-sophisticated video game set in an urban environment, and our target data is Cityscapes Cordts et al. (2016), an urban autonomous driving dataset. The supervised model is HRnet-W18, a fully convolutional network Shelhamer et al. (2017) in the high-resolution network family Wang et al. (2020). For this qualitative example, we run tent on a single image for multiple iterations, because an image is in effect a batch of pixels. This demonstrates adaptation to a target instance, without any further access to the target domain through usage of multiple images from the target distribution. 13Published as a conference paper at ICLR 2021 image label source-only tent, iteration 1 tent, iteration 5 tent, iteration 10 Figure 9: Adaptation for semantic segmentation with simulation-to-real shift from GTA Richter et al. (2017) to Cityscapes Cordts et al. (2016). Tent only uses the target data, and optimizes over a single image as a dataset of pixel-wise predictions. This episodic optimization in effect ﬁts a custom model to each image of the target domain. In only 10 iterations our method suppresses noise (see the completion of the street segment, in purple) and recovers missing classes (see the motorcycle and rider, center). 14Published as a conference paper at ICLR 2021 C F EATURE SHIFTS ACROSS LAYERS AND METHODS (a) Source (b) BN (c) Tent (d) Oracle Layer 2 Layer 5 Layer 8 Layer 11 Layer 14 Layer 18 Layer 20 Layer 23 Layer 26 Figure 10: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts the source features from the reference. BN shifts the features back to be more like the reference. Tent shifts features to be less like the reference, and more like an oracle that optimizes on target labels. 15",
      "meta_data": {
        "arxiv_id": "2006.10726v3",
        "authors": [
          "Dequan Wang",
          "Evan Shelhamer",
          "Shaoteng Liu",
          "Bruno Olshausen",
          "Trevor Darrell"
        ],
        "published_date": "2020-06-18T17:55:28Z",
        "pdf_url": "https://arxiv.org/pdf/2006.10726v3.pdf",
        "github_url": "https://github.com/DequanWang/tent"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces TENT (Test-time ENtropy Minimization), a novel method for fully test-time adaptation where a model adapts to new, shifted data during inference using only the test data and its own parameters, without access to source data or supervision. TENT minimizes the entropy of model predictions to reduce generalization error. It achieves state-of-the-art error rates on ImageNet-C for corruption robustness (44.0% error) and demonstrates effective source-free domain adaptation for digit recognition (SVHN to MNIST/MNIST-M/USPS) and semantic segmentation (GTA to Cityscapes), rivaling methods that use source data and more optimization. The method operates efficiently with one epoch of test-time optimization and does not require altering the original model training.",
        "methodology": "TENT adapts the model by minimizing the Shannon entropy of its predictions on target data during testing. This is achieved by modulating the model's features through two main steps: 1) estimating normalization statistics (mean µ and standard deviation σ) from the target data batch-by-batch during the forward pass, and 2) optimizing channel-wise affine transformation parameters (scale γ and shift β) for each normalization layer by the gradient of the prediction entropy during the backward pass. The original normalization layers of the pre-trained source model are repurposed for this. Only these low-dimensional, linear feature modulation parameters are updated, while the rest of the model parameters are fixed to maintain stability. The method supports both online adaptation (continuous iteration) and offline adaptation (initial update followed by inference).",
        "experimental_setup": "The method was implemented in PyTorch using the pycls library. Experiments were conducted on several benchmarks: Image classification for corruption robustness using ImageNet-C and CIFAR-10/100-C (datasets corrupted with 15 types at 5 severity levels). For domain adaptation, digit recognition was tested from SVHN (source) to MNIST, MNIST-M, and USPS (targets). Semantic segmentation was evaluated on a simulation-to-real shift from GTA (source) to Cityscapes (target). The VisDA-C challenge was used for object recognition from synthesized to real scenes. Models used include Residual Networks (R-26 for CIFAR, R-50 for ImageNet, R-26 for digit adaptation, ResNet-50 for VisDA-C) and HRNet-W18 for semantic segmentation, all equipped with batch normalization. Optimization utilized SGD with momentum for ImageNet (BS=64, LR=0.00025) and Adam for other datasets (BS=128, LR=0.001). Baselines included source-only, adversarial domain adaptation (RG), self-supervised domain adaptation (UDA-SS), test-time training (TTT), test-time normalization (BN), and pseudo-labeling (PL).",
        "limitations": "TENT's effectiveness can be limited on harder domain shifts, such as MNIST-to-SVHN, where its error increased significantly compared to other domain adaptation methods. While efficient, it might not always achieve the highest accuracy compared to source-free adaptation methods that employ more computation and tuning (e.g., DIRT-T, SHOT). The method does not improve generalization on natural, unknown shifts like those found in CIFAR 10.1 and ImageNetV2 datasets. Furthermore, TENT modulates only a subset of the model's parameters (normalization and affine transformations), keeping most parameters fixed, which might limit its adaptation capability. The entropy objective requires batches for optimization, making it unsuitable for episodic updates on single points, unlike some other test-time adaptation methods.",
        "future_research_directions": "Future work should focus on extending TENT to handle more diverse and challenging real-world shifts, including natural but unknown shifts (e.g., ImageNetV2, CIFAR 10.1) and adversarial shifts. There's a need to explore updating a broader range of model parameters beyond just feature modulations, while ensuring expressiveness and reliability; this might involve adapting input representations as a general alternative. Developing more effective and efficient loss functions for general, episodic test-time optimization that can handle single-point updates, or more local optimization on representations, is also a key direction. Finally, investigating the interaction of the entropy loss with model calibration could lead to better uncertainty estimation and adaptation.",
        "experimental_code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom copy import deepcopy\n\n\nclass Tent(nn.Module):\n    \"\"\"Tent adapts a model by entropy minimization during testing.\n\n    Once tented, a model adapts itself by updating on every forward.\n    \"\"\"\n    def __init__(self, model, optimizer, steps=1, episodic=False):\n        super().__init__()\n        self.model = model\n        self.optimizer = optimizer\n        self.steps = steps\n        assert steps > 0, \"tent requires >= 1 step(s) to forward and update\"\n        self.episodic = episodic\n\n        self.model_state, self.optimizer_state = \\\n            copy_model_and_optimizer(self.model, self.optimizer)\n\n    def forward(self, x):\n        if self.episodic:\n            self.reset()\n\n        for _ in range(self.steps):\n            outputs = forward_and_adapt(x, self.model, self.optimizer)\n\n        return outputs\n\n    def reset(self):\n        if self.model_state is None or self.optimizer_state is None:\n            raise Exception(\"cannot reset without saved model/optimizer state\")\n        load_model_and_optimizer(self.model, self.optimizer,\n                                 self.model_state, self.optimizer_state)\n\n\ndef softmax_entropy(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Entropy of softmax distribution from logits.\"\"\"\n    return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n\n\n@torch.enable_grad()  # ensure grads in possible no grad context for testing\ndef forward_and_adapt(x, model, optimizer):\n    \"\"\"Forward and adapt model on batch of data.\n\n    Measure entropy of the model prediction, take gradients, and update params.\n    \"\"\"\n    outputs = model(x)\n    loss = softmax_entropy(outputs).mean(0)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return outputs\n\n\ndef collect_params(model):\n    \"\"\"Collect the affine scale + shift parameters from batch norms.\n\n    Walk the model's modules and collect all batch normalization parameters.\n    Return the parameters and their names.\n\n    Note: other choices of parameterization are possible!\n    \"\"\"\n    params = []\n    names = []\n    for nm, m in model.named_modules():\n        if isinstance(m, nn.BatchNorm2d):\n            for np, p in m.named_parameters():\n                if np in ['weight', 'bias']:  # weight is scale, bias is shift\n                    params.append(p)\n                    names.append(f\"{nm}.{np}\")\n    return params, names\n\n\ndef copy_model_and_optimizer(model, optimizer):\n    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\n    model_state = deepcopy(model.state_dict())\n    optimizer_state = deepcopy(optimizer.state_dict())\n    return model_state, optimizer_state\n\n\ndef load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\n    \"\"\"Restore the model and optimizer states from copies.\"\"\"\n    model.load_state_dict(model_state, strict=True)\n    optimizer.load_state_dict(optimizer_state)\n\n\ndef configure_model(model):\n    \"\"\"Configure model for use with tent.\"\"\"\n    model.train()\n    model.requires_grad_(False)\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.requires_grad_(True)\n            m.track_running_stats = False\n            m.running_mean = None\n            m.running_var = None\n    return model\n\n\ndef check_model(model):\n    \"\"\"Check model for compatability with tent.\"\"\"\n    is_training = model.training\n    assert is_training, \"tent needs train mode: call model.train()\"\n    param_grads = [p.requires_grad for p in model.parameters()]\n    has_any_params = any(param_grads)\n    has_all_params = all(param_grads)\n    assert has_any_params, \"tent needs params to update: \" \\\n                           \"check which require grad\"\n    assert not has_all_params, \"tent should not update all params: \" \\\n                               \"check which require grad\"\n    has_bn = any([isinstance(m, nn.BatchNorm2d) for m in model.modules()])\n    assert has_bn, \"tent needs normalization for its optimization\"\n\n# From cifar10c.py\ndef setup_tent(model):\n    import tent # assuming tent.py functions are imported or available\n    from conf import cfg # assuming cfg is imported or available\n\n    model = tent.configure_model(model)\n    params, param_names = tent.collect_params(model)\n    optimizer = setup_optimizer(params)\n    tent_model = tent.Tent(model, optimizer,\n                           steps=cfg.OPTIM.STEPS,\n                           episodic=cfg.MODEL.EPISODIC)\n    return tent_model\n\n# From cifar10c.py\ndef setup_optimizer(params):\n    from conf import cfg # assuming cfg is imported or available\n\n    if cfg.OPTIM.METHOD == 'Adam':\n        return optim.Adam(params,\n                    lr=cfg.OPTIM.LR,\n                    betas=(cfg.OPTIM.BETA, 0.999),\n                    weight_decay=cfg.OPTIM.WD)\n    elif cfg.OPTIM.METHOD == 'SGD':\n        return optim.SGD(params,\n                   lr=cfg.OPTIM.LR,\n                   momentum=cfg.OPTIM.MOMENTUM,\n                   dampening=cfg.OPTIM.DAMPENING,\n                   weight_decay=cfg.OPTIM.WD,\n                   nesterov=cfg.OPTIM.NESTEROV)\n    else:\n        raise NotImplementedError",
        "experimental_info": "Method: TENT (Test-time Entropy Minimization)\n\nExperimental Settings (default values from conf.py):\n- Model Adaptation: 'tent'\n- Episodic Adaptation: False (updates persist across batches)\n- Optimization Steps per batch: 1\n- Optimizer Method: 'Adam'\n- Learning Rate (LR): 1e-3\n- Adam Betas: (0.9, 0.999)\n- Momentum (for SGD): 0.9\n- Dampening (for SGD): 0.0\n- Nesterov (for SGD): True\n- Weight Decay (L2 regularization): 0.0\n- Test Batch Size: 128\n- Dataset: 'cifar10' (CIFAR-10-C)\n- Corruption Types: ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression']\n- Corruption Severities: [5, 4, 3, 2, 1]\n- Number of examples evaluated per corruption: 10000"
      }
    },
    {
      "title": "Robust Test-Time Adaptation in Dynamic Scenarios",
      "abstract": "Test-time adaptation (TTA) intends to adapt the pretrained model to test\ndistributions with only unlabeled test data streams. Most of the previous TTA\nmethods have achieved great success on simple test data streams such as\nindependently sampled data from single or multiple distributions. However,\nthese attempts may fail in dynamic scenarios of real-world applications like\nautonomous driving, where the environments gradually change and the test data\nis sampled correlatively over time. In this work, we explore such practical\ntest data streams to deploy the model on the fly, namely practical test-time\nadaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA)\nmethod against the complex data stream in PTTA. More specifically, we present a\nrobust batch normalization scheme to estimate the normalization statistics.\nMeanwhile, a memory bank is utilized to sample category-balanced data with\nconsideration of timeliness and uncertainty. Further, to stabilize the training\nprocedure, we develop a time-aware reweighting strategy with a teacher-student\nmodel. Extensive experiments prove that RoTTA enables continual testtime\nadaptation on the correlatively sampled data streams. Our method is easy to\nimplement, making it a good choice for rapid deployment. The code is publicly\navailable at https://github.com/BIT-DA/RoTTA",
      "full_text": "Robust Test-Time Adaptation in Dynamic Scenarios Longhui Yuan Binhui Xie Shuang Li \f School of Computer Science and Technology, Beijing Institute of Technology {longhuiyuan,binhuixie,shuangli}@bit.edu.cn Abstract Test-time adaptation (TTA) intends to adapt the pre- trained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distri- butions. However, these attempts may fail in dynamic sce- narios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we ex- plore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Exten- sive experiments prove that RoTTA enables continual test- time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA 1. Introduction In recent years, many machine learning problems have made considerable headway with the success of deep neu- ral networks [13, 22, 33, 38]. Unfortunately, the perfor- mance of deep models drops significantly when training data and testing data come from different distributions [59], which limits their utility in real-world applications. To re- duce the distribution shift, a handful of works focus on transfer learning field [56], in particular, domain adapta- tion (DA) [17, 42, 45, 48, 69, 72] or domain generalization (DG) [40, 41, 52, 71, 83], in which one or more different but \fCorresponding author Test data stream Continual TTANon-i.i.d.TTAPractical  TTACategoryDistribution Fully TTA Correlation samplingDistributionchanging Figure 1. We consider the practical test-time adaptation (TTA) setup and compare it with related ones. First, Fully TTA [70] adapts models on a fixed test distribution with an independently sampled test stream. Then, on this basis, Continual TTA [73] takes the continually changing distributions into consideration. Next, Non-i.i.d. TTA [19] tries to tackle the correlatively sampled test streams on a single test distribution, where the label distribution among a batch of data deviates from that of the test distribution. To be more practical, Practical TTA strives to connect both worlds: distribution changing and correlation sampling. related labeled datasets (a.k.a. source domain) are collected to help the model generalize well to unlabeled or unseen samples in new datasets (a.k.a. target domain). While both DA and DG have extensively studied the problem of distribution shifts, they typically assume acces- sibility to the raw source data. However, in many practical scenarios like personal consumption records, the raw data should not be publicly available due to data protection reg- ulations. Further, existing methods have to perform heavy backward computation, resulting in unbearable training costs. Test-time adaptation (TTA) [3,11,16,24,26,54,65,81] attempts to address the distribution shift online at test time with only unlabeled test data streams. Unequivocally, TTA has drawn widespread attention in a variety of applications, e.g., 2D/3D visual recognition [2, 29, 49, 65, 82], multi- modality [63, 64] and document understanding [15]. Prior TTA studies [7, 20, 70, 73] mostly concentrate on a simple adaptation scenario, where test samples are inde- pendently sampled from a fixed target domain. To name a few, Sun et al. [65] adapt to online test samples drawn from a constant or smoothly changing distribution with an auxil- iary self-supervised task. Wang et al. [70] adapt to a fixed arXiv:2303.13899v1  [cs.CV]  24 Mar 2023Table 1. Comparison between our proposed practical test-time adaptation (PTTA) and related adaptation settings. Setting Adaptation StageAvailable Data Test Data Stream Train Test Source Target Distribution Sampling Protocol Domain Adaptation ! % ! ! - - Domain Generalization ! % ! % - - Test-Time Training [65] ! ! ! ! stationary independently Fully Test-Time Adaptation [70] % ! % ! stationary independently Continual Test-Time Adaptation [73]% ! % ! continually changing independently Non-i.i.d. Test-Time Adaptation [5, 19]% ! % ! stationary correlatively Practical Test-Time Adaptation (Ours)% ! % ! continually changing correlatively target distribution by performing entropy minimization on- line. However, such an assumption is violated when the test environments change frequently [73]. Later on, Boudiaf et al. [5] and Gonget al. [19] consider the temporal correlation ship within test samples. For example, in autonomous driv- ing, test samples are highly correlated over time as the car will follow more vehicles on the highway or will encounter more pedestrians in the streets. More realistically, the data distribution changes as the surrounding environment alerts in weather, location, or other factors. In a word, distribution change and data correlation occur simultaneously in reality. Confronting continually changing distributions, tradi- tional algorithms like pseudo labeling or entropy minimiza- tion become more unreliable as the error gradients cumu- late. Moreover, the high correlation among test samples re- sults in the erroneous estimation of statistics for batch nor- malization and collapse of the model. Driven by this analy- sis, adapting to such data streams will encounter two major obstacles: 1) incorrect estimation in the batch normaliza- tion statistics leads to erroneous predictions of test samples, consequently resulting in invalid adaptation; 2) the model will easily or quickly overfit to the distribution caused by the correlative sampling. Thus, such dynamic scenarios are pressing for a new TTA paradigm to realize robust adapta- tion. In this work, we launch a more realistic TTA setting, where distribution changing and correlative sampling oc- cur simultaneously at the test phase. We call this Practical Test-Time Adaptation, or briefly,PTTA. To understand more clearly the similarities and differences between PTTA and the previous setups, we visualize them in Figure 1 and sum- marize them in Table 1. To conquer this challenging prob- lem, we propose a Robust Test-Time Adaptation (RoTTA) method, which consists of three parts: 1) robust statistics es- timation, 2) category-balanced sampling considering time- liness and uncertainty and 3) time-aware robust training. More concretely, we first replace the erroneous statistics of the current batch with global ones maintained by the expo- nential moving average. It is a more stable manner to esti- mate the statistics in BatchNorm layers. Then, we simulate a batch of independent-like data in memory with category- balanced sampling while considering the timeliness and un- certainty of the buffered samples. That is, samples that are newer and less uncertain are kept in memory with higher priority. With this batch of category-balanced, timely and confident samples, we can obtain a snapshot of the current distribution. Finally, we introduce a time-aware reweight- ing strategy that considers the timeliness of the samples in the memory bank, with a teacher-student model to perform robust adaptation. With extensive experiments, we demon- strate that RoTTA can robustly adapt in the practical setup, i.e., PTTA. In a nutshell, our contributions can be summarized as: • We propose a new test-time adaptation setup that is more suitable for real-world applications, namely practical test-time adaptation (PTTA). PTTA considers both distribution changing and correlation sampling. • We benchmark the performance of prior methods in PTTA and uncover that they only consider one aspect of the problem, resulting in ineffective adaptation. • We propose a robust test-time adaptation method (RoTTA), which has a more comprehensive considera- tion of PTTA challenges. Ease of implementation and effectiveness make it a practical deployment option. • We extensively demonstrate the practicality of PTTA and the effectiveness of RoTTA on common TTA benchmarks [23], i.e., CIFAR-10-C and CIFAR-100- C and a large-scale DomainNet [58] dataset. RoTTA obtains state-of-the-art results, outperforming the best baseline by a large margin (reducing the averaged classification error by over 5.9%, 5.5% and 2.2% on CIFAR-10-C, CIFAR-100-C and DomainNet, respec- tively). 2. Related Work Domain adaptation (DA) studies the problem of transfer- ring the knowledge learned from a labeled source dataset to an unlabeled target dataset [8, 17, 43, 51, 67, 68]. Represen- tative techniques include latent distribution alignment [48, 77], adversarial training [17, 62], or self-training [75, 85]. The limitation of this setting, however, is that an unlabeled test dataset (target domain) is needed at training time, in addition to a labeled training dataset (source domain). Ac- cordingly, it might fail to handle more practical scenariosFeature 𝐹Robust batch normalization (RBN)Update𝜇௚, 𝜎௚ଶNormalizeFeature𝐹′Update bank with current sample  Training lossℒ௥in Eq. (7) Teacher StudentAdaptation with RBNMemorybankEMA 𝑡A stream of online dataUpdateTest timeCorrelationsamplingStrong & weakaugmentation flowDistributionsCategoryTeacherMajor classhas highest ℋin majorRemoveAddWhen ℋ>ℋSamples to beadded& removed Figure 2. Framework overview. Firstly, we replace the batch normalization layer with RBN which robustly normalizes the feature map. During the inference of the online test stream of PTTA, we utilize the predictions of samples to maintain a memory bank by category- balanced sampling with timeliness and uncertainty. Finally, we use the category-balanced, timely and confident data in the memory bank combined with a robust loss to adapt the model at test time. like test-time adaptation. Our practical test-time adaptation setting can be viewed as performing correlatively sample adaptation on the fly. It is worth noting that standard domain adaptation techniques might collapse when only continual data streams from multiple target domains are accessible. Domain generalization (DG) assumes that multiple source domains are available for model training and tries to learn models that can generalize well to any unseen domains [4, 26,40,41,52,84]. A broad spectrum of methodologies based on data augmentation [78, 84], meta-learning [14, 40], or domain alignment [50,52] has made great progress. In con- trast, this work instead aims to improve the performance of source pre-trained models at the test time by using unla- beled online data streams from multiple continually chang- ing target domains. Continual learning (CL) (also known as incremental learning, life-long learning) addresses the problem of learn- ing a model for many tasks sequentially without forgetting knowledge obtained from the preceding tasks. [1, 6, 31, 37, 60]. CL methods can often be categorized into replay- based [60, 66] and regularization-based [31, 44] methods. Ideas from continual learning are also adopted for continu- ous domain adaptation approaches [34, 74] In our work, we share the same motivation as CL and point out that prac- tical test-time adaptation (PTTA) also suffers catastrophic forgetting (i.e., performance degradation on new test sam- ples due to correlation sampling), which makes test-time adaptation approaches are unstable to deploy. Test-time adaptation (TTA) focus on more challenging settings where only source model and unlabeled target data are available [9, 18, 27, 28, 35, 46, 61]. A similar paradigm is source-free domain adaptation (SFDA) [10, 36, 47, 79], which also requires no access to the training (source) data. To name a few, Liang et al . [45] fit the source hypoth- esis by exploiting the information maximization and self- supervised pseudo-labeling. Kundu et al. [35] formalize a unified solution that explores SFDA without any category- gap knowledge. To fully utilize any arbitrary pre-trained model, Sun et al. [65] propose conducting adaptation on the fly with an auxiliary self-supervised task. Later on, Wanget al. [70] take a source pre-trained model and adapt it to the test data by updating a few trainable parameters in Batch- Norm layers [25] using entropy minimization [21]. While standard TTA has been widely studied in many tasks [2, 20, 63, 64, 70, 82], the fact remains that both dis- tribution changing [73] and data correlation sampling [19] has only been considered in isolation. For example, Gong et al. [19] propose instance-aware batch normalization and prediction-balanced reservoir sampling to address the chal- lenges of correlatively sampled test streams, however, it does not consider unstable adaptation resulting from long- term adaptation on continually changing distributions. On the other hand, Wang et al. [73] assume that the target test data is streamed from a continually changing environment and continually adapt an off-the-shelf source pre-trained model to the current test data. In this work, we launch PTTA, a more practical TTA setting to connect both worlds: distribution changing and correlation sampling. 3. Method 3.1. Problem Definition and Motivation Given a model fθ0 with parameter θ0 pre-trained on source domain DS = {(xS, yS)}, the proposed practical test-time adaptation (PTTA) aims to adapt fθ0 to a stream of online unlabeled samples X0, X1, ...,XT , where Xt is a batch of highly correlated samples from the distribution Ptest that changes with time t continually. More specifi- cally, at test time, with time going on, the test distribution Ptest changes continually as P0, P1, ...,P∞. At time step t, we will receive a batch of unlabeled and correlated samplesmotion distribution changing snow time  Distributions and Labels of PTTA T est Stream uniform 10 1 0.1 0.01 0.001 Dirichlet Parameter  Figure 3. Illustration of the labels and distributions of the test stream of CIFAR10-C under the setup PTTA. And we adopt Dirichlet distribution to simulate the process of correlative sam- pling. It is clear that as the concentration parameter δ decreases, the correlation among sampled data increases, which is reflected in the increasing aggregation of categories. Xt from Ptest. Next, Xt is fed into the model fθt and the model needs to adapt itself to the current test data streams and make predictions fθt (Xt) on the fly. As a matter of fact, this setup is largely driven the prac- tical demands of deploying models in dynamic scenarios. Taking for example the case of autonomous driving men- tioned in § 1, test samples are highly correlated and the data distribution changes continually with the weather or loca- tion. Another example is the situation of intelligent moni- toring, the camera will continuously capture more people at certain times, such as after work, but fewer of them during work time. Meanwhile, the light condition changes con- tinually from day to night. The deployed model should be robustly adapted in such dynamic scenarios. In a word, dis- tribution change and data correlation often happen simul- taneously in the real world. For this reason, existing TTA methods [7,9,19,28,70,73,81] might become unstable when the test stream is sampled from such dynamic scenarios. To obtain the test stream of PTTA, we adopt Dirich- let Distribution with parameter δ to simulate the correla- tion among test samples. We present the test data streams corresponding to different values of δ on the CIFAR10-C dataset in Figure 3. We can observe that the smaller δ is, the higher the correlation will be. For the sake of unity, we set δ = 0.1 as the default for all experiments. In the follow- ing, we present a robust test-time adaptation framework for the practical test-time adaptation setup defined above. An overview of our RoTTA is illustrated in Figure 2. 3.2. Robust Test-Time Adaptation Motivated by the fact that the statistics of current batch data, which are commonly used in previous TTA meth- ods [7, 20, 65, 70, 73], become unreliable when they en- counter correlative test data streams, we first turn to the global robust statistics for normalization. Then, to effec- tively adapt to the current distribution, we maintain a mem- ory bank by category-balanced sampling with considering timeliness and uncertainty, which captures a more stable snapshot of the distribution. Finally, we utilize the teacher- student model and design a timeliness-based reweighting strategy to train the model robustly. Robust batch normalization (RBN). Batch Normaliza- tion (BN) [25] is a widely-used training technique as it can accelerate the training and convergence speed of networks and stabilize the training process by reducing the risk of gradient explosion and vanishing. Given the feature map F ∈ RB×C×H×W as the input for a BN layer when train- ing, the channel-wise mean µ ∈ RC and variance σ2 ∈ RC are calculated as follows: µc = 1 BHW BX b=1 HX h=1 WX w=1 F(b,c,h,w) , (1) σ2 c = 1 BHW BX b=1 HX h=1 WX w=1 (F(b,c,h,w) − µc)2 . (2) Then the feature map is normalized and refined in a channel-wise manner as BN (F(b,c,h,w); µ, σ2) =γc F(b,c,h,w) − µc √σ2c + ϵ + βc , (3) where γ, β∈ RC are learnable parameters in the layer and ϵ > 0 is a constant for numerical stability. Meanwhile, during training, the BN layer maintains a group of global running mean and running variance (µs, σ2 s) for inference. Due to the domain shift at test time, the global statis- tics (µs, σ2 s) normalize test features inaccurately, causing significant performance degradation. To tackle the prob- lem above, some methods [55, 70, 73] use the statistics of the current batch to perform normalization. Unfortunately, when the test samples have a high correlation under PTTA setup, the statistics of the current batch also fail to correctly normalize the feature map, as demonstrated in Figure 4c. Specifically, the performance of BN [53] decreases rapidly as the data correlation increases. Based on the analysis above, we propose a robust batch normalization (RBN) module, which maintains a group of global statistics (µg, σ2 g) to normalize the feature map ro- bustly. Before the whole test-time adaptation, (µg, σ2 g) is initialized as the running mean and variance (µs, σ2 s) of the pre-trained model. When adapting the model, we update the global statistics first by exponential moving average as µg = (1− α)µg + αµ , (4) σ2 g = (1− α)σ2 g + ασ2 , (5) where (µ, σ2) is the statistics of the buffered samples in the memory bank. Then we normalize and affine the feature as Eq. (3) with (µg, σ2 g). When inferring for test samples, we directly utilize (µg, σ2 g) to calculate the output as Eq (3). Al- though simple, RBN is effective enough to tackle the prob- lem of normalization on test streams of PTTA.Category-balanced sampling with timeliness and uncer- tainty (CSTU). In the PTTA setup, the correlation among test samples Xt at time t leads to a deviation between the observed distribution bPtest and the test distribution Ptest. Specifically, the marginal label distribution p(y|t) tends to differ from p(y). Continuously learning with Xt over time t can lead to model adaptation to an unreliable distribution bPtest, resulting in ineffective adaptation and an increased risk of model collapse. To address this issue, we propose a category-balanced memory bank M with a capacity of N, which takes into account the timeliness and uncertainty of samples when up- dating. In particular, we adopt the predictions of test sam- ples as pseudo labels to guide the update ofM. Meanwhile, to guarantee the balance among categories, we distribute the capacity of M equally to each category, and samples of the major categories will be replaced first (refer to lines 5-9 in Algorithm 1). Furthermore, due to the continually changing test distribution, old samples in M are limited in value, and could even impair the ability of the model to adapt to the current distribution. Additionally, samples of high uncer- tainty always produce erroneous gradient information that can hinder model adaptation, as suggested by [55]. With this in mind, we attach each sample in M with a group of heuristics (A, U), where A, initialized as 0 and in- creasing with time t, is the age of the sample, and U the un- certainty calculated as the entropy of the prediction. Next, we combine the timeliness and uncertainty to calculate a heuristic score, i.e., category-balanced sampling with time- liness and uncertainty (CSTU), as follows: H = λt 1 1 + exp(−A/N) + λu U log C , (6) where λt and λu make the trade-off between timeliness and uncertainty, and for simplicity, λt and λu are set to 1.0 for all experiments, andC is the number of categories. We sum- marize our sampling algorithm in Algorithm 1. With CSTU, we can obtain a robust snapshot of the current test distribu- tion Ptest, and effectively adapt the model to it. Robust training with timeliness. Actually, after replacing BN layers with our RBN and obtaining the memory bank selected via CSTU, we can directly adopt the widely used techniques like pseudo labeling or entropy minimization to perform test-time adaptation. However, we notice that too old or unreliable instances still have the opportunity to stay in M since keeping the category balance is assigned the top priority. In addition, too aggressive updates of the model will make the category balance ofM unreliable, resulting in unstable adaptation. Meanwhile, error accumulation caused by the distribution change also makes the aforementioned approaches unworkable. To further reduce the risk of error gradients information from old and unreliable instances and stabilize the adapta- tion, we turn to the robust unsupervised learning method Algorithm 1: CSTU for one test sample. 1 Input: a test sample x and the teacher model fθT . 2 Define: memory bank M and its capacity N, number of classes C, per class occupation O ∈RC, total occupation Ω, classes to pop instance D. 3 Infer as p(y|x) =Softmax(fθT (x)). 4 Calculate the predicted category of x as ˆy = arg maxc p(c|x), the uncertainty as Ux = −PC c=1 p(c|x) log(p(c|x)), the age as Ax = 0, and the heuristic score Hx of x with Eq (6) 5 if Oˆy < N C then 6 if Ω <N: Search range D = ∅. 7 else: Search range D = {j|j = arg maxc Oc} 8 else 9 Search range D = {ˆy} 10 if D is ∅ then 11 Add (x, ˆy, Hx, Ux) into M. 12 else 13 Find the instance (ˆx, yˆx, Aˆx, Uˆx) with the highest value in Eq (6) Hˆx among D. 14 if Hx < Hˆx then 15 Remove (ˆx, yˆx, Aˆx, Uˆx) from M. 16 Add (x, ˆy, Hx, Ux) into M. 17 else 18 Discard x. 19 Increase the age of all instances in M. teacher-student model and propose a timeliness reweight- ing strategy. In addition, for the sake of time efficiency and stability, only affine parameters in RBN are trained during adaptation. At time step t, after inferring for the correlated data Xt with the teacher model fθT t and updating the memory bank M with Xt, we begin updating the student model fθS t and the teacher model fθT t . Firstly, we update parameters of stu- dent model θS t → θS t+1 by minimizing the following loss: Lr = 1 Ω ΩX i=1 L(xM i , Ai; θT t , θS t ) , (7) where Ω = |M| is the total occupation of the memory bank, and xM i and Ai(i = 1, ..., Ω) are instances in the memory bank and their age respectively. Subsequently, the teacher model is updated by exponential moving average as θT t+1 = (1− ν)θT t + νθS t+1 . (8) To calculate the loss value of an instancexM i from the mem- ory bank, the timeliness reweighting term is computed as E(Ai) = exp(−Ai/N) 1 + exp(−Ai/N) , (9)where Ai is the age of xM i , and N is the capacity of the bank. And then we calculate the cross entropy between the soft-max prediction pS(y|x′′ i ) of the strong-augmented view x′′ i from the student model and that pT (y|x′ i) of the weak- augmented view 1 x′ i from the teacher model as follows: ℓ(x′ i, x′′ i ) =−1 C CX c=1 pT (c|x′ i) logpS(c|x′′ i ) . (10) Finally, equipped with Eq. (9) and Eq. (10), the right-hand side of Eq. (7) reduces to L(xM i , Ai; θT t , θS t ) =E(Ai)ℓ(x′ i, x′′ i ) . (11) To sum up, equipped with RBN, CSTU, and robust training with timeliness, our RoTTA is capable of effectively adapt- ing any pre-trained models in dynamic scenarios. 4. Experiments 4.1. Setup Datasets. CIFAR10-C and CIFAR100-C [23] are the com- monly used TTA benchmarks to testify the robustness un- der corruptions. Both of them are obtained by applying 15 kinds of corruption with 5 different degrees of severity on their clean test images of original datasets CIFAR10 and CIFAR100 respectively. CIFAR10/CIFAR100 [32] have 50,000/10,000 training/test images, all of which fall into 10/100 categories. DomainNet [58] is the largest and hard- est dataset to date for domain adaptation and consists of about 0.6 million images with 345 classes. It consists of six different domains including Clipart (clp), Infograph (inf), Painting (pnt), Quickdraw (qdr), Real (rel), and Sketch (skt). We first pre-train a source model on the train set in one of six domains and testify all baseline methods on the test set of the remaining five domains. Implementation details. All experiments are conducted with PyTorch [57] framework. In the case of robustness to corruption, following the previous methods [55, 70, 73], we obtain the pre-trained model from RobustBench bench- mark [12], including the WildResNet-28 [80] for CIFAR10 → CIFAR10-C, and the ResNeXt-29 [76] for CIFAR100 → CIFAR100-C. Then, we change the test corruption at the highest severity 5 one by one to simulate that the test distri- bution continually changes with time in PTTA. And in the case of generalization under the huge domain gap, we train a ResNet-101 [22] by standard classification loss for each domain in DomainNet and adapt them continually to differ- ent domains except the source domain. Meanwhile, we uti- lize the Dirichlet distribution to simulate the correlatively sampled test stream for all datasets. For optimization, we adopt Adam [30] optimizer with learning rate 1.0 × 10−3, 1Weak augmentation is ReSize+CenterCrop. Strong augmentation is a combination nine operations like Clip, ColorJitter, and RandomAffine. β = 0.9. For a fair comparison, we set the batch size for all methods as 64 and the capacity of the memory bank of RoTTA as N = 64. Concerning the hyperparameters, we adopt a unified set of values for RoTTA across all experi- ments including α = 0.05, ν = 0.001, λt = 1.0, λu = 1.0, and δ = 0.1. More details are provided in the appendix. 4.2. Comparisons with the State-of-the-arts Robustness under corruptions. The classification error on CIFAR10→CIFAR10-C and CIFAR100→CIFAR100-C are shown in Table 2 and Table 3 respectively. We change the type of the current corruption at the highest severity 5 as time goes on, and sample data correlatively for infer- ence and adaptation simultaneously. The same test stream is shared across all compared methods. From Table 2 and Table 3, we can see that RoTTA achieves the best performance compared to previous meth- ods. Moreover, RoTTA has a significant performance gain to the second-best method that 5.9% improvement on CIFAR10 →CIFAR10-C and 5.5% improvement on CIFAR100→CIFAR100-C respectively, verifying the effec- tiveness of RoTTA to adapt the model under PTTA. In more detail, we can observe that BN [53], PL [39], TENT [70] and CoTTA [73] negatively adapt the model to the test streams of both datasets compared to Source (−6.5 ∼ −46.4%). This is attributed to the fact that these methods overlook the issues posed by correlation sampling, which can result in highly correlated data within a batch. As a consequence, traditional normalization statistics may be ineffective in appropriately normalizing the feature maps. Equipped with RBN and CSTU, RoTTA no longer suffers from this issue. Meanwhile, in Table 3, if focus on the adaptation procedure, we can see that the performance of PL [39], TENT [70] and NOTE [19] becomes worse and worse, and eventually, the model even collapses (error rate > 97%). This reveals that the impact of error accumula- tion on long-term adaptation can be catastrophic. To tackle this problem, RoTTA turns to robustly adapt the model with timeliness reweighting and confident samples in the mem- ory bank, and superior performance throughout the adapta- tion process demonstrates its effectiveness. In addition, we find that although LAME [5] never tunes the parameters of the model, it is still a competi- tive baseline for example it achieves the second-best result on CIFAR100→CIFAR100-C. However, its performance is very dependent on the performance of the pre-trained model e.g. negligible improvement on difficult corruptions (shot, gaussian, pixelate). On the contrary, our RoTTA is more flexible and achieves better and more robust results. Generalization under domain shift. We also evalu- ate RoTTA under a more challenging dataset DomainNet, where we continually adapt a source pre-trained model to correlatively sampled test streams of the rest domains. AsTable 2. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 3. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 4. Average classification error of DomainNet while continually adapting to different domains with correlatively sampled test stream. Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Sourceclp inf pnt qdr rel sktAvg. BN clp inf pnt qdr rel sktAvg. PL clp inf pnt qdr rel sktAvg.TENTclp inf pnt qdr rel sktAvg. clp N/A 83.9 65.4 88.6 48.0 59.1 69.0clp N/A 88.6 70.7 90.5 65.4 67.0 76.5clp N/A 94.5 98.9 99.5 99.7 99.7 98.5clp N/A 87.5 71.9 94.2 96.2 98.9 89.7inf 61.8 N/A 66.9 96.0 50.0 70.6 69.1inf 68.6 N/A 74.2 96.2 69.9 76.8 77.1inf 82.6 N/A 99.2 99.6 99.7 99.3 96.1inf 68.6 N/A 75.0 97.3 95.9 98.7 87.1pnt 56.5 83.7 N/A 94.2 42.6 63.4 68.1pnt 60.8 87.9 N/A 94.3 62.3 68.7 74.8pnt 78.6 99.4 N/A 99.7 99.6 99.7 95.4pnt 61.7 87.1 N/A 96.4 95.3 98.8 87.8qdr 89.2 99.0 98.6 N/A 95.0 92.3 94.8qdr 80.3 97.7 92.6 N/A 88.7 88.1 89.5qdr 81.7 99.5 99.6 N/A 99.7 99.8 96.1qdr 78.9 97.1 91.6 N/A 89.2 88.7 89.1rel 49.4 80.4 51.5 93.4 N/A 63.3 67.6rel 57.9 87.1 63.1 94.3 N/A 70.8 74.6rel 73.5 99.4 99.2 99.6 N/A 99.7 94.3rel 57.8 86.4 68.1 96.9 N/A 96.7 81.2skt 47.5 88.2 62.9 87.1 51.8 N/A 67.5skt 50.4 87.6 64.6 89.6 63.1 N/A 71.1skt 64.8 99.2 99.4 99.7 99.7 N/A 92.6skt 51.9 87.2 69.1 95.3 97.3 N/A 80.1Avg.60.9 87.0 69.1 91.9 57.5 69.7 72.7Avg.63.6 89.8 73.0 93.0 69.9 74.3 77.3Avg.76.2 98.4 99.3 99.6 99.7 99.6 95.5Avg.63.8 89.0 75.1 96.0 94.8 96.4 85.8 Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →LAMEclp inf pnt qdr rel sktAvg.COTTAclp inf pnt qdr rel sktAvg.NOTEclp inf pnt qdr rel sktAvg.RoTTAclp inf pnt qdr rel sktAvg. clp N/A 82.2 64.5 87.7 46.9 58.9 68.0clp N/A 90.6 77.9 89.3 76.3 72.7 81.4clp N/A 89.2 73.0 94.8 98.4 99.4 91.0clp N/A 85.5 62.0 82.0 49.3 59.8 67.7inf 60.1 N/A 65.7 95.4 48.5 69.4 67.8inf 74.5 N/A 82.0 95.7 80.2 81.5 82.8inf 75.4 N/A 78.7 98.7 98.1 99.5 90.1inf 61.8 N/A 63.7 91.5 52.5 67.6 67.4pnt 55.8 81.5 N/A 93.3 41.3 62.1 66.8pnt 66.3 89.8 N/A 93.4 74.0 75.4 79.8pnt 64.7 89.8 N/A 97.8 98.4 99.2 90.0pnt 53.3 84.1 N/A 89.1 47.3 61.4 67.0qdr 88.3 99.1 99.0 N/A 94.9 92.2 94.7qdr 82.3 98.2 94.6 N/A 92.5 90.1 91.5qdr 74.7 97.2 92.2 N/A 93.5 99.6 91.4qdr 77.5 97.0 89.8 N/A 80.3 82.2 85.3rel 48.0 79.3 50.1 91.6 N/A 60.2 65.8rel 64.0 90.3 73.2 93.5 N/A 77.6 79.7rel 61.3 89.2 68.9 98.8 N/A 99.2 83.5rel 49.1 82.3 50.3 88.0 N/A 61.1 66.2skt 45.6 87.1 59.5 83.9 49.9 N/A 65.2skt 56.1 89.2 71.9 89.2 73.5 N/A 76.0skt 55.2 89.7 70.1 96.9 98.3 N/A 82.0skt 42.6 83.7 54.4 80.9 47.5 N/A 61.8Avg.59.6 85.8 67.8 90.4 56.3 68.6 71.4Avg.68.6 91.6 79.9 92.2 79.3 79.5 81.9Avg.66.3 91.0 76.6 97.4 97.3 99.4 88.0Avg.56.8 86.5 64.0 86.3 55.4 66.469.2(+2.2) shown in Table 4, consistent with the previous analysis, most of the methods include BN [53], PL [39], TENT [70], CoTTA [73] and NOTE [19] even perform worse than the Source model ( −4.6 ∼ −22.8%). RoTTA consistently achieves the best performance and has 2.2% gain than the second method LAME [5], demonstrating RoTTA’s effec- tiveness again. 4.3. Ablation Study Effect of each component. To further investigate the effi- cacy of each component, we replace each part with the nor- mally used solutions to obtain three variants: (1) RoTTA w/o RBN, replace RBN with test-time BN in TENT [70]; (2) RoTTA w/o CSTU, directly adapt the model on test stream; (3) RoTTA w/o robust training (RT), directly adapt the model only with entropy minimization. As shown in Table 5, we can observe that significant performance degra- dation occurs for all variants, proving that every part of our proposed method is valid for PTTA. Take one com- ponent for a detailed example, without RBN robustly nor- malizing feature maps, the performance of RoTTA drops 50.2% and 16.3% on CIFAR10-C and CIFAR100-C respec- tively, proving that RBN is robust enough to tackle the prob- lem of normalization of correlatively sampled data streams. CSTU enables RoTTA to adapt to a more stable distribu- tion by maintaining a timely and confident snapshot of the test distribution. Meanwhile, robust training with timeliness greatly reduces the accumulation of errors. Every compo- nent behaves significantly to enable effective adaptation un- der PTTA. Effect of the distribution changing order. To exclude the effect of a fixed order of distribution changing, we con- ducted experiments on ten different sequences of changes on CIFAR10-C and CIFAR100-C with independently andBN PL TENT LAME CoTTA NOTE RoTTA0 10 20 30 40 50 60 70 80Classification error (%) Source CIFAR-10  CIFAR-10-C Independent Correlative (a) CIFAR10-C. BN PL TENT LAME CoTTA NOTE RoTTA0 20 40 60 80Classification error (%) Source CIFAR-100  CIFAR-100-C Independent Correlative (b) CIFAR100-C. uniform 10 1 0.1 0.01 0.001 30 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (c) δ. 16 32 64 128 256 512 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (d) Batch size. Figure 4. (a) & (b) we adapt the model continually to different corruptions of 10 different orders with independently and correlatively sampled test streams on CIFAR10-C and CFAR100-C respectively and report their average classification error. (c) & (d) we verify the effect of δ and batch size to different methods on CIFAR100-C respectively. Table 5. Classification error of different variants of our RoTTA. Variant CIFAR10-C CIFAR100-C Avg. RoTTA w/o RBN 75.4 51.3 63.4 RoTTA w/o CSTU 47.1 46.3 46.7 RoTTA w/o RT 78.2 95.0 81.6 RoTTA 25.2 35.0 30.1 correlatively sampled test streams respectively. As shown in Figure 4a and 4b, no matter what kind of setup, RoTTA can achieve excellent results. The detailed results on the correlatively sampled test streams are shown in Table 6, RoTTA achieves 4.3% and 4.7% progress on CIFAR10- C and CIFAR100-C respectively. This shows that RoTTA can adapt the model robustly and effectively in long-term scenarios where distribution continually changes and test streams are sampled either independently or correlatively, making it a good choice for model deployment. Effect of Dirichlet concentration parameter δ. We vary the value of δ on CIFAR100-C and compare RoTTA with other approaches in Figure 4c. As the value of δ increases, the performance of BN [53], PL [39], TENT [70] and CoTTA [73] drops quickly, because they never consider the increasing correlation among test samples. NOTE [19] is stable to correlatively sampled test streams but does not consider the distribution changing, causing ineffective adaptation. Meanwhile, the higher correlation between test samples will make the propagation of labels more accurate, which is why the result of LAME [5] slightly improves. Fi- nally, excellent and stable results once again prove the sta- bility and effectiveness of RoTTA. Effect of batch size. In real scenarios, considering deploy- ment environments may use different test batch sizes, we conduct experiments with different values of test batch sizes and results are shown in Figure 4d. For a fair comparison, we control the frequency of updating the model of RoTTA so that the number of samples involved in back-propagation is the same. As the batch size increases, we can see that all of the compared methods have a significant improvement except for lame which has a slight decrease. This is be- cause the number of categories in a batch increases with the Table 6. Average classification error of tasks CIFAR10 → CIFAR10-C and CIFAR100 → CIFAR100-C while continually adapting to different corruptions of 10 different orders at the high- est severity 5 with correlatively sampled test stream. Method CIFAR10-C CIFAR100-C Avg. Source 43.5 46.4 46.9 BN [53] 75.2 52.9 64.1 PL [39] 75.2 52.9 60.1 TENT [70] 82.3 93.2 87.8 LAME [5] 39.5 40.6 40.1 NOTE [19] 30.5 76.1 53.3 CoTTA [73] 83.1 52.8 67.9 RoTTA 26.2(+4.3) 35.9(+4.7) 31.1(+9.0) increasing batch size, causing the overall correlation to be- come lower but the propagation of labels to become more difficult. Most significantly, RoTTA achieves the best re- sults across different batch sizes, demonstrating its robust- ness in dynamic scenarios once again. 5. Conclusion This work proposes a more realistic TTA setting where distribution changing and correlative sampling occur si- multaneously at the test phase, namely Practical Test-Time Adaptation (PTTA). To tackle the problems of PTTA, we propose Robust Test-Time Adaptation (RoTTA) method against the complex data stream. More specifically, a group of robust statistics for the normalization of feature maps is estimated by robust batch normalization. Meanwhile, a memory bank is adopted to capture a snapshot of the test distribution by category-balanced sampling with consider- ing timeliness and uncertainty. Further, we develop a time- aware reweighting strategy with a teacher-student model to stabilize the adaptation process. Extensive experiments and ablation studies are conducted to verify the robustness and effectiveness of the proposed method. We believe this work will pave the way for thinking about adapting models into real-world applications by test-time adaptation algorithm. Acknowledgements. This paper was supported by National Key R&D Program of China (No. 2021YFB3301503), and also supported by the National Natural Science Foundation of China under Grant No. 61902028.References [1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben- gio. Gradient based sample selection for online continual learning. In NeurIPS, pages 11816–11825, 2019. 3 [2] Fatemeh Azimi, Sebastian Palacio, Federico Raue, J ¨orn Hees, Luca Bertinetto, and Andreas Dengel. Self-supervised test-time adaptation on video data. In WACV, pages 2603– 2612, 2022. 1, 3 [3] Mathilde Bateson, Herve Lombaert, and Ismail Ben Ayed. Test-time adaptation with shape moments for image segmen- tation. In MICCAI, pages 736–745, 2022. 1 [4] Gilles Blanchard, Gyemin Lee, and Clayton Scott. General- izing from several related classification tasks to a new unla- beled sample. In NeurIPS, pages 2178–2186, 2011. 3 [5] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In CVPR, pages 8344–8353, 2022. 2, 6, 7, 8, 13, 14, 15, 16, 17 [6] Francisco M Castro, Manuel J Mar ´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incre- mental learning. In ECCV, pages 233–248, 2018. 3 [7] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, pages 295–305, 2022. 1, 4 [8] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object de- tection in the wild. In CVPR, pages 3339–3348, 2018. 2 [9] Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang. Test- time fast adaptation for dynamic scene deblurring via meta- auxiliary learning. In CVPR, pages 9137–9146, 2021. 3, 4 [10] Boris Chidlovskii, St ´ephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In KDD, pages 451–460, 2016. 3 [11] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, pages 440–458, 2022. 1 [12] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Neurips, 2021. 6 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1 [14] Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, and Ling Shao. Learning to learn with variational information bottleneck for domain general- ization. In ECCV, pages 200–216, 2020. 3 [15] Sayna Ebrahimi, Sercan ¨O. Arik, and Tomas Pfister. Test- time adaptation for visual document understanding. CoRR, abs/2206.07240, 2022. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1 [17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17:59:1– 59:35, 2016. 1, 2 [18] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and Dimitris N. Metaxas. Vi- sual prompt tuning for test-time domain adaptation. CoRR, abs/2210.04831, 2022. 3 [19] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware BN and prediction-balanced memory. In NeurIPS, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [20] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and J Zico Kolter. Test time adaptation via conjugate pseudo-labels. In NeurIPS, 2022. 1, 3, 4 [21] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, pages 529– 536, 2004. 3 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 1, 6 [23] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and per- turbations. In ICLR, 2019. 2, 6 [24] Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, Hongfu Liu, and Ye Wang. Extrapolative continuous-time bayesian neural network for fast training-free test-time adap- tation. In NeurIPS, 2022. 1 [25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448–456, 2015. 3, 4 [26] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, pages 2427–2440, 2021. 1, 3 [27] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, pages 577–584, 2011. 3 [28] Minguk Jang and Sae-Young Chung. Test-time adaptation via self-training with nearest neighbor information. CoRR, abs/2207.10792, 2022. 3, 4 [29] Junho Kim, Inwoo Hwang, and Young Min Kim. Ev-tta: Test-time adaptation for event-based object recognition. In CVPR, pages 17724–17733, 2022. 1 [30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [31] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska- Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku- maran, and Raia Hadsell. Overcoming catastrophic forget- ting in neural networks. CoRR, abs/1612.00796, 2016. 3 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In NeurIPS, pages 1097–1105, 2012. 1 [34] Ananya Kumar, Tengyu Ma, and Percy Liang. Understand- ing self-training for gradual domain adaptation. In ICML, pages 5468–5479, 2020. 3 [35] Jogendra Nath Kundu, Naveen Venkat, Rahul M. V ., and R. Venkatesh Babu. Universal source-free domain adapta- tion. In CVPR, pages 4543–4552, 2020. 3 [36] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free do- main adaptation method. In WACV, pages 615–625, 2021. 3 [37] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying for- getting in classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366–3385, 2022. 3 [38] Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nat., 521(7553):436–444, 2015. 1 [39] Dong-Hyun Lee et al. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, page 896, 2013. 6, 7, 8, 12, 14, 15, 16, 17 [40] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, pages 3490–3497, 2018. 1, 3 [41] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In CVPR, pages 5400–5409, 2018. 1, 3 [42] Shuang Li, Binhui Xie, Qiuxia Lin, Chi Harold Liu, Gao Huang, and Guoren Wang. Generalized domain conditioned adaptation network. IEEE Trans. Pattern Anal. Mach. Intell., 44(8):4093–4109, 2022. 1 [43] Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Yulin Wang, and Wei Li. Transferable semantic augmen- tation for domain adaptation. In CVPR, pages 11516–11525, 2021. 2 [44] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2935–2947, 2018. 3 [45] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for un- supervised domain adaptation. In ICML, pages 6028–6039, 2020. 1, 3 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: when does self-supervised test-time training fail or thrive? In NeurIPS, pages 21808–21820, 2021. 3 [47] Yuang Liu, Wei Zhang, and Jun Wang. Source-free do- main adaptation for semantic segmentation. In CVPR, pages 1215–1224, 2021. 3 [48] Mingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Transferable representation learning with deep adaptation networks. IEEE Trans. Pattern Anal. Mach. Intell., 41(12):3071–3085, 2019. 1, 2 [49] Wenao Ma, Cheng Chen, Shuang Zheng, Jing Qin, Huimao Zhang, and Qi Dou. Test-time adaptation with calibration of medical image classification nets for label distribution shift. In MICCAI, pages 313–323, 2022. 1 [50] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In ICML, pages 7313– 7324, 2021. 3 [51] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009. 2 [52] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant fea- ture representation. In ICML, pages 10–18, 2013. 1, 3 [53] Zachary Nado, Shreyas Padhy, D. Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. CoRR, abs/2006.10963, 2020. 4, 6, 7, 8, 12, 14, 15, 16, 17 [54] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, pages 16888–16905, 2022. 1 [55] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, volume 162, pages 16888–16905, 2022. 4, 5, 6 [56] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22(10):1345–1359, 2010. 1 [57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019. 6 [58] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, pages 1406–1415, 2019. 2, 6 [59] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in ma- chine learning. 2008. 1 [60] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classi- fier and representation learning. InCVPR, pages 5533–5542, 2017. 3 [61] Amelie Royer and Christoph H Lampert. Classifier adapta- tion at prediction time. In CVPR, pages 1401–1409, 2015. 3 [62] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In CVPR, pages 3723–3732, 2018. 2 [63] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. MM-TTA: multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, pages 16907–16916, 2022. 1, 3[64] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test- time prompt tuning for zero-shot generalization in vision- language models. In NeurIPS, 2022. 1, 3 [65] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, pages 9229–9248, 2020. 1, 2, 3, 4 [66] Rishabh Tiwari, KrishnaTeja Killamsetty, Rishabh K. Iyer, and Pradeep Shenoy. GCR: gradient coreset based replay buffer selection for continual learning. In CVPR, pages 99– 108, 2022. 3 [67] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In CVPR, pages 7472–7481, 2018. 2 [68] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, pages 4068–4076, 2015. 2 [69] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, pages 2962–2971, 2017. 1 [70] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 6, 7, 8, 12, 13, 14, 15, 16, 17 [71] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Trans. Knowl. Data Eng., 2022. 1 [72] Mei Wang and Weihong Deng. Deep visual domain adapta- tion: A survey. Neurocomputing, 312:135–153, 2018. 1 [73] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, pages 7191– 7201, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [74] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre- mental adversarial domain adaptation for continually chang- ing environments. In ICRA, pages 4489–4495, 2018. 3 [75] Binhui Xie, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, and Guoren Wang. Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell., pages 1–17, 2023. 2 [76] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, pages 5987–5995, 2017. 6 [77] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In ICCV, pages 1426– 1435, 2019. 2 [78] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 3 [79] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adapta- tion. In ICCV, pages 8978–8987, 2021. 3 [80] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 6 [81] Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmenta- tion. In NeurIPS, 2022. 1, 4 [82] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, pages 2633–2642, 2022. 1, 3 [83] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 2022. 1 [84] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 [85] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic seg- mentation via class-balanced self-training. In ECCV, pages 289–305, 2018. 26. Appendix 6.1. Discussion Societal impact. RoTTA enables adapting pre-trained models on continually changing distributions with correl- atively sampled test streams without any more raw data or label requirements. Thus, our work may have a positive im- pact on communities to effectively deploy and adapt models in various real-world scenarios, which is economically and environmentally friendly. And since no training data is re- quired, this protects data privacy and has potential commer- cial value. We carry out experiments on benchmark datasets and do not notice any societal issues. It does not involve sensitive attributes. Future work. Our work suggests a few promising direc- tions for future work. Firstly, the proposed RoTTA is a preliminary attempt to perform test-time adaptation for the more realistic test stream under the setup PTTA. One could experiment to improve the algorithm by replacing some parts of RoTTA. More importantly, we hope that with this work, we can open a path to the original goal of test-time adaptation, which is performing test-time adaptation in real- world scenarios. Thus, one could improve PTTA to make it more realistic. Limitations. RoTTA achieves excellent performance on various tasks under the setup PTTA as demonstrated in Sec- tion 4 in the main paper, but we still find some limitations of it. Firstly, the adopted robust batch normalization (RBN) is a naive solution to the normalization of the correlatively sampled batch of data. This requires careful design of the value of α in RBN. Secondly, we observe that during the adaptation procedure of some methods like PL [39] and TENT [70], the model collapse finally. Although we de- sign many strategies to stabilize the adaptation and model collapse never occurs in the experiments of RoTTA, we are still missing a way to recover the model from the collapse state as a remedy. Thirdly, category similarity is only one kind of correlation. Although we conduct experiments on different datasets with Dirichlet distribution to simulate cor- relatively sampled test streams, we still need to validate our approach in some real-world scenarios. 6.2. Sensitivity to different hyper-parameters In this section, we conduct a detailed sensitivity analy- sis of the hyperparameters involved in RoTTA. All experi- ments are conducted on CIFAR100→CIFAR100-C, and the corruptions changes as motion, snow, fog, shot, defocus, contrast, zoom, brightness, frost, elastic, glass, gaussian, pixelate, jpeg, and impulse, and test streams are sampled correlatively with the Dirichlet parameter δ = 0.1. When we investigate the sensitivity to a specific hyperparameter, other hyperparameters are fixed to the default values, i.e., λt = 1.0, λu = 1.0, α = 0.05, and ν = 0.001, for all experiments. Table 7. Classification error with different value of λt/λu. λt/λu 0.0/2.0 0.5/1.5 1.0/1.0 1.5/ 0.5 2.0/ 0.0 CIFAR100-C 57.5 36.9 35.0 35.9 38.9 Trade-off between timeliness and uncertainty. When updating the memory bank, we take the timeliness and uncertainty of samples into account simultaneously, and λt and λu will make a trade-off between them. In Table 7, we show the results of RoTTA with varying λt/λu, i.e., λt/λu ∈ {0.0/2.0, 0.5/1.5, 1.0/1.0, 1.5/0.5, 2.0/0.0}. When we consider both of them, the results are relatively stable (35.0-36.9%). When we only think about one side, the performance drops significantly. For example, when we set λt/λu = 0.0/2.0 which means only considering uncer- tainty, the performance drops 22.5%. That’s because some confident samples get stuck in the memory bank, making it not work the way we design it. Table 8. Classification error with varying α α 0.5 0.1 0.05 0.01 0.005 0.001 CIFAR100-C 39.0 36.0 35.0 36.0 38.1 41.5 Sensitivity to α. We show the results of RoTTA with vary- ing α, i.e., α ∈ {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} in Ta- ble 8. A larger value of α means updating the global statis- tics faster and vice versa. We can see that RoTTA achieves competitive results (35.0 − 36.0%) at appropriate values of α, i.e., α ∈ {0.1, 0.05, 0.01}. Updating too aggressively or too gently can lead to unreliable estimates of statistics. Table 9. Classification error with varying ν ν 0.05 0.01 0.005 0.001 0.0005 0.0001 CIFAR100-C 44.8 39.1 37.1 35.0 37.6 43.6 Sensitivity to ν. We show the results of RoTTA with vary- ing ν, i.e., ν ∈ {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001} in Table 9. As we can see, the best performance is achieved at ν = 0.001. Updating the teacher model too quickly or too slowly can cause performance degradation. 6.3. Additional experiment details and results 6.3.1 Compared methods BN [53] utilizes statistics of the current batch of data to nor- malize their feature maps without tuning any parameters. PL [39] is based on BN [53], and adopts pseudo labels to train the affine parameters in BN layers.TENT [70] is the first to propose fully test-time adaptation. It adopts test-time batch normalization and utilizes entropy minimization to train the affine parameters of BN layers. We reimplement it following the released code https:// github.com/DequanWang/tent. LAME [5] adapts the output of the pre-trained model by optimizing a group of latent variables without tuning any in- ner parts of the model. We reimplement it following the re- leased code https://github.com/fiveai/LAME. CoTTA [73] considers performing test-time adapta- tion on continually changing distributions and pro- pose augmentation-averaged pseudo-labels and stochastic restoration to address error accumulation and catastrophic forgetting. We reimplement it following the released code https://github.com/qinenergy/cotta. NOTE [19] proposes instance-aware normalization and prediction-balanced reservoir sampling to stable the adapta- tion on temporally correlated test streams. We reimplement it following the released code https://github.com/ TaesikGong/NOTE. 6.3.2 Simulate correlatively sampling As we described in the scenarios of autonomous driving that the car will follow more vehicles on the highway or will en- counter more pedestrians on the sidewalk, so we use the same category to simulate correlation. From a macro point of view, the test distribution Ptest changes continually as P0, P1, ...,P∞. During the period when Ptest = Pt, we adopt Dirichlet distribution to simulate correlatively sam- pled test stream. More specifically, we consider dividing samples of C classes into T slots. Firstly, we utilize Dirich- let distribution with parameter γ to generate the partition criterion q ∈ RC×T . Then for each class c, we split samples into T parts according to qc and assign each part to each slot respectively. Finally, we concatenate all slots to sim- ulate the correlatively sampled test stream for Ptest = Pt. And as Ptest changes, we use the above method again to generate the test stream. 6.3.3 Detailed results of different orders We report the average classification error of ten different distribution changing orders in Table 6 of the main pa- per. And then we present the specific results here, includ- ing Table 10, 11, 12, 13, 14, 15, 16, 17, 18, and 19 for CIFAR10→CIFAR10-C and Table 20, 21, 22, 23, 24, 25, 26, 27, 28, and 29 for CIFAR100 →CIFAR100-C. We can see consistently superior performance of RoTTA. One thing to mention is that on DomainNet we use alphabetical order to determine the order of domain changes.Table 10. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 9.3 58.5 72.3 34.8 42.0 54.3 72.9 30.3 46.9 26.6 65.7 41.3 25.1 26.0 46.7 43.5BN [53] 71.1 75.2 76.8 74.2 73.7 80.1 79.3 77.5 73.8 77.7 77.2 73.3 73.8 72.7 71.7 75.2PL [39] 71.7 75.9 80.2 78.4 80.2 85.2 85.3 85.4 85.1 86.7 87.9 87.9 88.1 88.3 87.9 83.6TENT [70] 71.6 75.9 81.3 80.5 82.3 85.6 87.1 87.0 87.1 88.1 88.2 87.8 87.9 88.3 88.2 84.4LAME [5] 5.4 56.8 73.1 29.1 37.0 50.5 71.4 22.3 42.8 18.6 65.5 37.3 18.8 20.4 43.6 39.5CoTTA [73] 75.0 79.8 83.1 83.4 83.2 84.0 84.5 83.2 83.5 83.3 83.6 83.0 83.0 83.4 83.7 82.6NOTE [19] 10.1 29.9 47.1 23.4 28.4 48.4 46.1 41.8 26.9 36.1 37.5 25.0 25.0 23.2 14.2 30.9 RoTTA 10.4 26.6 37.5 23.9 17.0 40.9 39.7 30.1 18.0 29.9 30.1 23.6 21.7 17.6 19.0 25.7(+5.2) Table 11. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 30.3 65.7 42.0 41.3 46.7 26.0 46.9 26.6 72.3 9.3 54.3 72.9 58.5 25.1 34.8 43.5BN [53] 77.6 75.8 73.4 74.1 73.1 72.5 72.9 77.1 77.2 72.2 79.9 79.9 75.5 74.6 72.9 75.2PL [39] 77.6 77.1 76.6 78.3 77.5 79.8 82.0 84.8 86.1 83.5 87.8 87.1 86.5 85.6 85.7 82.4TENT [70] 78.5 78.2 79.2 81.8 84.8 84.8 86.4 87.3 87.9 86.7 87.3 87.8 87.2 87.5 87.1 84.8LAME [5] 22.5 65.2 37.0 37.1 44.0 20.3 41.7 18.7 72.8 5.2 51.2 71.5 57.0 19.0 29.4 39.5CoTTA [73]78.5 81.0 82.8 84.1 84.9 83.4 83.5 83.5 84.5 83.3 84.7 84.6 83.0 84.4 83.4 83.3NOTE [19]35.4 36.1 22.1 21.3 11.6 24.8 24.5 36.0 37.7 18.4 49.0 47.4 43.9 30.4 29.2 31.2 RoTTA 33.2 33.3 19.8 24.1 24.9 20.5 16.2 31.7 28.4 11.8 43.1 36.9 32.5 20.7 20.6 26.5(+4.7) Table 12. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 46.7 46.9 72.3 65.7 25.1 41.3 54.3 42.0 26.6 30.3 58.5 9.3 72.9 34.8 26.0 43.5BN [53] 72.3 72.6 76.9 77.1 74.8 73.5 80.0 73.2 77.4 78.6 76.4 71.0 79.1 73.9 71.5 75.2PL [39] 72.4 75.3 80.7 82.6 83.3 83.5 86.6 85.7 86.6 88.4 87.5 86.6 88.3 88.2 86.8 84.1TENT [70] 73.5 77.9 85.5 86.9 87.6 87.8 88.3 87.7 88.6 89.2 88.5 88.5 89.3 88.6 88.6 86.4LAME [5] 43.5 42.3 73.1 65.3 19.2 37.3 51.1 36.8 18.5 22.5 56.9 5.5 71.1 29.1 20.5 39.5CoTTA [73]79.4 80.3 83.8 83.9 83.9 83.4 85.0 83.2 85.1 84.3 83.9 83.3 84.7 83.9 82.5 83.4NOTE [19] 9.6 21.8 40.1 31.0 25.5 22.6 44.8 22.8 33.2 39.4 33.2 18.1 50.0 28.3 29.8 30.0 RoTTA 18.4 17.9 38.4 31.9 23.3 19.8 40.7 17.4 31.4 29.8 27.8 11.3 43.8 19.7 18.8 26.0(+4.0) Table 13. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 65.7 26.0 54.3 58.5 25.1 26.6 9.3 72.9 46.9 41.3 46.7 72.3 34.8 30.3 42.0 43.5BN [53] 76.4 72.0 80.4 76.2 74.8 77.0 71.1 79.6 73.8 74.4 73.0 77.0 72.5 78.3 72.5 75.3PL [39] 77.0 73.3 82.4 79.8 81.0 82.3 79.5 84.4 82.7 83.5 83.5 85.5 84.8 87.0 84.5 82.1TENT [70]76.9 74.6 82.3 81.7 82.0 84.9 84.8 87.3 86.6 87.3 87.6 89.2 88.3 88.9 87.3 84.6LAME [5] 65.3 20.6 50.9 56.7 19.2 18.8 5.4 71.8 42.8 37.2 43.3 73.2 29.4 22.6 36.9 39.6CoTTA [73]77.4 77.6 83.8 81.9 82.2 82.6 80.4 83.3 82.3 81.5 82.7 82.6 81.1 82.9 81.0 81.6NOTE [19]34.0 20.9 43.1 36.6 24.0 36.4 12.1 48.0 25.9 23.9 13.4 38.1 25.0 43.2 24.2 29.9 RoTTA 35.0 21.1 43.9 29.2 22.1 29.7 10.8 44.6 25.3 22.7 24.6 29.4 26.9 34.4 16.1 27.7(+2.2) Table 14. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 58.5 54.3 42.0 25.1 26.0 72.9 9.3 34.8 41.3 30.3 72.3 65.7 46.7 46.9 26.6 43.5BN [53] 76.0 79.6 73.3 75.2 72.9 79.8 71.1 73.5 74.1 78.6 77.4 76.1 72.0 73.8 76.4 75.3PL [39] 76.7 81.3 77.4 80.3 81.2 86.3 83.3 85.9 86.2 87.7 88.1 88.4 87.4 87.6 87.7 84.4TENT [70] 76.4 80.2 77.8 81.2 83.0 87.1 85.6 87.2 87.6 88.7 88.6 88.9 88.5 88.6 88.2 85.2LAME [5] 56.9 50.7 37.0 19.0 20.3 71.5 5.4 29.2 37.2 22.5 73.0 65.3 43.8 42.4 18.7 39.5CoTTA [73]77.1 83.6 84.1 84.8 84.4 85.2 84.0 84.3 84.9 84.9 85.0 84.7 85.3 84.4 84.3 84.1NOTE [19] 27.8 52.2 24.5 22.3 21.6 44.5 14.5 21.3 25.9 42.5 38.8 36.0 16.7 28.1 40.6 30.5 RoTTA 25.9 43.3 17.7 22.1 20.2 41.5 12.2 22.9 22.5 31.2 33.8 26.0 31.4 17.7 27.6 26.4(+4.1)Table 15. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 16. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 41.3 72.9 30.3 46.7 42.0 54.3 58.5 25.1 46.9 34.8 9.3 26.6 65.7 26.0 72.3 43.5BN [53] 73.8 79.1 77.9 73.0 73.7 80.1 75.7 74.4 73.7 74.0 71.7 77.0 75.9 72.8 76.2 75.3PL [39] 74.2 80.9 80.4 79.5 81.8 85.9 83.9 85.1 84.7 85.9 85.9 86.7 87.2 87.0 87.8 83.8TENT [70]73.9 80.3 81.8 81.6 83.6 86.3 85.6 85.7 86.4 87.7 87.4 88.8 88.8 88.5 88.4 85.0LAME [5] 37.4 71.8 22.4 43.5 37.0 50.5 57.0 19.0 42.8 29.1 5.4 18.7 65.2 20.4 72.9 39.5CoTTA [73]76.5 82.2 82.8 85.0 82.9 85.0 83.0 82.9 83.5 83.4 82.6 83.7 83.2 83.3 83.6 82.9NOTE [19]21.1 41.4 36.3 10.2 21.7 46.7 37.5 26.4 26.1 21.4 14.3 37.9 38.5 24.4 40.7 29.6 RoTTA 22.2 44.9 35.2 18.8 19.7 41.5 28.5 23.2 21.2 18.6 12.4 30.0 27.4 20.0 31.2 26.3(+3.3) Table 17. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 46.9 34.8 42.0 65.7 72.3 54.3 30.3 26.0 46.7 58.5 41.3 25.1 9.3 26.6 72.9 43.5BN [53] 72.8 72.7 73.3 77.2 77.3 80.0 77.6 72.6 73.3 76.6 73.8 74.1 70.3 77.5 79.0 75.2PL [39] 73.2 74.6 76.5 81.7 82.8 84.6 85.1 84.6 86.2 86.4 86.1 87.1 86.8 88.4 88.1 83.5TENT [70] 73.7 74.3 77.1 82.5 84.3 86.9 87.4 86.6 88.0 88.5 88.1 88.5 88.4 89.4 88.9 84.8LAME [5] 42.5 29.3 37.0 65.3 73.2 50.5 22.5 20.5 43.5 56.9 37.1 18.9 5.4 18.5 71.3 39.5CoTTA [73]76.3 79.8 82.4 83.3 83.8 84.5 83.1 82.7 84.7 82.9 83.0 83.3 81.4 83.8 83.8 82.6NOTE [19] 18.5 18.8 23.6 36.5 33.7 47.8 38.6 22.8 13.0 40.0 29.2 26.3 17.5 44.0 52.9 30.9 RoTTA 17.0 17.5 16.5 33.8 33.3 42.7 29.4 18.0 19.6 29.5 20.7 22.1 11.5 29.5 38.1 25.3(+5.6) Table 18. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.3 42.0 72.9 26.0 25.1 30.3 72.3 41.3 65.7 9.3 46.7 34.8 58.5 46.9 26.6 43.5BN [53] 79.7 72.3 79.8 73.2 74.7 77.7 76.6 73.2 77.1 72.2 73.0 73.3 75.5 73.8 76.4 75.2PL [39] 79.6 73.2 81.3 77.3 79.1 83.0 83.2 83.0 85.5 84.3 87.0 86.9 86.4 86.5 87.6 82.9TENT [70] 79.5 74.1 84.2 82.2 84.5 86.5 86.7 85.9 87.2 86.6 86.8 87.3 86.9 87.4 87.3 84.9LAME [5] 50.8 36.9 71.3 20.6 19.2 22.4 72.5 37.2 65.4 5.2 43.3 29.1 57.0 42.4 18.7 39.5CoTTA [73]81.5 79.4 85.2 84.1 84.5 84.2 84.8 84.0 84.8 83.2 85.2 83.8 83.2 84.6 83.6 83.7NOTE [19]45.0 21.2 42.3 21.0 21.6 38.4 36.4 21.4 33.1 16.7 14.6 25.4 43.5 29.1 38.5 29.9 RoTTA 42.6 17.6 48.1 23.9 21.9 32.6 32.1 20.7 30.2 12.0 21.9 20.0 33.7 16.4 28.1 26.8(+3.1) Table 19. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 46.7 72.3 46.9 42.0 41.3 54.3 30.3 26.0 58.5 26.6 65.7 72.9 25.1 34.8 9.3 43.5BN [53] 72.4 76.2 73.2 73.7 73.6 80.0 77.6 72.6 76.4 77.7 77.2 79.9 73.8 73.9 70.0 75.2PL [39] 73.0 78.2 76.7 79.7 81.6 85.6 86.0 85.3 87.2 88.2 88.3 88.9 88.5 89.2 88.2 84.3TENT [70] 73.6 80.9 83.1 85.6 87.1 88.5 88.8 88.4 89.2 89.3 89.0 89.0 89.3 89.9 89.1 86.7LAME [5] 43.5 73.2 42.3 37.0 37.2 50.5 22.5 20.5 57.0 18.6 65.5 71.5 18.8 29.1 5.6 39.5CoTTA [73]79.5 81.4 83.4 83.6 83.9 85.0 84.0 82.8 84.8 84.8 84.5 84.7 84.1 84.4 82.8 83.6NOTE [19] 9.6 43.6 26.5 24.8 23.9 46.9 38.0 23.4 34.0 41.2 41.5 45.0 27.6 25.8 19.0 31.4 RoTTA 18.4 36.0 21.1 15.6 23.0 41.7 30.8 19.1 34.1 31.1 31.3 39.9 26.0 18.8 12.8 26.6(+4.8)Table 20. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 29.5 74.7 73.0 30.8 28.8 54.1 39.4 41.2 29.3 37.2 68.0 45.8 39.5 50.3 55.1 46.4BN [53] 46.5 52.0 58.6 47.4 47.4 57.6 58.2 56.9 47.0 53.4 56.0 52.5 53.1 57.7 49.1 52.9PL [39] 48.5 60.7 77.1 85.9 91.5 95.5 95.8 96.6 96.8 96.9 97.3 97.5 97.6 97.7 97.9 88.9TENT [70] 49.8 69.4 92.2 96.0 96.7 97.3 97.5 97.9 97.5 97.9 98.0 98.2 98.2 98.2 98.2 92.2LAME [5] 21.7 75.1 72.7 22.9 20.6 49.0 32.1 33.3 21.2 28.0 66.8 40.0 30.6 43.9 51.3 40.6CoTTA [73] 46.8 48.4 54.7 48.7 48.6 53.5 55.4 52.8 49.8 51.8 53.5 52.9 54.1 56.7 53.6 52.1NOTE [19] 42.6 53.0 69.9 52.1 53.3 70.4 73.1 76.7 80.8 96.0 97.7 97.1 96.6 97.2 95.8 76.8 RoTTA 28.4 37.3 44.6 31.9 28.3 41.8 43.6 39.9 28.0 35.2 38.2 33.7 33.0 39.5 31.0 35.6(+5.0) Table 21. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 41.2 68.0 28.8 45.8 55.1 50.3 29.3 37.2 73.0 29.5 54.1 39.4 74.7 39.5 30.8 46.4BN [53] 58.3 56.8 47.8 51.8 48.9 57.3 46.8 53.5 57.8 45.5 57.1 58.5 51.7 53.3 48.8 52.9PL [39] 59.4 66.3 74.9 87.5 94.2 95.5 96.2 97.1 97.4 97.2 97.5 97.7 98.0 98.2 98.2 90.4TENT [70] 62.0 79.3 91.7 95.8 96.9 97.0 97.4 97.7 97.6 97.7 97.9 97.9 98.0 97.9 97.9 93.5LAME [5] 33.6 66.7 21.1 39.9 50.6 43.9 21.0 28.6 72.5 21.6 48.6 32.5 74.5 30.6 22.5 40.6CoTTA [73]54.6 54.1 49.6 52.1 52.7 58.0 50.3 53.3 55.0 49.1 55.4 55.7 51.0 54.6 52.1 53.2NOTE [19]60.4 63.0 49.9 55.7 47.0 65.2 59.4 76.6 90.9 87.2 96.8 97.0 97.3 96.7 96.8 76.0 RoTTA 43.9 45.3 31.0 37.3 35.7 41.2 27.7 34.8 39.7 26.6 39.5 41.9 32.0 33.0 30.5 36.0(+4.6) Table 22. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 55.1 29.3 73.0 68.0 39.5 45.8 54.1 28.8 37.2 41.2 74.7 29.5 39.4 30.8 50.3 46.4BN [53] 49.4 47.2 58.6 56.2 52.7 52.0 57.9 46.1 54.4 57.7 50.5 46.2 58.2 47.6 58.5 52.9PL [39] 54.8 64.2 83.3 92.4 95.5 96.5 96.9 96.4 97.2 97.4 97.8 97.8 97.9 97.7 98.0 90.9TENT [70] 60.2 83.1 95.2 96.5 96.9 97.3 97.0 97.3 97.8 97.8 97.6 97.9 97.8 97.9 98.1 93.9LAME [5] 51.3 21.3 72.7 66.3 30.2 40.0 48.6 20.9 27.7 33.3 75.0 21.5 32.2 22.5 43.8 40.5CoTTA [73]52.1 48.6 55.1 52.7 53.4 51.9 55.9 49.2 53.2 52.8 49.2 49.7 56.2 50.7 58.1 52.6NOTE [19] 39.5 45.9 68.8 61.8 57.4 58.5 71.4 66.5 80.8 90.9 94.2 94.9 97.0 95.5 96.6 74.6 RoTTA 41.7 30.5 44.9 40.5 35.4 34.1 40.5 28.2 34.5 39.5 31.1 26.7 43.3 31.4 38.8 36.1(+4.4) Table 23. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 68.0 50.3 54.1 74.7 39.5 37.2 29.5 39.4 29.3 45.8 55.1 73.0 30.8 41.2 28.8 46.4BN [53] 57.5 58.6 58.5 50.5 52.7 53.1 45.9 57.9 47.0 51.5 47.8 58.2 48.2 57.1 47.7 52.8PL [39] 59.5 72.9 85.1 89.6 94.5 96.8 97.1 97.9 97.8 98.0 98.3 98.2 98.0 98.0 98.2 92.0TENT [70]60.3 81.4 95.0 96.6 97.0 97.3 97.3 97.7 97.7 97.7 97.8 97.7 97.6 97.6 97.9 93.8LAME [5] 66.4 43.2 49.0 75.2 30.2 28.5 21.6 32.5 21.2 39.5 52.0 72.8 22.3 33.1 20.5 40.5CoTTA [73]54.5 58.4 55.6 50.0 53.9 53.4 50.3 56.7 51.3 53.2 53.7 56.1 52.0 54.5 51.5 53.7NOTE [19]61.8 60.2 63.4 55.6 59.8 65.9 58.6 75.1 77.8 93.8 94.2 97.0 95.0 95.5 94.4 76.5 RoTTA 45.5 44.5 43.5 35.6 35.1 35.7 26.2 44.0 29.7 34.2 32.0 40.7 31.4 39.4 27.7 36.3(+4.2) Table 24. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 74.7 54.1 28.8 39.5 50.3 39.4 29.5 30.8 45.8 41.2 73.0 68.0 55.1 29.3 37.2 46.4BN [53] 51.7 58.6 47.8 52.9 57.1 58.2 45.9 47.6 52.9 57.8 57.5 56.7 49.5 46.1 54.0 52.9PL [39] 52.4 68.0 73.4 87.9 93.7 96.1 95.7 96.0 96.5 96.7 97.5 97.7 97.7 97.3 97.7 89.6TENT [70] 53.5 77.8 91.1 96.0 97.0 97.6 97.4 97.6 97.9 98.1 98.1 98.0 98.1 97.9 98.1 92.9LAME [5] 74.8 48.2 21.1 30.6 43.4 32.5 21.6 23.0 39.6 33.3 72.7 66.5 51.5 20.7 27.5 40.5CoTTA [73]49.3 55.1 49.1 52.9 56.8 55.7 49.5 50.0 53.6 53.4 54.9 53.9 53.8 50.1 53.5 52.8NOTE [19] 52.2 64.9 47.5 57.0 61.9 67.3 60.4 67.8 77.4 90.6 97.1 96.8 92.8 95.9 96.6 75.1 RoTTA 36.4 44.4 29.7 36.5 41.0 44.1 26.8 29.5 33.0 40.3 40.3 38.2 33.9 28.5 34.9 35.8(+4.7)Table 25. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 26. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 45.8 39.4 41.2 55.1 28.8 54.1 74.7 39.5 29.3 30.8 29.5 37.2 68.0 50.3 73.0 46.4BN [53] 52.9 58.8 57.6 48.2 47.4 57.6 50.9 52.4 47.0 47.2 45.1 54.0 56.4 57.7 58.2 52.8PL [39] 56.9 73.3 86.7 94.4 95.8 97.3 97.2 97.4 97.6 97.4 97.7 97.6 97.8 98.3 98.1 92.2TENT [70]60.1 84.2 95.7 97.2 97.4 97.9 97.8 98.0 98.1 98.2 98.3 98.4 98.4 98.4 98.4 94.4LAME [5] 39.9 32.4 33.4 51.4 20.6 49.0 74.4 31.3 21.2 22.6 21.9 28.1 66.9 43.9 72.5 40.6CoTTA [73]51.5 55.3 54.3 51.8 49.4 55.3 50.7 54.2 51.4 50.6 49.5 53.6 55.0 57.1 55.8 53.0NOTE [19]51.6 60.9 60.3 45.4 54.3 70.8 68.8 75.0 75.7 87.1 94.7 95.6 96.7 96.4 97.2 75.4 RoTTA 40.0 46.3 42.8 36.4 29.2 42.3 33.2 34.4 28.4 29.2 26.4 34.5 38.5 39.8 39.3 36.0(+4.6) Table 27. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 29.3 30.8 28.8 68.0 73.0 54.1 41.2 50.3 55.1 74.7 45.8 39.5 29.5 37.2 39.4 46.4BN [53] 47.1 48.6 47.8 56.2 57.6 57.6 57.6 57.5 48.7 50.6 51.8 53.2 46.9 53.5 58.8 52.9PL [39] 48.8 58.7 69.9 88.0 95.1 96.6 96.7 96.9 97.4 97.4 98.2 98.2 98.2 98.3 98.5 89.1TENT [70] 51.0 67.6 85.8 95.9 97.2 97.5 97.2 97.7 98.1 97.9 97.7 97.7 98.0 98.0 98.2 91.7LAME [5] 21.2 22.8 21.1 66.3 72.8 49.0 33.3 44.8 51.7 74.9 39.8 31.2 21.3 27.3 32.3 40.6CoTTA [73]48.4 48.8 48.2 52.9 54.0 53.8 52.7 57.2 52.6 48.6 51.8 53.9 49.4 52.3 56.0 52.0NOTE [19] 45.1 46.7 49.1 67.3 65.5 69.4 75.5 80.3 83.8 96.0 97.6 97.1 96.1 97.9 98.7 77.7 RoTTA 29.6 31.3 28.8 43.9 41.5 41.3 40.9 39.8 32.1 32.6 33.1 33.0 26.5 34.5 42.9 35.4(+5.2) Table 28. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.1 28.8 39.4 50.3 39.5 41.2 73.0 45.8 68.0 29.5 55.1 30.8 74.7 29.3 37.2 46.4BN [53] 58.8 47.7 59.2 57.6 52.7 56.9 58.2 52.0 56.7 45.5 47.8 48.2 51.7 46.1 54.0 52.9PL [39] 60.1 59.5 75.1 85.7 91.5 94.6 96.5 97.1 97.4 97.3 98.0 97.7 97.9 97.8 97.7 89.6TENT [70] 61.6 71.5 91.0 95.9 96.6 97.1 96.9 97.3 97.4 97.2 97.9 98.0 98.1 97.9 97.8 92.8LAME [5] 48.6 20.6 32.3 44.4 30.2 33.6 72.4 40.0 66.3 21.6 52.0 22.8 74.6 20.7 27.5 40.5CoTTA [73]56.4 48.9 56.1 57.8 54.1 54.2 56.2 53.6 55.4 50.0 53.6 51.6 51.2 50.7 54.4 53.6NOTE [19]62.5 46.3 61.5 61.1 58.6 68.4 76.1 78.3 92.0 93.4 96.1 95.4 96.2 95.8 96.4 78.5 RoTTA 45.5 30.0 45.9 42.6 35.3 41.8 42.2 34.5 40.2 27.3 31.3 30.2 32.7 28.1 34.9 36.2(+4.3) Table 29. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 55.1 73.0 29.3 28.8 45.8 54.1 41.2 50.3 74.7 37.2 68.0 39.4 39.5 30.8 29.5 46.4BN [53] 49.5 58.8 47.0 46.5 52.2 57.6 57.6 57.6 51.7 53.5 56.0 58.5 53.1 47.6 46.3 52.9PL [39] 53.6 70.4 76.0 85.1 91.2 95.2 96.0 97.0 96.9 97.3 97.3 97.6 97.5 97.6 97.7 89.8TENT [70] 60.2 89.1 95.0 96.2 96.9 97.0 96.5 97.0 97.0 97.2 97.6 97.8 97.5 97.9 97.7 94.0LAME [5] 51.3 72.5 21.5 21.0 39.6 49.0 33.3 44.8 74.8 28.0 66.8 32.5 30.6 22.5 21.4 40.6CoTTA [73]52.3 55.3 49.5 48.1 52.1 54.8 52.7 56.9 50.6 52.6 53.7 55.8 54.6 50.6 50.5 52.7NOTE [19] 39.1 64.7 48.9 50.6 59.1 70.1 71.7 75.0 85.2 95.7 96.9 98.4 96.0 95.9 94.9 76.1 RoTTA 41.4 46.2 30.5 28.5 36.0 40.9 40.5 39.6 33.0 35.0 38.2 43.1 33.9 30.7 27.1 36.3(+4.3)",
      "meta_data": {
        "arxiv_id": "2303.13899v1",
        "authors": [
          "Longhui Yuan",
          "Binhui Xie",
          "Shuang Li"
        ],
        "published_date": "2023-03-24T10:19:14Z",
        "pdf_url": "https://arxiv.org/pdf/2303.13899v1.pdf",
        "github_url": "https://github.com/BIT-DA/RoTTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Practical Test-Time Adaptation (PTTA), a new and more realistic Test-Time Adaptation (TTA) setup for real-world applications. PTTA accounts for both continually changing data distributions and correlatively sampled test data streams, which are common in dynamic scenarios like autonomous driving. The main contribution is the proposed Robust Test-Time Adaptation (RoTTA) method, which addresses the challenges of PTTA through robust statistics estimation, category-balanced sampling with timeliness and uncertainty, and time-aware robust training. RoTTA significantly outperforms prior TTA methods on standard benchmarks (CIFAR-10-C, CIFAR-100-C, and DomainNet), demonstrating its effectiveness and ease of implementation for rapid deployment.",
        "methodology": "The proposed RoTTA method consists of three key components. First, Robust Batch Normalization (RBN) replaces standard BN layers. It maintains global statistics (mean and variance) via Exponential Moving Average (EMA) of buffered samples' statistics, initialized from the pre-trained model's running statistics, to robustly normalize feature maps in the presence of correlative data. Second, Category-Balanced Sampling with Timeliness and Uncertainty (CSTU) is used to maintain a memory bank. This memory bank stores samples by considering their age (timeliness) and prediction entropy (uncertainty), prioritizing newer and less uncertain samples while ensuring category balance through a heuristic score. Third, a time-aware robust training strategy employs a teacher-student model architecture. The student model's affine parameters in RBN are updated by minimizing a loss function applied to strong-augmented views of memory bank samples, reweighted by their timeliness. The teacher model is then updated via EMA of the student's parameters. The loss is computed as cross-entropy between the teacher's weak-augmented prediction and the student's strong-augmented prediction.",
        "experimental_setup": "Experiments were conducted on CIFAR10-C, CIFAR100-C (robustness to common corruptions), and DomainNet (generalization under huge domain shifts). For CIFAR-C, pre-trained models were WildResNet-28 (CIFAR10) and ResNeXt-29 (CIFAR100) from RobustBench. For DomainNet, ResNet-101 was pre-trained on each source domain. The PTTA scenario was simulated by continually changing corruption types (CIFAR-C, highest severity 5) or target domains (DomainNet) over time. Correlative sampling was simulated using a Dirichlet distribution with parameter δ=0.1. The Adam optimizer was used with a learning rate of 1.0 × 10−3. The batch size was set to 64, and the memory bank capacity (N) was 64. RoTTA's hyperparameters were α=0.05, ν=0.001, λt=1.0, and λu=1.0. Performance was evaluated using average classification error, and extensive ablation studies were performed on RoTTA's components, distribution changing order, Dirichlet parameter sensitivity, and batch size effects. Baselines included BN, PL, TENT, LAME, CoTTA, and NOTE.",
        "limitations": "The Robust Batch Normalization (RBN) component is considered a 'naive' solution, requiring careful tuning of its parameter (α). While RoTTA successfully avoids model collapse during experiments, the method currently lacks a mechanism to explicitly recover a model if it were to enter a collapsed state. The work primarily validates its approach through simulations of correlative sampling using a Dirichlet distribution on benchmark datasets, and further validation in real-world scenarios is needed to confirm its robustness beyond category similarity as a single type of correlation.",
        "future_research_directions": "Future work could focus on improving the RoTTA algorithm by replacing or enhancing some of its current components, such as developing a more sophisticated robust batch normalization scheme. More importantly, the authors hope this work paves the way for making Practical Test-Time Adaptation (PTTA) more realistic, encouraging further research into making the PTTA setup itself closer to real-world conditions for actual model deployment.",
        "experimental_code": "import torchimport torch.nn as nnfrom ..utils import memoryfrom .base_adapter import BaseAdapterfrom copy import deepcopyfrom .base_adapter import softmax_entropyfrom ..utils.bn_layers import RobustBN1d, RobustBN2dfrom ..utils.utils import set_named_submodule, get_named_submodulefrom ..utils.custom_transforms import get_tta_transformsclass RoTTA(BaseAdapter):    def __init__(self, cfg, model, optimizer):        super(RoTTA, self).__init__(cfg, model, optimizer)        self.mem = memory.CSTU(capacity=self.cfg.ADAPTER.RoTTA.MEMORY_SIZE, num_class=cfg.CORRUPTION.NUM_CLASS, lambda_t=cfg.ADAPTER.RoTTA.LAMBDA_T, lambda_u=cfg.ADAPTER.RoTTA.LAMBDA_U)        self.model_ema = self.build_ema(self.model)        self.transform = get_tta_transforms(cfg)        self.nu = cfg.ADAPTER.RoTTA.NU        self.update_frequency = cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY        self.current_instance = 0    @torch.enable_grad()    def forward_and_adapt(self, batch_data, model, optimizer):        with torch.no_grad():            model.eval()            self.model_ema.eval()            ema_out = self.model_ema(batch_data)            predict = torch.softmax(ema_out, dim=1)            pseudo_label = torch.argmax(predict, dim=1)            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)        for i, data in enumerate(batch_data):            p_l = pseudo_label[i].item()            uncertainty = entropy[i].item()            current_instance = (data, p_l, uncertainty)            self.mem.add_instance(current_instance)            self.current_instance += 1            if self.current_instance % self.update_frequency == 0:                self.update_model(model, optimizer)        return ema_out    def update_model(self, model, optimizer):        model.train()        self.model_ema.train()        sup_data, ages = self.mem.get_memory()        l_sup = None        if len(sup_data) > 0:            sup_data = torch.stack(sup_data)            strong_sup_aug = self.transform(sup_data)            ema_sup_out = self.model_ema(sup_data)            stu_sup_out = model(strong_sup_aug)            instance_weight = timeliness_reweighting(ages)            l_sup = (softmax_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()        l = l_sup        if l is not None:            optimizer.zero_grad()            l.backward()            optimizer.step()        self.update_ema_variables(self.model_ema, self.model, self.nu)    @staticmethod    def update_ema_variables(ema_model, model, nu):        for ema_param, param in zip(ema_model.parameters(), model.parameters()):            ema_param.data[:] = (1 - nu) * ema_param[:].data[:] + nu * param[:].data[:]        return ema_model    def configure_model(self, model: nn.Module):        model.requires_grad_(False)        normlayer_names = []        for name, sub_module in model.named_modules():            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):                normlayer_names.append(name)        for name in normlayer_names:            bn_layer = get_named_submodule(model, name)            if isinstance(bn_layer, nn.BatchNorm1d):                NewBN = RobustBN1d            elif isinstance(bn_layer, nn.BatchNorm2d):                NewBN = RobustBN2d            else:                raise RuntimeError()            momentum_bn = NewBN(bn_layer,                                self.cfg.ADAPTER.RoTTA.ALPHA)            momentum_bn.requires_grad_(True)            set_named_submodule(model, name, momentum_bn)        return modeldef timeliness_reweighting(ages):    if isinstance(ages, list):        ages = torch.tensor(ages).float().cuda()    return torch.exp(-ages) / (1 + torch.exp(-ages))",
        "experimental_info": "Robust Batch Normalization (RBN):\n- Replaces BatchNorm1d/2d layers with RobustBN1d/2d during `configure_model`.\n- Momentum for RBN (`alpha`) is set by `cfg.ADAPTER.RoTTA.ALPHA` (default: 0.05).\n- RBN affine parameters `weight` and `bias` are updated during training.\n\nCategory-Balanced Sampling with Timeliness and Uncertainty (CSTU) Memory Bank:\n- Memory bank (`self.mem`) is an instance of `memory.CSTU`.\n- Capacity is `cfg.ADAPTER.RoTTA.MEMORY_SIZE` (default: 64).\n- `lambda_t` and `lambda_u` for the heuristic score are `cfg.ADAPTER.RoTTA.LAMBDA_T` (default: 1.0) and `cfg.ADAPTER.RoTTA.LAMBDA_U` (default: 1.0).\n- Samples are added to memory with their data, pseudo-label (from teacher EMA model), and entropy (uncertainty).\n- `CSTU` manages sample age, prediction entropy, and category balance using a heuristic score to decide replacement.\n\nTime-aware Robust Training Strategy:\n- Teacher-student model architecture: `self.model_ema` (teacher) is an EMA copy of `self.model` (student).\n- Teacher model is updated via EMA with coefficient `nu` (default: 0.001) from `cfg.ADAPTER.RoTTA.NU`.\n- Student model is updated every `cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY` (default: 64) instances.\n- Training samples are drawn from the memory bank (`self.mem.get_memory()`).\n- Strong augmentation (`self.transform`) is applied to memory samples before feeding to the student model. The augmentation chain includes `ColorJitterPro`, `RandomAffine`, `GaussianBlur`, `RandomHorizontalFlip`, and `GaussianNoise`.\n- The loss function is `softmax_entropy(stu_sup_out, ema_sup_out)`, which is equivalent to negative KL divergence between student's strong-augmented prediction and teacher's weak-augmented prediction.\n- The loss is reweighted by `timeliness_reweighting(ages)` which applies `torch.exp(-ages) / (1 + torch.exp(-ages))`, where `ages` are normalized by memory capacity.\n- Optimizer parameters: Learning rate `cfg.OPTIM.LR` (default: 1e-3), method `cfg.OPTIM.METHOD` (default: Adam), `cfg.OPTIM.STEPS` (default: 1)."
      }
    },
    {
      "title": "What How and When Should Object Detectors Update in Continually Changing Test Domains?",
      "abstract": "It is a well-known fact that the performance of deep learning models\ndeteriorates when they encounter a distribution shift at test time. Test-time\nadaptation (TTA) algorithms have been proposed to adapt the model online while\ninferring test data. However, existing research predominantly focuses on\nclassification tasks through the optimization of batch normalization layers or\nclassification heads, but this approach limits its applicability to various\nmodel architectures like Transformers and makes it challenging to apply to\nother tasks, such as object detection. In this paper, we propose a novel online\nadaption approach for object detection in continually changing test domains,\nconsidering which part of the model to update, how to update it, and when to\nperform the update. By introducing architecture-agnostic and lightweight\nadaptor modules and only updating these while leaving the pre-trained backbone\nunchanged, we can rapidly adapt to new test domains in an efficient way and\nprevent catastrophic forgetting. Furthermore, we present a practical and\nstraightforward class-wise feature aligning method for object detection to\nresolve domain shifts. Additionally, we enhance efficiency by determining when\nthe model is sufficiently adapted or when additional adaptation is needed due\nto changes in the test distribution. Our approach surpasses baselines on widely\nused benchmarks, achieving improvements of up to 4.9\\%p and 7.9\\%p in mAP for\nCOCO $\\rightarrow$ COCO-corrupted and SHIFT, respectively, while maintaining\nabout 20 FPS or higher.",
      "full_text": "What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Jayeon Yoo1 Dongkwan Lee1 Inseop Chung1 Donghyun Kim2∗ Nojun Kwak1∗ 1Seoul National University 2Korea University 1{jayeon.yoo, biancco, jis3613, nojunk}@snu.ac.kr 2d kim@korea.ac.kr Abstract It is a well-known fact that the performance of deep learning models deteriorates when they encounter a dis- tribution shift at test time. Test-time adaptation (TTA) al- gorithms have been proposed to adapt the model online while inferring test data. However, existing research pre- dominantly focuses on classification tasks through the op- timization of batch normalization layers or classification heads, but this approach limits its applicability to various model architectures like Transformers and makes it chal- lenging to apply to other tasks, such as object detection. In this paper, we propose a novel online adaption approach for object detection in continually changing test domains, considering which part of the model to update, how to up- date it, and when to perform the update. By introducing architecture-agnostic and lightweight adaptor modules and only updating these while leaving the pre-trained backbone unchanged, we can rapidly adapt to new test domains in an efficient way and prevent catastrophic forgetting. Fur- thermore, we present a practical and straightforward class- wise feature aligning method for object detection to resolve domain shifts. Additionally, we enhance efficiency by deter- mining when the model is sufficiently adapted or when ad- ditional adaptation is needed due to changes in the test dis- tribution. Our approach surpasses baselines on widely used benchmarks, achieving improvements of up to 4.9%p and 7.9%p in mAP for COCO → COCO-corrupted and SHIFT, respectively, while maintaining about 20 FPS or higher. 1. Introduction Although deep learning models have demonstrated remark- able success in numerous vision-related tasks, they remain susceptible to domain shifts where the test data distribu- tion differs from that of the training data [3, 25, 40]. In real-world applications, domain shifts frequently occur at test-time due to natural variations, corruptions, changes in weather conditions (e.g., fog, rain) , camera sensor differ- Figure 1. We propose an online adaptation method for object detection in continually changing test domains. Object detectors trained with clean images suffer from performance degradation due to various corruption, such as camera sensor degradation or environmental changes (Direct-Test). Updating full parameters for online adaptation require a large number of test samples and vul- nerable to drastic domain changes (Full-Finetuning), while using only our lightweight adaptor is robust and quickly adapts within a few time steps (Ours). We can further improve efficiency by skip- ping unnecessary adaptation steps (Ours-Skip). ences (e.g., pixelate, defocus blur) , and various other fac- tors. Test-Time Adaptation (TTA) [3, 25, 30, 40, 43, 47] has been proposed to solve the domain shifts in test-time by adapting models to a specific target (test) distribution in an online manner. Furthermore, it is essential to take into account continuously changing test distributions, as the test distribution has the potential to undergo changes and devel- opments as time progresses (i.e., Continual Test-time Adap- tation (CTA)). For instance, autonomous driving systems may experience transitions from clear and sunny conditions to rainy or from daytime to nighttime, which causes contin- ually changing domain shifts [39]. While it is an important research topic, continual test-time adaptation for object de- tection has not been well explored. Recently, several TTA methods [6, 29, 36] tailored for 1 arXiv:2312.08875v1  [cs.CV]  12 Dec 2023object detection have been proposed. ActMAD [29] aligns all the output feature maps ( RC×H×W ) after Batch Nor- malization (BN) layers [14] to adapt the test domain to be similar to that of the training domain. However, this ap- proach requires significant memory during adaptation and does not explicitly consider the objects present in the image. TeST [36] and STFAR [6] adapt to a test domain by utiliz- ing weak and strong augmented test samples with a teacher- student network [37], but they significantly increase infer- ence costs since they require additional forward passes and update steps. Also, these methods update all network pa- rameters, making them highly inefficient in online adapta- tion and vulnerable to losing task-specific knowledge when the test domain experiences continual or drastic changes. In this paper, we aim to develop an efficient continual test-time adaptation (CTA) method for object detection. We investigate the following three key aspects to improve ef- ficiency; what to update: while previous TTA methods for object detection [6, 29, 36] use full fine-tuning, updating all parameters at test time, they are inefficient and prone to losing task-specific knowledge in relatively complex object detection tasks. Updating BN layers, as done in many TTA methods for classification [17, 30, 43, 47], is not as effective for object detection, given its smaller batch size compared to classification and the limitation in applying various back- bones, such as Transformer [26, 41].how to update: several previous TTA methods for object detection [6, 36] adapt the model by using teacher-student networks, resulting in a significant decrease in inference speed, which is detri- mental during test time. While another existing method [29] aligns feature distributions for adaptation, it does not con- sider each object individually, focusing only on image fea- tures, making it less effective for object detection. when to update: most TTA or CTA methods update models using all incoming test samples. However, it is inefficient to update continuously the model if it is already sufficiently adapted when the change of the test domain is not significant. To this end, (1) we propose an efficient continual test- time adaptation method for object detectors to adapt to continually changing test domains through the use of lightweight adaptors which require only 0.54%∼0.89% ad- ditional parameters compared to the full model. It exhibits efficiency in parameters, memory usage, and adaptation time, along with robustness to continuous domain shifts without catastrophic forgetting. Additionally, it demon- strates wide applicability to various backbone types com- pared to BN-based TTA methods [17, 22, 30, 43, 47, 48]. (2) To enhance the adaptation effectiveness in the object detec- tion task, we align the feature distribution of the test domain with that of the training domain at both the image-level and object-level using only the mean and variance of features. For estimating the mean of the test domain features, we employ Exponentially Moving Average (EMA) as we can leverage only the current incoming test samples, not the en- tire test domain data. Due to the unavailability of training data access, we utilize only the mean and variance of the features from a few training samples. (3) We also introduce two novel criteria that do not require additional resources to determine when the model needs adaptation to enhance efficiency in a continually changing test domain environ- ment. As illustrated in Fig. 1, our approach Ours, employ- ing adaptors, tends to adapt much faster to domain changes compared to full parameter updates. This enables efficient TTA by using only a few test samples to update the adaptor and skipping the rest of the updates as shown in Ours-Skip. Our main contributions are summarized as follows: • We introduce an architecture-agnostic lightweight adap- tor, constituting only a maximum of 0.89% of the total model parameters, into the backbone of the object de- tector to adapt the model in a continually changing test domain. This approach ensures efficiency in parameters, memory usage, and adaptation speed, demonstrating the robust preservation of task-specific knowledge owing to its inherent structural characteristics. • We propose a straightforward and effective adaptation loss for CTA in object detection tasks. This is achieved by aligning the distribution of training and test domain fea- tures at both the image and object levels, utilizing only the mean and variance of a few training samples and EMA- updated mean features of the test domain. • We also propose two criteria to determine when the model requires adaptation, enabling dynamic skipping or resum- ing adaptation as needed. This enhancement significantly boosts inference speed by up to about 2 times while main- taining adaptation performance. • Our adaptation method proves effective for diverse types of domain shifts, including weather changes and sensor variations, regardless of whether the domain shift is dras- tic or continuous. In particular, our approach consistently improves the mAP by up to 7.9% in COCO →COCO-C and SHIFT-Discrete/Continuous with higher than 20 FPS. 2. Related Work Test-time adaptation. Recently, there has been a surge of interest in research that adapts models online using unla- beled test samples while simultaneously inferring the test sample to address the domain shift problem, where the test data distribution differs from that of the training data. There are two lines for online adaptation to the test do- main, Test-time Training (TTT) and Test-time Adaptation (TTA). TTT [1, 2, 25, 40] involves modifying the model architecture during training to train it with self-supervised loss, allowing adaptation to the test domain in the test time by applying this self-supervised loss to the unlabeled test samples. On the other hand, TTA aims to adapt the trained model directly to the test domain without specifically tai- 2lored model architectures or losses during training time. NORM [35] and DUA [28] address the domain shifts by adjusting the statistics of batch normalization (BN) layers using the current test samples, without updating other pa- rameters, inspired by [21]. Following this, [22, 30, 43, 48] and [17] update the affine parameters of BN layers using unsupervised loss, entropy minimization loss to enhance the confidence of test data predictions, and feature distribution alignments loss, respectively. Several studies [15, 16] up- date the classifier head using the pseudo-prototypes from the test domain. However, these methods limit their appli- cability to architectures without BN layers or to object de- tection tasks that involve multiple objects in a single im- age. Others [29, 38, 47] update full parameters for online adaptation to the test domain in an online manner, but this approach is inefficient and susceptible to the noisy signal from the unsupervised loss. While existing TTA methods are oriented towards classification tasks, we aim to propose an effective and efficient method for online adaptation in the object detection task. Continual test-time adaptation. Recent studies [31, 44] point out that existing TTA methods have primarily focused on adapting to test domains following an i.i.d assumption and may not perform well when the test data distribution deviates from this assumption. [44] introduces a Contin- ual TTA (CTA) method designed for scenarios where the test domain continuously changes over time. This poses challenges in preventing the model from over-adapting to a particular domain shift and preserving the knowledge of the pre-trained model to avoid catastrophic forgetting. In the field of CTA, the self-training strategy adopting an Exponentially Moving Average (EMA) teacher-student structure is attracting interest as an effective algorithm en- abling robust representation to be learned through self- knowledge distillation. In many studies, the EMA teacher- student structure and catastrophic restoration of source model weights have been proposed as a solution to achieve the goal of CTA [4, 44, 45]. Approaches using source re- play [32], and anti-forgetting regularization [30] have also achieved good performances in robust continuous adapta- tion. Furthermore, there is growing attention on methods that mitigate the computational and memory challenges as- sociated with CTA, such as [12], which relies on updates to batch normalization statistics. Test-time adaptive object detection. Research on TTA for Object Detection (TTAOD) is progressively emerging [6, 29, 36, 42]. Most existing TTAOD methods [6, 36, 42] exploit a teacher-student network to adapt to the test do- main, following the self-training approach commonly em- ployed in Unsupervised Domain Adaptation for object de- tection [7, 18, 19, 34]. However, it is inefficient for TTA due to data augmentation requirements and additional for- ward and backward steps, resulting in slower inference speeds and higher memory usage. Another approach, Act- MAD [29], aligns the distributions of output feature maps after all BN layers along the height, width, and channel axes to adapt to the test domain. However, this location-aware feature alignment is limited to datasets with fixed location priors, such as driving datasets, and is less effective for nat- ural images like COCO. Additionally, CTA for Object De- tection (CTAOD)have not been thoroughly explored. There- fore, there is a need for an effective CTAOD method con- sidering memory and time efficiency. 3. Method To enable the efficient and effective Continual Test-time Adaptation of Object Detectors (CTAOD), we introduce an approach that specifies which part of the model should be updated, describes how to update those using unlabeled test data, and determines whether we perform model updates or not to improve efficiency. 3.1. Preliminary Assume that we have an object detector h ◦ gΘ, here h and g are the RoI head and the backbone, respectively with their parameters being Θ. The training dataset is denoted as Dtrain = {(xi, yi)}N i=1, where xi ∼ Ptrain(x) and yi = ( bboxi, ci), containing information on the bounding box (bbox) and class label ci ∈ C. Consider deploying the detector to the test environments where the test data at pe- riod T is denoted as xT j ∼ PT test(x), PT test ̸= Ptrain and PT test deviates from the i.i.d. assumption. In addition, the domain of PT test continually changes according to T (i.e., PT test ̸= PT−1 test ). Our goal is to adapt the detector h ◦ g to PT test using only test data xT j while making predictions. 3.2. What to update: Adaptation via an adaptor Previous methods [6, 29, 36, 42] adapt the model to the test domain by updating all parameters Θ, leading to in- efficiency at test time and a high risk of losing task knowl- edge from the training data. In contrast, we adapt the model by introducing an adaptor with an extremely small set of parameters and updating only this module while freezing Θ. We introduce a shallow adaptor in parallel for each block, inspired by [5, 13], where transformer-based mod- els are fine-tuned for downstream tasks through parameter- efficient adaptors, as shown in Fig. 2. Each adaptor consists of down-projection layers Wdown ∈ Rd×d r , up-projection layers Wup ∈ R d r ×d and ReLUs, where d denotes the in- put channel dimension and r is the channel reduction ratio set to 32 for all adaptors. We use MLP layers for the Trans- former block (Fig. 2a) and 1×1 convolutional layers for the ResNet block (Fig. 2b) to introduce architecture-agnostic adaptors. The up-projection layer is initialized to 0 values so that the adaptor does not modify the output of the block, 3(a) A block of Transformer  (b) A block of ResNet Figure 2. We attach an adaptor, which is a shallow and low-rank MLP or CNN, to every N block in parallel. We update only these adaptors while other parameters are frozen. Our approach can be applied to diverse architectures including CNNs and Transformers. but as the adaptor is gradually updated, it adjusts the output of the block to adapt to the test domain. Even as the adaptor is updated in the test domain, the original backbone param- eter Θ remains frozen and fully preserved. This structural preservation, as evident in Ours in Fig. 1, enables robust and efficient adaptation to domain changes by maintaining relatively complex task knowledge in object detection and updating very few parameters. 3.3. How to update: EMA feature alignment To adapt the object detector to the test domain, we align the feature distribution of the test domain with that of the training data, inspired by [17, 29, 38]. In contrast to these methods that solely align image feature distribution, we ad- ditionally align object-level features in a class-wise manner, considering class frequency, to enhance its effectiveness for object detection. As the training data is not accessible dur- ing test time, we pre-compute the first and second-order statistics, denoted as µtr = E[Ftr] and Σtr = Var[Ftr], where the operators E and Var represent the mean and vari- ance respectively. The features Ftr = {gΘ(xtr)} are com- puted using only 2,000 training samples, a small subset of the training data. Since a sufficient amount of test domain data is not available at once, and only the current incoming test data, whose features are denoted as Ft te, is accessible at time step t, we estimate the mean of test data features using an exponentially moving average (EMA) as follows: µt te = (1 − α) · µt−1 te + α · E[Ft te], s.t. µ0 te = µtr. (1) Considering the typically small batch size in object detec- tion compared to classification, we approximate the vari- ance of the test features as Σte ≃ Σtr to reduce instability. Image-level feature alignment. We estimate the training and test feature distributions as normal distributions and minimize the KL divergence between them as follows: Limg = DKL(N(µtr, Σtr), N(µt te, Σtr)). (2) Region-level class-wise feature alignment. In object de- tection, we deal with multiple objects within a single image, making it challenging to apply the class-wise feature align- ment proposed in [38], a TTA method for classification. To handle region-level features that correspond to an object, we use ground truth bounding boxes for the training data and utilize the class predictions of RoI pooled features, ft te, for unlabeled test data. In object detection, domain shifts often result in lower recall rates, as a significant number of proposals are predicted as background [20]. To mitigate this issue, we filter out features with background scores exceed- ing a specific threshold. Subsequently, we assign them to the foreground class with the highest probability, as follows: Fk,t te = {ft te|argmax c pfg = k, pbg < 0.5}, where hcls(ft te) = [pfg , pbg] = [p0, ..., pC−1, pbg]. (3) We estimate the class-wise feature distribution of the test domain by exploiting Fk,t te and Eq.1. Furthermore, we in- troduce a weighting scheme for aligning features of less frequently appearing classes, taking into account the severe class imbalance where specific instance ( e.g., person) may appear multiple times within a single image, as follows: Nk,t = Nk,t−1 + ||Fk,t te ||, s.t. Nk,0 = 0 wk,t = log \u0012maxi Ni,t Nk,t \u0013 + 0.01 Lobj = X k wk,t · DKL(N(µk tr, Σk tr), N(µk,t te , Σk tr)). (4) Here, the class-wise mean µk and variance Σk of the train- ing and test data are obtained in the same way as the image- level features. We can effectively adapt the object detector by updating the model to align the feature distribution at both the image and object levels as L = Limg + Lobj. 3.4. When to update: Adaptation on demand As shown in Fig. 1, Ours, which only updates the adaptor proposed in Sec. 3.2, efficiently adapts to changes in the test domain, even with a small subset of early test samples. We leverage its rapid adaptation characteristics to reduce com- putational costs by skipping model updates ( i.e., skipping backward passes) when the model has already sufficiently adapted to the current test domain and resuming model up- dates when confronted with a new test domain. Therefore, we introduce two criteria to determine when to update the model or not as follows: (Criterion 1) When the distribution gap exceeds the in- domain distribution gap. Recall that Limg (Eq. 2) mea- sures the distribution gap between the test and train distri- butions. We assume a model is well-adapted to the current test domain when Limg is closer to the in-domain distri- bution gap. We measure the in-domain distribution gap by 4(a) The ratio of Limg to Din KL (b) The ratio of Limg to Lt ema Figure 3. The test domain undergoes a shift every 4,000 time steps, and each metric reaches its peak at the same intervals. sampling two disjoint subsets, xi and xj, of training fea- tures Ftr from Sec. 3.3 as follows: Din KL = DKL(N(µi tr, Σi tr), N(µj tr, Σj tr)), (5) where µi tr, Σi tr are obtained from xi ∼ Ptrain(x) and µj tr, Σj tr from xj ∼ Ptrain(x). In other words, if Limg is noticeably higher than the in-domain distribution gapDin KL, we consider a model encountering a test domain whose dis- tribution differs from Ptrain(x) and needs to be updated. Based on this, we introduce a new index Limg Din KL . Fig. 3a plots the trend of this index during the model adaptation to a con- tinually changing test domain. It shows that the index has a large value in the early stages of a domain change, decreases rapidly, and then maintains a value close to 1. This index exhibits a similar trend regardless of the backbone type and dataset, as included in the appendix. Therefore, we establish the criterion that model updates are necessary when this in- dex exceeds a certain threshold, τ1, as Limg Din KL > τ1. (Criterion 2 ) When the distribution gap suddenly in- creases. Additionally, we can determine when the test dis- tribution changes and model updates are necessary by ob- serving the trend of the distribution gap ( i.e., Limg). The convergence of Limg indicates that a model is well-adapted to the current test domain. To put it differently, Limg will exhibit a sudden increase when the model encounters a new test domain. We introduce an additional index, denoted as Limg Ltema , representing the ratio of the currentLimg to its expo- nentially moving averageLt ema at time t. We calculate it us- ing the following formula:Lt ema = 0.99·Lt−1 ema+0.01·Limg. Fig. 3b illustrates the trend of the ratio of Limg over the timesteps. It tends to reach a value of 1 as the loss stabilizes at a specific level. Nevertheless, when the model encounters shifts in the test distribution, the ratio experiences a sharp increase, indicating the necessity of a model update when it exceeds a specific threshold, τ2, as Limg Ltema > τ2. If at least one of the two criteria is satisfied, we conclude that the model requires adaptation and proceed to update it. 4. Experiments Sec. 4.1 presents the two object detection benchmark datasets with test distributions that change continuously, ei- ther in a drastic or gradual manner, and our implementation detail is in 4.2. Sec. 4.4 compares our method with other TTA baselines described in Secs. 4.3.. We present detailed ablation studies of our method analyzing the effectiveness and efficiency of our method in terms of what, how, and when to update the models for CTAOD in Sec. 4.5. 4.1. Datasets We experiment with the following three scenarios. COCO → COCO-C simulates continuous and drastic real- istic test domain changes over a long sequence. MS-COCO [23] collects 80 classes of common objects in their natural context with 118k training images and 5k validation images. COCO-C is created by employing 15 types of realistic cor- ruptions [27], such as image distortion and various weather conditions, to simulate test domain changes. In the experi- ments, the model is only trained on the COCO train set and sequentially evaluated on each corruption in the COCO-C validation set during test-time for reproducing continually changing test domains. Finally, the model is evaluated on the original COCO validation set to assess how well it pre- serves knowledge of the original domain (denoted as Org.). SHIFT-(Discrete / Continuous) [39] is a synthetic driving image dataset with 6 classes under different conditions us- ing five weather attributes (clear, cloudy, overcast, fog, rain) and three time-of-day attributes ( daytime, dawn, night ). In SHIFT-Discrete, there are image sets for each attribute, and the model is sequentially evaluated on these attributes, cloudy → overcast → foggy → rainy → dawn → night → clear which contains 2.4k, 1.6k, 2.7k, 3.2k, 1.2k, 1.4k, and 2.8k validation images, respectively. This simulates scenar- ios where the domain undergoes drastic changes. InSHIFT- Continuous, the model is evaluated on four sequences, each consisting of 4k frames, continuously transitioning from clear to foggy (or rainy) and back to clear. 4.2. Implementation Detail We experiment with Faster-RCNN [33] models using ResNet50 [10] and Swin-Tiny [26] as a backbone with FPN [24]. For the COCO → COCO-C adaptation, we em- ploy the publicity available models trained on COCO re- leased in [46] and [26] for ResNet5- and Swin-Tiny-based Faster-RCNN, respectively. For SHIFT experiments, mod- els are trained on the training domain using the detectron2 framework following [33] and [26]. For test-time adapta- tion, we always set the learning to 0.001 for the SGD opti- mizer, and α of Eq. 1 to 0.01, while τ1 and τ2 are set to 1.1 5Table 1. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on COCO→ COCO- C. Our model consistently outperforms baselines on the two different backbones. Furthermore, Ours-Skip with ResNet notably reduces backward passes by as much as 90.5%, leading to a significantly improved frames per second (FPS) rate by up to 109.9%. Noise Blur Weather Digital # step Backbone Method Gau Sht Imp Def Gls Mtn Zm Snw Frs Fog Brt Cnt Els Px Jpg Org. Avg. For. Back. FPS Swin-T [26] Direct-Test 9.7 11.4 10.0 13.4 7.5 12.1 5.2 20.7 24.8 36.1 36.0 12.9 19.1 4.9 15.8 43.0 17.7 80K 0 21.5 ActMAD 10.7 12.0 9.4 12.3 5.7 9.5 4.5 15.3 17.5 27.6 28.2 1.1 16.7 2.6 8.7 36.3 13.9 80K 80K 8.3 Mean-Teacher 10.0 12.1 11.2 12.8 8.1 12.1 4.9 19.6 23.7 34.9 34.0 8.0 18.9 6.1 17.6 41.0 17.2 160K 80K 6.9 Ours 13.6 16.6 16.1 14.0 13.6 14.2 8.3 23.7 27.2 37.4 36.4 27.2 27.2 22.2 22.3 42.3 22.6 80K 80K 9.5 Ours-Skip 13.3 15.3 15.1 14.0 12.8 13.9 6.5 22.0 25.4 35.5 34.9 26.5 25.9 23.4 20.2 41.2 21.6 80K 9.7K 17.7 ResNet50 [10] Direct-Test 9.1 11.0 9.8 12.6 4.5 8.8 4.6 19.1 23.1 38.4 38.0 21.4 15.6 5.3 11.9 44.2 17.3 80K 0 25.8 NORM 9.9 11.9 11.0 12.6 5.2 9.1 5.1 19.4 23.5 38.2 37.6 22.4 17.2 5.7 10.3 43.4 17.5 80K 0 25.8 DUA 9.8 11.7 10.8 12.8 5.2 8.9 5.1 19.3 23.7 38.4 37.8 22.3 17.2 5.4 10.1 44.1 17.1 80K 0 25.8 ActMAD 9.1 9.6 7.0 11.0 3.2 6.1 3.3 12.8 14.0 27.7 27.8 3.9 12.9 2.3 7.2 34.3 10.5 80K 80K 9.6 Mean-Teacher 9.6 12.5 12.0 4.0 2.9 4.8 3.1 16.2 23.5 35.1 34.0 21.8 16.6 8.2 12.7 40.3 14.5 160K 80K 8.1 Ours 12.7 17.8 17.5 12.4 11.5 11.3 6.6 22.8 26.9 38.6 38.5 28.0 25.1 21.2 22.2 41.8 22.2 80K 80K 10.1 Ours-Skip 14.4 17.1 16.0 13.9 11.7 12.2 6.3 22.1 25.5 37.7 37.1 25.5 24.1 23.1 21.1 42.8 21.9 80K 7.6K 21.2 and 1.05, respectively. We use the same hyper-parameters across all backbones and datasets. All experiments are con- ducted with a batch size of 4. 4.3. Baselines Direct-Test evaluates the model trained in the training do- main without adaptation to the test domain. ActMAD [29] is a TTA method aligning the distribution of output features across all BN layers. To apply ActMAD to the Swin Trans- former-based model, we align the output features of the LN layers. We implement Mean-Teacher using a teacher- student network framework to reproduce as close as possi- ble to TeST [36], as its implementation is not publicly avail- able. We follow the FixMatch [37] augmentation method and report results after tuning all hyper-parameters in our scenario. NORM [35] and DUA [28], TTA methods ini- tially designed for classification, are directly applicable to detection tasks by either mixing a certain amount of current batch statistics or updating batch statistics via EMA. How- ever, these are only compatible with architectures contain- ing BN layers. Additional details are provided in Appendix. 4.4. Main Results We compare the performance of each method using mAP and efficiency metrics, including the number of forward and backward passes, as well as FPS during test-time adapta- tion. Results of COCO and SHIFT are in Tab. 1 and 2, re- spectively. COCO → COCO-C. Tab. 1 demonstrates the effective adaptation performance of Ours in the challenging COCO benchmark with 80 classes due to object-level class-wise feature alignment. ActMAD also aligns feature distribution for TTA, but is not effective since it only aligns whole fea- ture maps without considering specific classes in the im- age. NORM and DUA, applicable only to ResNet [10], show minimal performance improvement by adaptation as they are not specifically tailored for object detection and only modify batch statistics across the entire feature map. Ad- ditionally, ActMAD and Mean-Teacher, updating full pa- rameters, gradually lose task knowledge in the continually changing test distributions, resulting in much lower perfor- mance on Org. , the domain identical to the training data, than that of Direct-Test. In contrast, Ours effectively pre- vents catastrophic forgetting by freezing the original param- eters of the models and updating only the adaptor, obtain- ing performance on par with Direct-Test on the Org. do- main and consistently high performance across corrupted domains, with an average mAP improvement of 4.9%p compared to that of Direct-Test. Furthermore, leveraging the rapid adaptation ability of the adaptor,Ours-Skip, which skips unnecessary adaptation, allows using only a maxi- mum of about 12% of the total samples for adaptation with- out significant performance loss. This leads to a substantial improvement in inference speed, more than doubling com- pared to other TTA methods, reaching over 17.7 FPS. SHIFT-Discrete. Ours is also effective in SHIFT, which simulates continuous changes in weather and time in driv- ing scenarios according to the left section of Tab. 2. Espe- cially, Ours shows significant improvements in mAP by 7- 9%p, particularly for the foggy and dawn attributes where Direct-Test obtains lower performance due to severe do- main shift. In contrast, with ActMAD, catastrophic forget- ting takes place when adapting to the cloudy and overcast weather. This is due to the updating of the full parame- ters, despite that Direct-Test already shows proficient per- formance in these conditions. As a result, the performance in the later domains is worse than that of the Direct-Test. DUA, which updates batch statistics using EMA, shows a gradual decrease in performance as the domain contin- uously changes, resulting in much lower performance in the original clear domain ( i.e., clear ). On the other hand, NORM, which utilizes the statistics of the current batch samples, exhibits no catastrophic forgetting and relatively good adaptation, as SHIFT is a relatively easier task com- pared to COCO due to having only 6 classes. Compared to NORM, Ours shows better adaptation performance, and is 6Table 2. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on SHIFT-Discrete and SHIFT-Continuous. Baselines perform effectively in a particular setting but lack generalizability across various settings. Our method consistently achieves results that are either better or on par with the best model in all settings, demonstrating its strong stability. Ours-Skip also effectively reduces the number of backward passes without compromising mAP performance, resulting in a higher FPS. SHIFT-Discrete SHIFT-Continuous mAP # step mAP # Avg. step Backbone Method cloudy overc. fog rain dawn night clear Avg. For. Back. FPS clear↔fog clear ↔rain For. Back. FPS Swin-T [26] Direct-Test 50.0 38.9 23.1 45.1 26.9 39.5 45.9 38.5 15.3K 0 27.5 18.1 21.1 4K 0 28.3 ActMAD 49.8 38.4 21.4 43.1 19.0 32.0 44.8 35.5 15.3K 15.3K 9.3 15.6 16.3 4K 4K 9.8 Mean-Teacher 50.0 39.2 25.7 45.4 26.0 37.5 42.2 38.0 15.3K 15.3K 7.8 20.4 24.3 8K 4K 6.5 Ours 50.3 39.2 32.2 46.7 30.4 39.9 44.3 40.4 15.3K 15.3K 11.2 23.9 22.6 4K 4K 11.6 Ours-Skip 50.3 39.7 29.1 47.1 30.2 41.5 45.9 40.6 15.3K 6.1K 20.0 25.1 23.8 4K 0.83K 19.2 ResNet50 [10] Direct-Test 49.4 37.9 19.7 43.1 20.1 35.3 45.6 35.9 15.3K 0 30.1 12.1 15.4 4K 0 30.0 NORM 49.7 38.6 22.9 44.7 25.1 37.4 45.5 37.7 15.3K 0 30.1 16.9 19.4 4K 0 30.0 DUA 45.2 31.5 27.7 31.9 15.2 18.6 21.1 27.3 15.3K 0 30.1 22.5 22.4 4K 0 30.0 ActMAD 49.2 37.7 18.0 40.6 16.0 32.9 44.3 34.1 15.3K 15.3K 11.3 12.7 16.3 4K 4K 11.2 Mean-Teacher 49.6 38.4 26.8 43.4 26.6 33.1 41.6 37.1 15.3K 15.3K 9.9 16.0 20.8 8K 4K 9.8 Ours 49.7 38.7 27.4 46.3 27.4 37.6 43.8 38.7 15.3K 15.3K 12.9 20.9 21.9 4K 4K 13.9 Ours-Skip 49.7 38.8 26.9 46.2 27.6 38.8 45.0 39.0 15.3K 8.9K 21.5 20.0 22.5 4K 0.75K 21.3 Table 3. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to which part of the backbone is updated. SD / SC de- notes SHIFT-Discrete/Continuous, respectively. mAP # Params Cache Backbone Trainable Params SD SC Num Ratio Avg. Max Swin-T Full-params 38.4 20.6 27.7M 100% 0.86 11.0 LayerNorm 38.5 20.0 0.03M 0.1% 0.65 7.49 adaptor (Ours) 40.4 23.2 0.15M 0.5% 0.65 6.96 ResNet50 Full-params 37.6 20.4 23.7M 100% 1.65 9.29 BatchNorm 37.9 20.2 0.05M 0.2% 1.47 9.11 adaptor (Ours) 38.7 21.7 0.21M 0.9% 1.48 5.41 also applicable to BN-layer-free Swin Transformers. SHIFT-Continuous. In scenarios where the test domain gradually changes across the entire sequence, Ours also demonstrates effectiveness, improving mAP by up to 7%p, as shown in the right section of Tab. 2. WhileDUA performs well in the clear to foggy transition, it is prone to catas- trophic forgetting in situations where the sequence becomes longer, and the test domain changes more diversely, as seen in the left section. Our strategy for determining when model adaptation is necessary is particularly effective in SHIFT. It improves FPS by about 9, reaching about 20 FPS, while en- hancing mAP. This is likely due to avoiding overfitting that can occur when adapting to all repetitive frames in SHIFT, which consists of continuous frames, leading to improve- ments in both inference speed and adaptation performance. 4.5. Additional Analyses We aim to demonstrate the effectiveness and detailed anal- ysis of our proposed model in terms of 1) which parts of, 2) how, and 3) when the model should be updated. Which part to update? Tab. 3 shows how updating dif- ferent parts of the backbone model affects the performance and the memory usage during continual test-time adapta- Table 4. Ablation on each component of our loss. SHIFT-D / C denotes SHIFT-Discrete / Continuous, respectively. The left and right value in each cell corresponds to the mAP for the Swin-T and ResNet50 backbone, respectively. Limg Lobj COCO SHIFT-D. SHIFT-C. - - 17.7/ 17.3 38.5/ 35.9 19.6/ 13.8 ✔ - 16.7/ 18.1 36.6/ 37.0 19.1/ 16.0 ✔ no class weight 17.8/ 18.9 39.7/ 38.0 25.1/ 23.4 ✔ class weight wk,t 22.6/ 22.2 40.4/ 38.7 23.2/ 21.7 tion. We compare (1) updating full parameters, (2) affine parameters of the normalization layer, and (3) our proposed adaptor for each backbone on the SHIFT dataset. Although our adaptor has fewer parameters, about 0.9% or less of the full parameters, it demonstrates the best adaptation perfor- mance. Updating only the affine parameters of the normal- ization layer, while having fewer parameters, seems less ef- fective for adaptation in object detection compared to clas- sification [30, 43]. Additionally, our adaptor requires only about 60% of the memory compared to updating the full parameters, making it memory-efficient. Ablation study on each component in our loss. Tab. 4 presents the effects of image-level feature alignment,Limg, object-level feature class-wise alignment Lobj, and class frequency weighting wk,t proposed to address class im- balance. Aligning only the image-level feature distribu- tion with Limg (first row) leads to modest adaptation in the ResNet50 backbone, while performance in the Swin- T backbone is even lower than without adaptation. No- tably, aligning object-level features with Lobj leads to a substantial improvement, with the mAP increasing by approximately 10%p compared to the no-adaptation sce- nario. Introducing class-specific frequency-based weighting wk,t, despite a slight performance decrease in the SHIFT- Continuous setting, proves highly effective, particularly in scenarios with significant class imbalance, such as COCO 7(a) Swin Transformer backbone  (b) ResNet50 backbone Figure 4. Comparison of mAP and FPS fromOurs-Skip with vary- ing values of τ1 (♦) and τ2 (▲) against Evenly-Skip (×), adapting every N-th instances, on COCO→COCO-C using both (a) Swin- T and (b) ResNet50. The upward and rightward movement indi- cates a better strategy with higher mAP and faster inference speed, showing that Ours-Skip is consistently better than Evenly-Skip. (a) Accumulated number of backward steps (b) Number of backward steps and mAP of Direct-Test in each domain Figure 5. Analysis of the adaptation of Ours-Skip. with 80 classes, where it enhances the mAP by around 5%p. Trade-off between adaptation performance and effi- ciency according to different skipping strategies. Fig. 4 presents mAP and FPS depending on the values ofτ1 and τ2 in the Sec. 3.4 on COCO → COCO-C, which are used for two criteria to determine when the adaptation is needed. We also show the simple baselineEvenly-Skip, which adapts ev- ery N-th step and skips the rest. In Fig. 4, the blue lines (▲) show the results when τ1 is changing from 1.0 to infinity, where only criterion 2 is used, while τ2 is fixed at 1.05. As τ1 decreases, more adaptation is required, leading to slower FPS but higher mAP. The green lines (♦) show the results of changing τ2, where ‘τ2 = inf’ denotes using only criterion 1, without criterion 2. For all main experiments, we set τ1 and τ2 as 1.1 and 1.05, respectively, considering the balance between mAP and FPS. Additionally, our skipping strategy consistently outperforms Evenly-Skip, achieving higher val- ues in both mAP and FPS. This indicates that our criterion for deciding when to bypass model updates provides an ef- fective balance between accuracy and speed. When do models actually update? We analyze when the model actually skips adaptation and only performs infer- ence or actively utilizes test samples for model adaptation based on the two criteria we propose. This analysis is con- ducted in COCO to COCO-C with 15 corruption domains and 1 original domain. Fig. 5a plots the number of back- ward passes, i.e., the number of batches of test samples used for adaptation, with different values of τ1 for the two backbones. The horizontal and vertical axes represent se- quentially incoming test domains and the cumulative back- ward numbers, respectively. A steep slope in a region in- dicates frequent adaptation, while a gentle slope indicates skipping adaptation, performing only inference. Notably, even without explicit information about when the test do- main changes, the model actively performs adaptation, es- pecially right after the test domain changes. This trend is consistent regardless of changes in τ value or backbone type. Furthermore, it is evident that the number of backward passes is primarily determined by the value ofτ1 rather than the type of backbone, suggesting that a consistent τ1 value can be used irrespective of the backbone. Fig. 5b visually represents the adaptation tendencies by dividing backward steps for each domain in the case of Swin-T backbone with τ1 = 1.1. More clearly, it shows that adaptation occurs ac- tively around the points where each domain changes, and af- terward, adaptation happens intermittently or almost not at all. The light pink bars represent the performance ofDirect- Test, showing that domains with initially high model per- formance tend to have less adaptation, while domains with lower performance initially need more adaptation. In other words, the amount of skipping adaptation is proportional to the amount of the domain shift. Interestingly, the second do- main, ’Shot Noise’, shows almost no adaptation despite the lower performance of the Direct-Test. We conjecture that the preceding domain, ’Gaussian Noise’, shares a similar nature of noise, leading the model to decide that additional adaptation steps may not be necessary. As a result, our skip- ping strategy enables the model to efficiently adapt, consid- ering both the original domain the model is trained on and the previous domain the model has been adapted to. 5. Conclusion We introduce an efficient Continual Test-time Adaptation (CTA) method for object detection in the continually chang- ing domain. Our approach involves 1) lightweight adap- tors, 2) class-wise object-level feature alignment, and 3) skipping unnecessary adaptation. These contributions col- lectively yield a highly efficient and effective adaptation method, showcasing robustness to diverse domain shifts, and achieving notable improvements in mAP performance across various CTA scenarios without serious slowdown in the inference speed. 8What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Supplementary Material 6. Additional Details for Baselines We provide additional implementation details for each base- line model. Our framework incorporates all baseline models using the official code except Mean-Teacher. The results of the experiments are reported based on the optimal hyperpa- rameters that yield the best results in our scenario. ActMAD [29] As ActMAD exclusively conducts experi- ments on the KITTI dataset, where all images have a con- stant height and width (e.g., 370 x 1224), ensuring consis- tent feature map sizes for all samples. ActMAD can easily align them along the spatial axis. However, in the general setting of object detection tasks, such as the COCO bench- mark set, where image sizes and width-to-height ratios vary, aligning feature maps along the spatial axis becomes chal- lenging due to different sizes. To adapt ActMAD to our COCO → COCO-C scenario, we perform center cropping on the feature maps to match the size of training domain fea- ture maps and the current test sample feature maps. We em- ploy a learning rate of 1e-5 for COCO and 1e-4 for SHIFT, respectively. Mean-Teacher As the official code of TeST [36] is not available, we implement the EMA-updated Teacher and Student models following TeST [36], to conduct experi- ments in our scenarios. TeST involves three forward steps for a batch: forwarding weakly augmented samples through the student network, strong augmented samples through the teacher network, and original samples through the teacher network for outputs. However, for a fair comparison, we perform two forward steps, forwarding the original sample through the teacher network and strong augmented sam- ples through the student network, to make predictions be- fore adaptation for every samples. We utilize a learning rate of 1e-5 and set the EMA update rate for the teacher network to 0.999. NORM [35] We set the hyperparameter N that controls the trade-off between training statistics and estimated tar- get statistics as 128. DUA [28] We set the momentum decay as 0.94, minimum momentum constant as 1e-4, and the initial momentum de- cay as 1e-3. 7. The effect of Bottleneck Reduction Ratio in the Adaptor Table 5 shows the results for COCO → COCO-C, SHIFT- Discrete, and SHIFT-Continuous based on the dimension reduction ratio ( r) discussed in Section 3.2, representing Table 5. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to r of Sec. 3.2, the bottleneck reduction ratio in the adaptor. We set r as 32 for all our experiments in the main paper. SD / SC denotes SHIFT-Discrete / Continuous, respectively. mAP # Params Cache Backbone r COCO SD SC Num Ratio Avg. Max Swin-T 1 22.6 40.0 21.3 4.33M 15.7% 0.75 7.51 2 22.6 40.3 23.2 2.17M 7.85% 0.73 7.27 4 22.6 40.4 23.2 1.09M 3.95% 0.70 7.06 8 22.6 40.4 23.2 0.55M 2.00% 0.69 7.00 16 22.6 40.4 23.2 0.28M 1.02% 0.67 6.98 32 22.6 40.4 23.2 0.15M 0.54% 0.65 6.96 64 22.6 40.4 23.2 0.08M 0.29% 0.65 6.95 ResNet50 1 22.5 38.7 20.8 6.31M 26.7% 1.55 5.89 2 22.4 38.7 20.9 3.16M 13.4% 1.51 5.64 4 22.3 38.6 21.3 1.59M 6.71% 1.49 5.52 8 22.3 38.6 21.4 0.80M 3.39% 1.48 5.46 16 22.2 38.6 21.4 0.41M 1.73% 1.48 5.43 32 22.2 38.7 21.4 0.21M 0.89% 1.48 5.41 64 22.1 38.7 21.3 0.11M 0.48% 1.48 5.40 the ratio of bottleneck size compared to the input size in the adaptor. The adaptation performance remains consistent across different r values. However, in the case of r = 1 in SHIFT experiments, mAP decreases, potentially due to catastrophic forgetting resulting from a large number of adaptable parameters. Since increasing the value of r sig- nificantly reduces the number of learnable parameters and memory usage, we set r to 32 in all other experiments. 8. Results on the KITTI Dataset We conduct additional experiments on the KITTI [8] dataset, the commonly used object detection dataset consist- ing of driving scenes with 8 classes (car, van, truck, person, person sitting, cyclist, tram, misc). To simulate the continu- ally changing domains, we use the following scenario ( Fog → Rain → Snow → Clear) as done in [29]. We use the physics-based rendered dataset [9] forfog and rain and sim- ulate snow using the corruption library from [11]. We use the same split of [29], which divides the 7,441 training sam- ples into 3,740 training and 3,741 test samples. We train the Faster-RCNN using 3,741 training samples representing the Clear attribute with Swin-Transformer and ResNet50 back- bones, and evaluate it sequentially on Fog, Rain, Snow, and Clear test samples. We conduct all experiments with a batch size of 16 on 1 RTX A6000 GPU. Table 6 shows the mAP@50, the num- 1Table 6. Comparison of mAP, the number of backward and forward passes, FPS, and memory usage between baselines and our models on the continually changing KITTI datasets ( Fog → Rain → Snow → Clear). Our models improve mAP@50 by 15.1 and 11.3 for Swin-T and ResNet50 backbone, respectively, compared to Direct-Test while maintaining comparable FPS. All experiments are conducted with a batch size of 16. mAP@50 # For. Steps # Backward Steps FPS Cache Backbone Method Fog Rain Snow Clear Avg. All Fog Rain Snow Clear All Avg. Avg. Max Swin-T Direct-Test 46.9 69.5 28.7 89.6 58.7 936 0 0 0 0 0 24.7 0.4 5.5 ActMAD 53.3 78.1 41.2 90.7 65.8 936 234 234 234 234 936 16.8 0.8 21.9 Mean-Teacher 54.5 80.2 43.2 92.4 67.6 936 234 234 234 234 936 10.0 1.0 22.6 Ours 56.7 82.1 64.6 91.8 73.8 936 234 234 234 234 936 17.1 0.4 11.8 Ours-Skip 57.4 81.5 64.3 91.3 73.6 936 234 65 224 36 559 22.9 0.4 11.8 ResNet50 Direct-Test 33.4 63.5 29.8 88.6 53.8 936 0 0 0 0 0 27.7 0.8 4.3 NORM 38.4 66.4 35.9 87.3 57.0 936 0 0 0 0 0 27.7 0.8 4.3 DUA 34.8 67.7 30.9 89.0 55.6 936 0 0 0 0 0 27.7 0.8 4.3 ActMAD 40.4 66.5 42.7 84.5 58.5 936 234 234 234 234 936 18.5 1.6 22.6 Mean-Teacher 39.6 71.3 43.5 88.2 60.6 936 234 234 234 234 936 11.1 1.8 31.1 Ours 45.6 71.4 52.5 88.3 64.5 936 234 234 234 234 936 18.8 0.8 9.4 Ours-Skip 45.8 71.3 50.9 88.4 64.1 936 234 111 98 45 488 24.5 0.8 9.4 (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 6. Results of COCO images corrupted by Shot-Noise. In the analysis of Sec. 4.5, we conjecture that Ours largely skips adaptation in Shot-Noise domain, despite the low mAP of Direct-Test, because the model has already adapted to a similar domain, Gaussian-Noise. In (c), at the first step before adaptation to the Shot-Noise, our model already predicts ’Oven’ and ’Refrigerator’ which Direct-Test fails to detect. This results in a much faster adaptation, and Ours successfully detects various objects, including rare ones such as ’Fire Hydrants’, in the remaining images of the Shot-Noise domain. ber of forward and backward steps, FPS, and memory usage (Cache). Ours improves the mAP@50 by 15.1 and 10.7 for Swin-T and ResNet50 backbones, respectively, compared to Direct-Test. Compared to ActMAD and Mean-Teacher, our model not only improves the adaptation performance but also reduces memory usage, as we update only an ex- tremely small number of parameters of the adaptor. Further- more, using our skipping criteria of Sec. 3.4 with τ = 1.1 and β = 1.05, we can improve FPS by more than 5.8 with- out sacrificing mAP@50, resulting in much faster inference 2(a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 7. Results for COCO images corrupted by Pixelate. In the Pixelate domain, where the model has already experienced various corruptions in a long sequence, Ours initially incorrectly detects objects. In (c), it misidentifies a bed as a couch in the first step. However, it rapidly adapts to the Pixelate domain and effectively detects various objects. Notably, even in cases whereDirect-Testcorrectly identifies objects but with low confidence, Ours detects them with much higher confidence. (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 8. Results for SHIFT-Discrete with continually changing attributes, foggy → rainy → dawn → night. speed compared to other TTA baselines. 39. Qualitative Results Fig. 6 and 7 and Fig. 8 show the qualitative results of Ours and Direct-Test which predict the samples without adapta- tion for COCO → COCO-C and SHIFT, respectively. 9.1. COCO → COCO-C Fig. 6 and 7 compare the prediction results for COCO im- ages corrupted. When the model encounters test images with various corruptions sequentially ( Gaussian-Noise → Shot-Noise → Impulse-Noise → Defocus-Blur → Glass- Blur → Motion-Blur → Zoom-Blur → Snow → Frost → Fog → Brightness → Contrast → Elastic-Transform → Pixelate → JPEG-Compression → Original), Fig. 6 and 7 shows the results when the test images are corrupted by Shot-Noise and Pixelate, respectively. Compared to Direct- Test, our model adapts to the current domain within a few steps, such as 100 iterations, and detects various objects very well in the remaining incoming images. 9.2. SHIFT-Discrete Fig. 8 shows the qualitative results for SHIFT-Discrete. In the SHIFT-Discrete scenario, the model encounters environ- ments sequentially, transitioning from cloudy → overcast → foggy → rainy → dawn → night → clear. Figure. 8 se- lectively shows the foggy → rainy → dawn → night se- quence, where the domain gap from the original clear envi- ronments is relatively large. Compared to Direct-Test, Ours detects various objects such as ’cars’ and ’pedestrians’ re- gardless of distribution changes. References [1] Alexander Bartler, Florian Bender, Felix Wiewel, and Bin Yang. Ttaps: Test-time adaption by aligning prototypes using self-supervision. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2022. 2 [2] Alexander Bartler, Andre B ¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In International Conference on Artificial Intelligence and Statistics , pages 3080–3090. PMLR, 2022. 2 [3] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 8344–8353, 2022. 1 [4] Dhanajit Brahma and Piyush Rai. A probabilistic frame- work for lifelong test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3582–3591, 2023. 3 [5] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition.Advances in Neural Information Processing Systems, 35:16664–16678, 2022. 3 [6] Yijin Chen, Xun Xu, Yongyi Su, and Kui Jia. Stfar: Im- proving object detection robustness at test-time by self- training with feature alignment regularization.arXiv preprint arXiv:2303.17937, 2023. 1, 2, 3 [7] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Un- biased mean teacher for cross-domain object detection. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4091–4101, 2021. 3 [8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The Inter- national Journal of Robotics Research , 32(11):1231–1237, 2013. 1 [9] Shirsendu Sukanta Halder, Jean-Franc ¸ois Lalonde, and Raoul de Charette. Physics-based rendering for improving robustness to rain. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 10203–10212, 2019. 1 [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5, 6, 7 [11] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. 1 [12] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. Mecta: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2022. 3 [13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3 [14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In International conference on machine learn- ing, pages 448–456. pmlr, 2015. 2 [15] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for modelagnostic domain generaliza- tion. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 3 [16] Minguk Jang, Sae-Young Chung, and Hye Won Chung. Test- time adaptation via self-training with nearest neighbor infor- mation. In International Conference on Learning Represen- tations (ICLR), 2023. 3 [17] Sanghun Jung, Jungsoo Lee, Nanhee Kim, Amirreza Sha- ban, Byron Boots, and Jaegul Choo. Cafa: Class-aware fea- ture alignment for test-time adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, 2023. 2, 3, 4 [18] Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, and William G Macready. A robust learning approach to domain adaptive object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 480– 490, 2019. 3 [19] Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, and Chang- ick Kim. Self-training and adversarial background regular- ization for unsupervised domain adaptive one-stage object 4detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6092–6101, 2019. 3 [20] Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsuper- vised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelli- gence, pages 8474–8481, 2021. 4 [21] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical do- main adaptation, 2017. 3 [22] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. Ttn: A domain-shift aware batch normalization in test- time adaptation, 2023. 2, 3 [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 5 [24] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyra- mid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 2117–2125, 2017. 5 [25] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems , 34: 21808–21820, 2021. 1, 2 [26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. 2, 5, 6, 7 [27] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking ro- bustness in object detection: Autonomous driving when win- ter is coming. arXiv preprint arXiv:1907.07484, 2019. 5 [28] M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsuper- vised domain adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 14765–14775, 2022. 3, 6, 1 [29] Muhammad Jehanzeb Mirza, Pol Jan ´e Soneira, Wei Lin, Ma- teusz Kozinski, Horst Possegger, and Horst Bischof. Act- mad: Activation matching to align distributions for test-time- training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 24152– 24161, 2023. 1, 2, 3, 4, 6 [30] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In Interna- tional conference on machine learning, pages 16888–16905. PMLR, 2022. 1, 2, 3, 7 [31] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. arXiv preprint arXiv:2302.12400, 2023. 3 [32] Mario obler, Robert A Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 7704–7714, 2023. 3 [33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. 2016. 5 [34] Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin, Huaizu Jiang, Liangliang Cao, and Erik Learned-Miller. Automatic adaptation of object detectors to new domains using self-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3 [35] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in neural information processing sys- tems, 33:11539–11551, 2020. 3, 6, 1 [36] Samarth Sinha, Peter Gehler, Francesco Locatello, and Bernt Schiele. Test: Test-time self-training under distribution shift. In Proceedings of the IEEE/CVF Winter Conference on Ap- plications of Computer Vision, pages 2759–2769, 2023. 1, 2, 3, 6 [37] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596– 608, 2020. 2, 6 [38] Yongyi Su, Xun Xu, and Kui Jia. Revisiting realistic test- time training: Sequential inference and adaptation by an- chored clustering. Advances in Neural Information Process- ing Systems, 35:17543–17555, 2022. 3, 4 [39] Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc Van Gool, Bernt Schiele, Federico Tombari, and Fisher Yu. Shift: a synthetic driving dataset for continuous multi-task domain adaptation. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 21371–21382, 2022. 1, 5 [40] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229– 9248. PMLR, 2020. 1, 2 [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [42] Vibashan VS, Poojan Oza, and Vishal M Patel. Towards on- line domain adaptive object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 478–488, 2023. 3 [43] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020. 1, 2, 3, 7 5[44] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022. 3 [45] Zehao Xiao, Xiantong Zhen, Shengcai Liao, and Cees GM Snoek. Energy-based test sample adaptation for domain gen- eralization. arXiv preprint arXiv:2302.11215, 2023. 3 [46] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End- to-end semi-supervised object detection with soft teacher. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 5 [47] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems , 35: 38629–38642, 2022. 1, 2, 3 [48] Bowen Zhao, Chen Chen, and Shu-Tao Xia1. Delta: Degradation-free fully test-time adaptation. In International Conference on Learning Representations (ICLR), 2023. 2, 3 6",
      "meta_data": {
        "arxiv_id": "2312.08875v1",
        "authors": [
          "Jayeon Yoo",
          "Dongkwan Lee",
          "Inseop Chung",
          "Donghyun Kim",
          "Nojun Kwak"
        ],
        "published_date": "2023-12-12T07:13:08Z",
        "pdf_url": "https://arxiv.org/pdf/2312.08875v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes a novel online adaptation approach for object detection in continually changing test domains, addressing 'what, how, and when' to update the model. It introduces architecture-agnostic, lightweight adaptor modules (0.54%~0.89% additional parameters) that are updated while the pre-trained backbone remains frozen, effectively preventing catastrophic forgetting. A practical class-wise feature aligning method is proposed for object detection, combining image-level and object-level feature distribution alignment using mean and variance with Exponentially Moving Average (EMA). Additionally, two novel criteria are introduced to dynamically determine when the model requires adaptation or when adaptation can be skipped, enhancing efficiency. The approach significantly improves mAP by up to 4.9%p on COCO \" COCO-corrupted and 7.9%p on SHIFT benchmarks, while maintaining high inference speeds (around 20 FPS or higher) and reducing backward passes by up to 90.5%.",
        "methodology": "The proposed Continual Test-time Adaptation for Object Detectors (CTAOD) method involves three core components. First, for 'what to update,' lightweight adaptor modules are introduced in parallel for each block of the backbone (e.g., MLP for Transformer, 1x1 CNN for ResNet). These adaptors are the only parameters updated during test-time adaptation, preserving the original backbone parameters. Second, for 'how to update,' an EMA feature alignment strategy is employed. This includes both image-level feature alignment (minimizing KL divergence between training and test feature distributions, where test mean is estimated via EMA and test variance is approximated by training variance for stability) and region-level class-wise feature alignment. The object-level alignment uses RoI pooled features, filters background scores, and assigns to foreground classes, incorporating a weighting scheme for class imbalance. The total loss combines these image-level and object-level losses. Third, for 'when to update,' two criteria enable 'adaptation on demand': (1) updating when the distribution gap (Limg) between test and train features exceeds the in-domain distribution gap by a threshold (τ1), and (2) updating when the distribution gap suddenly increases, indicated by its ratio to its EMA (exceeding τ2). Adaptation is performed if at least one criterion is met.",
        "experimental_setup": "The method is evaluated using Faster-RCNN models with ResNet50 and Swin-Tiny backbones, equipped with FPN. Experiments are conducted on three scenarios: COCO \": COCO-C (MS-COCO with 15 types of realistic corruptions for continuous and drastic domain changes, and original COCO validation for knowledge preservation) and SHIFT-Discrete/Continuous (a synthetic driving dataset with 6 classes under various weather and time-of-day attributes, simulating drastic and gradual continuous changes). An additional experiment is performed on the KITTI dataset (driving scenes with 8 classes) to simulate Fog \": Rain \": Snow \": Clear transitions. Hyperparameters include a learning rate of 0.001 for SGD, EMA α of 0.01, and skipping criteria thresholds τ1 = 1.1 and τ2 = 1.05. Batch size is 4 for main experiments and 16 for KITTI. Baselines include Direct-Test (no adaptation), ActMAD, Mean-Teacher (implemented similar to TeST), and for ResNet-based models, NORM and DUA. Performance is measured by mAP, number of forward/backward passes, FPS, and memory usage.",
        "limitations": "The approximation of the test feature variance (Σte ≃ Σtr) to reduce instability is made due to the typically small batch sizes in object detection, which might limit adaptation effectiveness if test domain variance deviates significantly from the training domain's. Furthermore, the thresholds (τ1 and τ2) for determining when to skip adaptation are manually set to balance mAP and FPS, implying a reliance on hyperparameter tuning that may not generalize optimally across all diverse scenarios or datasets.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "NOTE: Robust Continual Test-time Adaptation Against Temporal Correlation",
      "abstract": "Test-time adaptation (TTA) is an emerging paradigm that addresses\ndistributional shifts between training and testing phases without additional\ndata acquisition or labeling cost; only unlabeled test data streams are used\nfor continual model adaptation. Previous TTA schemes assume that the test\nsamples are independent and identically distributed (i.i.d.), even though they\nare often temporally correlated (non-i.i.d.) in application scenarios, e.g.,\nautonomous driving. We discover that most existing TTA methods fail\ndramatically under such scenarios. Motivated by this, we present a new\ntest-time adaptation scheme that is robust against non-i.i.d. test data\nstreams. Our novelty is mainly two-fold: (a) Instance-Aware Batch Normalization\n(IABN) that corrects normalization for out-of-distribution samples, and (b)\nPrediction-balanced Reservoir Sampling (PBRS) that simulates i.i.d. data stream\nfrom non-i.i.d. stream in a class-balanced manner. Our evaluation with various\ndatasets, including real-world non-i.i.d. streams, demonstrates that the\nproposed robust TTA not only outperforms state-of-the-art TTA algorithms in the\nnon-i.i.d. setting, but also achieves comparable performance to those\nalgorithms under the i.i.d. assumption. Code is available at\nhttps://github.com/TaesikGong/NOTE.",
      "full_text": "NOTE: Robust Continual Test-time Adaptation Against Temporal Correlation Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee KAIST Daejeon, South Korea {taesik.gong,jongheonj,maxkim139,yewon.e.kim,jinwoos,profsj}@kaist.ac.kr Abstract Test-time adaptation (TTA) is an emerging paradigm that addresses distributional shifts between training and testing phases without additional data acquisition or labeling cost; only unlabeled test data streams are used for continual model adap- tation. Previous TTA schemes assume that the test samples are independent and identically distributed (i.i.d.), even though they are often temporally correlated (non-i.i.d.) in application scenarios, e.g., autonomous driving. We discover that most existing TTA methods fail dramatically under such scenarios. Motivated by this, we present a new test-time adaptation scheme that is robust against non-i.i.d. test data streams. Our novelty is mainly two-fold: (a) Instance-Aware Batch Normal- ization (IABN) that corrects normalization for out-of-distribution samples, and (b) Prediction-balanced Reservoir Sampling (PBRS) that simulates i.i.d. data stream from non-i.i.d. stream in a class-balanced manner. Our evaluation with various datasets, including real-world non-i.i.d. streams, demonstrates that the proposed ro- bust TTA not only outperforms state-of-the-art TTA algorithms in the non-i.i.d. set- ting, but also achieves comparable performance to those algorithms under the i.i.d. assumption. Code is available at https://github.com/TaesikGong/NOTE. 1 Introduction While deep neural networks (DNNs) have been successful in several applications, their performance degrades under distributional shifts between the training data and test data [32]. This distributional shift hinders DNNs from being widely deployed in many risk-sensitive applications, such as au- tonomous driving, medical imaging, and mobile health care, where new types of test data unseen during training could result in undesirable disasters. For instance, Tesla Autopilot has caused 12 “deaths” until recently [2]. To address this problem, test-time adaptation (TTA) aims to adapt DNNs to the target/unseen domain with only unlabeled test data streams, without any additional data acqui- sition or labeling cost. Recent studies reported that TTA is a promising, practical direction to mitigate distributional shifts [29, 33, 41, 4, 44]. Prior TTA studies typically assume (implicitly or explicitly) that a target test sample xt at time tand the corresponding ground-truth label yt (unknown to the learner) are independent and identically distributed (i.i.d.) following a target domain, i.e.,(xt,yt) is drawn independently from a time-invariant distribution PT(x,y). However, the distribution of online test samples often changes across the time axis, i.e., (xt,yt) ∼PT(x,y |t) in many applications; for instance, AI-powered self-driving car’s object encounter will be dominated by cars while driving on the highway, but less dominated by them on downtown where other classes such as pedestrians and bikes are visible. In human activity recognition, some activities last for a short term (e.g., a fall down), whereas certain activities last longer (e.g., a sleep). Figure 1 illustrates that some data distributions in the real world, such as autonomous driving and human activity recognition, are often temporally correlated. Considering that most 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2208.05117v3  [cs.LG]  11 Jan 2023… … … 𝑡𝑖𝑚𝑒 …… Car Cyclist Pedestrian Van Misc Person (sitting) (a) KITTI dataset. 𝑡𝑖𝑚𝑒 …… Running Walking Standing (b) HARTH dataset. Figure 1: Illustration of test sample distributions along the time axis from real-world datasets: (a) autonomous driving (KITTI [9]) and (b) human activity recognition (HARTH [ 25]). They are temporally correlated. /uni0000004b/uni00000010/uni0000004b/uni00000010/uni00000046/uni00000010/uni00000050/uni00000051/uni00000050/uni0000000f/uni0000004b/uni00000010/uni0000004b/uni00000010/uni00000046/uni00000010 /uni00000012 /uni00000014/uni00000012 /uni00000016/uni00000012 /uni00000018/uni00000012 /uni0000001a/uni00000012/uni00000025/uni0000004e/uni00000043/uni00000055/uni00000055/uni0000004b/uni00000048/uni0000004b/uni00000045/uni00000043/uni00000056/uni0000004b/uni00000051/uni00000050/uni00000002/uni00000047/uni00000054/uni00000054/uni00000051/uni00000054/uni0000000a/uni00000007/uni0000000b /uni00000032/uni00000047/uni00000054/uni00000048/uni00000051/uni00000054/uni0000004f/uni00000043/uni00000050/uni00000045/uni00000047/uni00000002/uni00000031/uni00000058/uni00000047/uni00000054/uni00000058/uni0000004b/uni00000047/uni00000059 /uni00000035/uni00000051/uni00000057/uni00000054/uni00000045/uni00000047 /uni00000024/uni00000030/uni00000002/uni00000035/uni00000056/uni00000043/uni00000056/uni00000055 /uni00000031/uni00000030/uni00000026/uni00000023 /uni00000032/uni0000002e /uni00000036/uni00000027/uni00000030/uni00000036 /uni00000025/uni00000051/uni00000036/uni00000036/uni00000023 N O T E Figure 2: Average classiﬁcation error (%) of ex- isting TTA methods and our method (NOTE) on CIFAR10-C [13]. The error rates signiﬁcantly increase under the non-i.i.d. setting compared with the i.i.d. setting. Lower is better. existing TTA algorithms simply use an incoming batch of test samples for adaptation [29, 33, 41, 44], the model might be biased towards these imbalanced samples under the temporally correlated test streams. Figure 2 compares the performance of the state-of-the-art TTA algorithms under the i.i.d. and non-i.i.d.1 conditions. While the TTA methods perform well under the i.i.d. assumption, their errors increase under the non-i.i.d. case. Adapting to temporally correlated test data results in overﬁtting to temporal distributions, which in turn harms the generalization of the model. Motivated by this, we present a NOn-i.i.d. TEst-time adaptation scheme, NOTE, that consists of two components: (a) Instance-Aware Batch Normalization (IABN) and (b) Prediction-Balanced Reservoir Sampling (PBRS). First, we propose a novel normalization layer, IABN, that eliminates the dependence on temporally correlated data for adaptation while being robust to distribution shifts. IABN detects out-of-distribution instances sample by sample and corrects via instance-aware normalization. The key idea of IABN is synthesizing Batch Normalization (BN) [16] with Instance Normalization (IN) [37] in a unique way; it calculates how different the learned knowledge (BN) is from the current observation (IN) and corrects the normalization by the deviation between IN and BN. Second, we present PBRS that resolves the problem of overﬁtting to non-i.i.d. samples by mimicking i.i.d. samples from non-i.i.d. streams. By utilizing predicted labels of the model, PBRS aims for both time-uniform sampling and class-uniform sampling from the non-i.i.d. streams and stores the ‘simulated’ i.i.d. samples in memory. With the i.i.d.-like batch in the memory, PBRS enables the model to adapt to the target domain without being biased to temporal distributions. We evaluate NOTE with state-of-the-art TTA baselines [29, 33, 22, 27, 4, 44] on multiple datasets, including common TTA benchmarks (CIFAR10-C, CIFAR100-C, and ImageNet-C [13]) and real- world non-i.i.d. datasets (KITTI [9], HARTH [25], and ExtraSensory [38]). Our results suggest that NOTE not only signiﬁcantly outperforms the baselines under non-i.i.d. test data, e.g., it achieves a 21.1% error rate on CIFAR10-C which is on average 15.1% lower than the state-of-the-art method [4], but also shows comparable performance even under the i.i.d. assumption, e.g., 17.6% error on CIFAR10-C where the best baseline [44] achieves 17.8% error. Our ablative study demonstrates the individual effectiveness of IABN and PBRS and further highlights their synergy when jointly used. Finally, we summarize the key characteristics of NOTE. First, NOTE is a batch-free inference algorithm (requiring a single instance for inference), different from the state-of-the-art TTA algo- rithms [29, 33, 41, 4, 44] where a batch of test data is necessary for inference to estimate normalization statistics (mean and variance). Second, while some recent TTA methods leverage augmentations to improve performance at the cost of additional forwarding passes [ 35, 44], NOTE requires only a single forwarding pass. NOTE updates only the normalization statistics and afﬁne parameters in IABN, which is, e.g., approximately 0.02% of the total trainable parameters in ResNet18 [12]. Third, NOTE’s additional memory overhead is negligible. It merely stores predicted labels of test data to run PBRS. These characteristics make NOTE easy to apply to any existing AI system and particularly, are beneﬁcial in latency-sensitive tasks such as autonomous driving and human health monitoring. 1We use the terms temporally correlated and non-i.i.d. interchangeably in the context of test-time adaptation. 22 Background 2.1 Problem setting: test-time adaptation with non-i.i.d. streams Test-time adaptation.Let DS= {XS,Y}be the data from the source domain and DT = {XT,Y} be the data from the target domain to adapt to. Each data instance and the corresponding ground-truth label pair (xi,yi) ∈X S ×Y in the source domain follows a probability distribution PS(x,y). Similarly, each target test sample and the corresponding label at test time t, (xt,yt) ∈X T ×Y, follows a probability distributionPT(x,y) where ytis unknown for the learner. The standard covariate shift assumption in domain adaptation is deﬁned as PS(x) ̸= PT(x) and PS(y|x) = PT(y|x) [32]. Unlike traditional domain adaptation that uses DS and XT collected beforehand for adaptation, test-time adaptation (TTA) continually adapts a pre-trained model fθ(·) from DS, by utilizing only xt obtained at test time t. TTA on non-i.i.d. streams.Note that previous TTA mechanisms typically assume that each target sample (xt,yt) ∈XT×Yis independent and identically distributed (i.i.d.) following a time-invariant distribution PT(x,y). However, the data obtained at test time is non-i.i.d. in many scenarios. By non-i.i.d., we refer to distribution changes over time, i.e., (xt,yt) ∼PT(x,y |t), which is a practical setting in many real world applications [46]. 2.2 Batch normalization in TTA Batch Normalization (BN) [16] is a widely-used training technique in deep neural networks as it reduces the internal covariant shift problem. Let f ∈RB×C×L denote a batch of feature maps in general, where B, C, and Ldenote the batch size, the number of channels, and the size of each feature map, respectively. Given the statistics of the feature maps for normalization, say mean µ and variance σ2, BN is channel-wise, i.e., µ,σ2 ∈RC and computes: BN(f:,c,:; µc,σ2 c) := γ·f:,c,: −µc√ σ2c + ϵ + β, (1) where γand βare the afﬁne parameters followed by the normalization, and ϵ> 0 is a small constant for numerical stability. Although a conventional way of computing BN in test-time is to set µ and σ2 as those estimated from training (or source) data, say ¯µ and ¯σ2, the state-of-the-art TTA methods based on adapting BN layers [29, 33, 41, 44] instead use the statistics computed directly from the recent test batch to de-bias distributional shifts at test-time, i.e.: ˆµc := 1 BL ∑ b,l fb,c,l, and ˆσ2 c := 1 BL ∑ b,l (fb,c,l −ˆµc)2. (2) This practice is simple yet effective under distributional shifts and is thus adopted in many recent TTA studies [29, 33, 41, 44]. Based on the test batch statistics, they often further adapt the afﬁne parameters via entropy minimization of the model outputs [41] or update the entire parameters with self-training [44]. 3 Method In the same vein as previous work [29, 33, 41], we focus on adapting BN layers in the given model to perform TTA, and this includes essentially two approaches: (a) re-calibrating (or adapting) channel- wise statistics for normalization (instead of using those learned from training), and (b) adapting the afﬁne parameters (namely, γand β) after the normalization with respect to a certain objective based on test samples, e.g., the entropy minimization of model outputs [41]. Under scenarios where test data are temporally correlated, however, naïvely adapting to the incoming batch of test samples [29, 33, 41, 44] could be problematic for both approaches: the batch is now more likely to (a) remove instance-wise variations that are actually useful to predict y, i.e., the “contents” rather than “styles” through normalization, and (b) include a bias in p(y) rather than uniform, which can negatively affect the test-time adaptation objective such as entropy minimization. 3Case1: Out of distribution Case2: In distribution 𝐈𝐈𝐈𝐈 𝐈𝐈𝐈𝐈 �𝝁𝝁c, �𝝈𝝈c �𝝁𝝁b,c, �𝝈𝝈b,c 𝐈𝐈𝐈𝐈: learned stats 𝐈𝐈𝐈𝐈: instance stats �𝝁𝝁b,c, �𝝈𝝈b,c�𝝁𝝁c, �𝝈𝝈c Non-i.i.d.  (temporally  correlated) 𝒙𝒙𝑡𝑡 𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 … (1) Normalize & Predict via 𝐈𝐈𝐈𝐈𝐈𝐈𝐈𝐈 (2) Manage Memory & Adapt IABN via 𝐏𝐏𝐈𝐈𝐏𝐏𝐏𝐏 𝐈𝐈𝐈𝐈 𝐈𝐈𝐈𝐈𝐈𝐈𝐈𝐈𝐈𝐈𝐈𝐈 �𝝁𝝁c, �𝝈𝝈c �𝝁𝝁b,c, �𝝈𝝈b,c 𝐈𝐈𝐈𝐈 𝐈𝐈𝐈𝐈𝐈𝐈𝐈𝐈 𝐈𝐈𝐈𝐈 �𝝁𝝁b,c, �𝝈𝝈b,c�𝝁𝝁c, �𝝈𝝈c = Correct BN stats Use BN stats 𝐈𝐈𝐈𝐈𝐈𝐈𝐈𝐈 = 𝐈𝐈𝐈𝐈+ 𝝍𝝍(𝐈𝐈𝐈𝐈−𝐈𝐈𝐈𝐈) 𝝍𝝍∗: soft-shrinkage 𝝁𝝁𝑡𝑡 ← 𝝁𝝁𝑡𝑡−1, �𝝁𝝁𝑡𝑡 𝝈𝝈𝑡𝑡 ← 𝝈𝝈𝑡𝑡−1, �𝝈𝝈𝑡𝑡 �𝝁𝝁𝑡𝑡, �𝝈𝝈𝑡𝑡 Time-uniform (𝒙𝒙𝑖𝑖, �𝑦𝑦𝑖𝑖) Prediction-uniform Old New Class 𝒃𝒃 Class 𝒂𝒂 Class 𝒄𝒄 (𝒙𝒙𝑡𝑡, �𝑦𝑦𝑡𝑡) Adapt learned stats 𝒙𝒙𝑡𝑡 ′ = 𝐈𝐈𝐈𝐈𝐈𝐈𝐈𝐈(𝒙𝒙𝑡𝑡) (𝝁𝝁𝑡𝑡, 𝝈𝝈𝒕𝒕)(𝝁𝝁𝑡𝑡−1, 𝝈𝝈𝑡𝑡−1) Normalized  output  Figure 3: An overview of the proposed methodology: Instance-Aware Batch Normalization (IABN) and Prediction-Balanced Reservoir Sampling (PBRS). IABN aims to detect non-i.i.d. streams and in turn corrects the normalization for inference. PBRS manages data in a time- and prediction-uniform manner from non-i.i.d. data streams and gradually adapts IABNs with the balanced data afterward. We propose two approaches to tackle each of the failure modes of adapting BN under temporal correlation. Our method consists of two components: (a) Instance-Aware Batch Normalization (IABN) (Section §3.1) to overcome the limitation of BN under distribution shift and (b) Prediction- Balanced Reservoir Sampling (PBRS) (Section §3.2) to combat with the temporal correlation of test batches. Figure 3 illustrates the overall workﬂow of NOTE with IABN and PBRS. 3.1 Instance-Aware Batch Normalization As described in Section §2.2, recent TTA algorithms rely solely on the test batch to re-calculate BN statistics. We argue that this common practice does not successfully capture the feature statistics to normalize the feature map f ∈RB×C×L under temporal correlation in the test batch B. In principle, standardizing a given feature map f:,c,: by the statistics ˆµc,ˆσ2 c computed across Band Lis posited on premise that averaging information across Bcan marginalize out uninformative instance-wise variations for predicting y. Under temporal correlation in B, however, this assumption is no longer valid, and averaging across Bmay not fully de-correlate useful information in f:,c,: from µc and σ2 c. In an attempt to bypass such an “over-whitening” effect of usingˆµcand ˆσ2 c in test-time under temporal correlation, we propose correcting normalization statistics on a per-sample basis: speciﬁcally, instead of completely switching from the original statistics of (¯µ,¯σ2) into (ˆµc,ˆσ2 c), our proposed Instance- Aware Batch Normalization(IABN) considers the instance-wise statistics ˜µ,˜σ2 ∈RB,C of f similarly to Instance Normalization (IN) [37], namely: ˜µb,c := 1 L ∑ l fb,c,l and ˜σ2 b,c := 1 L ∑ l (fb,c,l −˜µb,c)2. (3) We assume that ˜µb,c and ˜σ2 b,c follow the sampling distribution of a sample size Lin N(¯µ,¯σ2) as the population. Then the corresponding variances for the sample mean ˜µb,c and the sample variance ˜σ2 b,c can be calculated as: s2 ˜µ,c := ¯σ2 c L and s2 ˜σ2,c := 2¯σ4 c L−1. (4) IABN corrects (¯µ,¯σ2) only in cases when ˜µb,c (and ˜σ2 b,c) signiﬁcantly differ from ¯µc (and ¯σ2 c). Speciﬁcally, we propose to use the following statistics for TTA: µIABN b,c := ¯µc + ψ(˜µb,c −¯µc; αs˜µ,c), and (σIABN b,c )2 := ¯σ2 c + ψ(˜σ2 b,c −¯σ2 c; αs˜σ2,c), where ψ(x; λ) =    x−λ, if x>λ x+ λ, if x< −λ 0, otherwise is the soft-shrinkage function. (5) 4Algorithm 1Prediction-Balanced Reservoir Sampling Input: target stream xt ∼PT(x|t), memory bank M of capacity N 1: M[i] ←φfor i= 1,···N; and n[c] ←0 for c∈Y 2: for t∈{1,···,T}do 3: n[ˆyt] ←n[ˆyt] + 1 // increase the number of samples encountered for the class 4: m[c] ←|{(x,y) ∈M|y= c}|for c∈Y // count instances per class in memory 5: if |M|<N then // if memory is not full 6: Add (xt,ˆyt) to M 7: else 8: C∗←arg maxc∈Ym[c] // get majority class(es) 9: if ˆyt /∈C∗then // if the new sample is not majority ⊿Prediction-Balanced 10: Randomly pick M[i] := (xi, ˆyi) where ˆyi ∈C∗ 11: M[i] ←(xt,ˆyt) // replace it with a new sample 12: else ⊿Reservoir Sampling 13: Sample p∼Uniform(0,1) 14: if p<m [ˆyt]/n[ˆyt] then 15: Randomly pick M[i] := (xi, ˆyi) where ˆyi = ˆyt 16: M[i] ←(xt,ˆyt) // replace it with a new sample α≥0 is the hyperparameter of IABN that determines the conﬁdence level of the BN statistics. A high value of αrelies more on the learned statistics (BN), while a low value of αis in favor of the current statistics measured from the instance. Finally, the output of IABN can be described as: IABN(fb,c,:; ¯µc,¯σ2 c; ˜µb,c,˜σ2 b,c) := γ· fb,c,: −µIABN b,c√ (σIABN b,c )2 + ϵ + β. (6) Observe that IABN becomes IN and BN when α = 0 and α = ∞, respectively. If one chooses too small α≥0, IABN may remove useful features, e.g., styles, of input (as with IN), which can degrade the overall classiﬁcation (or regression) performance [30]. Hence, it is important to choose an appropriate α. Nevertheless, we found that a good choice of αis not too sensitive across tested scenarios, where we chose α= 4 for all experiments. This way, IABN can be robust to distributional shifts without the risk of eliminating crucial information to predict y. 3.2 Adaptation via Prediction-Balanced Reservoir Sampling Temporally correlated distributions lead to an undesirable bias in p(y), and thus adaptation with a batch of consecutive test samples negatively impacts the adaptation objective, such as entropy minimization [41]. To combat this imbalance, we propose Prediction-Balanced Reservoir Sampling (PBRS) that mimics i.i.d. samples from temporally correlated streams with the assistance of a small (e.g., a mini-batch size) memory. PBRS combines time-uniform sampling and prediction-uniform sampling to simulate i.i.d. samples from the non-i.i.d. streams. For time-uniform sampling, we adopt reservoir sampling (RS) [40], a proven random sampling algorithm to collect time-uniform data in a single pass on a stream without prior knowledge of the total length of data. For prediction-uniform sampling, we ﬁrst use the predicted labels to compute the majority class(es) in the memory. We then replace a random instance of the majority class(es) with a new sample. We detail the algorithm of PBRS as a pseudo-code in Algorithm 1. We found that these two heuristics can effectively balance samples among both time and class axes, which mitigates the bias in temporally correlated data. With the stored samples in the memory, we update the normalization statistics and afﬁne parameters in the IABN layers. Note that IABN assumes ˜µb,c and ˜σ2 b,c follow the sampling distribution of N(¯µ,¯σ2) and corrects the normalization if ˜µb,c and ˜σ2 b,c are out of distribution. While IABN is resilient to distributional shifts to a certain extent, the assumption might not hold under severe distributional shifts. Therefore, we aim to ﬁnd better estimates of ¯µ,¯σ2 in IABN under distributional shifts via PBRS. Speciﬁcally, we update the normalization statistics, namely the means µ and variances σ2, via exponential moving average: (a) µt = (1 −m)µt−1 + m N N−1 ˆµt and (b) σ2 t = (1 −m)σ2 t−1 + m N N−1 ˆσ2 t where mis a momentum and N is the size of the memory. We further 5optimize the afﬁne parameters, scaling factor γ and bias term β, via a single backward pass with entropy minimization, similar to a previous study [41]. These parameters account for only around 0.02% of the total trainable parameters in ResNet18 [ 12]. The IABN layers are adapted with the N samples in the memory every N test samples. We set the memory size N as 64 following the common batch size of existing TTA methods [33, 4, 41] to ensure a fair memory constraint. 3.3 Inference NOTE infers each sample via a single forward pass with IABN layers. Note that NOTE requires only a single instance for inference, different from the state-of-the-art TTA methods [29, 33, 41, 4, 44] that require batches for every inference. Moreover, NOTE requires only one forwarding pass for inference, while multiple forward passes are required in other TTA methods that utilize augmentations [35, 44]. The batch-free single-forward inference of NOTE is beneﬁcial in latency-sensitive tasks such as autonomous driving and human health monitoring. After inference, NOTE determines whether to store the sample and predicted label in the memory via PBRS. 4 Experiments We implemented NOTE and the baselines via the PyTorch framework [31].2 We ran all experiments with three random seeds and report the means and standard deviations. Additional experimental details, e.g., hyperparameters of the baselines and datasets, are speciﬁed in Appendix A. Baselines. We consider the following baselines including the state-of-the-art test-time adaptation algorithms: Source evaluates the model trained from the source data directly on the target data without adaptation. Test-time normalization (BN stats) [29, 33] updates the BN statistics from a batch of test data. Online Domain Adaptation (ONDA) [27] adapts batch normalization statistics to target domains via a batch of target data with an exponential moving average. Pseudo-Label (PL) [22] optimizes the trainable parameters in BN layers via hard pseudo labels. We update the BN layers only in PL, as done in previous studies [41, 44]. Test entropy minimization (TENT) [41] updates the BN parameters via entropy minimization. Laplacian Adjusted Maximum-likelihood Estimation (LAME) [4] takes a more conservative approach; it modiﬁes the classiﬁer’s output probability and not the internal parameters of the model itself. By doing so, it prevents the model parameters from over-adapting to the test batch. Continual test-time adaptation ( CoTTA) [44] reduces the error accumulation by using weight-averaged and augmentation-averaged predictions. It avoids catastrophic forgetting by stochastically restoring a part of the neurons to the source pre-trained weights. Adaptation and hyperparameters. We assume the model pre-trained with source data is available for TTA. In NOTE, we replaced BN with IABN during training. We set the test batch size as 64 and the adaptation epoch as one for adaptation, which is the most common setting among the baselines [33, 4, 41]. Similarly, we set the memory sizeNas 64 and adapt the model every 64 samples in NOTE to ensure a fair memory constraint. We conduct online adaptation and evaluation, where the model is continually updated. For the baselines, we adopt the best values for the hyperparameters reported in their papers or the ofﬁcial codes. We followed the guideline to tune the hyperparameters when such a guideline was available [44]. We use ﬁxed values for the hyperparameters of NOTE, soft-shrinkage width α= 4 and exponential moving average momentum m= 0.01, and update the afﬁne parameters via the Adam optimizer [18] with a learning rate of l= 0.0001 unless speciﬁed. We detailed hyperparameter information of the baselines in Appendix A.1. Datasets. We use CIFAR10-C, CIFAR100-C, and ImageNet-C [ 13] datasets that are common TTA benchmarks for evaluating the robustness to corruptions [ 29, 33, 41, 44, 4]. Both CI- FAR10/CIFAR100 [19] have 50,000/10,000 training/test data. ImageNet [7] has 1,281,167/50,000 training/test data. CIFAR10/CIFAR100/ImageNet have 10/100/1,000 classes, respectively. CIFAR10- C/CIFAR100-C/ImageNet-C apply 15 types of corruption to CIFAR10/CIFAR100/ImageNet test data. Similar to previous studies [29, 33, 41, 44], we use the most severe corruption level of 5. We use ResNet18 [12] as the backbone network and pre-trained it on the clean training data. Following prior studies [23, 15, 43, 42], we adopt Dirichlet distribution to generate synthetic non-i.i.d. test streams 2https://github.com/TaesikGong/NOTE 6Table 1: Average classiﬁcation error (%) and their corresponding standard deviations on CIFAR10- C/100-C and ImageNet-C under temporally correlated (non-i.i.d.) and uniformly distributed (i.i.d.) test data stream. Bold fonts indicate the lowest classiﬁcation errors, while Red fonts show performance degradation after adaptation. Values encompassed by parentheses refer to NOTE used directly with test batches (without using PBRS). Averaged over three runs. Temporally correlated test stream Uniformly distributed test stream Method CIFAR10-C CIFAR100-C ImageNet-C Avg CIFAR10-C CIFAR100-C ImageNet-C Avg Source 42.3 ± 1.1 66.6 ± 0.1 86.1 ± 0.0 65.0 42.3 ± 1.1 66.6 ± 0.1 86.1 ± 0.0 65.0 BN Stats [29] 73.4 ± 1.3 65.0 ± 0.3 96.9 ± 0.0 78.5 21.6 ± 0.4 46.6 ± 0.2 76.0 ± 0.0 48.1 ONDA [27] 63.6 ± 1.0 49.6 ± 0.3 89.0 ± 0.0 67.4 21.7 ± 0.4 46.5 ± 0.1 75.9 ± 0.0 48.0 PL [22] 75.4 ± 1.8 66.4 ± 0.4 98.9 ± 0.0 80.2 21.6 ± 0.2 43.1 ± 0.3 74.4 ± 0.2 46.4 TENT [41] 76.4 ± 2.7 66.9 ± 0.6 96.9 ± 0.0 80.1 18.8 ± 0.2 40.3 ± 0.2 76.0 ± 0.0 45.0 LAME [4] 36.2 ± 1.3 63.3 ± 0.3 82.7 ± 0.0 60.7 44.1 ± 0.5 68.8 ± 0.1 86.3 ± 0.0 66.4 CoTTA [44] 75.5 ± 0.7 64.2 ± 0.2 97.0 ± 0.0 78.9 17.8 ± 0.3 44.3 ± 0.2 71.5 ± 0.0 44.6 NOTE 21.1 ± 0.6 47.0 ± 0.1 80.6 ± 0.1 49.620.1 ± 0.5 (17.6 ± 0.3) 46.4 ± 0.0 (41.0 ± 0.2) 70.3 ± 0.0 (71.7 ± 0.0) 45.6 (43.4) timeClass distribution Dirichlet parameter 𝛿 0.001 0.01 0.1 1.0 10.0 uniform Figure 4: Illustration of syn- thetic non-i.i.d. streams sampled from Dirichlet distribution vary- ing δ on CIFAR10-C. uniform denotes an i.i.d. condition. The lower the δ, the more temporally correlated the distribution. /uni00000057/uni00000050/uni0000004b/uni00000048/uni00000051/uni00000054/uni0000004f/uni00000013/uni00000012/uni00000010/uni00000012/uni00000013/uni00000010/uni00000012/uni00000012/uni00000010/uni00000013/uni00000012/uni00000010/uni00000012/uni00000013/uni00000012/uni00000010/uni00000012/uni00000012/uni00000013 /uni00000026/uni0000004b/uni00000054/uni0000004b/uni00000045/uni0000004a/uni0000004e/uni00000047/uni00000056/uni00000002/uni00000052/uni00000043/uni00000054/uni00000043/uni0000004f/uni00000047/uni00000056/uni00000047/uni00000054/uni00000002 /uni00000014/uni00000012 /uni00000016/uni00000012 /uni00000018/uni00000012 /uni0000001a/uni00000012 /uni00000013/uni00000012/uni00000012/uni00000025/uni0000004e/uni00000043/uni00000055/uni00000055/uni0000004b/uni00000048/uni0000004b/uni00000045/uni00000043/uni00000056/uni0000004b/uni00000051/uni00000050/uni00000002/uni00000047/uni00000054/uni00000054/uni00000051/uni00000054/uni0000000a/uni00000007/uni0000000b /uni00000035/uni00000051/uni00000057/uni00000054/uni00000045/uni00000047 /uni00000024/uni00000030/uni00000002/uni00000035/uni00000056/uni00000043/uni00000056/uni00000055 /uni00000032/uni0000002e /uni00000031/uni00000030/uni00000026/uni00000023 /uni00000036/uni00000027/uni00000030/uni00000036 /uni0000002e/uni00000023/uni0000002f/uni00000027 /uni00000025/uni00000051/uni00000036/uni00000036/uni00000023 NOTE (a) Effect of Dirichlet concen- tration parameter δ. /uni00000013/uni00000018/uni00000015/uni00000014/uni00000018/uni00000016/uni00000013/uni00000014/uni0000001a/uni00000014/uni00000017/uni00000018/uni00000017/uni00000013/uni00000014 /uni00000024/uni00000043/uni00000056/uni00000045/uni0000004a/uni00000002/uni00000055/uni0000004b/uni0000005c/uni00000047 /uni00000014/uni00000012 /uni00000016/uni00000012 /uni00000018/uni00000012 /uni0000001a/uni00000012 /uni00000013/uni00000012/uni00000012/uni00000025/uni0000004e/uni00000043/uni00000055/uni00000055/uni0000004b/uni00000048/uni0000004b/uni00000045/uni00000043/uni00000056/uni0000004b/uni00000051/uni00000050/uni00000002/uni00000047/uni00000054/uni00000054/uni00000051/uni00000054/uni0000000a/uni00000007/uni0000000b (b) Effect of batch size. Figure 5: Average classiﬁcation error (%) under the non-i.i.d. setting with CIFAR10-C dataset. We vary (a) the Dirichlet con- centration parameter δ to investigate the impact of the degree of temporal correlation and (b) batch size to understand the be- haviors of the TTA methods. Averaged over three runs. Lower is better. from the originally i.i.d. CIFAR10/100 data. The details are provided in Appendix A.2. We vary the Dirichlet concentration parameter δto simulate diverse streams and visualize the resulting data in Figure 4. We use δ= 0.1 as the default value unless speciﬁed. For ImageNet, we sort the test stream as the number of test samples per class is not enough for generating temporally correlated streams via Dirichlet distribution. We additionally provide an experiment with MNIST-C data [28] in the appendix, which shows similar takeaways to our experiments with CIFAR10-C/CIFAR100-C/ImageNet-C. Overall result. Tables 1 shows the result under the temporally correlated (non-i.i.d.) data and the uniform (i.i.d.) data, respectively. We observe signiﬁcant performance degradation in the baselines under the temporally correlated setting. For BN Stats, PL, TENT, and CoTTA, this degradation is particularly due to the dependence on the test batch for the re-calculation of the BN statistics. Updating the batch statistics from test data via exponential moving average (ONDA) also suffers from the temporally correlated data. This indicates relying on the test batch for re-calculating the BN statistics indeed cancels out meaningful instance-wise variations under temporal correlation. Interestingly, LAME works better in the non-i.i.d. setting than in the i.i.d. setting, which is consistent with previous reports [4]. The primary reason is, as stated by the authors, it “discourages deviations from the predictions of the pre-trained model,” and thus it “does not noticeably help in i.i.d and class-balanced scenarios.” NOTE achieves on average 11.1% improvement over the best baseline (LAME) under the non-i.i.d. setting. With the i.i.d. assumption, NOTE still achieves comparable performance to the baselines. When we know the target samples are i.i.d., we can simply use the test batch without using PBRS. For 7this variant version of NOTE, we update IABN with incoming test batches directly using a ten times higher learning rate of 0.001 following previous work [41, 44]. We report the result of the variant version of NOTE in the parentheses, which achieves on average 2.2% improvement further when the i.i.d. assumption is known. Effect of the degree of temporal correlation.We also investigate the effect of the degree of temporal correlation for TTA algorithms. Figure 5a shows the result. The lowerδis, the severer the temporal correlation becomes. The error rates of most of the baselines deteriorate as δdecreases, which shows that the existing TTA baselines are susceptible to temporally correlated data. NOTE shows consistent performance among allδvalues, indicating its robustness under temporal correlation. Effect of batch size. While we experiment with a widely-used value of 64 as the batch size (or memory size in NOTE), one might be curious about the impact of batch size under temporally correlated streams. Figure 5b shows the result with six different batch sizes. As shown, NOTE is not much affected by the batch size, while most of the baselines recover performance degradation as the batch size increases. This is because a higher batch size has a better chance of adaptation with balanced samples under temporally correlated streams. Increasing the batch size, however, mitigates temporal correlation at the expense of inference latency and adaptation speed. 4.1 Real-distributions with domain shift Datasets. We evaluate NOTE under three real-world distribution datasets: object detection in autonomous driving (KITTI [9]), human activity recognition (HARTH [25]), and user behavioral context recognition (ExtraSensory [38]). Additional dataset-speciﬁc details are in Appendix A.2. KITTI is a well-known autonomous driving dataset that provides consecutive frames that contains natural temporal correlation in driving contexts. We adapted from KITTI to KITTI-Rain [ 11] - a dataset that converted KITTI images to rainy images. This contains 7,481/7,800 train/test samples with nine classes. We use ResNet50 [12] pre-trained on ImageNet [8] as the backbone network. HARTH was collected from 22 users in free-living environments for seven days. Each user was equipped with two three-axial Axivity AX3 accelerometers for recording human activities. We use 15 users collectively as the source domain and the remaining seven users as each target domain, which entails natural domain shifts from source users to target users as different physical conditions make domain shifts across users. HARTH contains 82,544/39,377 train/test samples with 12 classes. We report the average error over all target domains. We use four one-dimensional convolutional layers followed by one fully-connected layer as the backbone network for HARTH. The Extrasensory dataset collected users’ own smartphone sensory data (motion sensors, audio, etc.) in the wild for seven days, aiming to capture people’s authentic behaviors in their regular activities. We use 16 users as the source domain and seven users as target domains. ExtraSensory includes 17,777/4,862 train/test data with ﬁve classes. For ExtraSensory, we use two one-dimensional convolutional layers followed by one fully-connected layer as the backbone network. For both HARTH and ExtraSensory models, a single BN layer follows each convolutional layer. Result. Table 2 shows the result for the real-world datasets. The overall trend is similar to the tem- poral correlation experiments with CIFAR10-C/CIFAR100-C/ImageNet-C datasets, which indicates that the real-world datasets are indeed temporally correlated. NOTE consistently reduces errors after adaptation under real-world distributions. We believe this demonstrates NOTE is a promising method to be utilized in various real-world ML applications with distributional shifts. We illustrate real-time classiﬁcation error changes for real-world datasets in the appendix. 4.2 Ablation study We conduct an ablative study to further investigate the individual components’ effectiveness. Table 3 shows the result under both i.i.d. and non-i.i.d. settings. Using IABN alone signiﬁcantly reduces error rates over Source, demonstrating the effectiveness of correcting normalization for out-of-distribution samples. Using PBRS with BN shows comparable improvement with the IABN-only result. Note that there is only a marginal gap (around 1%) between the non-i.i.d. and i.i.d. results in PBRS. This indicates that PBRS could effectively simulate i.i.d. samples from non-i.i.d. streams. The joint use of 8Table 2: Average classiﬁcation error (%) and their corresponding standard deviations on three real test data streams: KITTI, HARTH, and ExtraSensory.Bold fonts indicate the lowest classiﬁcation errors, while Red fonts show performance degradation after adaptation. Averaged over three runs. Real test stream Method KITTI HARTH ExtraSensory Avg Source 12.3 ± 2.3 62.6 ± 8.5 50.2 ± 2.2 41.7 BN Stats [29] 35.4 ± 0.5 68.6 ± 1.1 56.0 ± 0.9 53.4 ONDA [27] 26.3 ± 0.5 69.3 ± 1.1 48.2 ± 1.5 47.9 PL [22] 39.0 ± 0.3 64.8 ± 0.6 56.0 ± 0.9 53.3 TENT [41] 39.6 ± 0.2 64.1 ± 0.7 56.0 ± 0.8 53.2 LAME [4] 11.3 ± 2.9 61.0 ± 10.0 50.7 ± 2.7 41.0 CoTTA [44] 35.4 ± 0.6 68.7 ± 1.1 56.0 ± 0.9 53.4 NOTE 10.9 ± 3.6 51.0 ± 5.6 45.4 ± 2.6 35.8 Table 3: Average classiﬁcation error (%) and corresponding standard deviations of varying ablation settings on CIFAR10-C/100-C under temporally correlated (non-i.i.d.) and uniformly distributed (i.i.d.) test data stream. Bold fonts indicate the lowest classiﬁcation errors. Averaged over three runs. Temporally correlated test stream Uniformly distributed test stream Method CIFAR10-C CIFAR100-C Avg CIFAR10-C CIFAR100-C Avg Source 42.3 ± 1.1 66.6 ± 0.1 54.4 42.3 ± 1.1 66.6 ± 0.1 54.4 IABN 24.6 ± 0.6 54.5 ± 0.1 39.5 24.6 ± 0.6 54.5 ± 0.1 39.5 PBRS 27.5 ± 1.0 51.7 ± 0.2 39.6 25.8 ± 0.2 51.3 ± 0.1 38.5 IABN+RS 20.5 ± 1.5 48.2 ± 0.2 34.3 20.7 ± 0.6 48.3 ± 0.3 34.5 IABN+PBRS 21.1 ± 0.6 47.0 ± 0.1 34.0 20.1 ± 0.5 46.4 ± 0.0 33.2 IABN and PBRS outperforms using either of them, meaning that PBRS provides IABN with better estimates for the normalizing operation. In addition, PBRS is better than Reservoir Sampling (RS) that has been a strong baseline in continual learning [17, 5]. This shows storing prediction-balanced sampling in addition to time-uniform sampling leads to better adaptation in TTA. We also investigated the joint use of IN and PBRS with the combination of IABN and PBRS on CIFAR100-C, and the result shows that IABN+PBRS (47.0%) achieves a lower error rate than IN+PBRS (52.5%) on CIFAR100-C under temporal correlation. 5 Related work Test-time adaptation. Test-time adaptation (TTA) attempts to overcome distributional shifts with test data without the cost of data acquisition or labeling. TTA adapts to the target domain with only test data on the ﬂy. Most existing TTA algorithms rely on a batch of test samples to adapt [29, 33, 41, 44] to re-calibrate BN layers on the test data. Simply using the statistics of a test batch in BN layers improves the robustness under distributional shifts [29, 33]. ONDA [27] updates the BN statistics with test data via exponential moving average. TENT [41] further updates the scaling and bias parameters in BN layers via entropy minimization. Latest TTA studies consider distribution changes of test data [4, 44]. LAME [4] corrects the output probabilities of a classiﬁer rather than tweaking the model’s inner parameters. By restraining the model from over-adapting to the test batch, LAME allows the model to be more robust under non- i.i.d. scenarios. However, LAME does not have noticeable performance gains in class-balanced, standard i.i.d. scenarios. The primary reason is, as stated by the authors, it “discourages deviations from the predictions of the pre-trained model,” and thus it “does not noticeably help in i.i.d and class-balanced scenarios.” CoTTA [44] aims to adapt to continually changing target environments via a weight-averaged teacher model, weight-averaged augmentations, and stochastic restoring. However, CoTTA assumes i.i.d. test data within each domain and updates the entire model which increases computational costs. 9There also exist works [35, 24] utilizing domain-speciﬁc self-supervision to resolve the distribution shift with test data, but are complementary to ours, i.e., we can also optimize the self-supervised loss instead of the entropy loss, and not applicable to our setups of real test data streams as designing good self-supervision for these domains is highly non-trivial. Replay memory. Replay memory is one of the major approaches in continual learning; it manages a buffer to replay previous data for future learning to prevent catastrophic forgetting. Reservoir sampling [40] is a random sampling algorithm that collects time-uniform samples from unknown sample streams with a single pass, and it has been proven to be a strong baseline in continual learning [17, 5]. GSS [1] stores samples to a memory in a way that maximizes the gradient direction among those samples. A recent study modiﬁes reservoir sampling to balance classes under imbalanced data when the labels are given [6]. Our memory management scheme (PBRS) is inspired by these studies to prevent catastrophic forgetting in test-time adaptations. 6 Discussion and conclusion This paper highlights that real-world distributions often change across the time axis, and existing test- time adaptation algorithms mostly suffer from the non-i.i.d. test data streams. To address this problem, we present a NOn-i.i.d. TEst-time adaptation algorithm, NOTE. Our experiments evaluated robustness under corruptions and domain adaptation on real-world distributions. The results demonstrate that NOTE not only outperforms the baselines under the non-i.i.d./real distribution settings, but it also shows comparable performance under the i.i.d. assumption. We believe that the insights and ﬁndings from this study are a meaningful step toward the practical impact of the test-time adaptation paradigm. Limitations. NOTE and most state-of-the-art TTA algorithms [ 29, 22, 27, 33, 41, 44] assume that the backbone networks are equipped with BN (or IABN) layers. While BN is a widely-used component in deep learning, several architectures, such as LSTMs [14] and Transformers [39], do not embed BN layers. A recent study uncovered that BN is advantageous in Vision Transformers [45], showing potential room to apply our idea to architectures without BN layers. However, more in-depth studies are necessary to identify the actual applicability of BN (or IABN) to those architectures. While LAME [4] is applicable to models without BN, its limitation is the performance drop in i.i.d. scenarios, as shown in both its paper and our evaluation. While NOTE shows its effectiveness in both non-i.i.d and i.i.d. scenarios, a remaining challenge is to design an algorithm that generalizes to any architecture. We believe the ﬁndings and contributions of our work could give valuable insights to future endeavors on this end. Potential negative societal impacts. As TTA relies on unlabeled test samples and changes the model accordingly, the model is exposed to potential data-driven biases after adaptation, such as fairness issues [ 3] and adversarial attacks [ 36]. In some sense, the utility of TTA comes at the expense of exposure to threats. This vulnerability is another crucial problem that both ML researchers and practitioners need to take into consideration. In addition, TTA entails additional computations for adaptation with test data, which may have negative impacts on environments, e.g., increasing electricity consumption and carbon emissions [ 34]. Nevertheless, we believe NOTE would not exacerbate this issue as it is computationally efﬁcient as mentioned in Section §1. Acknowledgments and Disclosure of Funding We thank the anonymous reviewers for their constructive feedback and suggestions to improve this paper. This work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No.NRF-2020R1A2C1004062) and Center for Applied Research in Artiﬁcial Intelligence (CARAI) grant funded by Defense Acquisition Program Administration (DAPA) and Agency for Defense Development (ADD) (UD190031RD). References [1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. Advances in neural information processing systems, 32, 2019. 10[2] Elon Bachman and I Capulet. Digital record of tesla crashes resulting in death, May 2022. [3] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. Nips tutorial, 1:2, 2017. [4] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of Conference on Computer Vision and Pattern Recognition, 2022. [5] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. Continual learning with tiny episodic memories. ICML Workshop: Multi-Task and Lifelong Reinforcement Learning, 2019. [6] Aristotelis Chrysakis and Marie-Francine Moens. Online continual learning from imbalanced data. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Confer- ence on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1952–1961. PMLR, 13–18 Jul 2020. [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, 2009. [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. [9] A Geiger, P Lenz, C Stiller, and R Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231–1237, 2013. [10] Clément Godard, Oisin Mac Aodha, and Gabriel J. Brostow. Unsupervised monocular depth estimation with left-right consistency, 2016. [11] Shirsendu Sukanta Halder, Jean-François Lalonde, and Raoul de Charette. Physics-based rendering for improving robustness to rain, 2019. [12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, Los Alamitos, CA, USA, jun 2016. IEEE Computer Society. [13] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019. [14] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. [15] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classiﬁcation. arXiv preprint arXiv:1909.06335, 2019. [16] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 448–456, Lille, France, 07–09 Jul 2015. PMLR. [17] David Isele and Akansel Cosgun. Selective experience replay for lifelong learning. In Proceed- ings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence and Thirtieth Innovative Applications of Artiﬁcial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, AAAI’18/IAAI’18/EAAI’18. AAAI Press, 2018. [18] Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna- tional Conference on Learning Representations (ICLR), 2015. [19] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s thesis, Department of Computer Science, University of Toronto, 2009. 11[20] Nicholas D. Lane, Emiliano Miluzzo, Hong Lu, Daniel Peebles, Tanzeem Choudhury, and Andrew T. Campbell. A survey of mobile phone sensing. IEEE Communications Magazine, 48(9):140–150, 2010. [21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [22] Dong-Hyun Lee et al. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, page 896, 2013. [23] Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos: An experimental study. arXiv preprint arXiv:2102.02079, 2021. [24] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34, 2021. [25] Aleksej Logacjov, Kerstin Bach, Atle Kongsvold, Hilde Bremseth Bårdstu, and Paul Jarle Mork. Harth: A human activity recognition dataset for machine learning. Sensors, 21(23), 2021. [26] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. 2017. [27] Massimiliano Mancini, Hakan Karaoguz, Elisa Ricci, Patric Jensfelt, and Barbara Caputo. Kit- ting in the wild through online domain adaptation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1103–1109. IEEE, 2018. [28] Norman Mu and Justin Gilmer. Mnist-c: A robustness benchmark for computer vision. arXiv preprint arXiv:1906.02337, 2019. [29] Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. [30] Hyeonseob Nam and Hyo-Eun Kim. Batch-instance normalization for adaptively style-invariant neural networks. Advances in Neural Information Processing Systems, 31, 2018. [31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high- performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché- Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. [32] Joaquin Quiñonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. Mit Press, 2008. [33] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 11539–11551. Curran Associates, Inc., 2020. [34] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. Commun. ACM, 63(12):54–63, nov 2020. [35] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International Conference on Machine Learning, pages 9229–9248. PMLR, 2020. [36] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel- low, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 12[37] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [38] Yonatan Vaizman, Katherine Ellis, and Gert Lanckriet. Recognizing detailed human context in the wild from smartphones and smartwatches. IEEE Pervasive Computing, 16(4):62–74, 2017. [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [40] Jeffrey S. Vitter. Random sampling with a reservoir. ACM Trans. Math. Softw., 11(1):37–57, mar 1985. [41] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. [42] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020. [43] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in neural information processing systems, 33:7611–7623, 2020. [44] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of Conference on Computer Vision and Pattern Recognition, 2022. [45] Zhuliang Yao, Yue Cao, Yutong Lin, Ze Liu, Zheng Zhang, and Han Hu. Leveraging batch nor- malization for vision transformers. In 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), pages 413–422, 2021. [46] Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin. Federated learning on non-iid data: A survey. Neurocomputing, 465:371–390, 2021. 13Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section §6. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section §6. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main ex- perimental results (either in the supplemental material or as a URL)? [Yes] Code is available at https://github.com/TaesikGong/NOTE. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section §4 and Appendix A. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] We ran the entire experiments with three different random seems (0,1,2) and reported the standard deviations. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix A. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [Yes] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [Yes] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14A Experimental details For all the experiments in the paper, we used three different random seeds (0, 1, 2) and reported the average errors (and standard deviations). We ran our experiments on NVIDIA GeForce RTX 3090 GPUs. A.1 Baseline details We referred to the ofﬁcial implementations of the baselines. We use the reported best hyperparam- eters from their paper or code. We further tuned hyperparameters if there exists a hyperparameter selection guideline. Here, we provide additional details of the baseline implementations, including hyperparameters. PL. Following the previous studies [ 41, 44], we update the BN layers only in PL. We set the learning rate as LR= 0.001 as the same as [41]. ONDA. ONDA [27] has two hyperparameters, the update frequencyN and the decay of the moving average m. The authors set N = 10 and m= 0.1 as the default values throughout the experiments, and we follow this choice unless speciﬁed. TENT. TENT [41] set the learning rate as LR= 0.001 for all datasets except for ImageNet, and we follow this choice. We referred to the ofﬁcial code3 for implementing TENT. LAME. LAME [4] needs an afﬁnity matrix and has hyperparameters related to it. We follow the authors’ hyperparameter selection speciﬁed in the paper and their ofﬁcial code. Namely, we use the kNN afﬁnity matrix with the value of k set as 5. We referred to the ofﬁcial code4 for implementing LAME. CoTTA. CoTTA [44] has three hyperparameters, augmentation conﬁdence threshold pth, restora- tion factor p, and exponential moving average (EMA) factor m. We follow the authors’ choice for restoration factor (p = 0.01) and EMA factor ( α = 0.999). For the augmentation conﬁdence threshold, the authors provide a guideline to choose it, using 5% quantile for the softmax predictions’ conﬁdence on the source domains. We follow this guideline, which results in pth = 0.92 for MNIST- C and CIFAR10-C, pth = 0.72 for CIFAR100-C, and pth = 0.55 for KITTI. For 1D time-series datasets (HARTH and ExtraSensory), the authors do not provide augmentations, and it is non-trivial to select appropriate augmentations for them. We thus do not use augmentations for these datasets. We referred to the ofﬁcial code5 for implementing CoTTA. A.2 Dataset details A.2.1 Robustness to corruptions MNIST-C. MNIST-C [28] applies 15 corruptions to the MNIST [ 21] dataset. Speciﬁcally, the corruptions include Shot Noise, Impulse Noise, Glass Blur, Motion Blur, Shear, Scale, Rotate, Brightness, Translate, Stripe, Fog, Spatter, Dotted Line, Zigzag, and Canny Edges, as illustrated in Figure 6. Note that the result of this dataset is included only in the supplementary material. In total, MNIST-C has 60,000 clean training data and 150,000 corrupted test data (10,000 for each corruption type). We use ResNet18 [12] as the backbone network. We train it on the clean training data to generate source models, using stochastic gradient descent with momentum=0.9 and cosine annealing learning rate scheduling [26] for 100 epochs with an initial learning rate of 0.1. CIFAR10-C/CIFAR100-C. CIFAR10-C/CIFAR100-C [13] are common TTA benchmarks for evaluating the robustness to corruptions [ 29, 33, 41, 44]. Both CIFAR10/CIFAR100 [ 19] have 50,000/10,000 training/test data. CIFAR10/CIFAR100 have 10/100 classes, respectively. CIFAR10- C/CIFAR100-C apply 15 types of corruptions to CIFAR10/CIFAR100 test data: Gaussian Noise, 3https://github.com/DequanWang/tent 4https://github.com/fiveai/LAME 5https://github.com/qinenergy/cotta 15Original Shot Noise Impulse Noise Glass Blur Motion Blur Shear Scale Rotate Brightness Translate Stripe Fog Spatter Dotted Line Zigzag Canny Edges Figure 6: Illustration of the 15 corruption types in the MNIST-C dataset. Original Gaussian Noise Shot Noise Impulse Noise Defocus Blur Glass Blur Motion Blur Zoom Blur Snow Frost Fog Brightness Contrast Elastic Pixelate JPEG Figure 7: Illustration of the 15 corruption types in the CIFAR10-C/CIFAR100-C/ImageNet-C dataset. Shot Noise, Impulse Noise, Defocus Blur, Frosted Glass Blur, Motion Blur, Zoom Blur, Snow, Frost, Fog, Brightness, Contrast, Elastic Transformation, Pixelate, and JPEG Compression, as illustrated in Figure 7. We use the most severe corruption level of 5, similar to previous studies [29, 33, 41, 44]. This results in a total of 150,000 test data for CIFAR10-C/CIFAR100-C, respectively. We use ResNet18 [12] as the backbone network. We train it on the clean training data to generate source models, using stochastic gradient descent with momentum=0.9 and cosine annealing learning rate scheduling [26] for 200 epochs with an initial learning rate of 0.1 and a batch size of 128. ImageNet-C. ImageNet-C is another common TTA benchmark for evaluating the robustness to corruptions [29, 33, 41, 44, 4]. ImageNet [7] has 1,281,167/50,000 training/test data. ImageNet-C applies the same 15 types of corruption used in CIFAR10-C and CIFAR100-C. We use a pre-trained ResNet18 [ 12] on ImageNet training data and ﬁne-tune it by replacing BN layers with IABN layers on the clean ImageNet training data. For ﬁne-tuning, we use stochastic gradient descent with momentum=0.9 for 30 epochs with a ﬁxed learning rate of 0.001 and a batch size of 256. Temporally correlated streams via Dirichlet distribution.Note that most public vision datasets are not time-series data, and existing TTA studies usually shufﬂed the order of these datasets resulting in i.i.d. streams, which might be unrealistic in real-world scenarios. To simulate non-i.i.d. streams from these “static” datasets, we utilize Dirichlet distribution that is widely used to simulate non-i.i.d. settings. [23, 15, 43, 42] Speciﬁcally, we simulate a non-i.i.d partition for T tokens on Cclasses. For each class c, we draw a T-dimensional vector qc ∼Dir(δp), where Dir(·) denotes the Dirichlet distribution, p is a prior class distribution over T classes, and δ >0 is a concentration parameter. We assign data from each class to each token t, following proportion qc[n]. To simulate the nature of real-world online data where sequences are temporally correlated, and data from the same classes appear multiple times (e.g., walking, jogging, and then walking, see Figure 9 and 10 for illustrations), we concatenate the generated T tokens to create a synthetic non-i.i.d. sequential data. We use δ= 0.1 as the default value if not speciﬁed. 16Samples in time order Car Van Truck Pedestrian Person (sitting) Cyclist Tram Misc (a) Visualization of the class distribution in the entire KITTI dataset. (b) Original data with an interval of three frames. (c) Rain data with an interval of three frames. Figure 8: Illustration of the test stream of the KITTI dataset. We apply a 200mm/hr rain intensity to the original data. A.2.2 Real-distributions with domain shift The following illustrates the summary and preprocessing steps of datasets collected in the real world or have a resemblance to class distributions in the real world. KITTI, KITTI-Rain. KITTI [ 9] is a well-known dataset used in numerous tasks such as object detection, object tracking, depth estimation, etc. It must be emphasized that the dataset was collected by driving around the city, in rural areas and on highways, which captures the real-world distribution. From the available tasks, we select the object tracking task; to utilize its temporal correlation. In order to reduce the task to a single image classiﬁcation task, we crop each frame with respect to the largest bounding box. Domain gap is introduced through synthetic generation of corresponding “rainy” frames, hereby denoted as KITTI-Rain [11]. KITTI-Rain is generated via a two-step procedure: (1) generation of a depth-map estimation of each frame, and (2) generation of rainy images from the vanilla frame and its corresponding depth map, as described in [11]. For the depth map generation, we used Monodepth [10], and for rainy image generation, we used the source code available in [11]. The rain intensity is set to 200mm/hr for training and testing. The ﬁnal source domain consists of 7,481 samples, and each of the target domains consists of 7,800 samples. We use ResNet50 [12] pre-trained on ImageNet [8] as the backbone network. We ﬁne-tune it on the KITTI training data to generate source models, using the Adam optimizer [18] and cosine annealing learning rate scheduling [26] for 50 epochs with an initial learning rate of 0.1 and a batch size of 64. HARTH. Human Activity Recognition Trondheim dataset [25] was collected from 22 users, with two three-axial Axivity AX3 accelerometers, each attached to the subject’s thigh and lower back. HARTH was also collected in a free-living environment and labeled through recorded video. We set the source domain as the accelerometer data collected from the back (15 users), and set the target domain as one collected from the thigh (from the remaining seven users). We deem such a setting to be natural, for one of the most dominant forms of domain shift in wearable sensory data is by the positioning of sensors on the human body [20]. We use a window size of 50 and min-max scaled (0-1) the data, following the original paper [25]. The ﬁnal source domain consists of 82,544 samples, and each of the seven target domains consists of {S008: 8,140, S018: 6,241, S019: 5,846, S021: 5,910, S022: 6,448, S028: 3,271, S029: 3,521} samples. We use four one-dimensional convolutional layers followed by one fully-connected layer as the backbone network. We train it on the source data to generate source models, using stochastic gradient descent with momentum=0.9 for 100 epochs and cosine annealing learning rate scheduling [26] with an initial learning rate of 0.1 and a batch size of 64. ExtraSensory. Extrasensory dataset [38] was collected from 60 users with the user’s own smart- phones over a seven-day period in the wild, i.e., data was collected from users who engaged in their regular natural behavior. As there were no constraints on the subject’s activity, the distribution 170 50000 100000 150000 200000 250000 300000 350000 400000 Raw data in time order 0.20 0.40 0.60 Accelerometer (x-axis) Walking Running Shuffling Stairs (ascending) Stairs (descending) Standing Sitting Lying Cycling (sit) Cycling (stand) Transport (sit) Transport (stand) (a) S008. 0 50000 100000 150000 200000 250000 300000 Raw data in time order 0.00 0.50 Accelerometer (x-axis) (b) S018. 0 50000 100000 150000 200000 250000 Raw data in time order 0.25 0.50 0.75 Accelerometer (x-axis) (c) S019. 0 50000 100000 150000 200000 250000 Raw data in time order 0.25 0.50 Accelerometer (x-axis) (d) S021. 0 50000 100000 150000 200000 250000 300000 Raw data in time order 0.25 0.50 0.75 Accelerometer (x-axis) (e) S022. 0 20000 40000 60000 80000 100000 120000 140000 160000 Raw data in time order 0.00 0.25 0.50 Accelerometer (x-axis) (f) S028. 0 20000 40000 60000 80000 100000 120000 140000 160000 Raw data in time order 0.00 0.50 Accelerometer (x-axis) (g) S029. Figure 9: Illustration of the target streams of the HARTH dataset. We specify x-axis accelerometer values only. varied from user to user. We select the ﬁve most frequently occurred, mutually exclusive activ- ities (lying down, sitting, walking, standing, running) and omit other labels. We further process the data to only those consisting of the following sensor modalities - accelerometer, gyroscope, magnetometer, and audio. We used a window size of ﬁve, with no overlap, and standardly scaled the datasets. After the pre-processing step, 23 users were left, 16 of them were used as source domains, and seven of them were used as target domains. The ﬁnal source domain consists of 17,777 samples, and each of the seven target domains consists of {4FC32141-E888-4BFF-8804- 12559A491D8C: 844, 59818CD2-24D7-4D32-B133-24C2FE3801E5: 401, 61976C24-1C50-4355- 9C49-AAE44A7D09F6: 776, 797D145F-3858-4A7F-A7C2-A4EB721E133C: 463, A5CDF89D- 180 500 1000 1500 2000 2500 3000 3500 4000 Raw data in time order 0.00 5.00 Accelerometer (x-axis) Lying down Sitting Walking Running Standing (a) 4FC. 0 250 500 750 1000 1250 1500 1750 2000 Raw data in time order -2.50 0.00 2.50 Accelerometer (x-axis) (b) 598. 0 500 1000 1500 2000 2500 3000 3500 Raw data in time order -2.50 0.00 2.50 Accelerometer (x-axis) (c) 619. 0 500 1000 1500 2000 Raw data in time order -2.50 0.00 2.50 Accelerometer (x-axis) (d) 797. 0 500 1000 1500 2000 2500 3000 3500 Raw data in time order -2.50 0.00 2.50 Accelerometer (x-axis) (e) A5D. 0 500 1000 1500 2000 2500 3000 3500 4000 Raw data in time order -2.50 0.00 2.50 Accelerometer (x-axis) (f) C48. 0 500 1000 1500 2000 2500 3000 3500 Raw data in time order -2.50 0.00 2.50 Accelerometer (x-axis) (g) D7D. Figure 10: Illustration of the target streams of the Extrasensory dataset. We specify x-axis accelerom- eter values only. Due to the length of the name of each domain, denoted here with the ﬁrst three characters. 02A2-4EC1-89F8-F534FDABDD96 : 734, C48CE857-A0DD-4DDB-BEA5-3A25449B2153 : 850, D7D20E2E-FC78-405D-B346-DBD3FD8FC92B: 794} samples. We use two one-dimensional con- volutional layers followed by one fully-connected layer as the backbone network. We train it on the source data to generate source models, using stochastic gradient descent with momentum=0.9 for 100 epochs and cosine annealing learning rate scheduling [26] with an initial learning rate of 0.1 and a batch size of 64. 190 1000 2000 3000 4000 5000 6000 7000 Samples in time order 0 25 50 75 100Classification error (%) BN Stats PL ONDA TENT LAME CoTTA NOTE (a) Rain-200. Figure 11: Illustration of the real-time cumulative classiﬁcation error change of different methods on the KITTI dataset. The x-axis denotes the samples in order, whereas the y-axis denotes the error rate in percentage. Note that some lines are not clearly visible due to overlap. Error on the source domain. We also measure the domain gap between the source and the targets in the three real-distribution datasets: Table 4 for KITTI, Table 5 for HARTH, and Table 6 for Extrasensory. As shown, there is a clear performance degradation from the source domain to the target domain. For HARTH and ExtraSensory, the performance degradation was severe (30∼40%p increased error rates compared with Source), indicating the importance of overcoming the domain shift problem in sensory applications. Table 4: Average classiﬁcation error (%) and their corresponding standard deviations on the KITTI dataset of the source model. Bold fonts indicate the lowest classiﬁcation errors. Averaged over three runs. Method Src domain Rain Avg Source 7.4 ± 1.0 12.3 ± 2.3 9.9 Table 5: Average classiﬁcation error (%) and their corresponding standard deviations on the HARTH dataset of the source model. Bold fonts indicate the lowest classiﬁcation errors. Averaged over three runs. Method Src domain S008 S018 S019 S021 S022 S028 S029 Avg Source 11.7 ± 0.786.2 ± 1.3 44.7 ± 2.1 50.4 ± 9.5 74.8 ± 3.8 72.0 ± 2.6 53.0 ± 24.0 57.0 ± 16.7 56.2 Table 6: Average classiﬁcation error (%) and their corresponding standard deviations on the ExtraSen- sory dataset of the source model. Bold fonts indicate the lowest classiﬁcation errors. Averaged over three runs. Method Src domain 4FC 598 619 797 A5C C48 D7D Avg Source 8.3 ± 0.734.6 ± 2.5 40.1 ± 0.7 63.8 ± 5.7 45.3 ± 2.4 64.6 ± 3.7 39.6 ± 6.8 63.0 ± 3.9 44.9 20B Domain-wise results B.1 Robustness to corruptions Table 7: Average classiﬁcation error (%) and their corresponding standard deviations on CIFAR10-C with temporally correlated test streams, shown per corruption. Bold fonts indicate the lowest classiﬁcation errors, while Red fonts show performance degradation after adaptation. Averaged over three runs. Method GaussianShotImpulseDefocusGlassMotionZoomSnowFrostFog BrightnessContrastElasticPixelateJPEGAvg Source 74.0 ± 3.3 66.8 ± 3.5 75.3 ± 4.2 43.3 ± 2.7 48.0 ± 2.7 32.6 ± 1.2 35.2 ± 2.6 22.0 ± 0.4 33.0 ± 2.5 25.9 ± 0.9 8.5 ± 0.3 66.1 ± 1.8 23.4 ± 0.7 53.6 ± 0.7 26.8 ± 0.7 42.3 BN Stats [29]77.2 ± 0.7 76.7 ± 1.0 78.9 ± 0.8 70.0 ± 1.7 78.6 ± 0.6 70.5 ± 1.5 71.1 ± 1.4 72.5 ± 1.4 71.9 ± 1.1 70.6 ± 1.6 68.7 ± 1.9 69.1 ± 1.9 75.1 ± 1.5 73.6 ± 1.4 76.8 ± 1.4 73.4 ONDA [27] 69.3 ± 1.0 68.5 ± 1.0 71.8 ± 0.6 58.5 ± 1.4 71.0 ± 0.2 59.9 ± 1.0 59.5 ± 1.0 62.4 ± 1.4 62.1 ± 1.0 59.6 ± 1.3 55.6 ± 1.4 58.4 ± 1.4 65.6 ± 1.0 63.9 ± 1.4 67.6 ± 1.1 63.6 PL [22] 78.3 ± 1.0 78.0 ± 1.5 80.4 ± 1.0 72.2 ± 1.6 80.1 ± 1.2 72.4 ± 2.2 73.1 ± 1.4 74.5 ± 2.5 73.9 ± 1.8 73.4 ± 1.7 71.5 ± 2.7 71.7 ± 2.5 77.3 ± 2.1 75.7 ± 1.5 78.6 ± 2.7 75.4 TENT [41] 79.0 ± 2.9 78.8 ± 2.8 80.6 ± 2.2 73.3 ± 1.7 80.5 ± 2.9 74.4 ± 2.4 74.5 ± 3.3 74.8 ± 2.2 75.0 ± 2.3 74.0 ± 2.2 72.3 ± 3.4 74.9 ± 3.2 78.2 ± 2.8 76.5 ± 2.9 79.0 ± 2.9 76.4 LAME [4] 73.6 ± 5.2 64.8 ± 4.6 74.8 ± 6.4 36.2 ± 4.4 37.7 ± 5.3 24.9 ± 1.6 27.9 ± 3.4 12.4 ± 1.0 22.4 ± 3.9 19.4 ± 0.9 3.6 ± 0.3 65.1 ± 1.5 12.6 ± 0.8 50.3 ± 0.9 16.4 ± 1.236.2 CoTTA [44] 77.0 ± 0.7 76.8 ± 0.6 79.0 ± 0.7 74.1 ± 0.9 79.6 ± 0.6 74.3 ± 0.5 74.0 ± 0.8 74.8 ± 1.1 73.3 ± 0.9 72.9 ± 0.5 72.2 ± 0.9 76.5 ± 0.8 76.5 ± 0.9 75.1 ± 0.8 76.6 ± 0.6 75.5 NOTE 34.9 ± 1.6 32.3 ± 3.1 39.6 ± 2.5 13.6 ± 0.5 35.8 ± 1.9 11.8 ± 0.8 14.5 ± 0.5 14.1 ± 0.6 15.2 ± 1.3 14.2 ± 0.6 7.7 ± 0.3 7.6 ± 0.6 20.8 ± 0.7 27.7 ± 2.6 26.4 ± 0.5 21.1 Table 8: Average classiﬁcation error (%) and their corresponding standard deviations on CIFAR100-C with temporally correlated test streams, shown per corruption. Bold fonts indicate the lowest classiﬁcation errors, while Red fonts show performance degradation after adaptation. Averaged over three runs. Method GaussianShotImpulseDefocusGlassMotionZoomSnowFrostFog BrightnessContrastElasticPixelateJPEGAvg Source 88.1 ± 0.2 86.8 ± 0.6 93.7 ± 0.6 64.9 ± 0.4 79.7 ± 0.9 55.5 ± 0.3 57.7 ± 0.2 53.8 ± 0.4 66.3 ± 0.8 59.3 ± 0.4 33.0 ± 0.3 81.4 ± 0.4 49.2 ± 0.4 73.6 ± 1.1 55.5 ± 0.3 66.6 BN Stats [29]73.9 ± 0.5 73.5 ± 0.4 77.2 ± 0.7 56.9 ± 0.2 72.3 ± 0.5 58.8 ± 0.3 57.9 ± 0.4 65.3 ± 0.4 65.0 ± 0.4 62.4 ± 0.6 55.6 ± 0.2 57.6 ± 0.4 64.6 ± 0.5 63.6 ± 0.3 71.0 ± 0.4 65.0 ONDA [27] 63.0 ± 0.7 62.5 ± 0.4 68.0 ± 0.5 37.3 ± 0.2 60.0 ± 0.2 40.0 ± 0.3 38.3 ± 0.1 49.6 ± 0.3 50.0 ± 0.6 45.2 ± 0.6 35.7 ± 0.2 40.9 ± 0.5 48.6 ± 0.5 46.9 ± 0.3 57.5 ± 0.2 49.6 PL [22] 71.9 ± 1.4 72.0 ± 0.5 76.3 ± 0.7 59.3 ± 0.8 73.8 ± 0.9 61.5 ± 0.9 59.9 ± 0.5 67.1 ± 0.9 66.7 ± 1.4 63.0 ± 1.0 57.9 ± 0.5 62.2 ± 1.5 67.6 ± 1.0 65.2 ± 0.3 71.1 ± 0.5 66.4 TENT [41] 71.8 ± 0.9 71.0 ± 0.4 76.4 ± 1.2 60.2 ± 0.6 75.0 ± 1.0 61.9 ± 0.9 60.2 ± 0.7 67.8 ± 0.5 67.8 ± 0.7 63.3 ± 1.1 58.4 ± 0.7 65.0 ± 1.8 68.4 ± 0.9 65.0 ± 0.2 71.8 ± 0.1 66.9 LAME [4] 89.0 ± 1.1 87.1 ± 0.8 94.5 ± 0.7 62.3 ± 1.2 79.7 ± 1.2 49.4 ± 1.0 52.8 ± 0.3 46.6 ± 0.4 63.9 ± 1.9 55.6 ± 1.2 25.2 ± 0.6 82.4 ± 0.2 40.8 ± 0.5 71.9 ± 1.4 47.8 ± 0.763.3 CoTTA [44] 68.6 ± 0.3 67.9 ± 0.4 71.4 ± 0.4 60.7 ± 0.4 69.9 ± 0.4 60.8 ± 0.5 60.2 ± 0.2 64.0 ± 0.3 62.9 ± 0.5 63.2 ± 0.6 56.7 ± 0.2 65.6 ± 0.3 64.5 ± 0.3 60.9 ± 0.0 65.3 ± 0.1 64.2 NOTE 66.2 ± 0.8 64.2 ± 1.6 72.6 ± 0.4 37.2 ± 0.8 61.1 ± 0.7 35.4 ± 0.3 37.4 ± 0.4 40.0 ± 0.4 42.5 ± 0.3 43.4 ± 0.5 29.4 ± 0.1 32.1 ± 0.5 44.3 ± 0.4 47.5 ± 0.6 51.3 ± 0.3 47.0 21Table 9: Average classiﬁcation error (%) and their corresponding standard deviations on ImageNet-C with temporally correlated test streams, shown per corruption. Bold fonts indicate the lowest classiﬁcation errors, while Red fonts show performance degradation after adaptation. Averaged over three runs. Method GaussianShotImpulseDefocusGlassMotionZoomSnowFrost Fog BrightnessContrastElasticPixelateJPEGAvg Source 98.4 ± 0.0 97.7 ± 0.0 98.4 ± 0.0 90.6 ± 0.0 92.5 ± 0.0 89.8 ± 0.0 81.8 ± 0.0 89.5 ± 0.0 85.0 ± 0.0 86.4 ± 0.0 51.1 ± 0.0 97.2 ± 0.0 85.3 ± 0.0 76.9 ± 0.0 71.7 ± 0.0 86.1 BN Stats 98.3 ± 0.0 98.1 ± 0.0 98.4 ± 0.0 98.7 ± 0.0 98.8 ± 0.0 97.8 ± 0.0 96.6 ± 0.0 96.2 ± 0.0 96.0 ± 0.0 95.1 ± 0.0 93.1 ± 0.0 98.6 ± 0.0 96.3 ± 0.0 95.6 ± 0.0 96.1 ± 0.0 96.9 ONDA 95.1 ± 0.0 94.7 ± 0.0 95.0 ± 0.0 96.2 ± 0.0 96.1 ± 0.0 92.5 ± 0.0 87.2 ± 0.0 87.4 ± 0.0 87.8 ± 0.0 82.7 ± 0.0 71.0 ± 0.0 96.4 ± 0.0 84.9 ± 0.0 81.7 ± 0.0 86.1 ± 0.0 89.0 PL 99.3 ± 0.0 99.3 ± 0.0 99.4 ± 0.0 99.5 ± 0.0 99.4 ± 0.0 99.5 ± 0.0 98.8 ± 0.0 99.1 ± 0.0 99.2 ± 0.0 98.1 ± 0.0 97.3 ± 0.1 99.8 ± 0.0 98.4 ± 0.0 98.5 ± 0.0 98.5 ± 0.0 98.9 TENT 98.3 ± 0.0 98.1 ± 0.0 98.4 ± 0.0 98.7 ± 0.0 98.8 ± 0.0 97.8 ± 0.0 96.6 ± 0.0 96.2 ± 0.0 96.0 ± 0.0 95.1 ± 0.0 93.1 ± 0.0 98.6 ± 0.0 96.3 ± 0.0 95.6 ± 0.0 96.1 ± 0.0 96.9 LAME 98.1 ± 0.0 97.1 ± 0.0 98.0 ± 0.0 87.9 ± 0.0 90.9 ± 0.0 87.1 ± 0.0 78.3 ± 0.0 87.1 ± 0.0 80.2 ± 0.0 81.5 ± 0.0 39.8 ± 0.0 96.4 ± 0.0 82.5 ± 0.0 70.7 ± 0.0 64.9 ± 0.082.7 CoTTA 98.2 ± 0.0 98.1 ± 0.0 98.3 ± 0.0 98.8 ± 0.0 98.8 ± 0.0 97.7 ± 0.0 96.8 ± 0.0 96.6 ± 0.1 96.3 ± 0.0 95.3 ± 0.0 93.5 ± 0.0 98.8 ± 0.0 96.5 ± 0.0 95.6 ± 0.0 96.2 ± 0.0 97.0 NOTE 94.7 ± 0.1 93.7 ± 0.3 94.5 ± 0.1 91.2 ± 0.1 91.0 ± 0.2 83.3 ± 0.1 79.0 ± 0.2 79.0 ± 0.4 78.7 ± 0.3 66.3 ± 0.6 48.0 ± 0.4 94.1 ± 0.1 76.9 ± 0.6 62.6 ± 0.7 76.6 ± 0.6 80.6 Table 10: Average classiﬁcation error (%) and their corresponding standard deviations on MNIST-C with temporally correlated test streams, shown per corruption. Bold fonts indicate the lowest classiﬁcation errors, while Red fonts show performance degradation after adaptation. Averaged over three runs. Method ShotImpulseGlassMotionShearScaleRotateBrightnessTranslateStripe Fog SpatterDotted lineZigzagCanny edgesAvg Source 3.7 ± 0.7 27.3 ± 5.5 20.4 ± 6.4 4.6 ± 0.5 2.2 ± 0.5 5.1 ± 1.0 6.5 ± 1.0 21.1 ± 22.9 13.8 ± 1.4 17.4 ± 17.0 66.6 ± 14.7 3.8 ± 0.4 3.7 ± 0.4 18.2 ± 3.0 26.4 ± 11.4 16.1 BN Stats [29]72.0 ± 0.6 75.2 ± 0.8 73.7 ± 1.0 72.1 ± 0.8 71.2 ± 1.1 71.4 ± 0.6 71.2 ± 0.3 71.6 ± 0.6 78.5 ± 0.2 72.3 ± 1.2 70.8 ± 1.2 71.6 ± 0.9 73.8 ± 0.7 74.6 ± 0.6 72.3 ± 0.3 72.8 ONDA [27] 53.3 ± 3.0 59.9 ± 3.0 59.2 ± 3.3 54.1 ± 3.5 51.6 ± 2.2 53.9 ± 2.5 54.6 ± 2.0 50.5 ± 2.3 65.2 ± 2.1 57.5 ± 0.7 54.8 ± 2.9 54.2 ± 3.0 55.4 ± 2.8 61.0 ± 2.2 56.7 ± 2.1 56.1 PL [22] 73.7 ± 1.0 76.4 ± 0.4 75.3 ± 0.5 74.7 ± 1.1 72.7 ± 0.9 73.3 ± 1.6 73.7 ± 0.9 73.7 ± 1.0 78.7 ± 0.3 74.1 ± 1.4 75.8 ± 2.6 72.5 ± 0.8 75.8 ± 0.6 76.9 ± 1.4 74.5 ± 0.1 74.8 TENT [41] 74.7 ± 1.1 78.1 ± 0.9 76.6 ± 0.6 76.1 ± 0.7 75.8 ± 1.1 73.7 ± 1.3 75.2 ± 1.1 75.4 ± 0.3 78.9 ± 0.2 76.7 ± 1.8 81.4 ± 1.7 73.9 ± 0.5 77.3 ± 0.7 79.2 ± 2.0 75.8 ± 1.0 76.6 LAME [4] 1.1 ± 0.3 17.0 ± 8.7 12.5 ± 6.5 1.1 ± 0.3 0.4 ± 0.2 1.5 ± 0.6 2.3 ± 0.6 17.2 ± 26.0 6.0 ± 2.3 12.3 ± 17.2 68.3 ± 15.8 0.7 ± 0.3 0.7 ± 0.4 13.2 ± 3.4 22.1 ± 12.3 11.8 CoTTA [44] 76.9 ± 0.5 79.4 ± 0.4 79.1 ± 0.5 77.6 ± 0.6 75.4 ± 0.4 76.2 ± 1.3 77.6 ± 0.2 76.0 ± 0.5 81.6 ± 0.9 76.8 ± 0.6 78.0 ± 0.4 77.6 ± 0.6 79.3 ± 0.4 80.6 ± 1.0 77.6 ± 0.5 78.0 NOTE 3.9 ± 1.3 13.8 ± 2.4 14.3 ± 1.5 3.3 ± 2.4 1.7 ± 0.2 3.8 ± 0.7 6.5 ± 0.3 0.9 ± 0.0 8.0 ± 1.2 14.4 ± 8.1 1.6 ± 0.3 3.9 ± 0.4 4.5 ± 1.2 12.6 ± 2.5 13.4 ± 3.9 7.1 22Table 11: Average classiﬁcation error (%) and their corresponding standard deviations on CIFAR10- C with uniformly distributed test streams, shown per domain. Bold fonts indicate the lowest classiﬁcation errors, while Red fonts show performance degradation after adaptation. NOTE* indicates NOTE used directly with test batches (without using PBRS). Averaged over three runs. Method GaussianShotImpulseDefocusGlassMotionZoomSnowFrostFog BrightnessContrastElasticPixelateJPEGAvg Source 74.0 ± 3.3 66.8 ± 3.5 75.3 ± 4.2 43.3 ± 2.7 48.0 ± 2.7 32.6 ± 1.2 35.2 ± 2.6 22.0 ± 0.4 33.0 ± 2.5 25.9 ± 0.9 8.5 ± 0.3 66.1 ± 1.8 23.4 ± 0.7 53.6 ± 0.7 26.8 ± 0.7 42.3 BN Stats [29]33.1 ± 0.9 31.1 ± 1.0 39.8 ± 0.9 12.3 ± 0.4 34.8 ± 0.3 13.7 ± 0.3 12.6 ± 0.4 18.3 ± 0.7 19.9 ± 0.6 14.5 ± 0.6 9.3 ± 0.3 13.0 ± 0.3 23.3 ± 0.3 20.8 ± 0.2 28.0 ± 0.6 21.6 ONDA [27] 33.4 ± 0.6 31.3 ± 0.9 40.0 ± 1.1 12.3 ± 0.4 34.6 ± 0.7 13.7 ± 0.3 12.4 ± 0.5 18.3 ± 0.6 19.8 ± 0.8 14.3 ± 0.4 9.1 ± 0.0 14.0 ± 0.2 23.3 ± 0.4 20.9 ± 0.2 28.0 ± 0.7 21.7 PL [22] 29.4 ± 1.1 26.3 ± 1.0 36.8 ± 1.6 13.7 ± 0.4 36.5 ± 1.1 14.0 ± 1.0 13.5 ± 0.2 19.7 ± 0.8 21.2 ± 0.6 15.6 ± 1.5 10.0 ± 0.6 14.8 ± 0.2 24.5 ± 2.0 20.1 ± 0.9 27.4 ± 1.3 21.6 TENT [41] 25.3 ± 0.8 23.1 ± 1.1 32.1 ± 1.2 11.7 ± 0.6 33.1 ± 3.0 13.2 ± 1.1 11.2 ± 0.1 15.9 ± 0.3 18.8 ± 0.7 12.9 ± 0.8 8.6 ± 0.3 14.4 ± 0.6 21.7 ± 0.9 16.5 ± 0.8 23.6 ± 0.7 18.8 LAME [4] 78.2 ± 3.6 70.6 ± 4.0 80.5 ± 4.5 46.6 ± 1.9 48.0 ± 3.8 34.2 ± 0.4 37.4 ± 1.5 20.8 ± 0.8 30.5 ± 4.1 26.9 ± 1.8 9.8 ± 0.2 71.9 ± 1.0 24.2 ± 0.9 56.4 ± 0.8 25.8 ± 0.9 44.1 CoTTA [44] 23.1 ± 0.7 21.5 ± 0.6 28.0 ± 0.3 11.7 ± 0.5 29.2 ± 0.6 13.3 ± 0.6 12.0 ± 0.5 16.6 ± 0.2 16.6 ± 0.3 13.8 ± 0.4 8.8 ± 0.2 14.9 ± 0.5 20.6 ± 0.7 17.3 ± 0.5 19.9 ± 0.417.8 NOTE 33.5 ± 1.7 30.0 ± 1.6 38.2 ± 0.9 12.6 ± 0.8 34.4 ± 0.8 11.5 ± 0.5 12.9 ± 0.6 14.1 ± 0.2 15.2 ± 0.8 14.0 ± 0.6 7.4 ± 0.2 7.8 ± 0.2 20.7 ± 0.3 24.7 ± 0.7 24.2 ± 0.4 20.1 NOTE* 23.8 ± 0.7 23.0 ± 0.9 31.1 ± 0.3 11.8 ± 0.6 30.9 ± 1.3 11.8 ± 0.4 11.9 ± 0.7 15.3 ± 1.3 14.0 ± 0.7 13.3 ± 0.7 8.6 ± 0.2 7.5 ± 0.3 21.2 ± 0.3 16.9 ± 0.6 23.0 ± 1.2 17.6 Table 12: Average classiﬁcation error (%) and their corresponding standard deviations on CIFAR100- C with uniformly distributed test streams, shown per domain. Bold fonts indicate the lowest classiﬁcation errors, while Red fonts show performance degradation after adaptation. Averaged over three runs. NOTE* indicates NOTE used directly with test batches (without using PBRS) Method GaussianShotImpulseDefocusGlassMotionZoomSnowFrostFog BrightnessContrastElasticPixelateJPEGAvg Source 88.1 ± 0.2 86.8 ± 0.6 93.7 ± 0.6 64.9 ± 0.4 79.7 ± 0.9 55.5 ± 0.3 57.7 ± 0.2 53.8 ± 0.4 66.3 ± 0.8 59.3 ± 0.4 33.0 ± 0.3 81.4 ± 0.4 49.2 ± 0.4 73.6 ± 1.1 55.5 ± 0.3 66.6 BN Stats [29]60.9 ± 0.8 59.9 ± 0.6 65.7 ± 0.8 33.7 ± 0.4 57.6 ± 0.4 36.5 ± 0.2 35.2 ± 0.4 46.7 ± 0.3 46.9 ± 0.4 42.8 ± 0.7 32.3 ± 0.4 35.6 ± 0.5 45.8 ± 0.3 43.6 ± 0.3 55.5 ± 0.2 46.6 ONDA [27] 60.8 ± 0.9 60.2 ± 0.5 66.0 ± 0.6 33.9 ± 0.4 57.5 ± 0.4 36.3 ± 0.4 34.6 ± 0.4 46.5 ± 0.3 47.2 ± 0.3 42.1 ± 0.6 32.1 ± 0.5 36.4 ± 0.4 45.5 ± 0.1 43.4 ± 0.8 55.1 ± 0.1 46.5 PL [22] 52.2 ± 0.9 50.3 ± 1.0 59.4 ± 0.9 33.5 ± 0.5 54.0 ± 0.6 35.7 ± 0.3 33.1 ± 0.5 42.8 ± 0.9 44.5 ± 1.6 39.2 ± 1.3 30.9 ± 0.2 35.5 ± 0.2 45.5 ± 1.0 39.9 ± 0.3 50.4 ± 1.3 43.1 TENT [41] 48.7 ± 0.8 47.2 ± 0.6 55.6 ± 0.9 31.5 ± 0.2 50.9 ± 0.5 33.5 ± 0.4 31.7 ± 0.2 39.6 ± 0.3 41.0 ± 0.1 36.8 ± 0.7 29.4 ± 0.3 33.6 ± 0.4 42.3 ± 0.6 36.8 ± 0.5 46.4 ± 0.540.3 LAME [4] 91.0 ± 1.0 89.5 ± 1.0 95.2 ± 0.7 68.1 ± 0.9 82.7 ± 1.1 57.1 ± 0.5 60.2 ± 0.3 54.7 ± 0.3 68.9 ± 1.2 61.8 ± 0.6 33.7 ± 0.5 85.2 ± 0.4 50.3 ± 0.2 76.7 ± 1.3 56.2 ± 0.5 68.8 CoTTA [44] 52.8 ± 0.7 51.0 ± 0.4 56.9 ± 0.6 35.8 ± 0.4 53.9 ± 0.2 37.9 ± 0.5 36.8 ± 0.1 45.2 ± 0.5 44.5 ± 0.1 44.0 ± 0.2 32.2 ± 0.5 41.3 ± 1.4 46.1 ± 0.1 39.7 ± 0.3 46.9 ± 0.7 44.3 NOTE 65.6 ± 1.0 62.6 ± 0.7 72.0 ± 0.2 36.8 ± 0.7 60.5 ± 0.7 34.9 ± 0.5 36.7 ± 0.2 39.6 ± 0.2 41.7 ± 0.6 42.3 ± 0.3 28.6 ± 0.2 32.3 ± 0.9 43.8 ± 0.2 47.7 ± 0.4 50.9 ± 0.2 46.4 NOTE* 51.8 ± 1.0 50.0 ± 0.3 60.7 ± 0.4 32.6 ± 0.2 54.4 ± 0.3 33.0 ± 0.2 33.5 ± 0.4 38.5 ± 0.3 38.6 ± 0.1 36.7 ± 0.3 29.7 ± 0.5 27.3 ± 0.3 43.2 ± 0.4 37.1 ± 0.2 47.6 ± 0.9 41.0 23Table 13: Average classiﬁcation error (%) and their corresponding standard deviations on ImageNet-C with temporally correlated test streams, shown per corruption. Bold fonts indicate the lowest classiﬁcation errors, while Red fonts show performance degradation after adaptation. Averaged over three runs. Method GaussianShotImpulseDefocusGlassMotionZoomSnowFrost Fog BrightnessContrastElasticPixelateJPEGAvg Source 98.4 ± 0.0 97.7 ± 0.0 98.4 ± 0.0 90.6 ± 0.0 92.5 ± 0.0 89.8 ± 0.0 81.8 ± 0.0 89.5 ± 0.0 85.0 ± 0.0 86.4 ± 0.0 51.1 ± 0.0 97.2 ± 0.0 85.3 ± 0.0 76.9 ± 0.0 71.7 ± 0.0 86.1 BN Stats 89.4 ± 0.0 88.5 ± 0.1 89.2 ± 0.2 90.8 ± 0.0 90.0 ± 0.0 81.3 ± 0.0 69.8 ± 0.2 72.6 ± 0.1 73.8 ± 0.0 62.6 ± 0.0 44.3 ± 0.3 92.1 ± 0.0 64.5 ± 0.1 60.3 ± 0.1 70.7 ± 0.0 76.0 ONDA 89.2 ± 0.0 88.2 ± 0.0 89.0 ± 0.1 90.9 ± 0.1 90.0 ± 0.1 81.6 ± 0.1 69.5 ± 0.0 72.6 ± 0.1 73.7 ± 0.0 62.7 ± 0.1 43.9 ± 0.0 92.1 ± 0.0 64.3 ± 0.0 60.1 ± 0.1 70.0 ± 0.0 75.9 PL 89.8 ± 1.9 86.1 ± 0.9 88.5 ± 1.6 93.0 ± 1.1 92.5 ± 0.6 82.2 ± 0.0 64.6 ± 0.3 70.2 ± 0.6 79.7 ± 0.4 55.8 ± 0.2 43.9 ± 0.1 97.2 ± 0.5 57.8 ± 0.1 52.7 ± 0.2 60.5 ± 0.174.4 TENT 91.1 ± 2.4 89.7 ± 1.6 91.0 ± 2.5 93.1 ± 3.2 92.2 ± 3.2 84.7 ± 4.9 72.4 ± 3.5 73.3 ± 1.1 78.7 ± 6.9 59.8 ± 4.0 44.5 ± 0.5 95.2 ± 4.3 61.6 ± 4.3 56.4 ± 5.6 67.4 ± 4.7 76.5 LAME 98.6 ± 0.0 97.8 ± 0.0 98.6 ± 0.0 90.7 ± 0.0 92.6 ± 0.0 89.9 ± 0.0 81.9 ± 0.0 89.8 ± 0.0 85.0 ± 0.0 86.5 ± 0.0 51.1 ± 0.0 97.3 ± 0.0 85.6 ± 0.0 77.0 ± 0.0 71.7 ± 0.0 86.3 CoTTA 85.7 ± 0.2 84.6 ± 0.1 85.4 ± 0.0 87.8 ± 0.3 86.4 ± 0.2 74.6 ± 0.0 64.2 ± 0.2 67.9 ± 0.0 69.7 ± 0.2 56.1 ± 0.1 42.7 ± 0.0 88.5 ± 0.8 60.0 ± 0.0 54.2 ± 0.1 64.9 ± 0.1 71.5 NOTE 87.6 ± 0.1 85.7 ± 0.1 87.2 ± 0.2 83.3 ± 0.2 83.2 ± 0.2 73.6 ± 0.0 65.4 ± 0.2 65.0 ± 0.0 68.6 ± 0.1 57.9 ± 0.0 43.5 ± 0.1 75.9 ± 0.1 61.2 ± 0.1 54.1 ± 0.0 62.8 ± 0.1 70.3 NOTE* 89.5 ± 0.4 87.9 ± 0.2 88.9 ± 0.3 84.6 ± 0.2 83.7 ± 0.2 74.4 ± 0.1 66.6 ± 0.1 66.1 ± 0.2 71.2 ± 0.1 58.2 ± 0.1 44.7 ± 0.1 78.8 ± 0.1 61.2 ± 0.2 54.8 ± 0.0 64.8 ± 0.1 71.7 Table 14: Average classiﬁcation error (%) and their corresponding standard deviations on MNIST- C with uniformly distributed test streams, shown per domain. Bold fonts indicate the lowest classiﬁcation errors, while Red fonts show performance degradation after adaptation. Averaged over three runs. NOTE* indicates NOTE used directly with test batches (without using PBRS). Method ShotImpulseGlassMotionShearScaleRotateBrightnessTranslateStripe Fog SpatterDotted lineZigzagCanny edgesAvg Source 3.7 ± 0.7 27.3 ± 5.5 20.4 ± 6.4 4.6 ± 0.5 2.2 ± 0.5 5.1 ± 1.0 6.5 ± 1.0 21.1 ± 22.9 13.8 ± 1.4 17.4 ± 17.0 66.6 ± 14.7 3.8 ± 0.4 3.7 ± 0.4 18.2 ± 3.0 26.4 ± 11.4 16.1 BN Stats [29]2.9 ± 0.7 7.0 ± 1.6 9.1 ± 1.0 3.0 ± 0.8 2.0 ± 0.3 3.8 ± 0.2 6.1 ± 0.7 1.1 ± 0.1 12.5 ± 0.8 6.5 ± 2.6 2.2 ± 0.5 3.3 ± 0.3 2.5 ± 0.2 11.4 ± 0.2 6.7 ± 0.9 5.3 ONDA [27] 2.6 ± 0.6 6.5 ± 1.4 8.6 ± 1.0 2.8 ± 0.8 1.8 ± 0.2 3.5 ± 0.2 5.7 ± 0.7 1.0 ± 0.1 11.7 ± 1.1 6.1 ± 2.6 2.6 ± 0.9 3.0 ± 0.4 2.2 ± 0.2 11.0 ± 0.4 6.2 ± 0.8 5.0 PL [22] 1.6 ± 0.3 3.5 ± 0.7 4.8 ± 0.8 1.7 ± 0.0 1.5 ± 0.0 2.3 ± 0.1 4.9 ± 0.7 0.8 ± 0.1 6.8 ± 0.8 2.7 ± 0.6 1.0 ± 0.0 2.2 ± 0.3 1.7 ± 0.2 5.3 ± 0.4 3.9 ± 0.9 3.0 TENT [41] 1.4 ± 0.1 2.8 ± 0.4 3.8 ± 0.5 1.5 ± 0.0 1.2 ± 0.0 1.8 ± 0.1 3.6 ± 0.2 0.7 ± 0.1 4.6 ± 0.7 1.9 ± 0.2 0.8 ± 0.0 1.7 ± 0.1 1.3 ± 0.1 4.5 ± 0.6 3.1 ± 0.5 2.3 LAME [4] 3.0 ± 0.8 30.7 ± 8.3 18.9 ± 5.8 3.4 ± 0.5 1.9 ± 0.3 4.2 ± 0.5 6.3 ± 0.9 25.9 ± 29.8 13.9 ± 1.9 18.5 ± 21.2 78.2 ± 9.8 3.3 ± 0.7 3.2 ± 0.3 19.3 ± 3.2 28.0 ± 12.7 17.2 CoTTA [44] 2.6 ± 0.6 6.6 ± 1.7 8.7 ± 0.9 2.7 ± 0.7 1.8 ± 0.3 3.2 ± 0.0 5.6 ± 0.8 1.0 ± 0.1 14.3 ± 1.1 7.7 ± 6.0 1.9 ± 0.5 2.9 ± 0.3 2.2 ± 0.1 13.6 ± 1.4 6.1 ± 0.6 5.4 NOTE 2.5 ± 0.8 10.7 ± 1.9 10.9 ± 2.0 2.0 ± 0.3 1.5 ± 0.0 2.4 ± 0.1 5.5 ± 0.3 0.9 ± 0.1 5.5 ± 0.2 12.1 ± 5.7 1.2 ± 0.1 2.8 ± 0.3 3.0 ± 0.1 10.9 ± 1.6 9.1 ± 0.4 5.4 NOTE* 1.3 ± 0.2 2.7 ± 0.1 3.8 ± 0.5 1.3 ± 0.1 1.1 ± 0.1 1.6 ± 0.0 3.5 ± 0.1 0.7 ± 0.0 2.8 ± 0.0 2.2 ± 0.1 0.7 ± 0.1 1.7 ± 0.4 1.4 ± 0.2 4.8 ± 1.1 3.5 ± 0.1 2.2 B.2 Real distributions with domain shift Since the adaptation is done from a single source domain to a single target domain in KITTI, no further per-domain tables are speciﬁed here. 24Table 15: Average classiﬁcation error (%) and their corresponding standard deviations on HARTH with real test streams, shown per domain. Bold fonts indicate the lowest classiﬁcation errors, while Red fonts show performance degradation after adaptation. Averaged over three runs. Method S008 S018 S019 S021 S022 S028 S029 Avg Source 86.2 ± 1.3 44.7 ± 2.1 50.4 ± 9.5 74.8 ± 3.8 72.0 ± 2.6 53.0 ± 24.0 57.0 ± 16.7 62.6 BN Stats [29] 70.3 ± 1.4 73.8 ± 1.3 68.1 ± 3.0 64.9 ± 0.9 68.5 ± 0.3 65.5 ± 0.5 69.4 ± 1.4 68.6 ONDA [27] 75.3 ± 4.0 60.4 ± 0.9 63.1 ± 4.6 67.9 ± 0.4 70.0 ± 3.8 73.6 ± 0.7 74.5 ± 4.4 69.3 PL [22] 60.4 ± 1.3 71.4 ± 1.5 62.9 ± 1.9 61.8 ± 1.2 63.1 ± 0.4 64.5 ± 0.8 69.4 ± 2.0 64.8 TENT [41] 59.5 ± 0.371.0 ± 1.6 62.2 ± 1.9 61.1 ± 1.1 61.7 ± 0.464.1 ± 0.5 69.3 ± 2.1 64.1 LAME [4] 85.5 ± 1.7 43.4 ± 2.0 48.8 ± 10.9 73.2 ± 3.8 70.7 ± 2.6 51.2 ± 29.4 54.1 ± 20.6 61.0 CoTTA [44] 70.4 ± 1.4 73.8 ± 1.3 68.2 ± 2.9 64.9 ± 1.0 68.5 ± 0.2 65.5 ± 0.5 69.4 ± 1.4 68.7 NOTE 84.8 ± 0.7 32.9 ± 1.8 36.3 ± 10.969.1 ± 2.4 67.1 ± 1.2 30.0 ± 13.8 36.6 ± 9.8 51.0 Table 16: Average classiﬁcation error (%) and their corresponding standard deviations on Extrasensory with real test streams, shown per domain. Bold fonts indicate the lowest classiﬁcation errors, while Red fonts show performance degradation after adaptation. Due to the length of the name of each domain, denoted here with the ﬁrst three characters. Averaged over three runs. Method 4FC 598 619 797 A5D C48 D7D Avg Source 34.6 ± 2.5 40.1 ± 0.7 63.8 ± 5.7 45.3 ± 2.4 64.6 ± 3.7 39.6 ± 6.8 63.0 ± 3.9 50.2 BN Stats[29] 61.7 ± 4.2 50.1 ± 5.1 51.6 ± 1.5 59.4 ± 1.1 54.4 ± 1.0 52.4 ± 2.8 62.6 ± 2.9 56.0 ONDA [27] 36.3 ± 3.5 44.0 ± 2.2 50.8 ± 2.456.1 ± 1.9 59.7 ± 2.7 43.5 ± 5.9 46.7 ± 4.248.2 PL [22] 62.2 ± 4.3 50.0 ± 5.1 51.7 ± 1.8 59.2 ± 1.1 53.9 ± 1.1 52.3 ± 2.9 62.8 ± 3.0 56.0 TENT [41] 62.1 ± 4.6 49.8 ± 5.0 51.6 ± 1.9 59.4 ± 1.2 53.9 ± 1.0 52.2 ± 2.9 62.8 ± 3.0 56.0 LAME [4] 33.1 ± 2.4 37.8 ± 0.468.0 ± 8.8 37.1 ± 6.773.2 ± 2.6 39.0 ± 7.6 66.4 ± 4.0 50.7 CoTTA [44] 61.7 ± 4.2 50.0 ± 4.9 51.6 ± 1.5 59.4 ± 1.1 54.4 ± 1.0 52.4 ± 2.8 62.6 ± 2.9 56.0 NOTE 41.7 ± 5.9 40.7 ± 0.8 55.5 ± 10.8 45.8 ± 4.6 45.8 ± 10.4 32.9 ± 1.155.5 ± 10.4 45.4 B.3 Ablation study Table 17: Average classiﬁcation error (%) and their corresponding standard deviations of varying ablation settings on CIFAR10-C with temporally correlated test streams, shown per domain. Bold fonts indicate the lowest classiﬁcation errors. Averaged over three runs. Method GaussianShotImpulseDefocusGlassMotionZoomSnowFrostFog BrightnessContrastElasticPixelateJPEGAvg Source 74.0 ± 3.3 66.8 ± 3.5 75.3 ± 4.2 43.3 ± 2.7 48.0 ± 2.7 32.6 ± 1.2 35.2 ± 2.6 22.0 ± 0.4 33.0 ± 2.5 25.9 ± 0.9 8.5 ± 0.3 66.1 ± 1.8 23.4 ± 0.7 53.6 ± 0.7 26.8 ± 0.7 42.3 IABN 44.5 ± 2.7 41.3 ± 2.3 48.0 ± 1.9 16.3 ± 1.0 39.9 ± 0.1 13.8 ± 0.7 16.1 ± 0.7 14.9 ± 0.3 17.8 ± 0.6 16.3 ± 0.6 7.6 ± 0.2 8.8 ± 0.3 22.5 ± 0.3 34.0 ± 1.2 26.7 ± 0.6 24.6 PBRS 45.2 ± 3.0 38.5 ± 4.9 46.8 ± 3.3 24.5 ± 2.2 38.2 ± 2.8 19.1 ± 0.9 20.0 ± 0.2 16.5 ± 0.2 19.1 ± 2.4 16.5 ± 0.4 7.1 ± 0.7 34.4 ± 3.0 21.5 ± 0.5 39.8 ± 4.7 25.2 ± 0.427.5 IABN + RS 33.7 ± 6.4 30.0 ± 6.7 37.6 ± 2.9 13.6 ± 0.3 34.9 ± 1.9 12.4 ± 1.2 14.5 ± 1.7 13.9 ± 1.1 15.0 ± 3.1 14.0 ± 1.3 7.2 ± 0.0 7.4 ± 0.7 21.1 ± 0.9 26.2 ± 4.4 25.9 ± 1.1 20.5 IABN + PBRS34.9 ± 1.6 32.3 ± 3.1 39.6 ± 2.5 13.6 ± 0.5 35.8 ± 1.9 11.8 ± 0.8 14.5 ± 0.5 14.1 ± 0.6 15.2 ± 1.3 14.2 ± 0.6 7.7 ± 0.3 7.6 ± 0.6 20.8 ± 0.7 27.7 ± 2.6 26.4 ± 0.5 21.1 25Table 18: Average classiﬁcation error (%) and their corresponding standard deviations of varying ablation settings on CIFAR100-C withtemporally correlated test streams, shown per domain.Bold fonts indicate the lowest classiﬁcation errors. Averaged over three runs. Method GaussianShotImpulseDefocusGlassMotionZoomSnowFrostFog BrightnessContrastElasticPixelateJPEGAvg Source 88.1 ± 0.2 86.8 ± 0.6 93.7 ± 0.6 64.9 ± 0.4 79.7 ± 0.9 55.5 ± 0.3 57.7 ± 0.2 53.8 ± 0.4 66.3 ± 0.8 59.3 ± 0.4 33.0 ± 0.3 81.4 ± 0.4 49.2 ± 0.4 73.6 ± 1.1 55.5 ± 0.3 66.6 IABN 79.3 ± 0.7 77.2 ± 0.7 84.2 ± 1.0 45.0 ± 0.6 69.6 ± 0.3 40.9 ± 0.3 43.1 ± 0.6 42.5 ± 0.4 48.6 ± 0.3 52.5 ± 0.5 30.4 ± 0.1 40.5 ± 0.7 47.6 ± 0.5 59.8 ± 1.1 56.2 ± 0.4 54.5 PBRS 68.8 ± 0.6 66.2 ± 0.4 73.3 ± 0.9 46.2 ± 0.6 64.9 ± 1.5 41.8 ± 0.6 41.7 ± 0.3 44.2 ± 0.4 48.5 ± 0.7 44.7 ± 0.2 28.3 ± 0.2 60.1 ± 0.4 44.2 ± 0.4 51.9 ± 0.8 50.5 ± 0.551.7 IABN + RS 66.8 ± 2.1 65.2 ± 0.3 73.1 ± 1.0 38.7 ± 0.4 63.0 ± 0.9 36.6 ± 0.0 38.0 ± 0.2 41.9 ± 0.8 43.9 ± 0.4 44.6 ± 0.5 29.5 ± 0.3 33.5 ± 0.7 46.0 ± 0.5 49.9 ± 0.9 52.4 ± 0.4 48.2 IABN + PBRS66.2 ± 0.8 64.2 ± 1.6 72.6 ± 0.4 37.2 ± 0.8 61.1 ± 0.7 35.4 ± 0.3 37.4 ± 0.4 40.0 ± 0.4 42.5 ± 0.3 43.4 ± 0.5 29.4 ± 0.1 32.1 ± 0.5 44.3 ± 0.4 47.5 ± 0.6 51.3 ± 0.3 47.0 Table 19: Average classiﬁcation error (%) and their corresponding standard deviations of varying ablation settings on CIFAR10-C with uniformly distributed test streams, shown per domain. Bold fonts indicate the lowest classiﬁcation errors. Averaged over three runs. Method GaussianShotImpulseDefocusGlassMotionZoomSnowFrostFog BrightnessContrastElasticPixelateJPEGAvg Source 74.0 ± 3.3 66.8 ± 3.5 75.3 ± 4.2 43.3 ± 2.7 48.0 ± 2.7 32.6 ± 1.2 35.2 ± 2.6 22.0 ± 0.4 33.0 ± 2.5 25.9 ± 0.9 8.5 ± 0.3 66.1 ± 1.8 23.4 ± 0.7 53.6 ± 0.7 26.8 ± 0.7 42.3 IABN 44.5 ± 2.7 41.4 ± 2.3 48.1 ± 1.9 16.3 ± 1.0 39.9 ± 0.1 13.9 ± 0.7 16.2 ± 0.7 14.9 ± 0.3 17.9 ± 0.6 16.4 ± 0.5 7.6 ± 0.2 8.8 ± 0.3 22.5 ± 0.4 34.1 ± 1.2 26.7 ± 0.6 24.6 PBRS 43.4 ± 0.8 37.9 ± 0.6 46.2 ± 1.5 21.8 ± 2.0 36.8 ± 1.0 18.1 ± 0.3 17.6 ± 0.8 16.1 ± 0.1 19.3 ± 0.5 15.2 ± 0.3 7.1 ± 0.4 32.5 ± 1.5 20.0 ± 0.2 30.7 ± 0.7 23.8 ± 0.125.8 IABN + RS 33.8 ± 1.6 31.1 ± 0.9 40.4 ± 1.3 13.3 ± 0.7 35.6 ± 0.2 11.8 ± 0.6 13.2 ± 0.3 14.6 ± 0.3 14.9 ± 0.6 14.7 ± 0.4 7.7 ± 0.2 8.1 ± 0.4 22.3 ± 0.5 24.6 ± 1.9 25.1 ± 1.2 20.7 IABN + PBRS33.5 ± 1.7 30.0 ± 1.6 38.2 ± 0.9 12.6 ± 0.8 34.4 ± 0.8 11.5 ± 0.5 12.9 ± 0.6 14.1 ± 0.2 15.2 ± 0.8 14.0 ± 0.6 7.4 ± 0.2 7.8 ± 0.2 20.7 ± 0.3 24.7 ± 0.7 24.2 ± 0.4 20.1 Table 20: Average classiﬁcation error (%) and their corresponding standard deviations of varying ablation settings on CIFAR100-C withuniformly distributed test streams, shown per domain.Bold fonts indicate the lowest classiﬁcation errors. Averaged over three runs. Method GaussianShotImpulseDefocusGlassMotionZoomSnowFrostFog BrightnessContrastElasticPixelateJPEGAvg Source 88.1 ± 0.2 86.8 ± 0.6 93.7 ± 0.6 64.9 ± 0.4 79.7 ± 0.9 55.5 ± 0.3 57.7 ± 0.2 53.8 ± 0.4 66.3 ± 0.8 59.3 ± 0.4 33.0 ± 0.3 81.4 ± 0.4 49.2 ± 0.4 73.6 ± 1.1 55.5 ± 0.3 66.6 IABN 79.3 ± 0.6 77.2 ± 0.6 84.3 ± 1.0 45.0 ± 0.5 69.6 ± 0.2 40.9 ± 0.3 43.1 ± 0.6 42.5 ± 0.4 48.6 ± 0.3 52.5 ± 0.5 30.5 ± 0.1 40.5 ± 0.7 47.6 ± 0.5 59.8 ± 1.1 56.2 ± 0.4 54.5 PBRS 68.6 ± 1.0 66.0 ± 0.3 72.9 ± 0.3 45.3 ± 0.3 64.1 ± 0.8 40.9 ± 0.5 41.6 ± 0.5 43.7 ± 0.2 47.9 ± 0.2 44.2 ± 0.3 28.3 ± 0.3 59.9 ± 0.7 44.2 ± 0.5 51.1 ± 1.6 50.4 ± 0.651.3 IABN + RS 67.1 ± 1.2 65.6 ± 0.3 74.0 ± 0.4 39.0 ± 0.3 61.4 ± 1.3 36.5 ± 0.1 38.7 ± 0.8 41.4 ± 0.2 44.0 ± 0.4 45.0 ± 0.2 30.0 ± 0.2 34.0 ± 0.2 46.0 ± 1.4 48.8 ± 1.3 52.5 ± 0.5 48.3 IABN + PBRS65.6 ± 1.0 62.6 ± 0.7 72.0 ± 0.2 36.8 ± 0.7 60.5 ± 0.7 34.9 ± 0.5 36.7 ± 0.2 39.6 ± 0.2 41.7 ± 0.6 42.3 ± 0.3 28.6 ± 0.2 32.3 ± 0.9 43.8 ± 0.2 47.7 ± 0.4 50.9 ± 0.2 46.4 26C Replacing BN with IABN during test time Table 21: Average classiﬁcation error (%) and corresponding standard deviations of varying ablation settings on CIFAR10-C/100-C under temporally correlated (non-i.i.d.) and uniformly distributed (i.i.d.) test data stream. IABN* refers to replacing BN with IABN during test time (no pre-training with IABN layers). Bold fonts indicate the lowest classiﬁcation errors. Averaged over three runs. Temporally correlated test stream Uniformly distributed test stream Method CIFAR10-C CIFAR100-C Avg CIFAR10-C CIFAR100-C Avg Source 42.3 ± 1.1 66.6 ± 0.1 54.4 42.3 ± 1.1 66.6 ± 0.1 54.4 IABN* 27.1 ± 0.4 60.8 ± 0.1 44.0 27.1 ± 0.4 60.8 ± 0.2 44.0 IABN 24.6 ± 0.6 54.5 ± 0.1 39.5 24.6 ± 0.6 54.5 ± 0.1 39.5 IABN*+PBRS 24.9 ± 0.2 55.9 ± 0.2 40.4 23.2 ± 0.4 55.3 ± 0.1 39.3 IABN+PBRS 21.1 ± 0.6 47.0 ± 0.1 34.0 20.1 ± 0.5 46.4 ± 0.0 33.2 For pre-trained models with BN layers such as ResNet [12], NOTE needs to re-train the model by replacing BN layers with IABN layers in order to utilize the effectiveness of IABN. This requires the additional computational cost of re-training, which might make it inconvenient to utilize off-the-shelf models. We further investigate whether simply switching BN to IABN without re-training still leads to performance gain. Table 21 shows the result of this experiment, where IABN* refers to replacing BN with IABN during test time. We note that IABN* still shows a signiﬁcant reduction of errors under CIFAR10-C and CIFAR100-C datasets compared with BN (Source). We interpret this as the normalization correction in IABN is somewhat valid without re-training the model. We notice that IABN* outperforms the baselines in CIFAR10-C with 27.1% error, while the second best (LAME) shows 36.2% error 1. In addition, IABN* also shows improvement combined with PBRS. This implies that IABN can be used without re-training the model, which aligns with the fully test-time adaptation paradigm introduced in a recent study [41]. D License of assets Datasets KITTI dataset (CC-BY-NC-SA 3.0), KITTI-rain dataset (CC-BY-NC-SA 3.0), CIFAR10, 100 (MIT License), ImageNet-C (Apache 2.0), MNIST-C (CC-BY-NC-SA 4.0), HARTH dataset (MIT License), and the Extrasensory dataset (CC-BY-NC-SA 4.0) Codes Code for rain augmentation on the KITTI dataset (Apache 2.0), torch-vision for ResNet18 and ResNet50 (Apache 2.0), code for depth estimation used in rain augmentation on the KITTI dataset (UCLB ACP-A License), code for generating Dirichlet distributions (Apache 2.0), the ofﬁcial repository of CoTTA (MIT License), the ofﬁcial repository of TENT (MIT License), and the ofﬁcial repository of LAME (CC BY-NC-SA 4.0). 270 1000 2000 3000 4000 5000 6000 7000 8000 Samples in time order 0 25 50 75 100Classification error (%) BN Stats PL ONDA TENT LAME CoTTA NOTE (a) S008. 0 1000 2000 3000 4000 5000 6000 Samples in time order 0 25 50 75 100Classification error (%) (b) S018. 0 1000 2000 3000 4000 5000 Samples in time order 0 25 50 75 100Classification error (%) (c) S019. 0 1000 2000 3000 4000 5000 Samples in time order 0 25 50 75 100Classification error (%) (d) S021. 0 1000 2000 3000 4000 5000 6000 Samples in time order 0 25 50 75 100Classification error (%) (e) S022. 0 500 1000 1500 2000 2500 3000 Samples in time order 0 25 50 75 100Classification error (%) (f) S028. 0 500 1000 1500 2000 2500 3000 3500 Samples in time order 0 25 50 75 100Classification error (%) (g) S029. Figure 12: Illustration of the real-time cumulative classiﬁcation error change of different methods on the HARTH dataset. The x-axis denotes the samples in order, whereas the y-axis denotes the error rate in percentage. Note that some lines are not clearly visible due to overlap. 280 100 200 300 400 500 600 700 800 Samples in time order 0 25 50 75 100Classification error (%) BN Stats PL ONDA TENT LAME CoTTA NOTE (a) 4FC. 0 50 100 150 200 250 300 350 Samples in time order 0 25 50 75 100Classification error (%) (b) 598. 0 100 200 300 400 500 600 700 Samples in time order 0 25 50 75 100Classification error (%) (c) 619. 0 50 100 150 200 250 300 350 400 Samples in time order 0 25 50 75 100Classification error (%) (d) 797. 0 100 200 300 400 500 600 700 Samples in time order 0 25 50 75 100Classification error (%) (e) A5D. 0 100 200 300 400 500 600 700 800 Samples in time order 0 25 50 75 100Classification error (%) (f) C48. 0 100 200 300 400 500 600 700 Samples in time order 0 25 50 75 100Classification error (%) (g) D7D. Figure 13: Illustration of the real-time cumulative classiﬁcation error change of different methods on the Extrasensory dataset. The x-axis denotes the samples in order, whereas the y-axis denotes the error rate in percentage. Note that some lines are not clearly visible due to overlap. 29",
      "meta_data": {
        "arxiv_id": "2208.05117v3",
        "authors": [
          "Taesik Gong",
          "Jongheon Jeong",
          "Taewon Kim",
          "Yewon Kim",
          "Jinwoo Shin",
          "Sung-Ju Lee"
        ],
        "published_date": "2022-08-10T03:05:46Z",
        "pdf_url": "https://arxiv.org/pdf/2208.05117v3.pdf",
        "github_url": "https://github.com/TaesikGong/NOTE"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the critical issue of Test-Time Adaptation (TTA) methods failing when applied to temporally correlated (non-i.i.d.) test data streams, a common scenario in real-world applications like autonomous driving. The main contributions include the proposal of NOTE (NOn-i.i.d. TEst-time adaptation), a novel TTA scheme robust against such non-i.i.d. streams. NOTE introduces two key components: (a) Instance-Aware Batch Normalization (IABN) to correct normalization for out-of-distribution samples on a per-instance basis, and (b) Prediction-Balanced Reservoir Sampling (PBRS) to simulate i.i.d. data streams from non-i.i.d. streams in a class-balanced manner. Experimental results demonstrate that NOTE significantly outperforms state-of-the-art TTA algorithms in non-i.i.d. settings and achieves comparable performance under i.i.d. assumptions. Additionally, NOTE is designed for batch-free, single-forward-pass inference with negligible memory overhead, making it practical for latency-sensitive tasks.",
        "methodology": "The proposed NOTE method focuses on adapting Batch Normalization (BN) layers. It comprises two main components. First, Instance-Aware Batch Normalization (IABN) is introduced to overcome the limitations of standard BN under distribution shift and temporal correlation. IABN synthesizes BN with Instance Normalization (IN) by calculating the deviation between instance-wise statistics (from IN) and learned statistics (from BN), then correcting the normalization using a soft-shrinkage function with a hyperparameter 'α' to control the confidence level. Second, Prediction-Balanced Reservoir Sampling (PBRS) is used to combat temporal correlation by mimicking i.i.d. samples from non-i.i.d. streams. PBRS combines time-uniform sampling (reservoir sampling) and prediction-uniform sampling (replacing majority class instances in memory based on predicted labels) to maintain a class-balanced memory bank. The normalization statistics (means and variances) in IABN layers are then updated via an exponential moving average using samples from this memory, and affine parameters (scaling factor γ and bias term β) are optimized via a single backward pass with entropy minimization. Inference with NOTE is batch-free and requires only a single forward pass.",
        "experimental_setup": "The methods were implemented using the PyTorch framework. Baselines included Source, Test-time normalization (BN Stats), Online Domain Adaptation (ONDA), Pseudo-Label (PL), Test entropy minimization (TENT), Laplacian Adjusted Maximum-likelihood Estimation (LAME), and Continual test-time adaptation (CoTTA). Evaluation utilized common TTA benchmarks: CIFAR10-C, CIFAR100-C, and ImageNet-C (with the most severe corruption level 5), and additional MNIST-C experiments in the appendix. Real-world non-i.i.d. datasets included KITTI (object detection in autonomous driving, adapted to KITTI-Rain), HARTH (human activity recognition), and ExtraSensory (user behavioral context recognition). Synthetic non-i.i.d. streams were generated using Dirichlet distribution for CIFAR10/100, while ImageNet-C streams were sorted. ResNet18 was used as the backbone for CIFAR/ImageNet/MNIST-C, ResNet50 for KITTI, and custom convolutional networks for HARTH and ExtraSensory. Models were pre-trained on source data, with BN replaced by IABN during NOTE's training. The test batch size and PBRS memory size were set to 64, with adaptation occurring every 64 samples. Hyperparameters for NOTE were α=4, EMA momentum m=0.01, and Adam optimizer learning rate of 0.0001 for affine parameters. All experiments were run with three random seeds, reporting means and standard deviations.",
        "limitations": "A significant limitation is that NOTE, along with most state-of-the-art TTA algorithms, assumes that backbone networks are equipped with Batch Normalization (BN) or IABN layers. This restricts their direct applicability to architectures that do not embed BN layers, such as LSTMs and many Transformers, although recent research on Vision Transformers suggests potential future applicability. Another challenge is to design an algorithm that generalizes to any network architecture. While LAME is applicable to models without BN, it suffers from performance drops in i.i.d. scenarios. Additionally, TTA's reliance on unlabeled test samples exposes models to potential data-driven biases, like fairness issues and adversarial attacks, which are crucial considerations for ML researchers and practitioners. While TTA generally entails additional computations, NOTE is designed to be computationally efficient to mitigate potential negative environmental impacts.",
        "future_research_directions": "Future research should focus on identifying the actual applicability of BN or IABN to network architectures that traditionally do not embed BN layers, such as LSTMs and Transformers. A key challenge is to design a TTA algorithm that can generalize to *any* architecture, moving beyond the current reliance on BN layers. Additionally, further work is needed to address the potential negative societal impacts of TTA, particularly mitigating data-driven biases (e.g., fairness issues) and vulnerabilities to adversarial attacks that arise from adapting models on unlabeled test samples. Exploring how to design effective self-supervision for real-world test data streams, as a complementary approach, also presents a promising direction.",
        "experimental_code": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport conf\nimport copy\n\ndef convert_iabn(module, **kwargs):\n    module_output = module\n    if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):\n        IABN = InstanceAwareBatchNorm2d if isinstance(module, nn.BatchNorm2d) else InstanceAwareBatchNorm1d\n        module_output = IABN(\n            num_channels=module.num_features,\n            k=conf.args.iabn_k,\n            eps=module.eps,\n            momentum=module.momentum,\n            affine=module.affine,\n        )\n\n        module_output._bn = copy.deepcopy(module)\n\n    for name, child in module.named_children():\n        module_output.add_module(\n            name, convert_iabn(child, **kwargs)\n        )\n    del module\n    return module_output\n\n\nclass InstanceAwareBatchNorm2d(nn.Module):\n    def __init__(self, num_channels, k=3.0, eps=1e-5, momentum=0.1, affine=True):\n        super(InstanceAwareBatchNorm2d, self).__init__()\n        self.num_channels = num_channels\n        self.eps = eps\n        self.k=k\n        self.affine = affine\n        self._bn = nn.BatchNorm2d(num_channels, eps=eps,\n                                  momentum=momentum, affine=affine)\n\n    def _softshrink(self, x, lbd):\n        x_p = F.relu(x - lbd, inplace=True)\n        x_n = F.relu(-(x + lbd), inplace=True)\n        y = x_p - x_n\n        return y\n\n    def forward(self, x):\n        b, c, h, w = x.size()\n        sigma2, mu = torch.var_mean(x, dim=[2, 3], keepdim=True, unbiased=True) #IN\n\n        if self.training:\n            _ = self._bn(x)\n            sigma2_b, mu_b = torch.var_mean(x, dim=[0, 2, 3], keepdim=True, unbiased=True)\n        else:\n            if self._bn.track_running_stats == False and self._bn.running_mean is None and self._bn.running_var is None: # use batch stats\n                sigma2_b, mu_b = torch.var_mean(x, dim=[0, 2, 3], keepdim=True, unbiased=True)\n            else:\n                mu_b = self._bn.running_mean.view(1, c, 1, 1)\n                sigma2_b = self._bn.running_var.view(1, c, 1, 1)\n\n\n        if h*w <=conf.args.skip_thres:\n            mu_adj = mu_b\n            sigma2_adj = sigma2_b\n        else:\n            s_mu = torch.sqrt((sigma2_b + self.eps) / (h * w))\n            s_sigma2 = (sigma2_b + self.eps) * np.sqrt(2 / (h * w - 1))\n\n            mu_adj = mu_b + self._softshrink(mu - mu_b, self.k * s_mu)\n\n            sigma2_adj = sigma2_b + self._softshrink(sigma2 - sigma2_b, self.k * s_sigma2)\n\n            sigma2_adj = F.relu(sigma2_adj) #non negative\n\n        x_n = (x - mu_adj) * torch.rsqrt(sigma2_adj + self.eps)\n        if self.affine:\n            weight = self._bn.weight.view(c, 1, 1)\n            bias = self._bn.bias.view(c, 1, 1)\n            x_n = x_n * weight + bias\n        return x_n\n\n\nclass InstanceAwareBatchNorm1d(nn.Module):\n    def __init__(self, num_channels, k=3.0, eps=1e-5, momentum=0.1, affine=True):\n        super(InstanceAwareBatchNorm1d, self).__init__()\n        self.num_channels = num_channels\n        self.k = k\n        self.eps = eps\n        self.affine = affine\n        self._bn = nn.BatchNorm1d(num_channels, eps=eps,\n                                  momentum=momentum, affine=affine)\n\n    def _softshrink(self, x, lbd):\n        x_p = F.relu(x - lbd, inplace=True)\n        x_n = F.relu(-(x + lbd), inplace=True)\n        y = x_p - x_n\n        return y\n\n    def forward(self, x):\n        b, c, l = x.size()\n        sigma2, mu = torch.var_mean(x, dim=[2], keepdim=True, unbiased=True)\n        if self.training:\n            _ = self._bn(x)\n            sigma2_b, mu_b = torch.var_mean(x, dim=[0, 2], keepdim=True, unbiased=True)\n        else:\n            if self._bn.track_running_stats == False and self._bn.running_mean is None and self._bn.running_var is None: # use batch stats\n                sigma2_b, mu_b = torch.var_mean(x, dim=[0, 2], keepdim=True, unbiased=True)\n            else:\n                mu_b = self._bn.running_mean.view(1, c, 1)\n                sigma2_b = self._bn.running_var.view(1, c, 1)\n\n        if l <=conf.args.skip_thres:\n            mu_adj = mu_b\n            sigma2_adj = sigma2_b\n        \n        else:\n            s_mu = torch.sqrt((sigma2_b + self.eps) / l) ##\n            s_sigma2 = (sigma2_b + self.eps) * np.sqrt(2 / (l - 1))\n\n            mu_adj = mu_b + self._softshrink(mu - mu_b, self.k * s_mu)\n            sigma2_adj = sigma2_b + self._softshrink(sigma2 - sigma2_b, self.k * s_sigma2)\n            sigma2_adj = F.relu(sigma2_adj)\n\n\n        x_n = (x - mu_adj) * torch.rsqrt(sigma2_adj + self.eps)\n\n        if self.affine:\n            weight = self._bn.weight.view(c, 1)\n            bias = self._bn.bias.view(c, 1)\n            x_n = x_n * weight + bias\n\n        return x_n\n\nimport conf\nimport random\nimport copy\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\nclass PBRS():\n\n    def __init__(self, capacity):\n        self.data = [[[], [], []] for _ in range(conf.args.opt['num_class'])] #feat, pseudo_cls, domain, cls, loss\n        self.counter = [0] * conf.args.opt['num_class']\n        self.marker = [''] * conf.args.opt['num_class']\n        self.capacity = capacity\n        pass\n    def print_class_dist(self):\n\n        print(self.get_occupancy_per_class())\n    def print_real_class_dist(self):\n\n        occupancy_per_class = [0] * conf.args.opt['num_class']\n        for i, data_per_cls in enumerate(self.data):\n            for cls in data_per_cls[3]:\n                occupancy_per_class[cls] +=1\n        print(occupancy_per_class)\n\n    def get_memory(self):\n\n        data = self.data\n\n        tmp_data = [[], [], []]\n        for data_per_cls in data:\n            feats, cls, dls = data_per_cls\n            tmp_data[0].extend(feats)\n            tmp_data[1].extend(cls)\n            tmp_data[2].extend(dls)\n\n        return tmp_data\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls[0])\n        return occupancy\n\n    def get_occupancy_per_class(self):\n        occupancy_per_class = [0] * conf.args.opt['num_class']\n        for i, data_per_cls in enumerate(self.data):\n            occupancy_per_class[i] = len(data_per_cls[0])\n        return occupancy_per_class\n\n    def update_loss(self, loss_list):\n        for data_per_cls in self.data:\n            feats, cls, dls, _, losses = data_per_cls\n            for i in range(len(losses)):\n                losses[i] = loss_list.pop(0)\n\n    def add_instance(self, instance):\n        assert (len(instance) == 5)\n        cls = instance[1]\n        self.counter[cls] += 1\n        is_add = True\n\n        if self.get_occupancy() >= self.capacity:\n            is_add = self.remove_instance(cls)\n\n        if is_add:\n            for i, dim in enumerate(self.data[cls]):\n                dim.append(instance[i])\n\n    def get_largest_indices(self):\n\n        occupancy_per_class = self.get_occupancy_per_class()\n        max_value = max(occupancy_per_class)\n        largest_indices = []\n        for i, oc in enumerate(occupancy_per_class):\n            if oc == max_value:\n                largest_indices.append(i)\n        return largest_indices\n\n    def remove_instance(self, cls):\n        largest_indices = self.get_largest_indices()\n        if cls not in largest_indices: #  instance is stored in the place of another instance that belongs to the largest class\n            largest = random.choice(largest_indices)  # select only one largest class\n            tgt_idx = random.randrange(0, len(self.data[largest][0]))  # target index to remove\n            for dim in self.data[largest]:\n                dim.pop(tgt_idx)\n        else:# replaces a randomly selected stored instance of the same class\n            m_c = self.get_occupancy_per_class()[cls]\n            n_c = self.counter[cls]\n            u = random.uniform(0, 1)\n            if u <= m_c / n_c:\n                tgt_idx = random.randrange(0, len(self.data[cls][0]))  # target index to remove\n                for dim in self.data[cls]:\n                    dim.pop(tgt_idx)\n            else:\n                return False\n        return True\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\n# https://discuss.pytorch.org/t/calculating-the-entropy-loss/14510\n# but there is a bug in the original code: it sums up the entropy over a batch. so I take mean instead of sum\nclass HLoss(nn.Module):\n    def __init__(self, temp_factor=1.0):\n        super(HLoss, self).__init__()\n        self.temp_factor = temp_factor\n\n    def forward(self, x):\n\n        softmax = F.softmax(x/self.temp_factor, dim=1)\n        entropy = -softmax * torch.log(softmax+1e-6)\n        b = entropy.mean()\n\n        return b\n\n\nimport conf\nfrom learner.dnn import DNN\nfrom torch.utils.data import DataLoader\n\nfrom utils import memory\n\nfrom utils.loss_functions import *\n\ndevice = torch.device(\"cuda:{:d}\".format(conf.args.gpu_idx) if torch.cuda.is_available() else \"cpu\")\nfrom utils.iabn import *\n\nclass NOTE(DNN):\n\n    def __init__(self, *args, **kwargs):\n        super(NOTE, self).__init__(*args, **kwargs)\n\n        # turn on grad for BN params only\n\n        for param in self.net.parameters():  # initially turn off requires_grad for all\n                param.requires_grad = False\n        for module in self.net.modules():\n\n            if isinstance(module, nn.BatchNorm1d) or isinstance(module, nn.BatchNorm2d):\n                # https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n\n                if conf.args.use_learned_stats:\n                    module.track_running_stats = True\n                    module.momentum = conf.args.bn_momentum\n                else:\n                    # With below, this module always uses the test batch statistics (no momentum)\n                    module.track_running_stats = False\n                    module.running_mean = None\n                    module.running_var = None\n\n                module.weight.requires_grad_(True)\n                module.bias.requires_grad_(True)\n\n            elif isinstance(module, nn.InstanceNorm1d) or isinstance(module, nn.InstanceNorm2d): #ablation study\n                module.weight.requires_grad_(True)\n                module.bias.requires_grad_(True)\n\n            if conf.args.iabn:\n                if isinstance(module, InstanceAwareBatchNorm2d) or isinstance(module, InstanceAwareBatchNorm1d):\n                    for param in module.parameters():\n                        param.requires_grad = True\n\n\n        self.fifo = memory.FIFO(capacity=conf.args.update_every_x) # required for evaluation\n\n\n    def train_online(self, current_num_sample):\n        \"\"\"\n        Train the model\n        \"\"\"\n\n        TRAINED = 0\n        SKIPPED = 1\n        FINISHED = 2\n\n        if not hasattr(self, 'previous_train_loss'):\n            self.previous_train_loss = 0\n\n        if current_num_sample > len(self.target_train_set[0]):\n            return FINISHED\n\n        # Add a sample\n        feats, cls, dls = self.target_train_set\n        current_sample = feats[current_num_sample - 1], cls[current_num_sample - 1], dls[current_num_sample - 1]\n        self.fifo.add_instance(current_sample)#for batch-based inferece\n\n        with torch.no_grad():\n\n            self.net.eval()\n\n            if conf.args.memory_type in ['FIFO', 'Reservoir']:\n                self.mem.add_instance(current_sample)\n\n            elif conf.args.memory_type in ['PBRS']:\n                f, c, d = current_sample[0].to(device), current_sample[1].to(device), current_sample[2].to(device)\n\n                logit = self.net(f.unsqueeze(0))\n                pseudo_cls = logit.max(1, keepdim=False)[1][0]\n                self.mem.add_instance([f, pseudo_cls, d, c, 0])\n\n        if conf.args.use_learned_stats: #batch-free inference\n            self.evaluation_online(current_num_sample, '', [[current_sample[0]], [current_sample[1]], [current_sample[2]]])\n\n        if current_num_sample % conf.args.update_every_x != 0:  # train only when enough samples are collected\n            if not (current_num_sample == len(self.target_train_set[\n                                                  0]) and conf.args.update_every_x >= current_num_sample):  # update with entire data\n\n                self.log_loss_results('train_online', epoch=current_num_sample, loss_avg=self.previous_train_loss)\n                return SKIPPED\n\n        if not conf.args.use_learned_stats: #batch-based inference\n            self.evaluation_online(current_num_sample, '', self.fifo.get_memory())\n\n\n        if conf.args.no_adapt: # for ablation\n            return TRAINED\n        # setup models\n        self.net.train()\n\n        if len(feats) == 1:  # avoid BN error\n            self.net.eval()\n\n        feats, _, _ = self.mem.get_memory()\n        feats = torch.stack(feats)\n        dataset = torch.utils.data.TensorDataset(feats)\n        data_loader = DataLoader(dataset, batch_size=conf.args.opt['batch_size'],\n                                 shuffle=True, drop_last=False, pin_memory=False)\n\n        entropy_loss = HLoss(temp_factor=conf.args.temperature)\n\n        for e in range(conf.args.epoch):\n\n            for batch_idx, (feats,) in enumerate(data_loader):\n                feats = feats.to(device)\n                preds_of_data = self.net(feats) # update bn stats\n\n\n                if conf.args.no_optim:\n                    pass # no optimization\n                else:\n                    loss = entropy_loss(preds_of_data)\n\n                    self.optimizer.zero_grad()\n                    loss.backward()\n                    self.optimizer.step()\n\n        self.log_loss_results('train_online', epoch=current_num_sample, loss_avg=0)\n\n        return TRAINED",
        "experimental_info": "Method Name: NOTE\n\nCore Components Configuration:\n- Instance-Aware Batch Normalization (IABN):\n  - Enabled: True (`--iabn` flag)\n  - Hyperparameter 'α' (k): 3.0 (`--iabn_k 3.0`)\n- Normalization Statistics Update:\n  - Mode: Exponential Moving Average (EMA) (`--use_learned_stats` flag)\n  - Momentum for EMA: 0.1 (`--bn_momentum 0.1`)\n- Memory Mechanism:\n  - Type: Prediction-Balanced Reservoir Sampling (PBRS) (`--memory_type PBRS`)\n  - Capacity: 1 sample (`--memory_size 1`)\n- Affine Parameters Optimization:\n  - Loss Function: Entropy Minimization (via HLoss)\n  - Temperature for HLoss: 1.0 (`--temperature 1.0`)\n\nOnline Adaptation Settings:\n- Online Learning: Enabled (`--online` flag)\n- Adaptation Frequency: Update every 1 target sample (`--update_every_x 1`)\n- Epochs per update: 1 (`--epoch 1`)\n\nData Stream Settings for Temporal Correlation:\n- Target Training Distribution: Dirichlet distribution (`--tgt_train_dist 4`)\n- Dirichlet Beta parameter: 0.1 (`--dirichlet_beta 0.1`)\n\nInference: Batch-free (implicit when `--use_learned_stats` is enabled, as running stats are maintained via EMA)."
      }
    },
    {
      "title": "Improved Test-Time Adaptation for Domain Generalization",
      "abstract": "The main challenge in domain generalization (DG) is to handle the\ndistribution shift problem that lies between the training and test data. Recent\nstudies suggest that test-time training (TTT), which adapts the learned model\nwith test data, might be a promising solution to the problem. Generally, a TTT\nstrategy hinges its performance on two main factors: selecting an appropriate\nauxiliary TTT task for updating and identifying reliable parameters to update\nduring the test phase. Both previous arts and our experiments indicate that TTT\nmay not improve but be detrimental to the learned model if those two factors\nare not properly considered. This work addresses those two factors by proposing\nan Improved Test-Time Adaptation (ITTA) method. First, instead of heuristically\ndefining an auxiliary objective, we propose a learnable consistency loss for\nthe TTT task, which contains learnable parameters that can be adjusted toward\nbetter alignment between our TTT task and the main prediction task. Second, we\nintroduce additional adaptive parameters for the trained model, and we suggest\nonly updating the adaptive parameters during the test phase. Through extensive\nexperiments, we show that the proposed two strategies are beneficial for the\nlearned model (see Figure 1), and ITTA could achieve superior performance to\nthe current state-of-the-art methods on several DG benchmarks. Code is\navailable at https://github.com/liangchen527/ITTA.",
      "full_text": "Improved Test-Time Adaptation for Domain Generalization Liang Chen1 Yong Zhang2* Yibing Song3 Ying Shan2 Lingqiao Liu1∗ 1 The University of Adelaide 2 Tencent AI Lab 3 AI3 Institute, Fudan University {liangchen527, zhangyong201303, yibingsong.cv}@gmail.com yingsshan@tencent.com lingqiao.liu@adelaide.edu.au Abstract The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Gen- erally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for up- dating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments in- dicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly consid- ered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, in- stead of heuristically defining an auxiliary objective, we pro- pose a learnable consistency loss for the TTT task, which con- tains learnable parameters that can be adjusted toward bet- ter alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adap- tive parameters during the test phase. Through extensive ex- periments, we show that the proposed two strategies are ben- eficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-art methods on several DG benchmarks. Code is available at https://github.com/liangchen527/ITTA. 1. Introduction Recent years have witnessed the rapid development of deep learning models, which often assume the training and test data are from the same domain and follow the same distribution. However, this assumption does not always hold in real-world scenarios. Distribution shift among the source and target domains is ubiquitous in related areas [35], such as autonomous driving or object recognition tasks, resulting *Corresponding authors. This work is done when L. Chen is an intern in Tencent AI Lab. 0.5 1.1 0.5 1.2 0.5 0.5 0.5 1.4 0.4 0.4 0.4 0.3 art cartoon photo sketch 79.9 75.4 94.4 75.8 83.3 76.0 94.4 76.7 84.7 78.0 94.5 78.2 Figure 1. Performance improvements from the proposed two strate- gies (i.e. introducing a learnable consistency loss and including additional adaptive parameters to improve TTT) for the baseline model (i.e. ResNet18 [30] with existing augmentation strategy [75]). Experiments are conducted on the PACS dataset [37] with the leave- one-out setting. Following [27], we use 60 sets of random seeds and hyper-parameters for each target domain. The reported average accuracy and error bars verify the effectiveness of our method. in poor performances for delicately designed models and hindering the further application of deep learning techniques. Domain generalization (DG) [2,8,16,23,24,31,38 –40,40, 44, 47, 51, 52, 69], designed to generalize a learned model to unseen target domains, has attracted a great deal of attention in the research community. The problem can be traced back to a decade ago [7], and various approaches have been pro- posed to push the DG boundary ever since. Those efforts in- clude invariant representation learning [28,47,49,58], adver- sarial learning [23,40,44,69], augmentation [9,41,42,66,75], or meta-learning [2, 16, 38, 39]. Despite successes on certain occasions, a recent study [27] shows that, under a rigorous evaluation protocol, most of these arts are inferior to the baseline empirical risk minimization (ERM) method [61]. This finding is not surprising, as most current arts strive to decrease the distribution shift only through the training data while overlooking the contributions from test samples. Recently, the test-time training (TTT) technique [60] has been gaining momentum for easing the distribution shift problem. TTT lies its success in enabling dynamic tuning of the pretrained model with the test samples via an auxil- iary TTT task, which seems to be a promising effort when arXiv:2304.04494v2  [cs.CV]  16 Apr 2023confronting data from different domains. However, TTT is not guaranteed to improve the performance. Previous arts [46, 63] indicate that selecting an appropriate auxiliary TTT task is crucial, and an inappropriate one that does not align with the main loss may deteriorate instead of improv- ing the performance. Meanwhile, it is pointed out in [63] that identifying reliable parameters to update is also essential for generalization, which is in line with our experimental findings in Sec. 5.3. Both of these two tasks are non-trivial, and there are limited efforts made to address them. This paper aims to improve the TTT strategy for better DG. First, different from previous works that empirically define auxiliary objectives and assume they are aligned with the main task, our work does not make such assumptions. Instead, we suggest learning an appropriate auxiliary loss for test-time updating. Specifically, encouraged by recent successes in multi-view consistency learning [13,26,29], we propose to augment the consistency loss by adding learn- able parameters based on the original implementation, where the parameters can be adjusted to assure our TTT task can be more aligned with the main task and are updated by en- forcing the two tasks share the same optimization direction. Second, considering that identifying reliable parameters to update is an everlasting job given the growing size of current deep models, we suggest introducing new adaptive param- eters after each block during the test phase, and we only tune the new parameters by the learned consistency loss while leaving the original parameters unchanged. Through extensive evaluations on the current benchmark [27], we illustrate that the learnable consistency loss performs more effectively than the self-supervised TTT tasks adopted in previous arts [60, 63], and by tuning only the new adaptive parameters, our method is superior to existing strategies that update all the parameters or part of them. This work aims to ease the distribution shift problem by improving TTT, and the main contributions are three-fold: • We introduce a learnable consistency loss for test-time adaptation, which can be enforced to be more aligned with the main loss by tuning its learnable parameters. • We introduce new adaptive parameters for the trained model and only update them during the test phase. • We conduct experiments on various DG benchmarks and illustrate that our ITTA performs competitively against current arts under the rigorous setting [27] for both the multi-source and single-source DG tasks. 2. Related Works 2.1. Domain Generalization. Being able to generalize to new environments while de- ploying is a challenging and practical requirement for cur- rent deep models. Existing DG approaches can be roughly categorized into three types. (1) Invariant representation learning: The pioneering work [5] theoretically proves that if the features remain invariant across different domains, then they are general and transferable to different domains. Guided by this finding, [47] uses maximum mean discrep- ancy (MMD) to align the learned features, and [25] proposes to use a multi-domain reconstruction auto-encoder to obtain invariant features. More recently, [58] suggests maximiz- ing the inner product of gradients from different domains to enforce invariance, and a similar idea is proposed in [52] where these gradients are expected to be similar to their mean values. (2) Optimization algorithms: Among the different optimization techniques adopted in DG, prevail- ing approaches resort to adversarial learning [23, 40, 44, 69] and meta-learning [2, 16, 38, 39]. Adversarial training is often used to enforce the learned features to be agnostic about the domain information. In [23], a domain-adversarial neural network (DANN) is implemented by asking the main- stream feature to maximize the domain classification loss. This idea is also adopted in [44], where adversarial training and an MMD constraint are employed to update an auto- encoder. Meanwhile, the meta-learning technique is used to simulate the distribution shifts between seen and unseen environments [2, 16, 38, 39], and most of these works are developed based on the MAML framework [20]. (3) Aug- mentation: Most augmentation skills applied in the general- ization tasks are operated in the feature level [34, 41, 48, 75] except for [11,66,68] which mix images [68] or its phase [66] to synthesize new data. To enable contrastive learning, we incorporate an existing augmentation strategy [75] in our framework. This method originated from AdaIN [32], which synthesizes new domain information by mixing the statistics of the features. Similar ideas can be found in [42, 48]. 2.2. Test-Time Training and Adaptation Test-Time Training (TTT) is first introduced in [60]. The basic paradigm is to employ a test-time task besides the main task during the training phase and update the pre- trained model using the test data with only the test-time objective before the final prediction step. The idea is empir- ically proved effective [60] and further developed in other related areas [3, 10, 12, 14, 21, 22, 43, 56, 63, 65, 73, 74]. Most current works focus on finding auxiliary tasks for updat- ing during the test phase, and the efforts derive from self- supervion [3, 10, 21, 22, 43, 60], meta-learning [65, 73, 74], information entropy [63], pseudo-labeling [12, 14], to name a few. However, not all empirically selected test-time tasks are effective. A recent study [46] indicates that only when the auxiliary loss aligns with the main loss can TTT improve the trained model. Inspired by that, we propose a learnable consistency loss and enforce alignment between the two ob- jectives. Results show that our strategy can be beneficial for the trained model (see Figure 1).subtract Figure 2. Training process of ITTA. We use x from the source domain as input for the feature extractor fθ(·) to obtain the repre- sentation z and its augmented version z′, where the augmentation skill from [75] is applied. The classifier fϕ(·) and weight subnet- work fw(·) are used to compute the main loss Lmain and learnable consistency loss Lwcont. Please refer to our text for details. Meanwhile, [63] suggests that auxiliary loss is not the only factor that affects the performance. Selecting reliable parameters to update is also crucial within the TTT frame- work. Given the large size of current models, correctly iden- tifying these parameters may require tremendous amounts of effort. To this end, instead of heuristically selecting candi- dates, we propose to include new adaptive parameters for up- dating during the test phase. Experimental results show that the proposed method can obtain comparable performances against existing skills. 3. Methodology In the task of DG, we are often given access to data from S (S ≥ 1) source domains Ds = {D1, D2, ..., DS} and expect a model to make good prediction on unseen target domains Dt = {D1, D2, ..., DT } (T ≥ 1). Our method aims to improve the test-time training (TTT) strategy for better DG. The improvements are two-fold. First, we pro- pose a learnable consistency loss for the TTT task, which could be enforced to align with the main objective by tuning its learnable weights. Second, we suggest including addi- tional adaptive parameters and only updating these adaptive parameters during the test phase. 3.1. A Learnable Consistency Loss for TTT The TTT strategies have shown promising performances when dealing with distribution shift problems [43, 63]. How- ever, their successes are depended on the empirically selected auxiliary TTT tasks, which may deteriorate the performances if chosen improperly. Motivated by the recent successes in multi-view consistency learning [13, 26, 29], we suggest adopting a consistency loss in our TTT task. Note that the naive consistency loss is still not guaranteed to be effective as prior art [46] indicates that only when the auxiliary loss aligns with the main loss, can TTT improves the perfor- mance. To this end, we propose to augment the auxiliary loss with learnable parameters that could be adjusted toward a better alignment between the TTT and main tasks. In our case, we make the adopted consistency loss learnable by introducing a weight subnetwork that allows flexible ways Algorithm 1 Pseudo code of the training phase of ITTA in a PyTorch-like style. # fθ, fϕ, fw: feature extractor, classifier, weight subnetwork # α, 0: weight paramter, all zero tensor # training process for x, yin training loader: # load a minibatch with N samples def forward process(x, y): z, z′ = fθ.forward(x) # computing losses Lmain = CrossEntropyLoss(fϕ.forward(z), y) Lmain+ =CrossEntropyLoss(fϕ.forward(z′), y) Lwcont = MSELoss(fw.forward(z − z′), 0) return Lmain, Lwcont # SGD update: feature extractor and classifier Lmain, Lwcont = forward process(x, y) ([fθ.params, fϕ.params]).zero grad() (Lmain + αLwcont).backward() update( \u0002 fθ.params, fϕ.params \u0003 ) # compute objectives for updating weight subnetwork Lmain, Lwcont = forward process(x, y) Lmain.backward() ˆgmain = fθ.params.grad.clone().normalize() fθ.params.zero grad() Lwcont.backward() ˆgwcont = fθ.params.grad.clone().normalize() # SGD update: weight subnetwork MSELoss(ˆgmain, ˆgwcont).backward() fw.params.zero grad() update(fw.params) to measure the consistency between two views of the same instance. We first introduce the pipeline of our training framework. Given the D dimensional representation z ∈ RD1 and its corresponding augmented version z′ that are obtained from a feature extractor (i.e. {z, z′} = fθ(x), where x is an input image from Ds, and fθ(·) is the feature extractor parame- terized by θ. In our implementation, we use the existing augmentation method [75] to obtain z′ by modifying the intermediate activation in fθ(x). We show in our supplemen- tary material that our framework can also thrive with other augmentation strategies), our learnable consistency loss is given by, Lwcont = ∥fw(z − z′)∥, (1) where ∥ · ∥denotes the L2 norm; fw(·) is the weight sub- network parameterized by w. To make the training process more stable and potentially achieve better performance, we apply a dimension-wise nonlinear function to map each di- mension of z − z′ before calculating the L2 norm. That is, ∀h ∈ RD, fw(h) is implemented by stacking layers of a nonlinear function: ReLU(a ∗ h + b), where a ∈ RD and b ∈ RD are the weight and bias from the nonlinear function, 1We omit the batch dimensions of the variables for simplicity.… … subtract Figure 3. Test adaptation process of ITTA. Different from that in the training stage, we include additional adaptive parameters fΘ after each block of the feature extractor fθ. For each test sample x, the intermediate representations zi and z′i obtained from fi θ are passed to fi Θ before going to the next block fi+1 θ . We use the learnable consistency loss Lwcont as the objective to update fΘ. Please refer to our text for details. and different layers of a, bform the parameter w in fw. In effect, this creates a piecewise-linear mapping function for h: depending on the value of h, the output could be 0, a constant, or a scaling-and-shifted version of h. More studies about the design of fw are provided in our supplementary material. Compared to the naive consistency learning with- out fw, our Lwcont can be more flexible with an adjustable fw, which we show in the following is the key for learning an appropriate loss in the improved TTT framework. Combining Lwcont with the main loss Lmain which applies the cross-entropy loss (CE) for both the origi- nal and augmented inputs ( i.e. Lmain = CE(fϕ(z), y) + CE(fϕ(z′), y), where fϕ is the classifier parameterized by ϕ, and y is the corresponding label), the objective for the feature extractor and classifier can be formulated into, min{θ,ϕ} Lmain + αLwcont, (2) where α is the weight parameter that balances the contri- butions from the two terms. A simple illustration of the workflow is shown in Figure 2. From Eq. (2), the expected gradients for the feature ex- tractor from Lmain and Lwcont can be represented as, \u001a gmain = ∇θ(CE(fϕ(z), y) + CE(fϕ(z′), y)), (3) gwcont = ∇θ∥fw(z − z′)∥. (4) We observe that the direction of gwcont is also determined by the weight subnetwork fw(·), which should be close with gmain to ensure alignment between Lmain and Lwcont [46, 60]. To this end, we propose a straightforward solution by enforcing equality between the normalized versions of gmain and gwcont, and we use this term as the objective for updating fw(·), which gives, min w Lalign, s.t. Lalign = ∥ˆgmain − ˆgwcont∥, (5) where ˆgmain = gmain−Egmain σgmain , and similar for ˆgwcont. In our implementation, we update {θ, ϕ} and w in an alternative manner. Pseudo code of the training process are shown in Algorithm 1. Algorithm 2 Pseudo code of the test phase of ITTA in a PyTorch-like style. # fθ, fϕ: feature extractor, classifier # fw, fΘ: weight subnetwork, additional adaptive blocks # m, 0: total number of blocks in fθ, all zero tensor # test process for x in test loader: # load a test batch def forward process(x): z1, z′1 = f1 Θ.forward((f1 θ .forward(x))) # first blocks for i in range(2, m + 1): # the following m − 1 blocks zi, z′i = fi θ.forward(zi−1), fi θ.forward(z′i−1) zi, z′i = fi Θ.forward(zi), fi Θ.forward(z′i) return zi, z′i # test adaptation phase: SGD update additional adaptive parameters z, z′ = forward process(x) Lwcont = MSELoss(fw.forward(z − z′), 0) fΘ.params.zero grad() Lwcont.backward() update(fΘ.params) # final prediction z, = forward process(x) result = fϕ.forward(z) 3.2. Including Additional Adaptive Parameters Selecting expressive and reliable parameters to update during the test phase is also essential in the TTT frame- work [63]. Some strategies decide to update all the parame- ters from the feature extractor [3, 43], while others use only the parameters from the specific layers for updating [63, 71]. Given the fact that the sizes of current deep models are often very large and still growing, exhaustively trying different combinations among the millions of candidates seems to be an everlasting job. As there are no consensuses on which parameter should be updated, we suggest another easy alter- native in this work. Specifically, assuming there are a total of m blocks in the pretrained feature extractor fθ(·), and the i-th block can be denoted as fi θ(·). Then the intermediate representation zi from fi θ(·) can be formulated as, zi = fi θ(zi−1), s.t. z1 = f1 θ (x). (6) We propose to include additional adaptive blockfΘ that is parameterized by Θ after each block of fθ during the test- time adaptation phase, which reformulates Eq. (6) into, zi = fi Θ(fi θ(zi−1)), s.t. z1 = f1 Θ(f1 θ (x)), (7) where fΘ(·) does not change the dimension and sizes of the intermediate representations. In our work, we use a structure similar to fw to implement fΘ. Note zm is simplified as z in this phase, and the same process is applied for obtaining z′. Then, in the test-time adaptation phase, we suggest only updating the new adaptive parameters via the learned con- sistency loss. The optimization process can be written as,Table 1. Multi sources domain generalization. Experiments are conducted on the DomainBed benchmark [27]. All methods are examined for 60 trials in each unseen domain. Top5 accumulates the number of datasets where a method achieves the top 5 performances. The score here accumulates the numbers of the dataset where a specific art obtains larger accuracy than ERM on account of the variance. Best results are colored as red. Among the 22 methods compared, less than a quarter outperforms ERM in most datasets (Score ≥ 3). PACS VLCS OfficeHome TerraInc DomainNet Avg. Top5↑ Score↑ MMD [40] 81.3 ± 0.8 74.9 ± 0.5 59.9 ± 0.4 42.0 ± 1.0 7.9 ± 6.2 53.2 1 2 RSC [33] 80.5 ± 0.2 75.4 ± 0.3 58.4 ± 0.6 39.4 ± 1.3 27.9 ± 2.0 56.3 0 1 IRM [1] 80.9 ± 0.5 75.1 ± 0.1 58.0 ± 0.1 38.4 ± 0.9 30.4 ± 1.0 56.6 0 1 ARM [72] 80.6 ± 0.5 75.9 ± 0.3 59.6 ± 0.3 37.4 ± 1.9 29.9 ± 0.1 56.7 0 0 DANN [23] 79.2 ± 0.3 76.3 ± 0.2 59.5 ± 0.5 37.9 ± 0.9 31.5 ± 0.1 56.9 1 1 GroupGRO [55] 80.7 ± 0.4 75.4 ± 1.0 60.6 ± 0.3 41.5 ± 2.0 27.5 ± 0.1 57.1 0 1 CDANN [44] 80.3 ± 0.5 76.0 ± 0.5 59.3 ± 0.4 38.6 ± 2.3 31.8 ± 0.2 57.2 0 0 VREx [36] 80.2 ± 0.5 75.3 ± 0.6 59.5 ± 0.1 43.2 ± 0.3 28.1 ± 1.0 57.3 1 1 CAD [53] 81.9 ± 0.3 75.2 ± 0.6 60.5 ± 0.3 40.5 ± 0.4 31.0 ± 0.8 57.8 1 2 CondCAD [53] 80.8 ± 0.5 76.1 ± 0.3 61.0 ± 0.4 39.7 ± 0.4 31.9 ± 0.7 57.9 0 1 MTL [6] 80.1 ± 0.8 75.2 ± 0.3 59.9 ± 0.5 40.4 ± 1.0 35.0 ± 0.0 58.1 0 0 ERM [61] 79.8 ± 0.4 75.8 ± 0.2 60.6 ± 0.2 38.8 ± 1.0 35.3 ± 0.1 58.1 1 - MixStyle [75] 82.6 ± 0.4 75.2 ± 0.7 59.6 ± 0.8 40.9 ± 1.1 33.9 ± 0.1 58.4 1 1 MLDG [38] 81.3 ± 0.2 75.2 ± 0.3 60.9 ± 0.2 40.1 ± 0.9 35.4 ± 0.0 58.6 1 1 Mixup [68] 79.2 ± 0.9 76.2 ± 0.3 61.7 ± 0.5 42.1 ± 0.7 34.0 ± 0.0 58.6 2 2 Fishr [52] 81.3 ± 0.3 76.2 ± 0.3 60.9 ± 0.3 42.6 ± 1.0 34.2 ± 0.3 59.0 2 2 SagNet [48] 81.7 ± 0.6 75.4 ± 0.8 62.5 ± 0.3 40.6 ± 1.5 35.3 ± 0.1 59.1 1 2 SelfReg [34] 81.8 ± 0.3 76.4 ± 0.7 62.4 ± 0.1 41.3 ± 0.3 34.7 ± 0.2 59.3 2 3 Fish [58] 82.0 ± 0.3 76.9 ± 0.2 62.0 ± 0.6 40.2 ± 0.6 35.5 ± 0.0 59.3 3 4 CORAL [59] 81.7 ± 0.0 75.5 ± 0.4 62.4 ± 0.4 41.4 ± 1.8 36.1 ± 0.2 59.4 2 3 SD [51] 81.9 ± 0.3 75.5 ± 0.4 62.9 ± 0.2 42.0 ± 1.0 36.3 ± 0.2 59.7 4 4 Ours 83.8 ± 0.3 76.9 ± 0.6 62.0 ± 0.2 43.2 ± 0.5 34.9 ± 0.1 60.2 4 4 min Θ ∥fw(z − z′)∥, s.t. {z, z′} = fΘ(fθ(x)). (8) Note that different from the training phase, x in this stage is from the target domain Dt, and we use the online setting in [60] for updating. A simple illustration of the test adaptation pipeline is shown in Figure 3. For the final step, we use the original representation ob- tained from the pretrained feature extractor and the adapted adaptive parameters for prediction. Pseudo code of the test stage are shown in Algorithm 2. 4. Experiments 4.1. Settings Datasets. We evalute ITTA on five benchmark datasets: PACS [37] which consists of 9,991 images from 7 cate- gories. This dataset is probably the most widely-used DG benchmark owing to its large distributional shift across 4 do- mains including art painting, cartoon, photo, and sketch; VLCS [18] contains 10,729 images of 5 classes from 4 different datasets (i.e. domains) including PASCAL VOC 2007 [17], LabelMe [54], Caltech [19], and Sun [64] where each dataset is considered a domain in DG;OfficeHome [62] is composed of 15,588 images from 65 classes in office and home environments, and those images can be categorized into 4 domains (i.e. artistic, clipart, product, and real world); TerraInc [4] has 24,788 images from 10 classes. Those images are wild animals taken from 4 different locations (i.e. domains) including L100, L38, L43, and L46; Domain- Net [50] which contains 586,575 images from 345 classes, and the images in it can be depicted in 6 styles (i.e. clipart, infograph, painting, quickdraw, real, and sketch). Implementation details. For all the experiments, we use the ImageNet [15] pretrained ResNet18 [30] backbone that with 4 blocks as the feature extractor fθ, which could en- large the gaps in DG compared to larger models [70]. Corre- spondingly, we also include 4 blocks of additional adaptive parameters (i.e. fΘ), and each block is implemented with 5 layers of learnable parameters with weight initialized as all ones and bias initialized as all zeros. For the weight subnet- work fw, we use 10 layers of learnable parameters with the initialization skill similar to that of fΘ. The classifier fϕ is an MLP layer provided by the Domainbed benchmark [27]. For the weight parameter α in Eq. (2), we set it to be 1 for all experiments (please refer to our supplementary material for analysis). The random seeds, learning rates, batch size, and augmentation skills are all dynamically set for all the compared arts according to [27].Table 2. Single source domain generalization. Experiments are conducted on the PACS dataset [37]. Here A, C, P, and S are the art, cartoon, photo, and sketch domains in PACS. A→C represents models trained on the art domain and tested on the cartoon domain, and similar for others. All methods are examined for 60 trials in each unseen domain. Best results are colored as red. A→C A →P A →S C →A C →P C →S P →A P →C P →S S →A S →C S →P Avg. RSC 66.3 ±1.3 88.2±0.6 57.2±3.1 65.8±1.5 82.4±0.6 68.7±2.5 60.5±2.0 41.3±6.0 53.1±2.8 53.8±1.6 65.9±0.7 48.4±1.9 62.6 Fish 67.1 ±0.5 89.2±1.8 57.0±0.2 66.7±1.0 85.6±0.4 64.5±3.6 55.1±2.1 33.9±2.3 51.2±4.2 59.1±3.2 67.1±0.9 58.4±1.2 62.9 CDANN 66.5±1.7 92.2±0.6 65.0±0.9 70.6±0.1 82.9±1.4 67.7±3.0 60.6±0.3 42.2±6.4 46.9±9.9 51.4±2.3 60.7±1.2 51.9±0.4 63.2 SelfReg 63.9±1.9 90.1±1.0 56.8±2.2 70.2±2.3 85.4±0.3 70.2±2.2 60.9±2.6 38.8±4.0 50.5±3.2 54.5±4.7 66.2±1.2 51.7±4.1 63.3 DANN 67.5 ±1.6 91.2±1.3 67.5±1.3 70.6±1.0 81.4±0.4 66.6±1.1 54.1±2.3 33.5±2.7 52.8±2.3 53.8±1.7 64.4±0.7 58.9±0.8 63.5 CAD 67.1 ±1.5 89.6±0.4 60.2±0.2 67.7±3.1 83.7±1.4 70.2±2.6 60.6±2.6 38.3±3.7 53.8±3.2 50.7±1.6 65.8±1.3 54.4±1.7 63.5 GroupGRO66.5±1.2 90.5±1.5 58.9±2.5 70.8±0.9 85.7±1.2 69.7±1.8 62.3±2.1 41.1±2.7 48.2±4.1 54.8±0.5 65.2±1.6 53.9±1.4 64.0 MTL 67.3 ±1.0 90.1±1.0 58.9±0.7 70.2±1.8 84.2±2.2 71.9±0.7 58.3±2.7 38.5±2.7 52.8±1.5 55.4±3.1 66.1±1.3 55.2±2.6 64.1 IRM 67.5 ±1.8 93.0±0.5 62.9±4.7 67.6±1.3 83.8±0.4 68.9±0.8 63.7±1.8 39.9±3.7 49.0±5.4 54.9±1.4 63.1±2.1 54.9±1.4 64.1 ARM 66.0 ±2.4 91.2±0.7 58.7±6.9 70.6±0.8 84.2±1.0 69.1±0.9 59.2±1.8 42.1±5.6 52.1±3.0 60.0±0.6 62.9±3.3 53.8±2.0 64.2 Mixup 65.5 ±0.8 87.8±0.3 57.2±1.0 71.4±1.1 83.1±1.8 68.0±3.0 59.6±1.7 37.2±2.7 56.5±3.8 55.0±2.2 66.2±1.5 62.7±4.2 64.2 CORAL 66.8±0.5 90.3±0.7 61.5±1.9 67.9±2.1 85.4±0.3 70.4±1.3 55.9±2.9 40.4±4.9 49.8±8.5 55.8±2.1 67.6±0.9 58.9±3.8 64.2 SD 67.1 ±1.3 91.7±1.2 63.7±4.1 70.3±0.9 84.4±0.7 69.4±2.3 57.5±2.5 42.6±0.8 47.7±1.7 55.9±2.4 65.7±0.8 55.8±2.1 64.3 MMD 67.1 ±1.4 88.0±0.8 63.6±1.6 70.0±1.1 83.6±0.2 70.2±1.0 58.8±2.6 40.3±1.0 52.3±2.4 57.4±1.9 68.7±0.9 52.7±3.7 64.4 MLDG 67.3±2.0 90.8±0.5 64.4±0.9 70.8±1.0 84.2±0.3 69.7±1.8 61.6±1.0 41.3±5.1 50.4±0.2 49.9±2.5 66.8±0.4 58.7±3.4 64.7 CondCAD66.9±1.4 92.3±0.7 60.8±4.5 71.0±0.6 84.7±1.1 72.6±0.5 61.2±1.5 40.7±3.6 55.7±1.6 52.3±1.7 64.2±0.4 55.3±1.2 64.8 ERM 67.3 ±0.7 91.7±0.9 60.1±4.7 70.4±0.6 82.3±2.7 68.1±0.9 59.6±1.8 44.7±2.8 56.5±2.7 52.8±2.3 68.1±0.7 58.4±0.9 65.0 VREx 67.1 ±1.5 91.0±1.0 62.6±3.5 71.1±2.4 84.1±0.9 71.7±1.3 62.4±3.1 37.7±3.3 53.6±2.3 60.6±1.6 66.7±0.8 57.5±1.4 65.5 Fishr 67.9 ±1.9 92.7±0.3 62.4±4.7 71.2±0.5 83.4±0.6 70.2±1.1 60.0±2.3 42.7±3.2 57.1±3.9 55.7±3.7 68.4±1.0 62.0±3.1 66.1 SagNet 67.6±1.4 92.3±0.5 59.5±1.7 71.8±0.3 82.8±0.6 69.9±1.8 62.5±2.5 45.2±2.5 64.1±2.0 55.8±1.1 65.7±1.4 55.9±3.5 66.1 MixStyle 68.5±2.0 91.2±1.6 65.1±0.7 73.2±1.3 85.0±0.8 71.7±1.5 63.6±1.7 46.3±1.1 51.6±3.7 54.2±1.5 67.0±3.4 58.3±1.4 66.3 Ours 68.9 ±0.6 92.4±0.1 62.5±0.6 75.3±0.4 85.9±0.3 70.2±1.4 66.5±1.1 52.2±2.7 63.8±1.1 57.6±3.7 68.0±1.3 57.9±2.0 68.4 Training and evaluation details. For all the compared methods, we conduct 60 trials on each source domain, and each with 5,000 iteration steps. During the training stage, we split the examples from training domains to 8:2 (train:val) where the training and validation samples are dynamically selected among different training trials. During test, we select the model that performs the best in the validation samples and test it on the target domains. The strategy is referred to as the “training-domain validate set” model selec- tion method in [27]. For each domain in different datasets, the final performance is the average accuracy from the 60 trials. 4.2. Multi-Source Generalization In these experiments, all five benchmark datasets afore- mentioned are used for evaluation, and the leave-one-out strategy is adopted for training (i.e. with S = |Ds ∪Dt|2 −1, and T = 1). Results are shown in Table 1. We note that ERM method obtains favorable performance against existing arts. In fact, as a strong baseline, ERM is superior to half of the methods in the term of average accuracy, and only 5 arts (i.e. SelfReg [34], Fish [58], CORAL [59], SD [51], and ours) among the compared 22 methods outperforms ERM in most datasets (i.e. with Score ≥ 3). In comparison, the proposed ITTA is more effective than all other models on average. In particular, ITTA achieves the best performances in 3 out of the 5 benchmarks (i.e. PACS, VLCS, and TerraInc datasets) and 4 in the top 5. Note that although our method does not obtain the best performances in the OfficeHome and DomainNet benchmarks, it still outperforms more than half 2We use | · |to denote the number of domains in the environment. of the existing models. The results validate the effectiveness of our method when tested in the multi-source setting. We present results of average accuracy in each domain from different datasets in the supplementary material. Please refer to it for details. 4.3. Single-Source Generalization In these experiments, we adopt the widely-used PACS [37] benchmark for evaluation, and the models are trained on one domain while tested on the remaining three (i.e. with S = 1, and T = 3). Although some approaches, such as MLDG [38] and Fishr [52], may require more than one domain information for their trainings, we can simu- late multi-domain information using only the source domain, and thus the experimental settings are still feasible for them. Compared to the multi-source generalization task, the single- source generalization is considered more difficult due to the limited domain information during the training phase. Evalu- ation results are presented in Table 2. We note that the ERM method outperforms most state-of-the-art models, and only 5 models, including VREx [36], Fishr [52], SagNet [48], MixStyle [75], and the proposed ITTA, can obtain better re- sults than ERM in the term of average accuracy. Meanwhile, our method achieves the best performances when trained in 5 out of the 12 source domain, and it obtains the best perfor- mance on average, leading more than 2% than the second best (i.e. MixStyle [75]) and 3% the ERM method. In line with the findings in [27], we notice that the naive ERM method [61] can indeed perform favorably against most existing models under rigorous evaluation protocol. As a matter of fact, the proposed method is the only one that consistently outperforms ERM in both the multi-sourceTable 3. Evaluations of different TTT-based models in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch Baseline 79.9 ±0.5 75.4±1.1 94.4±0.5 75.8±1.2 81.4±0.5 TTT [60] 81.5±0.8 77.6±0.6 94.3±0.2 78.4±0.7 83.0±0.2 MT3 [3] 82.0 ±1.0 76.5±1.0 94.1±0.2 77.7±1.3 82.6±0.6 TENT [63] 80.2±0.9 77.2±0.8 94.4±0.2 77.4±0.1 82.3±0.5 Ours 84.7 ±0.4 78.0±0.4 94.5±0.4 78.2±0.3 83.8±0.3 and single-source settings. These results indicate that DG remains challenging for current efforts that aim to ease the distribution shift only through training data, and using the proposed improved TTT strategy may be a promising direc- tion for solving DG. 5. Analysis All experiments in this section are conducted on the widely-used PACS benchmark [37] with the leave-one-out strategy. The experimental settings are the same as that illus- trated in Sec. 4.1. Please refer to our supplementary material for more analysis. 5.1. Compared with Other TTT-Based Models Using test-time adaptation to ease the distribution shift problem has been explored in previous works, such as the original TTT method [60] and MT3 [3]. Their differences lie in that TTT uses a rotation estimation task for the test-time objective, and MT3 adopts a contrastive loss for the task and implements the overall framework using MAML [20]. There is also a recently proposed TENT [63] that aims to minimize the entropy of the final results by tuning the parameters from the batch normalization (BN) layers. To analyze the overall effectiveness of our method, we compare ITTA with these arts using the same baseline (i.e. ResNet18 [30] backbone with the existing augmentation skill [75]). Results are shown in Table 3. We observe that all the com- pared TTT-based methods can improve the baseline model in almost all target domains except for the “Photo” domain, which might be due to the ImageNet pretraining [67]. This phenomenon demonstrates that the TTT strategy may be a promising effort for easing the distribution shift problem. Meanwhile, we observe that the proposed ITTA is superior to all other approaches in most target domains and leads in the term of average accuracy. The main reason is that compared to the empirically designed TTT tasks adopted in previous works, the proposed learnable consistency loss is enforced to be more aligned with the main loss, thus more suitable for the test-time adaptation task [46]. Meanwhile, compared to the strategies that update the original param- eters from the trained model, the adaptation of the newly included parameters is also more effective for the overall (a) Input (b) Ours w/o fw (c) Ours (d) Main Figure 4. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels from the four target do- mains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). Ours w/o fw is the naive consis- tency loss with fw disabled in Eq. (1). The proposed learnable consistency loss can align well with the main classification task. TTT framework. In the following, we provide more analysis to support these claims. 5.2. Effectiveness of the Learnable Consistency Loss To examine the effectiveness of our learnable consistency loss, we conduct ablation studies by comparing our method with the following variants. (1) Ours w/o fw: we disable fw when computing the learnable consistency loss in Eq. (1), which uses the naive consistency loss for the auxiliary TTT task. (2) Ours w/ Ent.: after training the model using the baseline settings (i.e. ResNet18 with the augmentation strat- egy [75]), we use the entropy minimization task in [63] for the TTT task. (3) Ours w/ Rot.: we use the rotation estimation task in [60] for the TTT task. To ensure fair com- parisons, we use the same baseline settings and include the same additional adaptive parameters for all the variants. Results are shown in the 4th to 6th rows Table 4. We find that the results from the naive consistency loss ( i.e. Ours w/o fw) are slightly better than that from the other two specially-designed objectives (i.e. Ours w/ Ent. and Ours w/ Rot.) on average. Besides the possibility of deteriorating the performance [46], our results indicate that empirically select- ing a TTT task may also be far from optimal. Meanwhile, we observe that when enabling fw, the proposed learnable consistency loss is superior to that withoutfw in all target do-Table 4. Comparison between different TTT tasks and parameter selecting strategies in the unseen domain from the PACS benchmark [37]. Here the “Ent.”, “Rot.”, and “Lwcont” denotes the entropy minimization task in [63], the rotation estimation task in [60], and the proposed learnable consistency objective, the “All”, “BN”, and “Ada.” are the strategies that update all the parameters, parameters from the batch normalization layer, and the proposed strategy that updates only the new additional adaptive parameters. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model TTT tasks Param selectings Target domain Avg.Ent. Rot. Lwcont All BN Ada. Art Cartoon Photo Sketch Ours − − ✓ − − ✓ 84.7±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 Ours w/ofw − − − − − ✓ 83.1±0.4 74.6 ±0.6 94.0 ±0.5 78.0 ±0.8 82.5 ±0.1 Ours w/ Ent. ✓ − − − − ✓ 79.9±2.4 77.3 ±0.3 94.8 ±0.8 77.6 ±0.4 82.4 ±0.8 Ours w/ Rot. − ✓ − − − ✓ 81.1±1.0 75.2 ±0.5 94.9 ±0.3 77.3 ±0.6 82.1 ±0.3 Ours w/o TTT − − ✓ − − − 83.3±0.5 76.0 ±0.5 94.4 ±0.5 76.7 ±1.4 82.8 ±0.3 Ours w/ All − − ✓ ✓ − − 83.0±0.7 77.0 ±1.4 94.5 ±0.7 77.4 ±0.9 83.0 ±0.2 Ours w/ BN − − ✓ − ✓ − 81.8±0.5 75.6 ±0.3 94.4 ±0.3 77.9 ±1.1 82.4 ±0.5 mains, and it leads in the term of average accuracy among the variants compared, illustrating its advantage against other adopted TTT tasks. These results are not surprising. By comparing the Grad-CAM [57] visualizations from the main classification task with the learnable and naive consistency losses in Figure 4, we find that the proposed learnable objec- tive can well align with the main loss when fw is enabled as the hot zones activated by these two tasks are similar, which guarantees the improvement for the test-time adapta- tion [46, 60]. Please refer to our supplementary material for more visualizations. 5.3. Effectiveness of the Adaptive Parameters We compare ITTA with three variants to demonstrate the effectiveness of the proposed additional adaptive parameters. (1) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model. (2) Ours w/ ALL: similar to the updating strategy in the original TTT method [60], we update all the parameters from the feature extractor during the test phase. (3) Ours w/ BN: following the suggestion from TENT [63], only parameters from the BN layers of the feature extractor are updated. Note the same pretrained model is shared for all variants in these experiments, and the objectives during the test adaptation phase are to minimize the same learned consistency loss. We list the results in the last three rows in Table 4. We observe that when only updating parameters from the BN layers, the performance is inferior to the strategy without test-time adaptation, and updating all the parameters does not ensure improvements in all target domains. The observations are in line with the findings in [63] that selecting reliable parameters to update is essential in the TTT system and may also interact with the choice of the TTT task. In comparison, when including additional adaptive parameters for updating, the pretrained model can be boosted in all environments. The results validate that our adaptive parameters are more effective than that selected with existing strategies [60, 63] when applied with the proposed learnable test-time objective. 5.4. Limitation Although the proposed learned loss can bring satisfaction improvements, we are aware that the lunch is not free. When the weight subnetwork fw is disabled, updating the joint loss in Eq. (2) only costs 1 forward and 1 backward. However, in order to update fw, we have to compute the second-order derivative in Eq. (5), which will require 1 more forward and 3 more backward processes, bringing extra burden to the system. Our future efforts aim to simplify the overall optimization process and reduce the cost for ITTA. 6. Conclusion In this paper, we aim to improve the current TTT strategy for alleviating the distribution shift problem in DG. First, given that the auxiliary TTT task plays a vital role in the over- all framework, and an empirically selecting one that does not align with the main task may potentially deteriorate instead of improving the performance, we propose a learnable con- sistency loss that can be enforced to be more aligned with the main loss by adjusting its learnable parameters. This strategy is ensured to improve the model and shows favorable perfor- mance against some specially-designed objectives. Second, considering that selecting reliable and effective parameters to update during the test phase is also essential while exhaus- tively trying different combinations may require tremendous effort, we propose a new alternative by including new ad- ditional adaptive parameters for adaptation during the test phase. This alternative is shown to outperform some pre- vious parameter selecting strategies via our experimental findings. By conducting extensive experiments under a rig- orous evaluation protocol, we show that our method can achieve superior performance against existing arts in both the multi-source and single-source DG tasks. Acknowledgements. Liang Chen is supported by the ChinaScholarship Council (CSC Student ID 202008440331). References [1] Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. 5, 15, 16, 17 [2] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel- lappa. Metareg: Towards domain generalization using meta- regularization. In NeurIPS, 2018. 1, 2, 14, 15 [3] Alexander Bartler, Andre B¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In AISTATS, 2022. 2, 4, 7 [4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018. 5, 17 [5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NeurIPS, 2006. 2 [6] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017. 5, 15, 16, 17 [7] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generaliz- ing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011. 1 [8] Chaoqi Chen, Jiongcheng Li, Xiaoguang Han, Xiaoqing Liu, and Yizhou Yu. Compound domain generalization via meta- knowledge encoding. In CVPR, 2022. 1 [9] Chaoqi Chen, Luyao Tang, Feng Liu, Gangming Zhao, Yue Huang, and Yizhou Yu. Mix and reason: Reasoning over se- mantic topology with data mixing for domain generalization. In NeurIPS, 2022. 1 [10] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, 2022. 2 [11] Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and Jue Wang. Self-supervised learning of adversarial example: Towards good generalizations for deepfake detection. In CVPR, 2022. 2 [12] Liang Chen, Yong Zhang, Yibing Song, Jue Wang, and Lingqiao Liu. Ost: Improving generalization of deepfake detection via one-shot test-time training. In NeurIPS, 2022. 2, 12 [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof- frey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3 [14] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, 2022. 2 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5 [16] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, 2019. 1, 2 [17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 88(2):303–338, 2010. 5 [18] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013. 5, 16 [19] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener- ative visual models from few training examples: An incre- mental bayesian approach tested on 101 object categories. In CVPR worksho, 2004. 5 [20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 2, 7 [21] Francois Fleuret et al. Uncertainty reduction for model adap- tation in semantic segmentation. In CVPR, 2021. 2 [22] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 2 [23] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marc- hand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 17(1):2096–2030, 2016. 1, 2, 5, 15, 16, 17 [24] Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE TPAMI, 39(7):1414–1430, 2016. 1 [25] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In ICCV, 2015. 2 [26] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer- sch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 2, 3 [27] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021. 1, 2, 5, 6, 14, 15, 16, 17 [28] Sivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, et al. Unsupervised domain general- ization by learning a bridge across domains. In CVPR, 2022. 1 [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual repre- sentation learning. In CVPR, 2020. 2, 3 [30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 5, 7, 14 [31] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In UAI, 2020. 1 [32] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. 2 [33] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In ECCV, 2020. 5, 15, 16, 17[34] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regular- ization for domain generalization. In ICCV, 2021. 2, 5, 6, 15, 16, 17 [35] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribu- tion shifts. In ICML, 2021. 1 [36] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In ICML, 2021. 5, 6, 15, 16, 17 [37] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In ICCV, 2017. 1, 5, 6, 7, 8, 12, 13, 14, 15 [38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, 2018. 1, 2, 5, 6, 15, 16, 17 [39] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic training for domain generalization. In ICCV, 2019. 1, 2 [40] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In CVPR, 2018. 1, 2, 5, 15, 16, 17 [41] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple feature augmentation for domain generalization. In ICCV, 2021. 1, 2, 12, 14 [42] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out- of-distribution generalization. In ICLR, 2022. 1, 2 [43] Yizhuo Li, Miao Hao, Zonglin Di, Nitesh Bharadwaj Gun- davarapu, and Xiaolong Wang. Test-time personalization with a transformer for human pose estimation. In NeurIPS, 2021. 2, 3, 4 [44] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza- tion via conditional invariant adversarial networks. In ECCV, 2018. 1, 2, 5, 15, 16, 17 [45] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heterogeneous do- main generalization. In ICML, 2019. 14, 15 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In NeurIPS, 2021. 2, 3, 4, 7, 8, 12, 14, 15 [47] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant feature representation. In ICML, 2013. 1, 2 [48] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, 2021. 2, 5, 6, 15, 16, 17 [49] Prashant Pandey, Mrigank Raman, Sumanth Varambally, and Prathosh Ap. Generalization on unseen domains via inference- time label-preserving target projections. In CVPR, 2021. 1 [50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, 2019. 5, 17 [51] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient star- vation: A learning proclivity in neural networks. In NeurIPS, 2021. 1, 5, 6, 15, 16, 17 [52] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution gen- eralization. In ICML, 2022. 1, 2, 5, 6, 15, 16, 17 [53] Yangjun Ruan, Yann Dubois, and Chris J Maddison. Optimal representations for covariate shift. In ICLR, 2022. 5, 15, 16, 17 [54] Bryan C Russell, Antonio Torralba, Kevin P Murphy, and William T Freeman. Labelme: a database and web-based tool for image annotation. IJCV, 77(1):157–173, 2008. 5 [55] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst- case generalization. In ICLR, 2020. 5, 15, 16, 17 [56] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 2 [57] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad- cam: Visual explanations from deep networks via gradient- based localization. In ICCV, 2017. 7, 8, 11, 13 [58] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In ICLR, 2021. 1, 2, 5, 6, 15, 16, 17 [59] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016. 5, 6, 15, 16, 17 [60] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, 2020. 1, 2, 4, 5, 7, 8, 11, 12, 13 [61] Vladimir Vapnik. The nature of statistical learning theory . Springer science & business media, 1999. 1, 5, 6, 15, 16, 17 [62] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017. 5, 16 [63] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 2, 3, 4, 7, 8, 11, 12, 13 [64] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recog- nition from abbey to zoo. In CVPR, 2010. 5 [65] Zehao Xiao, Xiantong Zhen, Ling Shao, and Cees GM Snoek. Learning to generalize across domains on single test samples. In ICLR, 2022. 2 [66] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generaliza- tion. In CVPR, 2021. 1, 2 [67] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 7[68] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020. 2, 5, 15, 16, 17 [69] Fu-En Yang, Yuan-Chia Cheng, Zu-Yun Shiau, and Yu- Chiang Frank Wang. Adversarial teacher-student representa- tion learning for domain generalization. In NeurIPS, 2021. 1, 2 [70] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu. Ood-bench: Quantifying and understanding two dimensions of out-of- distribution generalization. In CVPR, 2022. 5 [71] Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 4 [72] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: A meta-learning approach for tackling group distri- bution shift. arXiv preprint arXiv:2007.02931, 2020. 5, 15, 16, 17 [73] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: Learning to adapt to domain shift. NeurIPS, 2021. 2 [74] Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, and Jin Tang. Meta-dmoe: Adapting to domain shift by meta- distillation from mixture-of-experts. In NeurIPS, 2022. 2 [75] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 1, 2, 3, 5, 6, 7, 12, 15, 16, 17 Appendix In this supplementary material, we provide, 1. Resource usage for ITTA in Section 7. 2. Grad-CAM visualizations of different loss terms in Section 8. 3. Parameter analysis of ITTA in Section 9; 4. Using a different augmentation skill for ITTA in Sec- tion 10. 5. Using different updating steps or a strategy for ITTA during the test phase in Section 11. 6. Using different network structures for the learnable consistency loss and adaptive parameters in Section 12. 7. Comparisons with other related methods in Section 13. 8. Detailed experimental results in the DomainBed bench- mark in Section 14. 7. Resource Usage Comparisons Between ITTA and the Baseline Model Requiring extra resources for our ITTA is a common lim- itation for existing test-time-based arts. To further evaluate our method, in this section, we compare FLOPS, model size, and inference time in Table 5. We compare only with ERM as most existing methods utilize the same network during in- ferences. We note that compare to the baseline model, ITTA requires extra Flops and processing time, this is because the adaptation process uses extra forward and backward steps during the test phase. While the parameters between the two models are similar because the newly included adaptive blocks are much smaller in size compared to the original model. Table 5. Resource comparisons during testing. Here inc. and exc. columns in ITTA indicate to include and exclude the TTA phase. Model Flops (G) Params (M) Time (s) Baseline 1.82 11.18 0.004 ITTA (inc.| exc.) 6.12 | 1.83 14.95 | 14.94 0.021 | 0.005 8. Grad-CAM Visualizations of Different Self- Supervised Objectives In Section 5 of the manuscript, we provide Grad-CAM [57] visualizations of our learnable consistency and the main losses to illustrate their alignment. To further show the differences between several TTT tasks [60, 63], we present more visual examples in this section. Results are shown in Figure 5. We observe that the entropy minimization [63] and rotation estimation [60] objectives do not activate the same regions as the main loss. As shown in the first row, for the class label of giraffe, both the main loss and our learned loss can correctly locate the two giraffes in the image, while the rotation estimation task can only locate one target, the same observation can be found when the learned weightsare disabled in our loss term. Meanwhile, although the two objects can be found for the entropy minimization task, the corresponding hot region does not align with that of the main loss. Similar phenomena can be observed in other samples. These visual examples demonstrate that our learned objective can better align with the main task than the TTT tasks adopted in previous works [60, 63], explaining why using the proposed learnable consistency loss can better improve TTT. 9. Parameter Analysis In this section, we analyze the hyper-parameter used in ITTA. We use the weight parameterα to balance the contri- butions from the main loss and weighted consistency loss (i.e. Lmain + αLwcont in Eq. (2) of our manuscript). To analyze the sensitivity of ITTA regarding different values of α, we conduct ablation studies in the PACS benchmark [37]. Results are listed in Table 6. We observe that the proposed ITTA can obtain favorable performances when α is in the range of 0.1 to 10, and it performs the best on average when setting as 1. We thus fix the parameter as 1 in all experi- ments. 10. A Different Augmentation Skill for ITTA In our manuscript, we use the existing augmentation strat- egy from [75] to obtain the augmented feature. In this sec- tion, we replace this implementation with that from [41] to further verify if our ITTA can still thrive with another aug- mentation skill. Different from [75] that mixes the statics of the feature to synthesize new information, [41] uses an affine transformation to create new features, where the weight for the transformation is sampled from a normal distribution with the mean value of one and standard value of zero, and the bias for the transformation is sampled from a normal distribution with the mean and standard values both zero. Experiments are conducted on the PACS benchmark [37] with the leave-one-out strategy. We compare ITTA with several different variants. (1) Ours w/o fw & TTT: this variant is the baseline model which uses the naive consistency loss for training and does not include TTT during the test phase. (2) Ours w/o fw: we disable the fw in our consistency loss, which uses the naive consistency loss for the test-time updating. (3) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model when replacing the augmentation strategy. We also compare these variants with the ERM method to show their effectivenesses. Results are listed in Table 7. We observe that ERM per- forms favorably against the baseline model, indicating that this augmentation strategy may not be beneficial for the training process. Meanwhile, we observe that when fw is disabled, the performances seem to decrease in 3 out of 4 target domains, and the average accuracy is also inferior to the baseline (i.e. Ours w/o fw & TTT). This result is in line with the finding in [46] that an inappropriate TTT task may deteriorate the performance. In comparison, we note that the performances are both improved when fw is enabled (i.e. Ours w/o TTT and Ours), which once again demonstrates that the proposed learnable consistency loss can improve the trained model. Moreover, we can also observe that when combining fw and TTT, our model is superior to other vari- ants and the ERM method. These results demonstrate that the proposed two strategies can improve the current TTT framework despite a less effective augmentation strategy. 11. Different Updating Steps or Strategies for ITTA In the manuscript, we use one TTT step for ITTA before during the testing step. In this section, we conduct experi- ments to evaluate the performances of ITTA with different TTT steps. Experiments are conducted on the PACS bench- mark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper- parameter settings. Results are listed in Table 8. We observe that the average accuracies of using more TTT steps are not improved greatly while the computational times are propor- tional to the TTT steps. To this end, we use one TTT step for ITTA as a compromise between accuracy and efficiency. We use the online setting from TTT [60] for all arts, which assumes test samples arrive sequentially and updates the adaptive blocks based on the states optimized from a previous sample. In this section, we also test ITTA in an episodic manner (i.e. Epi) [12]. Results in Table 8 suggest that while the episodic updating strategy performs slightly worse than the current scheme, and it still outperforms the baseline. 12. Different Network Structures for the Learnable Consistency Loss and Adaptive Parameters In our implementation, we use 10 layers of learnable pa- rameters for fw, and we use 5 layers of learnable parameters for fΘ after each block. In this section, we evaluate our ITTA with different network structures for these two mod- ules. Specifically, we compare the original implementation with the variants that use 1, 5, and 15 layers for fw and 1, 10, and 15 layers for fΘ to evaluate the performances of dif- ferent structures. Similarly, we conduct experiments on the PACS benchmark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper-parameter settings. Evaluation results are listed in Table 9. We observe that their differences in the average accuracy are rather subtle on account of the variances. To(a) Input (b) Entropy (c) Rotation (d) Ours w/o fw (e) Ours (f) Main Figure 5. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels (i.e. giraffe, elephant, house, and horse from top to bottom) from the four target domains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). “Entropy” and “Rotation” here denote the entropy minimization and rotation estimation tasks in [63] and [60]. Ours w/o fw is the learnable consistency loss in Eq. (1) in the manuscript (i.e. ∥fw(z − z′)∥) when fw is disabled. The proposed learnable consistency loss can align well with the main classification task. Table 6. Sensitivity analysis of ITTA regarding different values ofα in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Values Target domain Avg.Art Cartoon Photo Sketch α = 0.1 83.9 ± 0.7 76.2 ± 1.1 94.8 ± 0.2 78.8 ± 0.8 83.4 ± 0.2 α = 1 (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 α = 10 83.9 ± 0.5 77.4 ± 0.6 94.2 ± 0.7 77.3 ± 0.8 83.2 ± 0.3 α = 100 81.5 ± 1.2 77.0 ± 0.6 92.6 ± 0.7 78.9 ± 2.1 82.5 ± 0.9 this end, we use the original implementation with 10 layers of learnable parameters for fw and 5 layers of learnable pa- rameters for fΘ, which performs relatively better than other variants. Since the adaptive blocks fΘ are attached after each layer of the network, one may wonder how the varying locations of the adaptive blocks affect the performance of ITTA. To answer this question, we further conduct experiments by adding the adaptive blocks after different layers of the orig- inal network. Denoting as Loc = lan given the n layers in the original network, we note that the model performs less effectively when the adaptive block is placed after the 1st layer of the network, and using all four adaptive blocks (i.e. ours) is more effective than other alternatives. 13. Comparisons with Other Related Methods Apart from the proposed ITTA, some other works also propose to include learnable parameters in their auxiliaryTable 7. Performances of our method with another augmentation strategy from [41] in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch ERM 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 Ours w/o fw & TTT 74.9 ± 0.4 74.1 ± 0.8 90.6 ± 0.3 79.7 ± 0.7 79.8 ± 0.4 Ours w/o fw 77.1 ± 1.0 73.6 ± 1.1 89.9 ± 0.4 78.4 ± 0.8 79.7 ± 0.2 Ours w/o TTT 77.5 ± 0.3 73.2 ± 0.6 92.4 ± 0.4 78.0 ± 1.0 80.3 ± 0.3 Ours (w/ fw & TTT) 79.2 ± 0.8 74.9 ± 1.1 92.2 ± 0.3 76.9 ± 0.7 80.8 ± 0.4 Table 8. Evaluations of ITTA in the unseen domain from PACS [37] with different TTT steps and updating strategies during the testing phase. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. The time consumption (TC) is computed using one image with the size of 224 × 224. Epi. denotes updating ITTA in an episodic manner. Steps Target domain Avg. TCArt Cartoon Photo Sketch 1 step (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 2.4 ms 2 step 84.2 ± 0.9 77.5 ± 0.6 94.4 ± 0.4 79.1 ± 1.0 83.8 ± 0.1 4.2 ms 3 step 84.5 ± 1.2 77.6 ± 0.6 94.0 ± 0.6 79.3 ± 0.1 83.9 ± 0.3 6.1 ms Epi. 83.6 ± 0.7 77.9 ± 0.5 95.2 ± 0.1 76.6 ± 0.5 83.3 ± 0.4 losses. Examples include MetaReg [2] and Feature-Critic [45] which both suggest using meta-learning to produce more general models. The main difference between these arts and ITTA is that parameters in the auxiliary loss from [2,45] are gradually refined by episode training, and they are updated via a gradient alignment step in ITTA (see Sec. 3.1 in the manuscript), which is much simpler. In this sec- tion, we compare ITTA with these two arts in the PACS dataset [37] using the same settings aforementioned. Be- cause MetaReg [2] does not release codes, we thus directly cite the data from their paper in the comparison. Different from others, the results in [2] are averaged by 5 trials accord- ing to their paper, which is much less than our experimental settings. Meanwhile, we also compare with TTT++ [46] which suggests storing the momentum of the features from the source domain and enforcing the similarity between mo- mentums of features from the source and target domains. We use the same setting in Section 5.1 from the manuscript to evaluate TTT++. Results are listed in Table 10. We observe that our method consistently outperforms that from [2,45,46] for both the cases with and without TTT, indicating that the proposed learnable consistency loss and updating method is not only simpler but also more effective than the losses in [2, 45]. 14. Detailed Results in the DomainBed Bench- mark [27] this section presents the average accuracy in each domain from different datasets. As shown in Table 11, 12, 13, 14, and 15, these results are detailed illustrations of the results in Table 2 in our manuscript. For all the experiments, we use the “training-domain validate set” as the model selection method. A total of 22 methods are examined for 60 trials in each unseen domain, and all methods are trained with the leave-one-out strategy using the ResNet18 [30] backbones.Table 9. Performances of our method with different network structures for the consistency loss (i.e. fw) and adaptive parameters (i.e. fΘ) in the unseen domain from PACS [37]. Here ‘Loc=lan’ locates the adaptive block after the n-th layer of the model (‘la4’ is the last layer). The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Structures Target domain Avg.Art Cartoon Photo Sketch Structures offw 1 layer 83.5 ±1.2 76.0 ±1.0 95.3 ±0.2 78.7 ±1.5 83.4 ±0.4 5 layers 83.7 ±0.6 76.8 ±0.9 94.6 ±0.3 78.8 ±0.3 83.5 ±0.3 10 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 15 layers 84.1 ±0.4 75.8 ±0.2 94.3 ±0.3 79.5 ±0.4 83.4 ±0.2 Structures offΘ 1 layer 84.0 ±0.6 77.4 ±0.5 94.4 ±0.5 78.3 ±0.4 83.5 ±0.3 5 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 10 layers 84.8 ±0.3 76.0 ±0.6 94.1 ±0.5 78.3 ±0.1 83.3 ±0.3 15 layers 83.9 ±0.8 76.0 ±0.5 93.8 ±0.4 78.7 ±1.4 83.1 ±0.6 Locations offΘ Loc=la1 83.4±0.7 76.8 ±0.3 94.4 ±0.3 77.8 ±0.3 83.1 ±0.3 Loc=la2 83.4±0.6 77.7 ±0.6 94.2 ±0.5 78.0 ±0.5 83.3 ±0.3 Loc=la3 84.0±0.4 77.5 ±0.3 94.4 ±0.1 77.8 ±0.1 83.4 ±0.2 Loc=la4 84.1±0.7 77.8 ±0.5 94.8 ±0.2 76.9 ±1.5 83.4 ±0.4 Table 10. Compare with learnable losses in [2, 45] in the unseen domain from PACS [37]. The reported accuracies ( %) and standard deviations are computed from 60 trials in each target domain except for [2] where the numbers are directly cited from their paper. Model Target domain Avg.Art Cartoon Photo Sketch MetaReg [2] 83.7 ± 0.2 77.2 ± 0.3 95.5 ± 0.2 70.3 ± 0.3 81.7 Feture-Critic [45] 78.4 ± 1.6 75.4 ± 1.2 92.6 ± 0.5 73.3 ± 1.4 80.0 ± 0.3 TTT++ [46] 84.3 ± 0.1 78.4 ± 0.5 93.8 ± 1.3 73.2 ± 3.2 82.4 ± 1.1 Ours w/o TTT 83.3 ± 0.5 76.0 ± 0.5 94.4 ± 0.5 76.7 ± 1.4 82.8 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 Table 11. Average accuracies on the PACS [37] datasets using the default hyper-parameter settings in DomainBed [27]. art cartoon photo sketch Average ERM [61] 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 IRM [1] 76.9 ± 2.6 75.1 ± 0.7 94.3 ± 0.4 77.4 ± 0.4 80.9 ± 0.5 GroupGRO [55] 77.7 ± 2.6 76.4 ± 0.3 94.0 ± 0.3 74.8 ± 1.3 80.7 ± 0.4 Mixup [68] 79.3 ± 1.1 74.2 ± 0.3 94.9 ± 0.3 68.3 ± 2.7 79.2 ± 0.9 MLDG [38] 78.4 ± 0.7 75.1 ± 0.5 94.8 ± 0.4 76.7 ± 0.8 81.3 ± 0.2 CORAL [59] 81.5 ± 0.5 75.4 ± 0.7 95.2 ± 0.5 74.8 ± 0.4 81.7 ± 0.0 MMD [40] 81.3 ± 0.6 75.5 ± 1.0 94.0 ± 0.5 74.3 ± 1.5 81.3 ± 0.8 DANN [23] 79.0 ± 0.6 72.5 ± 0.7 94.4 ± 0.5 70.8 ± 3.0 79.2 ± 0.3 CDANN [44] 80.4 ± 0.8 73.7 ± 0.3 93.1 ± 0.6 74.2 ± 1.7 80.3 ± 0.5 MTL [6] 78.7 ± 0.6 73.4 ± 1.0 94.1 ± 0.6 74.4 ± 3.0 80.1 ± 0.8 SagNet [48] 82.9 ± 0.4 73.2 ± 1.1 94.6 ± 0.5 76.1 ± 1.8 81.7 ± 0.6 ARM [72] 79.4 ± 0.6 75.0 ± 0.7 94.3 ± 0.6 73.8 ± 0.6 80.6 ± 0.5 VREx [36] 74.4 ± 0.7 75.0 ± 0.4 93.3 ± 0.3 78.1 ± 0.9 80.2 ± 0.5 RSC [33] 78.5 ± 1.1 73.3 ± 0.9 93.6 ± 0.6 76.5 ± 1.4 80.5 ± 0.2 SelfReg [34] 82.5 ± 0.8 74.4 ± 1.5 95.4 ± 0.5 74.9 ± 1.3 81.8 ± 0.3 MixStyle [75] 82.6 ± 1.2 76.3 ± 0.4 94.2 ± 0.3 77.5 ± 1.3 82.6 ± 0.4 Fish [58] 80.9 ± 1.0 75.9 ± 0.4 95.0 ± 0.4 76.2 ± 1.0 82.0 ± 0.3 SD [51] 83.2 ± 0.6 74.6 ± 0.3 94.6 ± 0.1 75.1 ± 1.6 81.9 ± 0.3 CAD [53] 83.9 ± 0.8 74.2 ± 0.4 94.6 ± 0.4 75.0 ± 1.2 81.9 ± 0.3 CondCAD [53] 79.7 ± 1.0 74.2 ± 0.9 94.6 ± 0.4 74.8 ± 1.4 80.8 ± 0.5 Fishr [52] 81.2 ± 0.4 75.8 ± 0.8 94.3 ± 0.3 73.8 ± 0.6 81.3 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3Table 12. Average accuracies on the VLCS [18] datasets using the default hyper-parameter settings in DomainBed [27]. Caltech LabelMe Sun VOC Average ERM [61] 97.7 ± 0.3 62.1 ± 0.9 70.3 ± 0.9 73.2 ± 0.7 75.8 ± 0.2 IRM [1] 96.1 ± 0.8 62.5 ± 0.3 69.9 ± 0.7 72.0 ± 1.4 75.1 ± 0.1 GroupGRO [55] 96.7 ± 0.6 61.7 ± 1.5 70.2 ± 1.8 72.9 ± 0.6 75.4 ± 1.0 Mixup [68] 95.6 ± 1.5 62.7 ± 0.4 71.3 ± 0.3 75.4 ± 0.2 76.2 ± 0.3 MLDG [38] 95.8 ± 0.5 63.3 ± 0.8 68.5 ± 0.5 73.1 ± 0.8 75.2 ± 0.3 CORAL [59] 96.5 ± 0.3 62.8 ± 0.1 69.1 ± 0.6 73.8 ± 1.0 75.5 ± 0.4 MMD [40] 96.0 ± 0.8 64.3 ± 0.6 68.5 ± 0.6 70.8 ± 0.1 74.9 ± 0.5 DANN [23] 97.2 ± 0.1 63.3 ± 0.6 70.2 ± 0.9 74.4 ± 0.2 76.3 ± 0.2 CDANN [44] 95.4 ± 1.2 62.6 ± 0.6 69.9 ± 1.3 76.2 ± 0.5 76.0 ± 0.5 MTL [6] 94.4 ± 2.3 65.0 ± 0.6 69.6 ± 0.6 71.7 ± 1.3 75.2 ± 0.3 SagNet [48] 94.9 ± 0.7 61.9 ± 0.7 69.6 ± 1.3 75.2 ± 0.6 75.4 ± 0.8 ARM [72] 96.9 ± 0.5 61.9 ± 0.4 71.6 ± 0.1 73.3 ± 0.4 75.9 ± 0.3 VREx [36] 96.2 ± 0.0 62.5 ± 1.3 69.3 ± 0.9 73.1 ± 1.2 75.3 ± 0.6 RSC [33] 96.2 ± 0.0 63.6 ± 1.3 69.8 ± 1.0 72.0 ± 0.4 75.4 ± 0.3 SelfReg [34] 95.8 ± 0.6 63.4 ± 1.1 71.1 ± 0.6 75.3 ± 0.6 76.4 ± 0.7 MixStyle [75] 97.3 ± 0.3 61.6 ± 0.1 70.4 ± 0.7 71.3 ± 1.9 75.2 ± 0.7 Fish [58] 97.4 ± 0.2 63.4 ± 0.1 71.5 ± 0.4 75.2 ± 0.7 76.9 ± 0.2 SD [51] 96.5 ± 0.4 62.2 ± 0.0 69.7 ± 0.9 73.6 ± 0.4 75.5 ± 0.4 CAD [53] 94.5 ± 0.9 63.5 ± 0.6 70.4 ± 1.2 72.4 ± 1.3 75.2 ± 0.6 CondCAD [53] 96.5 ± 0.8 62.6 ± 0.4 69.1 ± 0.2 76.0 ± 0.2 76.1 ± 0.3 Fishr [52] 97.2 ± 0.6 63.3 ± 0.7 70.4 ± 0.6 74.0 ± 0.8 76.2 ± 0.3 Ours 96.9 ± 1.2 63.7 ± 1.1 72.0 ± 0.3 74.9 ± 0.8 76.9 ± 0.6 Table 13. Average accuracies on the OfficeHome [62] datasets using the default hyper-parameter settings in DomainBed [27]. art clipart product real Average ERM [61] 52.2 ± 0.2 48.7 ± 0.5 69.9 ± 0.5 71.7 ± 0.5 60.6 ± 0.2 IRM [1] 49.7 ± 0.2 46.8 ± 0.5 67.5 ± 0.4 68.1 ± 0.6 58.0 ± 0.1 GroupGRO [55] 52.6 ± 1.1 48.2 ± 0.9 69.9 ± 0.4 71.5 ± 0.8 60.6 ± 0.3 Mixup [68] 54.0 ± 0.7 49.3 ± 0.7 70.7 ± 0.7 72.6 ± 0.3 61.7 ± 0.5 MLDG [38] 53.1 ± 0.3 48.4 ± 0.3 70.5 ± 0.7 71.7 ± 0.4 60.9 ± 0.2 CORAL [59] 55.1 ± 0.7 49.7 ± 0.9 71.8 ± 0.2 73.1 ± 0.5 62.4 ± 0.4 MMD [40] 50.9 ± 1.0 48.7 ± 0.3 69.3 ± 0.7 70.7 ± 1.3 59.9 ± 0.4 DANN [23] 51.8 ± 0.5 47.1 ± 0.1 69.1 ± 0.7 70.2 ± 0.7 59.5 ± 0.5 CDANN [44] 51.4 ± 0.5 46.9 ± 0.6 68.4 ± 0.5 70.4 ± 0.4 59.3 ± 0.4 MTL [6] 51.6 ± 1.5 47.7 ± 0.5 69.1 ± 0.3 71.0 ± 0.6 59.9 ± 0.5 SagNet [48] 55.3 ± 0.4 49.6 ± 0.2 72.1 ± 0.4 73.2 ± 0.4 62.5 ± 0.3 ARM [72] 51.3 ± 0.9 48.5 ± 0.4 68.0 ± 0.3 70.6 ± 0.1 59.6 ± 0.3 VREx [36] 51.1 ± 0.3 47.4 ± 0.6 69.0 ± 0.4 70.5 ± 0.4 59.5 ± 0.1 RSC [33] 49.0 ± 0.1 46.2 ± 1.5 67.8 ± 0.7 70.6 ± 0.3 58.4 ± 0.6 SelfReg [34] 55.1 ± 0.8 49.2 ± 0.6 72.2 ± 0.3 73.0 ± 0.3 62.4 ± 0.1 MixStyle [75] 50.8 ± 0.6 51.4 ± 1.1 67.6 ± 1.3 68.8 ± 0.5 59.6 ± 0.8 Fish [58] 54.6 ± 1.0 49.6 ± 1.0 71.3 ± 0.6 72.4 ± 0.2 62.0 ± 0.6 SD [51] 55.0 ± 0.4 51.3 ± 0.5 72.5 ± 0.2 72.7 ± 0.3 62.9 ± 0.2 CAD [53] 52.1 ± 0.6 48.3 ± 0.5 69.7 ± 0.3 71.9 ± 0.4 60.5 ± 0.3 CondCAD [53] 53.3 ± 0.6 48.4 ± 0.2 69.8 ± 0.9 72.6 ± 0.1 61.0 ± 0.4 Fishr [52] 52.6 ± 0.9 48.6 ± 0.3 69.9 ± 0.6 72.4 ± 0.4 60.9 ± 0.3 Ours 54.4 ± 0.2 52.3 ± 0.8 69.5 ± 0.3 71.7 ± 0.2 62.0 ± 0.2Table 14. Average accuracies on the TerraInc [4] datasets using the default hyper-parameter settings in DomainBed [27]. L100 L38 L43 L46 Average ERM [61] 42.1 ± 2.5 30.1 ± 1.2 48.9 ± 0.6 34.0 ± 1.1 38.8 ± 1.0 IRM [1] 41.8 ± 1.8 29.0 ± 3.6 49.6 ± 2.1 33.1 ± 1.5 38.4 ± 0.9 GroupGRO [55] 45.3 ± 4.6 36.1 ± 4.4 51.0 ± 0.8 33.7 ± 0.9 41.5 ± 2.0 Mixup [68] 49.4 ± 2.0 35.9 ± 1.8 53.0 ± 0.7 30.0 ± 0.9 42.1 ± 0.7 MLDG [38] 39.6 ± 2.3 33.2 ± 2.7 52.4 ± 0.5 35.1 ± 1.5 40.1 ± 0.9 CORAL [59] 46.7 ± 3.2 36.9 ± 4.3 49.5 ± 1.9 32.5 ± 0.7 41.4 ± 1.8 MMD [40] 49.1 ± 1.2 36.4 ± 4.8 50.4 ± 2.1 32.3 ± 1.5 42.0 ± 1.0 DANN [23] 44.3 ± 3.6 28.0 ± 1.5 47.9 ± 1.0 31.3 ± 0.6 37.9 ± 0.9 CDANN [44] 36.9 ± 6.4 32.7 ± 6.2 51.1 ± 1.3 33.5 ± 0.5 38.6 ± 2.3 MTL [6] 45.2 ± 2.6 31.0 ± 1.6 50.6 ± 1.1 34.9 ± 0.4 40.4 ± 1.0 SagNet [48] 36.3 ± 4.7 40.3 ± 2.0 52.5 ± 0.6 33.3 ± 1.3 40.6 ± 1.5 ARM [72] 41.5 ± 4.5 27.7 ± 2.4 50.9 ± 1.0 29.6 ± 1.5 37.4 ± 1.9 VREx [36] 48.0 ± 1.7 41.1 ± 1.5 51.8 ± 1.5 32.0 ± 1.2 43.2 ± 0.3 RSC [33] 42.8 ± 2.4 32.2 ± 3.8 49.6 ± 0.9 32.9 ± 1.2 39.4 ± 1.3 SelfReg [34] 46.1 ± 1.5 34.5 ± 1.6 49.8 ± 0.3 34.7 ± 1.5 41.3 ± 0.3 MixStyle [75] 50.6 ± 1.9 28.0 ± 4.5 52.1 ± 0.7 33.0 ± 0.2 40.9 ± 1.1 Fish [58] 46.3 ± 3.0 29.0 ± 1.1 52.7 ± 1.2 32.8 ± 1.0 40.2 ± 0.6 SD [51] 45.5 ± 1.9 33.2 ± 3.1 52.9 ± 0.7 36.4 ± 0.8 42.0 ± 1.0 CAD [53] 43.1 ± 2.6 31.1 ± 1.9 53.1 ± 1.6 34.7 ± 1.3 40.5 ± 0.4 CondCAD [53] 44.4 ± 2.9 32.9 ± 2.5 50.5 ± 1.3 30.8 ± 0.5 39.7 ± 0.4 Fishr [52] 49.9 ± 3.3 36.6 ± 0.9 49.8 ± 0.2 34.2 ± 1.3 42.6 ± 1.0 Ours 51.7 ± 2.4 37.6 ± 0.6 49.9 ± 0.6 33.6 ± 0.6 43.2 ± 0.5 Table 15. Average accuracies on the DomainNet [50] datasets using the default hyper-parameter settings in DomainBed [27]. clip info paint quick real sketch Average ERM [61] 50.4 ± 0.2 14.0 ± 0.2 40.3 ± 0.5 11.7 ± 0.2 52.0 ± 0.2 43.2 ± 0.3 35.3 ± 0.1 IRM [1] 43.2 ± 0.9 12.6 ± 0.3 35.0 ± 1.4 9.9 ± 0.4 43.4 ± 3.0 38.4 ± 0.4 30.4 ± 1.0 GroupGRO [55] 38.2 ± 0.5 13.0 ± 0.3 28.7 ± 0.3 8.2 ± 0.1 43.4 ± 0.5 33.7 ± 0.0 27.5 ± 0.1 Mixup [68] 48.9 ± 0.3 13.6 ± 0.3 39.5 ± 0.5 10.9 ± 0.4 49.9 ± 0.2 41.2 ± 0.2 34.0 ± 0.0 MLDG [38] 51.1 ± 0.3 14.1 ± 0.3 40.7 ± 0.3 11.7 ± 0.1 52.3 ± 0.3 42.7 ± 0.2 35.4 ± 0.0 CORAL [59] 51.2 ± 0.2 15.4 ± 0.2 42.0 ± 0.2 12.7 ± 0.1 52.0 ± 0.3 43.4 ± 0.0 36.1 ± 0.2 MMD [40] 16.6 ± 13.3 0.3 ± 0.0 12.8 ± 10.4 0.3 ± 0.0 17.1 ± 13.7 0.4 ± 0.0 7.9 ± 6.2 DANN [23] 45.0 ± 0.2 12.8 ± 0.2 36.0 ± 0.2 10.4 ± 0.3 46.7 ± 0.3 38.0 ± 0.3 31.5 ± 0.1 CDANN [44] 45.3 ± 0.2 12.6 ± 0.2 36.6 ± 0.2 10.3 ± 0.4 47.5 ± 0.1 38.9 ± 0.4 31.8 ± 0.2 MTL [6] 50.6 ± 0.2 14.0 ± 0.4 39.6 ± 0.3 12.0 ± 0.3 52.1 ± 0.1 41.5 ± 0.0 35.0 ± 0.0 SagNet [48] 51.0 ± 0.1 14.6 ± 0.1 40.2 ± 0.2 12.1 ± 0.2 51.5 ± 0.3 42.4 ± 0.1 35.3 ± 0.1 ARM [72] 43.0 ± 0.2 11.7 ± 0.2 34.6 ± 0.1 9.8 ± 0.4 43.2 ± 0.3 37.0 ± 0.3 29.9 ± 0.1 VREx [36] 39.2 ± 1.6 11.9 ± 0.4 31.2 ± 1.3 10.2 ± 0.4 41.5 ± 1.8 34.8 ± 0.8 28.1 ± 1.0 RSC [33] 39.5 ± 3.7 11.4 ± 0.8 30.5 ± 3.1 10.2 ± 0.8 41.0 ± 1.4 34.7 ± 2.6 27.9 ± 2.0 SelfReg [34] 47.9 ± 0.3 15.1 ± 0.3 41.2 ± 0.2 11.7 ± 0.3 48.8 ± 0.0 43.8 ± 0.3 34.7 ± 0.2 MixStyle [75] 49.1 ± 0.4 13.4 ± 0.0 39.3 ± 0.0 11.4 ± 0.4 47.7 ± 0.3 42.7 ± 0.1 33.9 ± 0.1 Fish [58] 51.5 ± 0.3 14.5 ± 0.2 40.4 ± 0.3 11.7 ± 0.5 52.6 ± 0.2 42.1 ± 0.1 35.5 ± 0.0 SD [51] 51.3 ± 0.3 15.5 ± 0.1 41.5 ± 0.3 12.6 ± 0.2 52.9 ± 0.2 44.0 ± 0.4 36.3 ± 0.2 CAD [53] 45.4 ± 1.0 12.1 ± 0.5 34.9 ± 1.1 10.2 ± 0.6 45.1 ± 1.6 38.5 ± 0.6 31.0 ± 0.8 CondCAD [53] 46.1 ± 1.0 13.3 ± 0.4 36.1 ± 1.4 10.7 ± 0.2 46.8 ± 1.3 38.7 ± 0.7 31.9 ± 0.7 Fishr [52] 47.8 ± 0.7 14.6 ± 0.2 40.0 ± 0.3 11.9 ± 0.2 49.2 ± 0.7 41.7 ± 0.1 34.2 ± 0.3 Ours 50.7 ± 0.7 13.9 ± 0.4 39.4 ± 0.5 11.9 ± 0.2 50.2 ± 0.3 43.5 ± 0.1 34.9 ± 0.1",
      "meta_data": {
        "arxiv_id": "2304.04494v2",
        "authors": [
          "Liang Chen",
          "Yong Zhang",
          "Yibing Song",
          "Ying Shan",
          "Lingqiao Liu"
        ],
        "published_date": "2023-04-10T10:12:38Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04494v2.pdf",
        "github_url": "https://github.com/liangchen527/ITTA"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the distribution shift problem in domain generalization (DG) by improving Test-Time Training (TTT). The main contributions are: (1) proposing a learnable consistency loss for the TTT task, which includes learnable parameters (fw) to ensure better alignment with the main prediction task; (2) introducing additional adaptive parameters (fΘ) to the trained model and suggesting to update only these parameters during the test phase; (3) demonstrating superior performance of the Improved Test-Time Adaptation (ITTA) method against state-of-the-art methods on several DG benchmarks for both multi-source and single-source DG tasks.",
        "methodology": "The Improved Test-Time Adaptation (ITTA) method improves TTT in two ways. First, it introduces a learnable consistency loss (Lwcont) for the auxiliary TTT task, defined as the L2 norm of the output of a weight subnetwork (fw) applied to the difference between original and augmented feature representations (z - z'). This fw network contains learnable parameters adjusted to align Lwcont with the main classification loss (Lmain). Alignment is enforced by minimizing the L2 norm between the normalized gradients of Lmain and Lwcont with respect to the feature extractor's parameters (∇θLalign = ||ĝmain - ĝwcont||). Second, ITTA incorporates additional adaptive parameters (fΘ) after each block of the feature extractor fθ. During test-time adaptation, only these new adaptive parameters (fΘ) are updated using the learned consistency loss, while the original feature extractor parameters (fθ) and classifier (fϕ) remain unchanged.",
        "experimental_setup": "ITTA is evaluated on five benchmark datasets: PACS, VLCS, OfficeHome, TerraInc, and DomainNet. For all experiments, an ImageNet-pretrained ResNet18 backbone with 4 blocks serves as the feature extractor (fθ). The additional adaptive parameters (fΘ) also consist of 4 blocks, each implemented with 5 layers of learnable parameters. The weight subnetwork (fw) uses 10 layers of learnable parameters. The classifier (fϕ) is an MLP layer from the Domainbed benchmark. The weight parameter α for balancing Lmain and Lwcont is set to 1. Experiments are conducted under the rigorous DomainBed evaluation protocol, examining methods for 60 trials in each unseen domain, with 5,000 iteration steps per trial. Training data is split 8:2 for train:val, and model selection uses the 'training-domain validate set' method. Both multi-source (leave-one-out) and single-source (trained on one domain, tested on three) DG tasks are evaluated. Performance is measured by average accuracy and standard deviations.",
        "limitations": "The proposed method, while effective, introduces an additional computational burden. While disabling the weight subnetwork (fw) in the joint loss (Eq. 2) requires only one forward and one backward pass, updating fw necessitates computing a second-order derivative in Eq. (5), which requires one more forward and three more backward processes, thus adding extra overhead to the system.",
        "future_research_directions": "Future research aims to simplify the overall optimization process of ITTA and reduce its computational cost, particularly concerning the additional forward and backward passes required for updating the learnable weight subnetwork (fw).",
        "experimental_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.autograd as autograd\n\nimport copy\nimport numpy as np\nfrom collections import defaultdict, OrderedDict\n\nfrom domainbed import networks\nfrom domainbed.lib.misc import (\n    random_pairs_of_minibatches, ParamDict, MovingAverage, l2_between_dicts\n)\n\n\nALGORITHMS = [\n    'ERM',\n    'Fish',\n    'IRM',\n    'GroupDRO',\n    'Mixup',\n    'MLDG',\n    'CORAL',\n    'MMD',\n    'DANN',\n    'CDANN',\n    'MTL',\n    'SagNet',\n    'ARM',\n    'VREx',\n    'RSC',\n    'SD',\n    'ANDMask',\n    'SANDMask',\n    'IGA',\n    'SelfReg',\n    \"Fishr\",\n    'TRM',\n    'IB_ERM',\n    'IB_IRM',\n    'ITTA',\n    'RIDG',\n    'CauseEB',\n    'LFME',\n    'ERMPlus'\t\n]\n\ndef get_algorithm_class(algorithm_name):\n    \"\"\"Return the algorithm class with the given name.\"\"\"\n    if algorithm_name not in globals():\n        raise NotImplementedError(\"Algorithm not found: {}\".format(algorithm_name))\n    return globals()[algorithm_name]\n\nclass Algorithm(torch.nn.Module):\n    \"\"\"\n    A subclass of Algorithm implements a domain generalization algorithm.\n    Subclasses should implement the following:\n    - update()\n    - predict()\n    \"\"\"\n    def __init__(self, input_shape, num_classes, num_domains, hparams):\n        super(Algorithm, self).__init__()\n        self.hparams = hparams\n\n    def update(self, minibatches, unlabeled=None):\n        \"\"\"\n        Perform one update step, given a list of (x, y) tuples for all\n        environments.\n\n        Admits an optional list of unlabeled minibatches from the test domains,\n        when task is domain_adaptation.\n        \"\"\"\n        raise NotImplementedError\n\n    def predict(self, x):\n        raise NotImplementedError\n\n\nclass ITTA(Algorithm):\n    \"\"\"\n    Improved Test-Time Adaptation (ITTA)\n    \"\"\"\n\n    def __init__(self, input_shape, num_classes, num_domains, hparams):\n        super(ITTA, self).__init__(input_shape, num_classes, num_domains,\n                                  hparams)\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.featurizer = networks.ResNet_ITTA(input_shape, self.hparams)\n        self.classifier = networks.Classifier(\n            self.featurizer.n_outputs,\n            num_classes,\n            self.hparams['nonlinear_classifier'])\n        self.test_mapping = networks.MappingNetwork() #specialized for resnet18\n        self.test_optimizer = torch.optim.Adam(self.test_mapping.parameters(), lr=self.hparams[\"lr\"]*0.1)\n        self.optimizer = torch.optim.Adam([\n            {'params': self.featurizer.parameters()},\n            {'params': self.classifier.parameters()}],\n            lr=self.hparams[\"lr\"],\n            weight_decay=self.hparams['weight_decay']\n        )\n        self.MSEloss = nn.MSELoss()\n        self.adaparams = networks.Adaparams() #specialized for resnet18\n        self.adaparams_optimizer = torch.optim.Adam(self.adaparams.parameters(), lr=self.hparams[\"lr\"]*0.1)\n\n    def _get_grads(self, loss):\n        self.optimizer.zero_grad()\n        loss.backward(inputs=list(self.featurizer.parameters()),\n                          retain_graph=True, create_graph=True)\n        dict = OrderedDict(\n            [\n                (name, weights.grad.clone().view(weights.grad.size(0),-1))\n                for name, weights in self.featurizer.named_parameters()\n            ]\n        )\n\n        return dict\n\n    def update(self, minibatches, unlabeled=None):\n        all_x = torch.cat([x for x,y in minibatches])\n        all_y = torch.cat([y for x,y in minibatches])\n        ############################# this is for network update\n        #############################\n        z_ori, z_aug = self.featurizer(all_x)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + \\\n                   F.cross_entropy(self.classifier(z_aug), all_y)\n        loss = loss_reg + loss_cla\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        ############################# this is for adaparams update\n        #############################\n        z_ori, z_aug = self.featurizer(all_x)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + \\\n                   F.cross_entropy(self.classifier(z_aug), all_y)\n        dict_reg = self._get_grads(loss_reg)\n        dict_cla = self._get_grads(loss_cla)\n        penalty = l2_between_dicts(dict_reg, dict_cla, normalize=True) * 0.1\n        self.adaparams_optimizer.zero_grad()\n        penalty.backward(inputs=list(self.adaparams.parameters()))\n        self.adaparams_optimizer.step()\n\n        return {'loss': loss_cla.item(), 'reg': loss_reg.item()}\n\n    def test_adapt(self, x):\n        z_ori, z_aug = self.featurizer(x)\n        z_ori, z_aug = self.test_mapping.fea1(z_ori), self.test_mapping.fea1(z_aug)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.test_mapping.fea2(z_ori), self.test_mapping.fea2(z_aug)\n        z_ori, z_aug = self.featurizer.fea3(z_ori), self.featurizer.fea3(z_aug)\n        z_ori, z_aug = self.test_mapping.fea3(z_ori), self.test_mapping.fea3(z_aug)\n        z_ori, z_aug = self.featurizer.fea4(z_ori), self.featurizer.fea4(z_aug)\n        z_ori, z_aug = self.test_mapping.fea4(z_ori), self.test_mapping.fea4(z_aug)\n        z_ori, z_aug = self.featurizer.flat(z_ori), self.featurizer.flat(z_aug)\n        ########## small lr for large datasets\n        loss_reg = self.MSEloss(self.adaparams(z_aug-z_ori), torch.zeros_like(z_ori)) * self.hparams['ada_lr']\n        self.test_optimizer.zero_grad()\n        loss_reg.backward(inputs=list(self.test_mapping.parameters()))\n        self.test_optimizer.step()\n\n    def predict(self, x):\n        z_ori, z_aug = self.featurizer(x)\n        z_ori = self.test_mapping.fea1(z_ori)\n        z_ori, z_aug = self.featurizer.fea2(z_ori,z_aug)\n        z_ori = self.test_mapping.fea2(z_ori)\n        z_ori = self.featurizer.fea3(z_ori)\n        z_ori = self.test_mapping.fea3(z_ori)\n        z_ori = self.featurizer.fea4(z_ori)\n        z_ori = self.test_mapping.fea4(z_ori)\n        z_ori = self.featurizer.flat(z_ori)\n        return self.classifier(z_ori)\n\n\n\nclass MappingNetwork(torch.nn.Module):\n    def __init__(self, depth=5):\n        super().__init__()\n        self.depth = depth\n        self.weight1 = nn.ParameterList()\n        self.bias1 = nn.ParameterList()\n        self.weight2 = nn.ParameterList()\n        self.bias2 = nn.ParameterList()\n        self.weight3 = nn.ParameterList()\n        self.bias3 = nn.ParameterList()\n        self.weight4 = nn.ParameterList()\n        self.bias4 = nn.ParameterList()\n        for i in range(depth):\n            self.weight1.append(nn.Parameter(torch.ones((64,56,56))))\n            self.bias1.append(nn.Parameter(torch.zeros((64,56,56))))\n\n            self.weight2.append(nn.Parameter(torch.ones((128,28,28))))\n            self.bias2.append(nn.Parameter(torch.zeros((128,28,28))))\n\n            self.weight3.append(nn.Parameter(torch.ones((256,14,14))))\n            self.bias3.append(nn.Parameter(torch.zeros((256,14,14))))\n\n            self.weight4.append(nn.Parameter(torch.ones((512, 7, 7))))\n            self.bias4.append(nn.Parameter(torch.zeros((512, 7, 7))))\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def fea1(self, x):\n        for i in range(self.depth-1):\n            x = self.relu(self.weight1[i] * x + self.bias1[i])\n        x = self.weight1[i+1] * x + self.bias1[i+1]\n        return x\n\n    def fea2(self, x):\n        for i in range(self.depth - 1):\n            x = self.relu(self.weight2[i] * x + self.bias2[i])\n        x = self.weight2[i + 1] * x + self.bias2[i + 1]\n        return x\n\n    def fea3(self, x):\n        for i in range(self.depth - 1):\n            x = self.relu(self.weight3[i] * x + self.bias3[i])\n        x = self.weight3[i + 1] * x + self.bias3[i + 1]\n        return x\n\n    def fea4(self, x):\n        for i in range(self.depth-1):\n            x = self.relu(self.weight4[i] * x + self.bias4[i])\n        x = self.weight4[i+1] * x + self.bias4[i+1]\n        return x\n\n\nclass Identity(nn.Module):\n    \"\"\"An identity layer\"\"\"\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, x):\n        return x\n\nclass Adaparams(nn.Module):\n    def __init__(self, depth=10):\n        super(Adaparams, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.depth = depth\n        self.weight = nn.ParameterList()\n        self.bias = nn.ParameterList()\n        for i in range(depth):\n            self.weight.append(nn.Parameter(torch.ones(512)))\n            self.bias.append(nn.Parameter(torch.zeros(512)))\n\n    def forward(self, x):\n        for i in range(self.depth-1):\n            x = self.relu(self.weight[i] * x + self.bias[i])\n        x = self.weight[i+1] * x + self.bias[i+1]\n        return x\n        \n\nclass ResNet_ITTA(torch.nn.Module):\n    \"\"\"ResNet with the softmax chopped off and the batchnorm frozen\"\"\"\n    def __init__(self, input_shape, hparams):\n        super(ResNet_ITTA, self).__init__()\n        if hparams['resnet18']:\n            self.network = torchvision.models.resnet18(pretrained=True)\n            self.n_outputs = 512\n        else:\n            self.network = torchvision.models.resnet18(pretrained=True)\n            self.n_outputs = 2048\n\n        nc = input_shape[0]\n        if nc != 3:\n            tmp = self.network.conv1.weight.data.clone()\n\n            self.network.conv1 = nn.Conv2d(\n                nc, 64, kernel_size=(7, 7),\n                stride=(2, 2), padding=(3, 3), bias=False)\n\n            for i in range(nc):\n                self.network.conv1.weight.data[:, i, :, :] = tmp[:, i % 3, :, :]\n\n        # save memory\n        self.network.fc = Identity()\n        self.isaug = True\n        self.freeze_bn()\n        self.hparams = hparams\n        self.dropout = nn.Dropout(hparams['resnet_dropout'])\n        self.eps = 1e-6\n\n    def mixstyle(self, x):\n        alpha = 0.1\n        beta = torch.distributions.Beta(alpha, alpha)\n        B = x.size(0)\n        mu = x.mean(dim=[2, 3], keepdim=True)\n        var = x.var(dim=[2, 3], keepdim=True)\n        sig = (var + self.eps).sqrt()\n        mu, sig = mu.detach(), sig.detach()\n        x_normed = (x - mu) / sig\n        lmda = beta.sample((B, 1, 1, 1))\n        lmda = lmda.to(x.device)\n        perm = torch.randperm(B)\n        mu2, sig2 = mu[perm], sig[perm]\n        mu_mix = mu * lmda + mu2 * (1 - lmda)\n        sig_mix = sig * lmda + sig2 * (1 - lmda)\n        return x_normed * sig_mix + mu_mix\n\n    def fea_forward(self, x):\n        x = self.fea3(x)\n        x = self.fea4(x)\n\n        x = self.flat(x)\n        return x\n\n    def fea2(self, x, aug_x):\n        x = self.network.layer2(x)\n        aug_x = self.network.layer2(aug_x)\n        if not self.isaug:\n            aug_x = self.mixstyle(aug_x)\n        return x, aug_x\n\n    def fea3(self, x):\n        x = self.network.layer3(x)\n        return x\n\n    def fea4(self, x):\n        x = self.network.layer4(x)\n        return x\n\n    def flat(self, x):\n        x = self.network.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.network.fc(x)\n        x = self.dropout(x)\n        return x\n\n    def forward(self, x):\n        \"\"\"Encode x into a feature vector of size n_outputs.\"\"\"\n        x = self.network.conv1(x)\n        x = self.network.bn1(x)\n        x = self.network.relu(x)\n        x = self.network.maxpool(x)\n\n        x = self.network.layer1(x)\n        if random.random() > 0.5:\n            self.isaug = True\n            aug_x = self.mixstyle(x)\n        else:\n            self.isaug = False\n            aug_x = x\n\n        return x, aug_x\n\n    def train(self, mode=True):\n        \"\"\"\n        Override the default train() to freeze the BN parameters\n        \"\"\"\n        super().train(mode)\n        self.freeze_bn()\n\n    def freeze_bn(self):\n        for m in self.network.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\ndef l2_between_dicts(dict_1, dict_2, normalize=False):\n    assert len(dict_1) == len(dict_2)\n    dict_1_values = [dict_1[key] for key in sorted(dict_1.keys())]\n    dict_2_values = [dict_2[key] for key in sorted(dict_1.keys())]\n    dict_1_tensor = torch.cat(tuple([t.view(-1) for t in dict_1_values]))\n    dict_2_tensor = torch.cat(tuple([t.view(-1) for t in dict_2_values]))\n    if normalize:\n        dict_1_tensor = (dict_1_tensor-dict_1_tensor.mean().item()) / dict_1_tensor.std().item()\n        dict_2_tensor = (dict_2_tensor-dict_2_tensor.mean().item()) / dict_2_tensor.std().item()\n        dict_2_tensor = dict_2_tensor.detach()\n    return (dict_1_tensor-dict_2_tensor).pow(2).mean()\n\ndef accuracy_tsc(network, loader, weights, device):\n    correct = 0\n    total = 0\n    weights_offset = 0\n    network.featurizer.requires_grad_(False)\n    network.classifier.requires_grad_(False)\n    for x, y in loader:\n        x = x.to(device)\n        y = y.to(device)\n        network.train()\n        #############\n        for i in range(1):\n            network.test_adapt(x)\n        network.eval()\n        with torch.no_grad():\n            p = network.predict(x)\n        ##############\n        if weights is None:\n            batch_weights = torch.ones(len(x))\n        else:\n            batch_weights = weights[weights_offset : weights_offset + len(x)]\n            weights_offset += len(x)\n        batch_weights = batch_weights.to(device)\n        if p.size(1) == 1:\n            correct += (p.gt(0).eq(y).float() * batch_weights.view(-1, 1)).sum().item()\n        else:\n            correct += (p.argmax(1).eq(y).float() * batch_weights).sum().item()\n        total += batch_weights.sum().item()\n    network.train()\n    network.featurizer.requires_grad_(True)\n    network.classifier.requires_grad_(True)\n\n    return correct / total\n",
        "experimental_info": "The ITTA algorithm is implemented as a `torch.nn.Module` named `ITTA` inheriting from `Algorithm` in `domainbed/algorithms.py`. It uses a custom ResNet architecture called `ResNet_ITTA` as its feature extractor, and introduces `MappingNetwork` (for adaptive parameters fΘ) and `Adaparams` (for weight subnetwork fw).\n\n**Training-time Adaptation (Meta-learning for adaparams):**\n1.  **Featurization:** `ITTA.featurizer(all_x)` produces two feature representations `z_ori` and `z_aug` by applying style randomization (mixstyle) to intermediate features in `ResNet_ITTA`.\n2.  **Consistency Loss (Lwcont):** `loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))` calculates the L2 norm of the output of the `Adaparams` network applied to the difference between augmented and original features.\n3.  **Classification Loss (Lmain):** `loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + F.cross_entropy(self.classifier(z_aug), all_y)` computes the standard cross-entropy loss on both original and augmented features.\n4.  **Feature Extractor & Classifier Update:** The combined loss `loss = loss_reg + loss_cla` is used to update the main `featurizer` and `classifier` network parameters.\n5.  **Adaparams Update (Alignment):**\n    *   Gradients of `loss_reg` (`dict_reg`) and `loss_cla` (`dict_cla`) with respect to `featurizer` parameters are computed using `_get_grads`.\n    *   `penalty = l2_between_dicts(dict_reg, dict_cla, normalize=True) * 0.1` calculates the L2 norm between normalized gradients, acting as `∇θLalign`.\n    *   This `penalty` is then used to update the `adaparams` (fw network) parameters using `self.adaparams_optimizer`.\n\n**Test-time Adaptation:**\n1.  During evaluation, the `test_adapt(x)` method is called before `predict(x)`.\n2.  `test_adapt` passes the input `x` through the `featurizer` to get `z_ori` and `z_aug`.\n3.  It then applies the `test_mapping` (fΘ) adaptively after `featurizer.layer1`, `featurizer.layer2`, `featurizer.layer3`, and `featurizer.layer4` blocks.\n4.  `loss_reg = self.MSEloss(self.adaparams(z_aug-z_ori), torch.zeros_like(z_ori)) * self.hparams['ada_lr']` calculates the consistency loss, but this time only `test_mapping` parameters are updated.\n\n**Prediction:**\n1.  `predict(x)` also passes `x` through the `featurizer` and applies `test_mapping` sequentially to `z_ori` before passing it to the `classifier`.\n\n**Hyperparameters (`hparams_registry.py`):**\n*   `ada_lr`: Learning rate for the test-time adaptive parameters (`test_mapping`). It is set to `0.1` for 'DomainNet' and `1e-6` for other datasets.\n\n**Evaluation (`domainbed/scripts/train.py`, `domainbed/lib/misc.py`):**\n*   The `accuracy_tsc` function is specifically used for evaluating ITTA. It performs the `test_adapt` step a fixed number of times (1 in the provided code snippet) on the test data `x` before making a `predict` call. During this process, `featurizer` and `classifier` are frozen (`requires_grad_(False)`)."
      }
    },
    {
      "title": "Test Time Adaptation via Conjugate Pseudo-labels",
      "abstract": "Test-time adaptation (TTA) refers to adapting neural networks to distribution\nshifts, with access to only the unlabeled test samples from the new domain at\ntest-time. Prior TTA methods optimize over unsupervised objectives such as the\nentropy of model predictions in TENT [Wang et al., 2021], but it is unclear\nwhat exactly makes a good TTA loss. In this paper, we start by presenting a\nsurprising phenomenon: if we attempt to meta-learn the best possible TTA loss\nover a wide class of functions, then we recover a function that is remarkably\nsimilar to (a temperature-scaled version of) the softmax-entropy employed by\nTENT. This only holds, however, if the classifier we are adapting is trained\nvia cross-entropy; if trained via squared loss, a different best TTA loss\nemerges. To explain this phenomenon, we analyze TTA through the lens of the\ntraining losses's convex conjugate. We show that under natural conditions, this\n(unsupervised) conjugate function can be viewed as a good local approximation\nto the original supervised loss and indeed, it recovers the best losses found\nby meta-learning. This leads to a generic recipe that can be used to find a\ngood TTA loss for any given supervised training loss function of a general\nclass. Empirically, our approach consistently dominates other baselines over a\nwide range of benchmarks. Our approach is particularly of interest when applied\nto classifiers trained with novel loss functions, e.g., the recently-proposed\nPolyLoss, where it differs substantially from (and outperforms) an\nentropy-based loss. Further, we show that our approach can also be interpreted\nas a kind of self-training using a very specific soft label, which we refer to\nas the conjugate pseudolabel. Overall, our method provides a broad framework\nfor better understanding and improving test-time adaptation. Code is available\nat https://github.com/locuslab/tta_conjugate.",
      "full_text": "Test-Time Adaptation via Conjugate Pseudo-labels Sachin Goyal⋆1 Mingjie Sun⋆1 Aditi Raghunathan1 Zico Kolter1,2 1Carnegie Mellon University, 2Bosch Center for AI {sachingo, mingjies, raditi, zkolter}@cs.cmu.edu Abstract Test-time adaptation (TTA) refers to adapting neural networks to distribution shifts, with access to only the unlabeled test samples from the new domain at test-time. Prior TTA methods optimize over unsupervised objectives such as the entropy of model predictions in TENT [50], but it is unclear what exactly makes a good TTA loss. In this paper, we start by presenting a surprising phenomenon: if we attempt to meta-learn the “best” possible TTA loss over a wide class of functions, then we recover a function that isremarkably similar to (a temperature-scaled version of) the softmax-entropy employed by TENT. This only holds, however, if the classiﬁer we are adapting is trained via cross-entropy loss; if the classiﬁer is trained via squared loss, a different “best” TTA loss emerges. To explain this phenomenon, we analyze test-time adaptation through the lens of the training losses’sconvex conjugate. We show that under natural conditions, this (unsupervised) conjugate function can be viewed as a good local approximation to the original supervised loss and indeed, it recovers the “best” losses found by meta-learning. This leads to a generic recipe that can be used to ﬁnd a good TTA loss for any given supervised training loss function of a general class. Empirically, our approach consistently dominates other TTA alternatives over a wide range of domain adaptation benchmarks. Our approach is particularly of interest when applied to classiﬁers trained with novel loss functions, e.g., the recently-proposed PolyLoss [25] function, where it differs substantially from (and outperforms) an entropy-based loss. Further, we show that our conjugate based approach can also be interpreted as a kind of self-training using a very speciﬁc soft label, which we refer to as the conjugate pseudo-label. Overall, our method provides a broad framework for better understanding and improving test-time adaptation. Code is available at https://github.com/locuslab/ tta_conjugate. 1 Introduction Modern deep networks perform exceeding well on new test inputs that are close to the training distribution. However, this performance dramatically decreases on test inputs drawn from a different distribution. While there is a large body of work on improving the robustness of models, most robust training methods are highly specialized to the setting they cater to. For e.g., they assume pre-speciﬁed perturbations, subpopulations, and spurious correlations, or access to unlabeled data from the target distribution, and most methods offer close to no improvement on general distribution shifts beyond what they were trained for [12, 21]. In practice, it is often cumbersome (or even impossible) to precisely characterize all possible distri- bution shifts a model could encounter and then train accordingly. Instead, a model already trained on some source data must be able to adapt at test-time to new inputs from a different domain. This setting of test-time adaptation (TTA) has gained interest in recent years [ 6, 47, 50, 54]. TTA is typically accomplished by updating the source model parameters via a few steps of optimization on an unsupervised objective involving the new test sample from the target distribution. The choice ⋆ Equal Contribution 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2207.09640v2  [cs.LG]  23 Nov 2022of this unsupervised objective, which we call the TTA loss, dictates the success of the adaptation procedure. [47] uses a self-supervised objective on the test sample, [50] uses the entropy of model predictions, and several follow-ups have proposed variants or alternatives [ 40, 54]. However, it remains unclear as to how to choose or guide the selection of this TTA loss, and thus far the choice of these losses has remained largely heuristic in nature. In this work, we begin by presenting a set of intriguing experiments where we attempt to learn the “best” TTA loss for a given source classiﬁer and distribution shift. We parameterize the TTA loss by another neural network whose parameters are learnt via meta-learning [ 3, 9] where we differentiate through the adaptation process to ﬁnd the TTA loss that achieves the best adaptation on distribution shifts. Surprisingly, we ultimately learn a TTA loss that looksremarkably similar to (a temperature-scaled version of) the softmax-entropy loss, which was already proposed by [50]. Why did we recover the commonly used softmax-entropy loss despite the fact that the procedure is capable of learning a very general class of losses and the meta-learning process could potentially specialize to both the source classiﬁer and the distribution shift of interest? Furthermore, we ﬁnd that this pattern only holds when the loss used to train the source classiﬁer is cross-entropy loss; when a different loss such as squared loss is used instead, the meta-learning procedure recovers a TTA loss that itself looks more like a negative squared error, and is very different from the softmax-entropy loss (Section 3). In order to explain this phenomenon, we propose to consider TTA through the lens of the convex conjugate function. Speciﬁcally, given a hypothesis function h(x) and label y, several common losses (cross-entropy and the squared loss amongst them, but not limited to these) can be written in the form L(h(x),y) = f(h(x)) −yTh(x) for some function f. In these cases, we show that “natural” TTA loss for such classiﬁers is precisely the (negation of) the convex conjugate evaluated at the gradient of h, LTTA(x) = −f∗(∇f(h(x)), where f∗is the convex conjugate of f. This framework not only recovers the results of our meta-learning experiments, but also justiﬁes why some speciﬁc choices of TTA loss in the previous literature work well (e.g., this framework recovers TENT’s choice of softmax-entropy for cross-entropy-trained classiﬁer). Moreover, it also provides a broad framework for what the TTA loss should be when the source model is trained using various different loss functions (for example the recently-proposed PolyLoss [25, 29]) as is becoming increasingly common in machine learning. Further, we show that our proposed conjugate adaptation loss is in fact a kind of self-training with pseudo-labels [42], a classic approach in machine learning. Various formulations of the pseudo-label have been proposed in the literature, and our conjugate analysis provides a general recipe for the “correct” choice of soft pseudo-labels given byˆy(x) = ∇f(h(x)). We thus refer to these as conjugate pseudo-labels (Conjugate PL’s), and believe our work provides a broad framework for understanding adaptation with unlabeled data in general. Finally, we empirically verify the effectiveness of our proposed conjugate adaptation loss across several datasets and training losses, such as cross-entropy and squared loss, along with the recently- proposed PolyLoss [ 25] (which itself has shown higher standard test accuracy on a wide range of vision tasks). Over all models, datasets and training losses, we ﬁnd our proposed conjugate pseudo-labeling consistently outperforms prior TTA losses and improves TTA performance over the current state of the art. 2 Background and preliminaries. Test-time adaptation. We are interested in mapping an input x∈Rd to a label y∈Y. We learn a model hθ : Rd ↦→R|Y|parameterized by θthat maps an input xto predictions hθ(x). We assume access to a trained source model and adapt at test-time over the test input, before making the ﬁnal prediction. This is the standard test-time adaptation (TTA) setting [47, 50]. During TTA, we update the model parameters on an unsupervised objective L(x,hθ). For example, in TENT [50], this loss is the entropy of the softmax-normalized predictions of the model. At each time step of adaptation, we observe a batch of test inputs and we take a gradient step towards optimizing the TTA loss on this test batch. As is standard, we measure the average online performance of models across all steps (number of test batch inputs seen) in the adaptation process. Meta learning the loss function. In order to explore the existence of different TTA losses, we employ the meta-learning procedure where we attempt to learn the TTA loss. We use a similar procedure as prior work on meta-learning loss functions [3, 37] and parameterize the loss function via a neural network mφ : R|Y| ↦→R that takes in the model predictions/logits and outputs a loss value. We want to learn parameter φsuch that when we update θvia the loss function mφ, our ﬁnal 2performance is optimal. In order to do so, let xbe the unlabeled test samples to adapt to, and ybe the corresponding labels. We update θand φalternatively as follows. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt , φt+1 ←φt −β∂L(hθt+1 (x′),y′) ∂φt , (1) where Lis some supervised surrogate loss function such as cross-entropy. Please refer to Appendix A3 for further details regarding meta-learning setup. Note that the meta-learning process above assumes access to labels yof test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We discuss our ﬁndings from this exploration in the next section. 3 Test-time Adaptation via Meta-Learnt Losses The objective used in TENT is the softmax-entropy of the model predictions which essentially makes the classiﬁer more conﬁdent in its current predictions. The same can be achieved by various other loss formulations such as those mentioned in [40]. With so many possible choices for the loss function, what should we use for TTA? In this section, we attempt to answer this empirically and present some intriguing observations. (a)  (b) Figure 1: Visualization of meta loss (blue) by varying one input prediction score. (a) For cross-entropy loss trained model, the learnt meta loss can be approximated with a scaled softmax-entropy function (dashed red). (b) When the source model is trained with a squared loss for classiﬁcation, the learnt meta loss (blue) can be ﬁtted closely with a quadratic function (dashed red), shown in Figure 1b. The range (max/min) of the prediction score (logit) in x-axis is chosen to cover the empirical range of the predicted logits. Experiment 1. We learn the TTA loss parameterized by a neural network via meta-learning as described in Section 2. Our source classiﬁer is a ResNet-26 trained on CIFAR-10 and we adapt to distribution shifts in CIFAR-10-C. We use the 4 labeled validation noises in CIFAR-10-C to learn the meta-loss network parameters and we denote the resulting learnt loss function by meta-TTA loss. We then adapt the source classiﬁer to the test set of 15 corruptions by optimizing the meta-TTA loss. Observations. First, we ﬁnd that TTA using meta-TTA loss performs better than TENT (12.35% vs 13.14%), suggesting that there are better TTA losses than previous losses based on softmax-entropy. However, on examining this meta-TTA loss, we ﬁnd a surprising observation. Figure 1a (blue curve) visualizes the learnt meta-loss over model predictions as we vary a single class prediction with the rest ﬁxed. Qualitatively, the learnt meta-loss looks very similar to softmax-entropy in one dimension. In fact, we can ﬁt it closely with a scaled softmax-entropy function (dashed red curve): α·H(softmax(hθ(x)/T)), where αis a magnitude parameter and T is a temperature scaler. We want to test if the meta-loss is basically learning the softmax-entropy function. Hence, we perform test-time adaptation with the ﬁtted softmax-entropy function instead (dashed red curve) and achieve an error of 12.32%, essentially recovering the performance of meta-TTA. 3Despite the ability to represent many different loss functions and potentially specialize to the CIFAR- 10-C setting, the meta-loss procedure gave back the standard entropy objective.Do we always recover a loss that looks like softmax-entropy? Experiment 2. In an attempt to isolate when we get back the entropy objective, we vary several things. We tried different architectures for the source classiﬁer, different lossesLduring the meta- learning process (1) and different training losses for the source classiﬁer. Results. We observed that we consistently recovered the temperature scaled softmax-entropy function in all cases except when we varied the training loss for the source classiﬁer (Appendix A.10). On using the squared loss function [18], a strikingly different meta-TTA loss emerges. Figure 1b (blue curve) shows the learnt meta-loss (13.48% error) for this network. Here again, the meta-TTA loss outperforms entropy (14.57%) but it is not simply due to a scaling factor. The loss now looks like the negative squared error (red curve). Like previously, we tried ﬁtting a quadratic loss directly to the meta loss in Figure 1b, and this time we even slightly outperformed the meta-TTA loss. To summarize, we used a meta-learning procedure to search for the “best” TTA loss, where the loss itself was parameterized by a neural network that could potentially represent arbitrarily complex loss functions. However, we ended up with loss functions displaying remarkable structure: across different architectures and different variants of meta-learning, for a classiﬁer trained with cross-entropy, the meta-TTA loss was temperature scaled softmax-entropy and for a classiﬁer trained with squared loss, the meta-TTA loss was a negative squared loss. This is interesting from both a practical and conceptual standpoint where the “best” TTA loss depends on the loss used to train the source classiﬁer in a clean fashion. We attempt to understand and explain this phenomenon in the next section. 4 Conjugate Pseudo Labels Results in the previous section raise an obvious question: why does softmax-entropy as used in TENT seem to be the “best” possible test time adaptation loss for classiﬁers trained via cross-entropy (at least, best in the sense that meta-learning consistently recovers something which essentially mimics softmax-entropy, even though meta-loss is parameterized by a neural network and hence could learn much more complex functions speciﬁc to the model and the particular shift)? And why, alternatively, does a quadratic TTA loss seem to perform best when the classiﬁer is trained via squared loss? In this section, we offer an explanation of this phenomenon via the construct of the convex conjugate function [1]. As we will see, our method recovers softmax-entropy and quadratic loss as the “natural” objectives for classiﬁers trained via cross-entropy and squared loss respectively. Furthermore, for classiﬁers trained via other loss functions, as is becoming increasingly common in deep learning, our approach naturally suggests corresponding test-time adaptation losses, which we show in the next section to comparatively outperform alternatives. Thus, we argue that our framework overall provides a compelling recipe for specifying the “correct” method for TTA for a large class of possible losses. 4.1 Losses and the convex conjugate We begin by formally considering loss functions between a hypothesis outputhθ(x) (e.g., the logit outputs of a classiﬁer, or the direct prediction of a regressor) and targetythat take the following form L(hθ(x),y) = f(hθ(x)) −yThθ(x) (2) for some function f; when there is no risk of confusion, we will use hin place of hθ(x) for simplicity of notation. While not every loss can be expressed in such a form, this captures a wide variety of common losses (possibly scaled by a constant value). For example, cross-entropy loss corresponds to the choice f(h) = log ∑ iexp(hi) and where y denotes a one-hot encoding of the class label; similarly, squared loss corresponds to the choice f(h) = 1 2 ∥h∥2 2. When training an over-parameterized classiﬁer, we can roughly view the training process as (approxi- mately) attaining the minimum over hypotheses hfor each training example min θ 1 t t∑ i=1 L(hθ(xi),yi) ≈1 t t∑ i=1 min h L(h,yi) (3) 4where t is the number of training samples. However, in the case of losses in the form (2), the minimization over hin this form represents a very speciﬁc and well-known optimization problem: it is known as the convex conjugate [1] of the function f min h L(h,y) = min h {f(h) −yTh}= −f⋆(y) (4) where f⋆ denotes the convex conjugate of f. f⋆ is a convex function in y(and indeed, is convex regardless of whether or not f is convex). Furthermore, for the case that f is convex differentiable, the optimality condition of this minimization problem is given by ∇f(hopt) = y, so we also have that f⋆(y) = f⋆(∇f(hopt)) (5) where hopt refers to the optimal classiﬁer (used interchangeably with hθopt ). Putting this all together, we can state (admittedly, in a rather informal manner) that under the assumption that θopt is chosen so as to approximately minimize the empirical loss on the source data in the over-parameterized setting, we have that for tinputs 1 t t∑ i=1 L(hθopt (xi),yi) ≈1 t t∑ i=1 −f⋆(∇f(hθopt (xi))) (6) i.e., the empirical loss can be approximated by the (negative) conjugate applied to the gradient of the f, at least in a region close to the optimal θopt that minimizes the empirical loss. But the later expression has the notable beneﬁt that it does not require any label yi in order to compute the loss, and thus can be used as a basis for TTA on target domain of the hypothesis function hθopt . Deﬁnition 1 (conjugate adaptation loss) Consider a loss function that takes the form given in 2, used for training a hypothesis hθ in the over-parameterized regime. We deﬁne the conjugate adaptation loss Lconj(hθ(x)) : R|Y|↦→R as follows. Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))⊤hθ(x). (7) 4.2 Recovery of existing test-time adaptation strategies Cross-entropy The interesting aspect to this formalism is that when applied to classiﬁers trained with cross-entropy, it recovers exactly the TENT approach to TTA : minimizing the softmax-entropy of hθ(x). And indeed, this loss was also recovered when using meta-learning to learn the “optimal” test-time adaptation loss. To see this, note that for cross-entropy, we have thatf(h) = log ∑ iexp(hi), giving the optimality condition y= ∇f(hopt) = exp(hopt)∑ iexp(hopt i ) and the conjugate function f⋆(y) = { ∑ iyilog yi if ∑ iyi = 1 ∞ otherwise . (8) In other words, Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (9) i.e. softmax-entropy of the model prediction, which is exactly the TTA loss that TENT uses. Squared loss For the squared loss, we have thatf(h) = 1 2 ∥h∥2 2, leading to the optimality condition y = hand conjugate function f⋆(y) = 1 2 ∥y∥2 2. Hence, the adaptation loss in this case would be simply given by Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = −1 2 ∥h∥2 2 which is also what we observed in the meta-learning experiments discussed in Section 3. 4.3 Conjugate pseudo-labels We now emphasize that by the nature of our approximations, there is an additional simple interpre- tation of the conjugate loss: it is also equal to the original loss (2) applied to the “psuedo-labels” ˜yCPL θ (x) = ∇f(hθ(x)), where CPL refers to conjugate pseudo-labels, i.e., Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))Thθ(x) = L(hθ(x),∇f(hθ(x))). (10) 5This property is known as the Fenchel-Young inequality, that isf(x) + f⋆(u) ≥xTuholding with equality when u = ∇f(x). In other words, our conjugate adaptation loss is precisely equivalent to self-training under the speciﬁc soft pseudo-labels given by ˜yCPL = ∇f(hθ(x)). And indeed, for many cases, this may be a more convenient form to compute than explicitly computing the conjugate function at all. For this reason, we refer to our method as that of conjugate pseudo-labels. In the case of cross-entropy loss, this approach then corresponds exactly to self-training using labels given by the softmax applied to the current hypothesis. We must emphasize, however, that while our conjugate formulation indeed has this “simple” form for the case of cross-entropy loss, the real advantage comes in that it provides the “correct”pseudo-label for use with other losses, which may result in pseudo-labels different from the “common” softmax operation. Example: conjugate pseudo-labels for PolyLoss. PolyLoss [25] is a recently-proposed simple alternative to cross-entropy loss than has been shown to improve performance across a wide variety of compute tasks. This loss is given by the form Lpoly(hθ(x),y) = Lce(hθ(x),y) + ϵ·yT(1 −softmax(hθ(x))) (11) We note that this can be put exactly into our conjugate form (equation 2) by writing the loss in a slightly more involved fashion, which we refer to as the expanded conjugate form Lpoly(hθ(x),y) = f(hθ(x)) −yTg(hθ(x)). (12) where f is the log-sum-exp function as before, and g(h) = h−ϵ(1 −softmax(h)). In order to formally put this into the form of the previous loss function (equation 2), we can simply deﬁne an alternative hypothesis as the function h′ θ(x) = g(hθ(x)), and then deﬁne PolyLoss in the conjugate form as Lpoly(h′ θ(x),y) = f(g−1(h′ θ(x))) −yTh′ θ(x). (13) Typically, however, it is easier to simply operate on the expanded conjugate form, which yields the optimality condition for the pseudo-label ∇f(hopt) = Dg(hopt)˜yCPL θ (x), where D is the Jacobian operator. For the case of PolyLoss, this leads to the conjugate pseudo-label of the following form: ˜yCPL θ (x) = (I+ ϵdiag(z) −ϵzzT)−1z, z ≡softmax(hθ(x)). Test-time adaptation. Finally, we note that the above discussion doesn’t actually address any topics related to test-time adaptation to OOD data, but merely provides a generic characterization of a self- training procedure for generic loss functions of the form(2). However, the application toTTA on OOD data is fairly straightforward: as long as the learnt source parameters θis a reasonable approximation to the true optimal θopt on the shifted domain, self-training with the conjugate pseudo-labels provides a reasonable proxy for ﬁne-tuning the network on the true OOD loss. We emphasize that, common to most approaches for TTA , there are still some amount of design decisions that must be put in place; these are detailed in Section 5.1. In practice, we observe OOD generalization typically beneﬁts (across all baselines) from an additional “temperature” scaling, i.e., applying the TTA loss to hθ(x)/T for some ﬁxed temperature T, although it requires a held-out validation dataset for tuningT. However, we should emphasize that truly unsupervisedTTA would require making an informed guess for the value of these hyper-parameters. The full procedure for test time adaptation via conjugate pseudo-labels is shown in Algorithm 1. Algorithm 1 Conjugate pseudo-labeling (Conjugate PL) Input: Source classiﬁer θ0 trained using loss L(hθ(x),y) = f(hθ(x)) −hθ(x)⊤y. N batches of test data Dtest = [x1,x2,...,x N] Hyperparams: learning rate ηand temperature T. Let ¯hθ(x) def = hθ(x)/T be the temperature scaled predictor. Let ˜yCPL θ (x) denote the conjugate pseudo-label function ˜yCPL θ (x) = ∇(f(¯hθ(x))). for n= 0,1,...N −1 do θn+1 = θn −η∇L ( ¯hθ(xn),˜yCPL θ (xn) ) [Self-training with conjugate pseudo-labels] 65 Experiments In this section, we empirically evaluate the effectiveness and generality of the proposed conjugate pseudo-labeling procedure (Algorithm 1) for test-time adaptation on a variety of datasets. 5.1 Setup Datasets. We evaluate on the three common corruption benchmarks: adapting a classiﬁer trained on CIFAR-10 to CIFAR-10-C, CIFAR-100 to CIFAR-100-C and ImageNet to ImageNet-C [ 15]. Following the previous works [47, 50], we report the error averaged across corruptions at the highest severity for CIFAR-10/100-C and averaged across corruptions and severity level for ImageNet-C. We also evaluate on three domain adaptation datasets: adapting a classiﬁer trained on SVHN to MNIST, an ImageNet classiﬁer to ImageNet-R [16] and adapting from synthetic to real data in VISDA-C [38]. Models and Training losses. Following previous works on TTA[47, 50], we use ResNet-26 [14] as the source classiﬁer architecture for CIFAR-10/100 experiments, ResNet-18 for SVHN to MNIST and a ResNet-50 for ImageNet and source synthetic data on VisDA-C. We consider source classiﬁers trained via the following loss functions: the de-facto cross-entropy, recently proposed polyloss [25] and squared loss [18]. Baselines. Our proposed conjugate pseudo-label is the classic approach of self-training with a speciﬁc form of pseudo-labels. In self-training, we replace the label ywith a pseudo-label ˜y(x) and adapt by optimizing the loss function L(hθ(x),˜y(x)). Note that we could either instantaneously update the pseudo-labels using the current classiﬁer, or generate pseudo-labels once with just the source classiﬁer. Instantaneous updates have been shown to work better for domain adaptation [7, 40], and we perform instantaneous updates for all methods. While we propose using ˜yCPL(x) = ∇f(hθ(x)) (See Section 4.3), we compare to the standard pseudo-labels used in the literature: • (i) the “hard” pseudo-label (hard PL) where ˜y(x) = arg maxi ( hθ(x) ) i is the most likely class as predicted by hθ. As is common in the self-training literature, we perform conﬁdence thresholding. • (ii) The “soft” pseudo-label (soft PL) where ˜y(x) is obtained by applying a softmax function to the model predictions hθ(x). We also compare with the following recently proposed test-time adaptation methods. • Entropy Minimization (ENT) [50] minimizes the entropy of model predictions. • Robust Pseudo-Label [40] where we minimize a robust classiﬁcation loss, Lrpl = q−1(1 −p(i|x)q) where i= argmaxjp(j|x) and q∈[0,1]. • MEMO [54] minimizes entropy of a model’s outputs across different augmentations of a test input. We implement a batch version, where we see multiple test points at once, for fair comparisons. TTA methodology. Following [ 50] and [40], we ﬁne-tune by updating the learnable scale and shift parameters of the batch normalization layers across all adaptation losses. For each batch, batch normalization statistics is also updated, as suggested in [41]. We report performance at the end of one round of test-time adaptation over the entire test set. We tune the learning rate (LR) and temperature (T) on the validation noises in the corruption benchmark by grid-search. LR is selected from {1e−1,1e−2,... 1e−4}and T from {1,2 ... 5}. All the experiments have been performed on A6000 GPU’s. On domain adaptation benchmarks, where there is no held-out target domain, we set T to be 1 and use the LR suggested by [ 6, 50]. We use the same hyperparameter tuning protocol across all methods. We single out temperature as a very important hyperparameter, as we discuss in the results below. 5.2 Results on classiﬁers trained with cross-entropy We study the effectiveness of our proposed conjugate pseudo-labels when the source classiﬁer is trained via cross-entropy loss. In this case, baselines Softmax PL and ENT are the same as Conjugate PL. Thus we omit them in our results. Table 1, reports the performance of various TTA methods. When the source classiﬁer is trained via cross-entropy, our conjugate pseudo-label algorithm exactly corresponds to entropy minimization with an additional temperature scaling. Entropy minimization as 7Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 13.95 (±0.06) 13.97 ( ±0.04) 12.60(±0.04) 13.07 (±0.05) \u0013 13.95 (±0.06) 12.85 ( ±0.04) 12.51(±0.01) 12.51(±0.03) CIFAR-100-C \u0017 45.22 (±0.4) 39.80 ( ±0.18) 38.52(±0.16) 41.15 (±0.25) \u0013 45.22 (±0.4) 36.37 ( ±0.10) 37.38 ( ±0.06) 36.10(±0.07) ImageNet-C \u0017 45.43(±0.05) 45.68 ( ±0.01) 48.91( ±0.03) 45.82(±0.01) \u0013 45.43 (±0.05) 45.61 ( ±0.01) 48.91( ±0.04) 45.36(±0.01) Table 1: Mean errors when adapting to corruptions using a source classiﬁer trained via cross- entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. With the right temperature scaling, softmax-entropy minimization matches or outperforms other approaches. Prior reported gains of other methods over softmax-entropy minimization disappear when we use temperature scaling. For additional context, the source classiﬁer errors without adaptation are: CIFAR-10-C (29.54%), CIFAR-100-C (62.26%), ImageNet-C (61.89%) proposed in prior work [50] does not tune the temperature parameter, and some newer objectives such as robust PL or MEMO outperform vanilla entropy minimization. For example, on CIFAR-100-C, vanilla ENT obtaines 41.15% average error, while robust PL improves this to39.80% and MEMO to 38.52%. However, with the right temperature scaling, entropy minimization obtains 36.10% error which outperforms the newer objectives (with and without temperature scaling). A similar observation holds for CIFAR-10-C and ImageNet-C as well. Essentially, the gains over vanilla entropy minimization vanish when we do temperature scaling, and entropy minimization (i.e. conjugate pseudo-labeling corresponding to cross-entropy) turns out to be the best objective after all. 5.3 Results on classiﬁers trained with polyloss and squared loss In the case of cross-entropy, conjugate pseudo-labeling reduces to the familiar notion of entropy minimization. We now explore the performance of our method on different loss functions where the conjugate pseudo-labels differ substantially from entropy minimization (section 4.3). Table 2 presents the results on the corruption benchmarks and Table 3 presents the results on the other domain adaptation datasets for source classiﬁers trained with PolyLoss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C \u0017 13.81(±0.12) 14.23(±0.02) 13.46(±0.06) 13.23(±0.07) 14.64(±0.11) 13.02(±0.09) \u0013 13.81(±0.12) 12.45(±0.05) 12.23(±0.06) 12.33(±0.04) 12.26(±0.04) 12.08(±0.05) CIFAR-100-C\u0017 40.47(±0.05) 42.86(±0.11) 40.12(±0.08) 39.90(±0.05) 41.00(±0.11) 38.17(±0.17) \u0013 40.47(±0.05) 39.80(±0.08) 38.23(±0.05) 39.23(±0.04) 37.04(±0.06) 36.83(±0.08) ImageNet-C \u0017 45.44(±0.21) 46.27(±0.03) 46.10(±0.03) 48.21(±0.05) 44.63(±0.03) 44.01(±0.01) \u0013 45.44(±0.21) 46.27(±0.03) 45.50(±0.02) 48.21(±0.04) 44.45(±0.03) 44.01(±0.01) Table 2: Mean errors when adapting to corruptions using a source classiﬁer trained via recently proposed Poly-1 Loss [ 25]. Conjugate pseudo-labeling consistently outperforms all previous ap- proaches. For additional context, source classiﬁer errors without adaptation : CIFAR-10-C (30.22%), CIFAR-100-C (63.91%) and ImageNet-C (62.18%). First, we note that, across all datasets in Table 2 and Table 3, our conjugate PL approach outperforms all other TTA losses. With polyloss classiﬁers, entropy minimization is no longer the best method—on CIFAR-100-C, entropy minimization achieves38.23% error while our conjugate PL achieves36.83%. We see similar consistent gains on CIFAR-10-C, ImageNet-C, ImageNet-R and VisDA-C. On digit adaptation tasks from SVHN to MNIST/USPS/MNISTM, where there is a larger shift between source and target, the gains are especially pronounced. Figure 2 compares how the task loss (polyloss ϵ= 6) on the test data decreases as we adapt the model through conjugate PL and other baselines. We use CIFAR-10-C as an example. Observe that our proposed conjugate PL indeed reduces the task loss the most among other baselines. 8Dataset Source Error Hard PL Robust PL EntropySoftmax PL Conjugate PL Ours SVHN→MNIST 28.33 20.21 19.73 14.28 16.54 10.73 SVHN→USPS 31.58 23.32 26.12 23.12 24.07 21.62 SVHN→MNISTM61.69 50.73 51.35 49.33 50.47 47.59 ImageNet-R 64.19 58.52 59.46 58.25 56.62 55.63 VisDA-C 58.13 40.43 45.44 44.11 39.63 38.42 Table 3: Target error when adapting models trained via polyloss on source domains across different domain adaptation bench- marks. Conjugate pseudo-labeling offers consistent and substan- tial gains over previous approaches across three datasets. Figure 2: Task Loss (PolyLoss ϵ= 6) evaluated on CIFAR-10-C test data during test-time adaptation. Furthermore, on CIFAR-10-C and ImageNet-C, we ﬁnd that adapting polyloss classiﬁers via conjugate PL improves the performance over all methods applied to cross-entropy trained source classiﬁers. For e.g., on ImageNet-C, the performance improves from 45.34% to 44.01%. However, this is only true when using the proposed conjugate PL. If we just did softmax-entropy minimization (even with temperature scaling), the ﬁnal adapted performance of a polyloss classiﬁer (45.5%) is in fact worse than that of a cross-entropy classiﬁer (45.34%). Our results suggest that as we develop new training losses that improve the source classiﬁers, it is important to adapt via conjugate pseudo-labeling to reap the maximum gains. Similarly, we experiment with the case when the source classiﬁer is trained using squared loss on the CIFAR-10 and CIFAR-100 datasets, and observe consistent gains using the proposed conjugate pseudo-labels over the baselines. For example, on CIFAR-10-C, TTA using conjugate PL gives and error of 12.87%, outperforming baselines like ENT (13.24%) and Softmax PL (31.81%). Table 5 in Appendix A.7 shows the detailed results. Comparing Table 1 and Table 2, we see that the relative ordering between the various baselines differs. This is further evidence that the adaptation loss has to depend on the training loss, and we believe our conjugate pseudo-label approach captures this appropriately by offering consistent gains across the various settings we experimented with. 6 Related Works Test-time adaptation methods. In recent years, the setting of test-time adaptation has gained a lot of interest with a host of different approaches proposed in the literature. One family of TTA approaches update the source classiﬁer by minimizing an unsupervised loss on the target distribution [4, 6, 20, 22, 35, 36, 40, 43, 44, 50, 51, 54]. TENT [ 50] proposes to minimize the entropy of model predictions at test time. Several follow ups like [ 6, 35, 40, 44, 54] propose alternative TTA objectives, e.g. robust pseudo-labelling [40], likelihood ratio loss [35], entropy of marginal probability averaged across augmentations [54] and self-supervised contrastive losses [6, 49]. However, most of these objectives are heuristically designed or chosen. In this paper, we provide a principled approach of designing unsupervised objectives for TTA . Another family of approaches for test-time adaptation such as [ 2, 8, 13, 31, 34, 47] leverage an auxiliary self-supervised task (e.g. rotation prediction [ 47], masked autoencoders [10]) to update model parameters on each test sample. Crucially, these methods require modifying the source model training by augmenting the supervised training objective with an auxiliary self-supervised loss. Hence it cannot be applied to typical standard classiﬁers that are trained by minimizing a supervised loss on the source data. Source-free domain adaptation. A very related setting to test-time adaptation is source-free domain adaptation, where a trained source classiﬁer must be adapted to a target distribution of interest, although the entire target unlabeled data is available at once. SHOT [28] proposes to optimize the source hypothesis (i.e. feature extractor) with a combination of entropy minimization, diversity and self-training on pseudo-labels on the unlabeled target data. [53] promotes feature clustering on features from target distributions. [24, 26] use generative modeling to estimate the underlying source distributions for enforcing feature invariance. Such approaches typically require multiple epochs over the target data and cannot be easily adopted to work in an online fashion. 9Unsupervised domain adaptation. The most canonical setting of domain adaptation involves access to labeled source data and unlabeled target data, all during training. The availability of source and target data during training lends itself to approaches that “align” the source and target representations in some way: [ 32, 33, 45, 48] match distribution statistics, [ 11] uses a discriminator, [ 46] uses self-supervised learning. However, such approaches require access to source data which might not always be feasible due to data privacy and efﬁciency issues. Pseudo-labels and self-training. Self-training is a classic idea for leveraging unlabeled data, devel- oped ﬁrst for the semi-supervised setting. Self-training generates pseudo-labels on the unlabeled data, allowing us to use any “supervised” loss on this pseudo-labeled data. Self-training has shown promising results in various settings like semi-supervised learning [ 19] and improving adversarial robustness [ 5]. Self-training has also been gaining attention in the setting of unsupervised domain adaptation [28, 39], where pseudo-labels generated on the unlabeled data from target domain is used to supervise the adaptation process. [ 7, 23, 52] provide theoretical insights into how self-training with pseudo-labels can help under distribution shift. TENT [50] (i.e entropy minimization) can be viewed as a form of self-training with instantaneous softmax pseudo-labels. Our work provides a general framework for the choice of soft pseudo-labels based on the conjugate analysis of the source training objective. Some prior works like [7, 17, 27, 30, 55, 56] have documented the improvement in performance when using instantaneous pseudo-labels over pre-computed pseudo-labels, and thus lend further support to the beneﬁts of our proposed conjugate pseudo-labeling approach. The ex- periment results presented in this work supporting conjugate pseudo-labels suggest that conjugate pseudo-labels is a promising direction of pseudo-labeling in a broader context. 7 Conclusion, Limitations and Future Directions In this work, we proposed a general test-time adaptation loss, based on the convex conjugate formulation which in turn was motivated by the intriguing meta learning experiments. The fact that meta-learning recovers the proposed loss hints at some kind of optimality of the loss. In Section 4, we prove that for a broad set of loss functions, the proposed (unsupervised) conjugate loss is close to the oracle supervised loss. However, this still does not completely answer what the optimal test-time adaptation loss is and why. The meta-learning framework in this work was constrained to learn functions over the logits of each individual input. It can be expanded to more involved setups, where we consider functions over the intermediate representations too and also consider learning functions over a batch of input while accounting for their interactions. Beyond the choice of the adaptation loss itself, achieving good test-time adaptation generally involves several heuristics like updating only the batch norm parameters [50]. While our work was motivated by the loss function, via the meta-learning experiments, we discovered that temperature scaling is another important hyper-parameter that improves the performance of all previous baselines as well. At a high level, test-time adaptation has to be appropriately regularized to prevent the updates over batches from taking the model too far: updating only a few batch norm parameters is one way to do that, and perhaps temperature scaling provides a similar beneﬁcial regularization effect by making the network predictions on unlabeled inputs less conﬁdent. Understanding the role of these heuristics more concretely is an interesting direction for future work. It also remains an open problem to understand under what sort of real-world distribution shifts would self-training based approaches would help. Finally, it is also worth extending and applying the conjugate pseudo-labeling to other settings like semi-supervised learning. 8 Acknowledgments We thank Shubhang Bhatnagar and Asher Trockman for helping with running the ImageNet experi- ments. We thank Zhili Feng for useful feedback. Sachin Goyal and Mingjie Sun were supported by funding from the Bosch Center for Artiﬁcial Intelligence. Aditi Raghunathan was supported by an Open Philanthropy AI Fellowship. 10References [1] https://en.wikipedia.org/wiki/Convex_conjugate. [2] Pratyay Banerjee, Tejas Gokhale, and Chitta Baral. Self-supervised test-time learning for reading comprehension. In Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2021. [3] Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, and Franziska Meier. Meta-learning via learned loss. arXiv preprint arXiv:1906.05374, 2019. [4] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [5] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Un- labeled data improves adversarial robustness. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf. [6] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [7] Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. In Advances in Neural Information Processing Systems, 2020. [8] Mohammad Zalbagi Darestani, Jiayu Liu, and Reinhard Heckel. Test-time training can close the natural distribution shift performance gap in deep learning based compressed sensing. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017. [10] Yossi Gandelsaman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. In Advances in Neural Information Processing Systems, 2022. [11] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, 2016. [12] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. InInternational Conference on Learning Representations, 2021. [13] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenya, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In International Conference on Learning Representations, 2021. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [15] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. [16] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [17] Yosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Advancing momentum pseudo-labeling with conformer and initialization strategy. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. 11[18] Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy in classiﬁcation tasks. In International Conference on Learning Representations, 2021. [19] Dong hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. [20] Yusuke Iwasawa and Yutaka Matsuo. Test-time classiﬁer adjustment module for model-agnostic domain generalization. In Advances in Neural Information Processing Systems, 2021. [21] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. [22] Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. Robustifying vision transformer without retraining from scratch by test-time class-conditional feature alignment. In International Joint Conference on Artiﬁcial Intelligence, 2022. [23] Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. In Proceedings of the 37 th International Conference on Machine Learning (ICML), 2020. [24] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2021. [25] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, and Dragomir Anguelov. Polyloss: A polynomial expansion perspective of classiﬁcation loss functions. In International Conference on Learning Representations, 2022. [26] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsuper- vised domain adaptation without source data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [27] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele. Learning to self-train for semi-supervised few-shot classiﬁcation. In Advances in Neural Information Processing Systems, 2019. [28] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. InProceedings of the 37th International Conference on Machine Learning (ICML), 2020. [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In IEEE/CVF International Conference on Computer Vision (ICCV), 2017. [30] Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation. In Advances in Neural Information Processing Systems, 2021. [31] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In Advances in Neural Information Processing Systems, 2021. [32] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer feature learning with joint distribution adaptation. In IEEE/CVF International Conference on Computer Vision (ICCV), 2013. [33] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32nd International Conference on Machine Learning, 2015. [34] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. In SIGGRAPH, 2020. 12[35] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-Time Adaptation to Distribution Shift by Conﬁdence Maximization and Input Transformation. arXiv preprint arXiv: 2106.14999, 2021. [36] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efﬁcient test-time model adaptation without forgetting. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [37] Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado P van Hasselt, Satinder Singh, and David Silver. Discovering reinforcement learning algorithms. In Advances in Neural Information Processing Systems, 2020. [38] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge, 2017. [39] Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [40] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. If your data distribution shifts, use self- learning, 2022. URL https://openreview.net/forum?id=1oEvY1a67c1. [41] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems, 2020. [42] H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transac- tions on Information Theory, 1965. [43] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. [44] Prabhu Teja Sivaprasad and François Fleuret. Test time adaptation through perturbation robust- ness. arXiv preprint arXiv: 2110.10232, 2021. [45] Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. arXiv preprint arXiv: 1612.01939, 2016. [46] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A. Efros. Unsupervised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. [47] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2019. [48] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014. [49] Dequan Wang, Shaoteng Liu, Sayna Ebrahimi, Evan Shelhamer, and Trevor Darrell. On-target adaptation. arXiv preprint arXiv: 2109.01087, 2021. [50] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. [51] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [52] Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations, 2021. 13[53] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [54] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. [55] Yang Zou, Zhiding Yu, B. V . K. Vijaya Kumar, and Jinsong Wang. Domain adaptation for semantic segmentation via class-balanced self-training. European Conference on Computer Vision, 2018. [56] Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V . K. Vijaya Kumar, and Jinsong Wang. Conﬁdence regularized self-training. In IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 14A Appendix A.1 Conjugate Derivations Cross-Entropy Loss : L(h,y) = − c∑ i=1 yilog exp(hi)∑c j=1 exp(hj) = − c∑ i=1 yi ∗hi + log c∑ j=1 exp(hj) = f(h) −y⊤h, (14) where f(h) is log ∑c j=1 exp(hj) and the constraint that ∑c i=1 yi = 1. Now, the conjugate f⋆(y) is given by : f⋆(y) = −min h {f(h) −yTh}= −min h {log c∑ j=1 exp(hj) −yTh} (15) with the constraint ∑c i=1 yi = 1. At the optimality, yi = (∇f(h))i = exp(hi)∑ jexp(hj) (16) Then, f⋆(y) = −log c∑ j=1 exp(hj) + c∑ i=1 hi exp(hi)∑ jexp(hj) = ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj), (17) if the constraint ∑c i=1 yi = 1 is satisﬁed, otherwise f⋆(y) = ∞by duality. This in turn gives, the conjugate loss for cross-entropy (when the constraint is satisﬁed) : Lconj(h) = −f⋆(y) = −f⋆(∇f(h)) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (18) Squared Loss : L(h,y) = 1 2||h−y||2 2 ≈1 2||h||2 2 −y⊤h [ignoring the constant term] = f(h) −y⊤h, (19) Now, the conjugate f⋆(y) is given by: f⋆(y) = −min h {f(h) −yTh}= −min h {1 2||h||2 2 −yTh} = −1 2||h||2 2 (20) A.2 Experiments on Binary Classiﬁcation with Exponential Loss Here we present the results on a binary classiﬁcation task over a synthetic dataset of 100 dimensional gaussian clusters. 15Dataset Creation For the binary classiﬁcation task, we create a synthetic dataset similar to [23]. Speciﬁcally, let the data X ∼ N(µ,Σ) ∈ R100 and labels Y ∈ {−1,+1}. We sample µ ∼ N(k,I100). For Σ, similar to [ 23], we sample a diagonal matrix D, where each entry is sampled uniformly from a speciﬁed range, and a rotation matrix U from a HAAR distribution, giving Σ = UDUT. For the source data, we sample µ−1 s ,µ+1 s ,Σ−1 s ,Σ+1 s as speciﬁed above with k= 0. Now to create a distribution shifted data of various severity, we sampleµ−1 t ,µ+1 t ,Σ−1 t ,Σ+1 t as speciﬁed above with k= 1, which are then used to sample the shifted data as follows : µ1 λ = λµ1 t + (1 −λ)µ1 s µ−1 λ = λµ−1 t + (1 −λ)µ−1 s Σ1 λ = λΣ1 t + (1 −λ)Σ1 s Σ−1 λ = λΣ−1 t + (1 −λ)Σ−1 s Xλ ∼N(µλ,Σλ) In the following experiments, easy shift refers to λ= 0.6, moderate shift to λ= 0.65 and hard shift to λ= 0.7. Exponential Loss for Binary Classiﬁcation Let zbe the classiﬁcation score hθ(x). For logistic training loss, conjugate adaptation loss would default to entropy with sigmoid probability. Thus, here we experiment with a different but also commonly used surrogate loss to 0/1 loss: exponential loss, which is deﬁned as: Lexp(z,y) = exp(−yz) (21) where y∈{−1,+1}. It can be rewritten in the expanded conjugate form of: Lexp(z,y) = 1 2 · ( ez + e−z) −1 2 ·y· ( ez −e−z) (22) For exponential loss, the conjugate pseudo-label function and the conjugate pseudo-label loss are: yCPL exp (z) = ez −e−z ez + e−z, LCPL exp (z) = 2 ez + e−z (23) The model is adapted on shifted gaussian clusters and we compare the conjugate loss with two baseline approaches: 1) Hard pseudo-labelling exp(−yhard pl ·z); 2) Entropy applied to sigmoid probability P(y= +1) = σ(z). The losses are compared on three degrees of shift (easy, moderate and hard), which is controlled by the drifted distance of Gaussian clusters. The results are shown in Figure 3, where we plot the accuracy curve with respect to adaptation iterations. With easy and moderate shift, conjugate loss (green) generalizes faster to shifted test data; with hard shift, only conjugate loss improves model accuracy on shifted test data while entropy (blue) deteriorates model performance. Figure 3: Test-time adaptation result on synthetic data with three shift levels ranging from easy, moderate and hard (detailed in section A.2). The source model is a linear classiﬁer trained with exponential loss Lexp = e−yhθ(x). Adaptation with the conjugate loss generalizes better compared to baseline losses. 16A.3 Meta Learning Experiment Details In section 3 we talked about learning the meta-loss function parameterized by a neural network mφ : R|Y|↦→R, that takes in the model predictions/logits and outputs a loss value. Here we discuss the architecture chosen and the implementation details. Further, in Appendix A.4 we empirically show that the learnt meta-loss is not affected by the choice of task loss / surrogate loss used in meta learning (Lin Equation 1). Note that the task loss / surrogate loss function is used to update the meta-loss mφ during meta-learning. The surrogate loss is calculated on updated source model’s predictions on labeled samples from test domain. The surrogate loss tries to update the meta-loss in the outer loop such that when meta-loss is later used to update the source model in the inner loop, the source model generalizes better to the test domain. Architecture and Implementation Details Figure 4 gives an overall schema for meta-learning the loss function and algorithm 2 gives the pseudo-code for meta-learning the loss function. Below we describe this in further detail. We use a transformer (denoted by T) with a MLP (denoted by P) over the output of transformer as the architecture for mφ, i.e. mφ(x) = P(T(x)). Speciﬁcally, for a given source trained model hθ and input x∼Dtest : 1. Let hθ(x) ∈R|Y|be the model predictions/logits, where |Y|denotes the number of classes. 2. Let hj θ(x) ∈R,∀j ∈|Y| be the prediction corresponding to class j. 3. The input to transformer is then given by z ∈R|Y|×(1+e), where zj ∈R1+e,∀j ∈|Y| is the concatenation of hj θ(x) and the learnable positional embedding pej ∈Re. 4. The transformer output is given by w= T(z) ∈Rd, where ddenotes the feed-forward dimension of the transformer. 5. The transformer output wis ﬁnally passed through a MLP to get the meta-loss valuemφ(hθ(x)) = P(w) ∈R 6. The source model is updated by optimizing over the meta-loss. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt (24) 7. The updated source model is then used to update the meta-loss by optimizing over some supervised loss function Ltask. φt+1 ←φt −β∂Ltask(hθt+1 (x′),y′) ∂φt , where (x′,y′) ∼Dtest (25) Note that the last step assumes access to labels of test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We select the trasformer input embedding dimension (1 + e) from {16,32,64}and transformer feed-forward dimension dfrom {32,64,128}. The number of transformer layers and the hidden layers in MLP are selected from {1,2}. We use Adam optimizer with a learning rate of 1e−3 for learning the meta-loss (i.e. the transformer + MLP). We train the meta-loss for 100 epochs with a batch size of 200. A.4 Effect of Task Loss in Meta Learning In section 3, we show that the meta losses learned on different source classiﬁers differ substantially if the source classiﬁers are trained using different source loss functions. Here we further empirically verify that the learnt meta loss is not affected by the task loss used in meta learning (Lin Equation 1). Thus the learnt meta loss is determined by the source model. In Figure 5, we show the meta loss learnt on a ResNet-26 trained with Cross Entropy loss for two meta task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. We plot the meta loss as a function over one of its input prediction scores, while keeping other ﬁxed. We can see that the task loss barely affects the learnt meta loss. Similar observations can be made for the classiﬁer trained with squared loss Figure 6. 17Meta-Loss  Backpropogate  Figure 4: Meta-Loss learning procedure : The model predictions hθt(x) are passed through the parameterized loss function mφt, which outputs a loss value. We optimize φ such that when optimizing the source model over the loss mφt(hθt(x)), the updated θt+1 has a better performance on the test domain. To do this, we take one gradient step over the meta-loss to get the update source model parameters θt+1, and then update φby evaluating θt+1 on the labeled validation data using some task loss Ltask. Algorithm 2 Learning the Meta-Loss Input: Source trained classiﬁer hθ0 . Randomly initialized meta-loss mφ0 . Task loss / Surrogate loss Ltask like cross-entropy or squared loss for meta learning N batches of test data Dtest = [(x1,y1),..., (xN,yN)] Hyperparams: learning rates αand β. for epoch= 0,1,2,... do for n= 0,1,...N −1 do θt+1 ←θt −α ∂mφt(hθt(xn)) ∂θt Sample (xr,yr) ∼Dtest. φt+1 ←φt −β∂Ltask(hθt+1 (xr),yr) ∂φt A.5 Test-Time Adaptation Detail For completeness, we also give the test-time adaptation setup in Algorithm 3. A.6 ImageNet results on each severity level In continuation with results shown in Table 2 in Section 5.3, Table 4 shows the mean errors averaged across the 15 corruption types for each of the severity level on ImageNet-C, for a source classiﬁer trained with PolyLoss (ϵ= 8). A.7 Square Loss Trained Source Classiﬁer In Section 5.3, we brieﬂy discussed that similar to the other source training losses like cross-entropy and polyloss, our proposed conjugate loss outperforms the baselines when the source classiﬁer is 18(a)  (b) Figure 5: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Cross Entropy. Here we show meta loss trained by two different task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. (a)  (b) Figure 6: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Squared Loss. Here we show meta loss trained by two different task losses: Cross Entropy Figure 6a and Squared Loss Figure 6b. Algorithm 3 Test-Time Adaptation Input: Source classiﬁer θ0 trained using loss L(hθ(x),y), An unsupervised loss function for test-time adaptation Ltta(x), N batches of test data Dtest = [x1,...,x N] Hyperparams: learning rate η. for n= 0,1,...N −1 do θn+1 = θn −η∇Ltta(xn) ˆyn = hθn+1 (xn) [Predictions for the nth batch] 19Corrution Severity Temperature Robust PL Entropy MEMO Softmax PL Conjugate 1 \u0017 34.27 33.17 34.39 32.49 32.26 \u0013 34.27 32.84 34.39 32.70 32.26 2 \u0017 41.25 39.04 40.38 37.78 37.40 \u0013 41.25 38.50 40.38 37.75 37.40 3 \u0017 47.37 44.04 45.67 42.30 41.72 \u0013 47.37 43.33 45.67 42.14 41.72 4 \u0017 56.63 51.88 54.49 49.61 48.84 \u0013 56.63 51.03 54.49 49.39 48.84 5 \u0017 67.11 62.53 66.13 60.94 59.90 \u0013 67.11 61.80 66.13 60.30 59.90 Mean \u0017 49.32 46.13 48.21 44.62 44.02 \u0013 49.32 45.50 48.21 44.45 44.02 Table 4: Mean Errors across the 15 noises for various severity level on the ImageNet-C dataset, with source model trained using Poly-1 Loss. Note that Temperature scaling helped only in the case of Entropy and Softmax PL. trained using a squared loss. Table 5 shows a detailed comparison with the baselines. We note that for the conjugate of squared loss, the temperature scaling can be wrapped into the learning rate as shown in Section 4.2. Further, on the CIFAR-10-C dataset we observe temperature scaling doesn’t help any of the other baselines too, hence we do not include the temperature row in CIFAR-10-C. Dataset Temperature Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL CIFAR-10-C \u0017 13.71 (±0.07) 13.06 (±0.05) 13.24 (±0.02) 13.22 (±0.04) 14.85 (±0.08)12.99(±0.04) CIFAR-100-C \u0017 50.82 (±0.31) 44.53 (±0.13) 43.55 (±0.12) 51.35 (±0.04) 51.99 (±0.03)43.39(±0.11) \u0013 50.82 (±0.31) 43.99 (±0.15)43.21(±0.08) 51.35 (±0.04) 51.99 (±0.03) 43.39 (±0.11) Table 5: Mean Errors on the common corruptions datasets for source classiﬁer trained using squared loss. We note that temperature scaling didn’t help on the CIFAR-10-C dataset. Source Classiﬁer Errors without adaptation : CIFAR-10-C (28.34%), CIFAR-100-C (68.79%) Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,1 e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1 e−2, 2 SGD,5 e−3, 3 Adam,1e−3, 2 CIFAR-100-C \u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,5 e−3, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD,1e−2, 2 ImageNet-C \u0017 SGD,1e−2, 1 SGD,2.5 e−3, 1 SGD,1 e−3, 1 SGD,2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1.5 SGD,1e−3, 1 SGD,2.5e−3, 1.5 Table 6: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 1, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using cross-entropy loss. A.8 Hyper-Parameters We share the exact hyper-parameters found using gridsearch over the 4 validation noises for the common corruptions dataset. 20Cross Entropy Classiﬁer Experiments In Section 5.2, Table 1 shows the results when adapting a cross entropy trained classiﬁer on various common corruptions dataset. Table 6 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. PolyLoss Classiﬁer Experiments In Section 5.3, Table 2 shows the results when adapting a polyloss trained classiﬁer on various common corruptions dataset. Table 7 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−3, 1 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,5 e−3, 1 SGD, 1e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1e−2, 3 SGD,1 e−2, 3 SGD,5 e−3, 3 SGD, 1e−3, 2 SGD, 1e−3, 1.5 CIFAR-100-C\u0017 SGD,1e−2, 1 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD, 1e−2, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 Adam,1e−3, 3 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD, 1e−2, 2.5 SGD, 1e−2, 1.5 ImageNet-C\u0017 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1 SGD,5e−3, 1 SGD, 2.5e−3, 1 SGD, 2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1.5 SGD,5e−3, 1 SGD, 2.5e−3, 2 SGD, 2.5e−3, 1 Table 7: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 2, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using poly-loss. Squared Loss Classiﬁer Experiments In Section 5.3, we brieﬂy discussed the results when adapt- ing a squared loss trained classiﬁer on various common corruptions dataset. Table 8 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss for the results in Table 5. Digit Adaptation Datasets For the experiments on digits adaptation tasks, we do not have any validation set. Hence, we don’t use temperature scaling here (T = 1) and ﬁx the optimizer and LR as Adam and 1e−2 respectively for all the baselines. A.9 Additional Experiments on Digit Adaptation Datasets Similar to the setting of Table 1, we perform additional experiments on digit adaptation datasets when the source classiﬁer is trained using the cross-entropy loss. Note that when the source classiﬁer is trained using cross-entropy loss, the conjugate loss is equal to the softmax-entropy. In the absence of validation dataset in digit adaptation benchmarks, we used a ﬁxed learning rate of 0.01 for all the baselines, optimizer as Adam and an informed temperature scaling guess of T=2. Table 9 compares softmax-entropy minimization with various baselines. Here, again we observe that on SVHN →MNIST benchmark, without temperature scaling, MEMO (10.67% error) outperforms softmax-entropy (14.41% error). However, similar to the observations in Table 1, with temperature scaling, softmax-entropy minimization (9.26% error) is able to match the performance of MEMO (9.36% error). Further, on the SVHN →USPS benchmark, softmax-entropy (conjugate) and MEMO perform similar even without temperature scaling. A.10 Additional Meta Learning the TTA Loss Experiments In Section 3, we tried to learn a test-time adaptation (TTA) loss via meta-learning for adapting a CIFAR10 trained ResNet26 to distribution shifts on CIFAR10 corruptions. Figure 1 showed that the learnt meta-loss looks like a temperature scaled softmax-entropy. In this section, we show the learnt meta loss across a range of settings as described below : 1. Digit Adaptation: Figure 7a and 7b show the learnt meta-loss when adapting a SVHN trained ResNet26 to MNIST dataset and USPS dataset respectively. We observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 2. Various Noise Types: In Figure 8, we show the learnt meta-loss when adapting a ResNet26 trained on CIFAR10 dataset using cross-entropy loss, to various noise types like speckle, gaussian, saturate and spatter. The severity level is kept ﬁxed at the maximum i.e. 5. 21Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD,1e−2, 1 SGD,1 e−4, 1 SGD,1e−2, 1 CIFAR-100-C\u0017 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam, 1e−4, 1 Adam, 1e−3, 1 \u0013 Adam,1e−3, 1 Adam,1e−3, 0.5 Adam,1e−3, 2 Adam,1e−3, 2 Adam, 1e−4, 2.5 Adam, 1e−3, 1 Table 8: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 5, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using squared loss. Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) SVHN→MNIST \u0017 21.54 27.44 10.67 14.41 \u0013 21.54 13.26 9.36 9.26 SVHN→USPS \u0017 26.06 26.81 22.72 22.57 \u0013 26.06 22.32 22.42 22.27 Table 9: Mean errors when adapting to digit adaptation benchmarks using a source classiﬁer trained via cross-entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. Again we observe that with the right temperature scaling, softmax-entropy minimization matches other approaches. For additional context, the source classiﬁer errors without adaptation are: SVHN →MNIST (34.17%), SVHN →USPS (31.84%). 20  10  0 10 20 prediction score 5 0 5 10loss value meta loss (error 10.44%) softmax entropy (error 14.41) fitted entropy (error 9.26) Meta Loss for SVHN -> MNIST (a) 20  10  0 10 20 prediction score 6 4 2 0 2 4 6 8 loss value meta loss (error 20.13%) softmax entropy (error 22.57) fitted entropy (error 22.22) Meta Loss for SVHN -> USPS adpatation (b) Figure 7: Visualizations of the learnt meta-loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with cross-entropy on the SVHN dataset. (a) The learnt meta-loss when adapting to the MNIST test dataset. (b) The learnt meta-loss when adapting to the USPS test dataset. 3. Various Severity Levels: In Figure 9, we vary the severity level of the noise, keeping the noise type ﬁxed. 4. Dataset and Architecture: In Figure 10, we compare the learnt meta-loss when adapting to speckle noise, for different source classiﬁer architectures (ResNet26 and ResNet50) and different source training dataset (CIFAR10 and CIFAR100). In all the cases, we again observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 5. Squared Loss : Finally, in Figure 11 we show the learnt meta-loss for classiﬁers trained with squared loss function instead of cross-entropy. We observe that in this case, the learnt meta loss mimics a quadratic function as expected from the conjugate formulation. 22For each of the learnt meta losses, we also show the values (α,T,C ) we use to ﬁt the meta loss with softmax entropy function: α·H(softmax(x/T)) −C. Note that although the learnt meta-loss can be approximated by the conjugate, the parameters α,T,C differ across the settings. In the case of classiﬁers trained with squared loss, we ﬁt the meta loss with a quadratic function∑K i=1(A·x2 i + C), where Kis the number of classes and xis the logit vector. Again, we also show the ﬁtted parameter value A,C. The meta loss follows the trend of a quadratic function. The ﬁtted quadratic function performs better or similar as the meta loss, while the parameters of the ﬁtted quadratic function remain different across the meta learning setup (base classiﬁer architectures and noise types). (a)  (b) (c)  (d) Figure 8: Visualization of meta loss (blue) learnt from various noise types in CIFAR-10-C validation set, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 23(a)  (b) (c)  (d) Figure 9: Visualization of meta loss (blue) learnt on speckle noise with different severity level for CIFAR-10-C, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 24(a)  (b) (c)  (d) Figure 10: Visualization of meta loss (blue) learnt across datasets (CIFAR-10-C/CIFAR-100-C) and base classiﬁer architectures (ResNet-26/ResNet-50), where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters ( α,T,C ) in the ﬁtted entropy. (a)  (b) Figure 11: Visualization of meta loss (blue), where base classiﬁer is trained with quadratic loss. We show the error of meta loss, softmax entropy and ﬁtted quadratic function for test-time adaptation on the corresponding noise types. We also show the parameters ( A,B,C ) in the ﬁtted quadratic function. 25",
      "meta_data": {
        "arxiv_id": "2207.09640v2",
        "authors": [
          "Sachin Goyal",
          "Mingjie Sun",
          "Aditi Raghunathan",
          "Zico Kolter"
        ],
        "published_date": "2022-07-20T04:02:19Z",
        "pdf_url": "https://arxiv.org/pdf/2207.09640v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper reveals that the 'best' Test-Time Adaptation (TTA) loss is dependent on the source classifier's training loss, a phenomenon discovered through meta-learning experiments. It proposes a generic recipe for deriving TTA losses for a broad class of supervised training loss functions by leveraging the convex conjugate function. The work interprets this conjugate adaptation loss as a form of self-training using 'conjugate pseudo-labels' (gradient of f with respect to model predictions). Empirically, the proposed approach consistently outperforms existing TTA alternatives across various datasets and training losses, particularly demonstrating substantial gains when the source model is trained with novel loss functions like PolyLoss, where it differs significantly from entropy-based losses.",
        "methodology": "The research starts by employing a meta-learning procedure to empirically learn the 'best' TTA loss, parameterizing the loss function via a neural network and differentiating through the adaptation process. This exploratory step revealed that the optimal TTA loss structure varied with the source classifier's training loss. To explain this, the authors propose a theoretical framework based on the convex conjugate function. For a general class of loss functions `L(h(x),y) = f(h(x)) −yTh(x)`, they derive the conjugate adaptation loss as `Lconj(hθ(x)) = −f⋆(∇f(hθ(x)))`, which can be equivalently viewed as self-training with conjugate pseudo-labels `˜yCPLθ(x) = ∇f(hθ(x))`. The adaptation process involves updating the source model parameters (specifically, the learnable scale and shift parameters of batch normalization layers) via gradient steps on this unsupervised conjugate adaptation loss, often incorporating temperature scaling.",
        "experimental_setup": "Experiments were conducted on common corruption benchmarks (CIFAR-10-C, CIFAR-100-C, ImageNet-C) and domain adaptation datasets (SVHN to MNIST, ImageNet-R, VISDA-C). Source classifiers included ResNet-26 (for CIFAR), ResNet-18 (for SVHN-MNIST), and ResNet-50 (for ImageNet, VISDA-C). Models were trained with Cross-Entropy, PolyLoss, and Squared Loss. Baselines for comparison included Hard Pseudo-Labels, Soft Pseudo-Labels (softmax), Entropy Minimization (TENT), Robust Pseudo-Label, and MEMO. Performance was evaluated as average error across corruptions (highest severity for CIFAR, all severities for ImageNet-C). Hyperparameters like learning rate and temperature were tuned using grid-search on validation noises for corruption benchmarks, while fixed values (T=1, specific LRs) were used for domain adaptation tasks due to the absence of validation sets. Adaptation involved fine-tuning batch normalization layers and updating batch statistics.",
        "limitations": "The study does not fully elucidate the fundamental reasons behind the optimality of the derived conjugate loss for TTA. The meta-learning framework used for discovery was constrained to learning functions solely over the logits of individual inputs, suggesting potential limitations in exploring more complex loss functions involving intermediate representations or batch interactions. The paper also acknowledges that practical TTA success still relies on various heuristics, such as updating only batch normalization parameters, and the exact role and benefits of temperature scaling (which significantly improved performance across all baselines) require more concrete understanding. Furthermore, it remains an open problem to precisely define the types of real-world distribution shifts under which self-training based approaches, including the proposed one, would be most effective.",
        "future_research_directions": "Future work could involve expanding the meta-learning framework to explore TTA loss functions that operate on intermediate representations of the neural network or consider interactions within a batch of inputs. A deeper understanding of the empirical heuristics commonly used in TTA, such as updating only batch normalization parameters and the regularization effects of temperature scaling, is another important direction. Research could also focus on identifying the specific characteristics of real-world distribution shifts where self-training-based adaptation methods are most beneficial. Finally, the conjugate pseudo-labeling framework could be extended and applied to other machine learning settings beyond TTA, such as semi-supervised learning."
      }
    },
    {
      "title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
      "abstract": "Test-time adaptation (TTA) addresses distribution shifts for streaming test\ndata in unsupervised settings. Currently, most TTA methods can only deal with\nminor shifts and rely heavily on heuristic and empirical studies.\n  To advance TTA under domain shifts, we propose the novel problem setting of\nactive test-time adaptation (ATTA) that integrates active learning within the\nfully TTA setting.\n  We provide a learning theory analysis, demonstrating that incorporating\nlimited labeled test instances enhances overall performances across test\ndomains with a theoretical guarantee. We also present a sample entropy\nbalancing for implementing ATTA while avoiding catastrophic forgetting (CF). We\nintroduce a simple yet effective ATTA algorithm, known as SimATTA, using\nreal-time sample selection techniques. Extensive experimental results confirm\nconsistency with our theoretical analyses and show that the proposed ATTA\nmethod yields substantial performance improvements over TTA methods while\nmaintaining efficiency and shares similar effectiveness to the more demanding\nactive domain adaptation (ADA) methods. Our code is available at\nhttps://github.com/divelab/ATTA",
      "full_text": "Published as a conference paper at ICLR 2024 ACTIVE TEST-TIME ADAPTATION : T HEORETICAL ANALYSES AND AN ALGORITHM Shurui Gui∗ Texas A&M University College Station, TX 77843 shurui.gui@tamu.edu Xiner Li* Texas A&M University College Station, TX 77843 lxe@tamu.edu Shuiwang Ji Texas A&M University College Station, TX 77843 sji@tamu.edu ABSTRACT Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies. To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting. We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques. Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods. Our code is available at https://github.com/divelab/ATTA. 1 I NTRODUCTION Deep learning has achieved remarkable success across various fields, attaining high accuracy in numerous applications (Krizhevsky et al., 2017; Simonyan and Zisserman, 2014). Nonetheless, When training and test data follow distinct distributions, models often experience significant performance degradation during test. This phenomenon, known as the distribution shift or out-of-distribution (OOD) problem, is extensively studied within the context of both domain generalization (DG) (Gulra- jani and Lopez-Paz, 2020; Koh et al., 2021; Gui et al., 2022) and domain adaptation (DA) (Ganin et al., 2016; Sun and Saenko, 2016). While these studies involve intensive training of models with considerable generalization abilities towards target domains, they overlook an important application property; namely, continuous adaptivity to real-time streaming data under privacy, resource, and efficiency constraints. This gap leads to the emergence of test-time adaptation (TTA) tasks, targeting on-the-fly adaptation to continuous new domains during the test phase or application deployment. The study of TTA encompasses two main categories; namely test-time training (TTT) methods (Sun et al., 2020; Liu et al., 2021c) and fully test-time adaptation (FTTA) (Niu et al., 2023; Wang et al., 2021). The TTT pipeline incorporates retraining on the source data, whereas FTTA methods adapt arbitrary pre-trained models to the given test mini-batch by conducting entropy minimization, without access to the source data. Nevertheless, most TTA methods can only handle corrupted distribution shifts (Hendrycks and Dietterich, 2019b) (e.g., Gaussian noise,) and rely heavily on human intuition or empirical studies. To bridge this gap, our paper focuses on tackling significant domain distribution shifts in real time with theoretical insights. We investigate FTTA, which is more general and adaptable than TTT, particularly under data ac- cessibility, privacy, and efficiency constraints. Traditional FTTA aims at adapting a pre-trained model to streaming test-time data from diverse domains under unsupervised settings. However, recent works (Lin et al., 2022; Pearl, 2009) prove that it is theoretically infeasible to achieve OOD generalization without extra information such as environment partitions. Since utilizing environment partitions requires heavy pretraining, contradicting the nature of TTA, we are motivated to incorporate extra information in a different way,i.e., integrating a limited number of labeled test-time samples to alleviate distribution shifts, following the active learning (AL) paradigm (Settles, 2009). To this end, we propose the novel problem setting of active test-time adaptation (ATTA) by incorporating ∗Equal contributions 1 arXiv:2404.05094v1  [cs.LG]  7 Apr 2024Published as a conference paper at ICLR 2024 AL within FTTA. ATTA faces two major challenges; namely, catastrophic forgetting (CF) (Kemker et al., 2018; Li and Hoiem, 2017) and real-time active sample selection. CF problem arises when a model continually trained on a sequence of domains experiences a significant performance drop on previously learned domains, due to the inaccessibility of the source data and previous test data. Real-time active sample selection requires AL algorithms to select informative samples from a small buffer of streaming test data for annotation, without a complete view of the test distribution. In this paper, we first formally define the ATTA setting. We then provide its foundational analysis under the learning theory’s paradigm to guarantee the mitigation of distribution shifts and avoid CF. Aligned with our empirical validations, while the widely used entropy minimization (Wang et al., 2021; Grandvalet and Bengio, 2004) can cause CF, it can conversely become the key to preventing CF problems with our sample selection and balancing techniques. Building on the analyses, we then introduce a simple yet effective ATTA algorithm, SimATTA, incorporating balanced sample selections and incremental clustering. Finally, we conducted a comprehensive experimental study to evaluate the proposed ATTA settings with three different settings in the order of low to high requirement restrictiveness, i.e., TTA, Enhanced TTA, and Active Domain Adaptation (ADA). Intensive experiments indicate that ATTA jointly equips with the efficiency of TTA and the effectiveness of ADA, rendering an uncompromising real-time distribution adaptation direction. Comparison to related studies. Compared to TTA methods, ATTA requires extra active labels, but the failure of TTA methods (Sec. 5.1) and the theoretical proof of Lin et al. (2022); Pearl (2009) justify its necessity and rationality. Compared to active online learning, ATTA focuses on lightweight real-time fine-tuning without round-wise re-trainings as Saran et al. (2023) and emphasizes the importance of CF avoidance instead of resetting models and losing learned distributions. In fact, active online learning is partially similar to our enhanced TTA setting (Sec. 5.2. Compared to ADA methods (Prabhu et al., 2021; Ning et al., 2021), ATTA does not presuppose access to source data, model parameters, or pre-collected target samples. Furthermore, without this information, ATTA can still perform on par with ADA methods (Sec. 5.3). The recent source-free active domain adaptation (SFADA) method SALAD (Kothandaraman et al., 2023) still requires access to model parameter gradients, pre-collected target data, and training of additional networks. Our ATTA, in contrast, with non-regrettable active sample selection on streaming data, is a much lighter and more realistic approach distinct from ADA and SFADA. More related-work discussions are provided in Appx. C. 2 T HE ACTIVE TEST-TIME ADAPTATION FORMULATION TTA methods aim to solve distribution shifts by dynamically optimizing a pre-trained model based on streaming test data. We introduce the novel problem setting of Active Test-Time Adaptation (ATTA), which incorporates active learning during the test phase. In ATTA, the model continuously selects the most informative instances from the test batch to be labeled by an explicit or implicit oracle (e.g., human annotations, self-supervised signals) and subsequently learned by the model, aiming to improve future adaptations. Considering the labeling costs in real-world applications, a “budget” is established for labeled test instances. The model must effectively manage this budget distribution and ensure that the total number of label requests throughout the test phase does not surpass the budget. We now present a formal definition of the ATTA problem. Consider a pre-trained modelf(x; ϕ) with parameters ϕ trained on the source dataset DS = (x, y)|DS|, with each data sample x ∈ Xand a label y ∈ Y. We aim to adapt model parameters θ, initialized as ϕ, to an unlabeled test-time data stream. The streaming test data exhibit distribution shifts from the source data and varies continuously with time, forming multiple domains to which we must continuously adapt. The test phase commences at time step t = 1 and the streaming test data is formulated in batches. The samples are then actively selected, labeled (by the oracle) and collected as Dte(t) = ActAlg(Ute(t)), where ActAlg(·) denotes an active selection/labeling algorithm. The labeled samples Dte(t) are subsequently incorporated into the ATTA training setDtr(t). Finally, we conclude time step t by performing ATTA training, updating model parameters θ(t) using Dtr(t), with θ(t) initialized as the previous final state θ(t − 1). Definition 1 (The ATTA problem). Given a model f(x; θ), with parameters θ, initialized with parameters θ(0) = ϕ obtained by pre-training on source domain data, and streaming test data batches Ute(t) continually changing over time, the ATTA task aims to optimize the model at any time stept (with test phase commencing at t = 1) as θ(t)∗ := argmin θ(t) (E(x,y,t)∈Dtr(t)[ℓCE (f(x; θ(t)), y)] + E(x,t)∈Ute(t)[ℓU (f(x; θ(t)))]), (1) 2Published as a conference paper at ICLR 2024 where Dtr(t) = ( ∅, t = 0 Dtr(t − 1) ∪ Dte(t), t ≥ 1, s.t. |Dtr(t)| ≤ B, (2) Dte(t) = ActAlg(Ute(t)) is actively selected and labeled, ℓCE is the cross entropy loss, ℓU is an unsupervised learning loss, and B is the budget. 3 T HEORETICAL STUDIES In this section, we conduct an in-depth theoretical analysis of TTA based on learning theories. We mainly explore two questions: How can significant distribution shifts be effectively addressed under the TTA setting? How can we simultaneously combat the issue of CF? Sec. 3.1 provides a solution with theoretical guarantees to the first question, namely, active TTA (ATTA), along with the conditions under which distribution shifts can be well addressed. Sec. 3.2 answers the second question with an underexplored technique, i.e., selective entropy minimization, building upon the learning bounds established in Sec. 3.1. We further validate these theoretical findings through experimental analysis. Collectively, we present a theoretically supported ATTA solution that effectively tackles both distribution shift and CF. 3.1 A LLEVIATING DISTRIBUTION SHIFTS THROUGH ACTIVE TEST-TIME ADAPTATION Traditional TTA is performed in unsupervised or self-supervised context. In contrast, ATTA introduces supervision into the adaptation setting. In this subsection, we delve into learning bounds and establish generalization bounds to gauge the efficacy of ATTA in solving distribution shifts. We scrutinize the influence of active learning and evidence that the inclusion of labeled test instances markedly enhances overall performances across incremental test domains. Following Kifer et al. (2004), we examine statistical guarantees for binary classification. A hypothesis is a function h : X → {0, 1}, which can serve as the prediction function within this context. In the ATTA setting, the mapping ofh varies with time as h(x, t). We use H∆H-distance following Ben- David et al. (2010), which essentially provides a measure to quantify the distribution shift between two distributions D1 and D2, and can also be applied between datasets. The probability that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} according to distribution D is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source data is inaccessible under ATTA settings, we consider the existence of source dataset DS for accurate theoretical analysis. Thus, we initialize Dtr as Dtr(0) = DS. For every time step t, the test and training data can be expressed asUte(t) and Dtr(t) = DS ∪Dte(1) ∪Dte(2) ∪···∪ Dte(t). Building upon two lemmas (provided in Appx. D), we establish bounds on domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesish at time t. Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), ··· , Ute(t), ··· , Si are unlabeled samples of sizem sampled from each of thet+1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λ = (λ0, ··· , λt). If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w = (w0, ··· , wt) on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. The adaptation performance on a test domain is majorly bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. Further theoretical analysis are in Appx. D. 3Published as a conference paper at ICLR 2024 Figure 1: (a) Empirical validation of Thm. 1. We train a series of models on N = 2000 samples from the PACS (Li et al., 2017) dataset given differentλ0 and w0 and display the test domain loss of each model. Red points are the test loss minimums given a fixed λ0. The orange line is the reference where w0 = λ0. We observe that w0 with loss minimums are located closed to the orange line but slightly smaller than λ0, which validates our findings in Eq. (4). (b) Empirical analysis with an uncertainty balancing. Given source pre-trained models, we fine-tune the models on 500 samples with different λ0 and w0, and display the combined error surface of test and source error. Although a small λ0 is good for test domain error, it can lead to non-trivial source error exacerbation. Therefore, we can observe that the global loss minimum (green X) locates in a relatively high-λ0 region. If we consider the multiple test data distributions as a single test domain,i.e., St i=1 Ute(i), Thm. 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . Given the optimal test/source hypothesis h∗ T (t) = arg minh∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), we have |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (3.a), with approximatelyB = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (4) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. The following theorem offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (5) Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. All proofs are provided in Appx. E. Finally, we support the theoretical findings with experimental analysis and show the numerical results of applying the principles on real-world datasets, as shown in Fig. 1. For rigorous analysis, note that our theoretical results rest on the underlying condition that N should at least be of the same scale as d, according to the principles of VC-dimension theory. The empirical alignment of our experiments with the theoretical framework can be attributed to the assumption that fine-tuning a model is roughly equivalent to learning a model with a relatively small d. Experiment details and other validations can be found in Appx. H. 4Published as a conference paper at ICLR 2024 3.2 M ITIGATING CATASTROPHIC FORGETTING WITH BALANCED ENTROPY MINIMIZATION Catastrophic forgetting (CF), within the realm of Test-Time Adaptation (TTA), principally manifests as significant declines in overall performance, most notably in the source domain. Despite the lack of well-developed learning theories for analyzing training with series data, empirical studies have convincingly illustrated the crucial role of data sequential arrangement in model learning, thereby accounting for the phenomenon of CF. Traditionally, the mitigation of CF in adaptation tasks involves intricate utilization of source domain data. However, under FTTA settings, access to the source dataset is unavailable, leaving the problem of CF largely unexplored in the data-centric view. Table 1: Correlation analysis of high/low en- tropy samples and domains. We use a source pre-trained model to select samples with low- est/highest entropy, and 1.retrain the model on 2000 samples; 2.fine-tune the model on 300 sam- ples. We report losses on source/test domains for each setting, showing that low-entropy samples form distributions close to the source domain. Sample type Retrain Fine-tune ϵS ϵT ϵS ϵT Low entropy 0.5641 0.8022 0.0619 1.8838 High entropy 2.5117 0.3414 0.8539 0.7725 To overcome this challenge of source dataset ab- sence, we explore the acquisition of “source-like” data. In TTA scenarios, it is generally assumed that the amount of source data is considerably large. We also maintain this assumption in ATTA, practically assuming the volume of source data greatly surpasses the test-time budget. As a re- sult, we can safely assume that the pre-trained model is well-trained on abundant source do- main data DS. Given this adequately trained source model, we can treat it as a “true” source data labeling function f(x; ϕ). The model es- sentially describes a distribution, Dϕ,S(X, Y) = {(x, ˆy) ∈ (X, Y) | ˆy = f(x; ϕ), x∈ DS}. The entropy of the model prediction is defined as H(ˆy) = −P c p(ˆyc) logp(ˆyc), ˆy = f(x; ϕ), where c denotes the class. Lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction, which can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model recognizes the sample as being similar to those it was trained on. Thus entropy can be used as an indicator of how closely a sample x aligns with the model distribution Dϕ,S. Since the model distribution is approximately the source distribution, selecting (and labeling) low-entropy samples using f(x; ϕ) essentially provides an estimate of sampling from the source dataset. Therefore, in place of the inaccessible DS, we can feasibly include the source-like dataset into the ATTA training data at each time stept: Dϕ,S(t) = {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el}, (6) where el is the entropy threshold. The assumption that Dϕ,S(t) is an approximation of DS can be empirically validated, as shown by the numerical results on PACS in Tab. 1. In contrast, high-entropy test samples typically deviate more from the source data, from which we select Dte(t) for active labeling. Following the notations in Thm. 1, we are practically minimizing the empirical weighted error of hypothesis h(t) as ˆϵ′ w(h(t)) = tX j=0 wjˆϵj(h(t)) = w0 λ0N X x∈Dϕ,S(t) |h(x, t) − f(x; ϕ)| + tX j=1 wj λjN X x,y∈Dte(j) |h(x, t) − y|. (7) By substituting DS with Dϕ,S(t) in Thm. 1, the bounds of Thm. 1 continue to hold for the test domains. In the corollary below, we bound the source error for practical ATTA at each time stept. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Thm. 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Thm. 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Further analysis and proofs are in Appx. D and E. The following corollary provides direct theoretical support that our strategy conditionally reduces the error bound on the source domain. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Thm. 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight 5Published as a conference paper at ICLR 2024 <latexit sha1_base64=\"NxhXSyFABPQk4q8627/odirDspg=\">AAAB9XicbVDLSgMxFM34rPVVdekmWARXZab4WhbcuKzYF7S1ZNI7bWgmMyR3lDL0P9y4UMSt/+LOvzHTdqGtBwKHc87l3hw/lsKg6347K6tr6xubua389s7u3n7h4LBhokRzqPNIRrrlMwNSKKijQAmtWAMLfQlNf3ST+c1H0EZEqobjGLohGygRCM7QSg/3mIWFGtAaGOwVim7JnYIuE29OimSOaq/w1elHPAlBIZfMmLbnxthNmUbBJUzyncRAzPiIDaBtqWIhmG46vXpCT63Sp0Gk7VNIp+rviZSFxoxD3yZDhkOz6GXif147weC6mwoVJwiKzxYFiaQY0awC2hcaOMqxJYxrYW+lfMg042iLytsSvMUvL5NGueRdli7uysXK+byOHDkmJ+SMeOSKVMgtqZI64USTZ/JK3pwn58V5dz5m0RVnPnNE/sD5/AFnsJJq</latexit> Streaming Test <latexit sha1_base64=\"a41BOKrutEYSWO9+8CjkPZKHvb8=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69BIvgqSTiR48FLx4r2A9oQ9lsN+3SzSbuToQQ+ie8eFDEq3/Hm//GTZuDtj4YeLw3w8w8PxZco+N8W6W19Y3NrfJ2ZWd3b/+genjU0VGiKGvTSESq5xPNBJesjRwF68WKkdAXrOtPb3O/+8SU5pF8wDRmXkjGkgecEjRSbzAhmKWzyrBac+rOHPYqcQtSgwKtYfVrMIpoEjKJVBCt+64To5cRhZwKNqsMEs1iQqdkzPqGShIy7WXze2f2mVFGdhApUxLtufp7IiOh1mnom86Q4EQve7n4n9dPMGh4GZdxgkzSxaIgETZGdv68PeKKURSpIYQqbm616YQoQtFElIfgLr+8SjoXdfe6fnV/WWs2ijjKcAKncA4u3EAT7qAFbaAg4Ble4c16tF6sd+tj0Vqyiplj+APr8wfpIY/e</latexit> ˆy <latexit sha1_base64=\"SJEOE2ZYxLL1SU/QahOlMH6fop4=\">AAAB8HicbVBNSwMxEM3Wr1q/qh69BItQL2VX/Oix4MVjBbettEvJptk2NMkuyaxQlv4KLx4U8erP8ea/MW33oK0PBh7vzTAzL0wEN+C6305hbX1jc6u4XdrZ3ds/KB8etUycasp8GotYd0JimOCK+cBBsE6iGZGhYO1wfDvz209MGx6rB5gkLJBkqHjEKQErPfr9DNi0Cuf9csWtuXPgVeLlpIJyNPvlr94gpqlkCqggxnQ9N4EgIxo4FWxa6qWGJYSOyZB1LVVEMhNk84On+MwqAxzF2pYCPFd/T2REGjORoe2UBEZm2ZuJ/3ndFKJ6kHGVpMAUXSyKUoEhxrPv8YBrRkFMLCFUc3srpiOiCQWbUcmG4C2/vEpaFzXvunZ1f1lp1PM4iugEnaIq8tANaqA71EQ+okiiZ/SK3hztvDjvzseiteDkM8foD5zPH2KnkB4=</latexit> U te ( t ) <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model SimATTA <latexit sha1_base64=\"bhVea6W/pzUPuDRNfs2xbDF7qAk=\">AAAB73icbVC7SgNBFL3rM8ZX1NJmMAhWYTf4KgM2FhYRzAOSJcxOZpMhs7PrzF0hhPyEjYUitv6OnX/jbLKFJh4YOJxzD3PvCRIpDLrut7Oyura+sVnYKm7v7O7tlw4OmyZONeMNFstYtwNquBSKN1Cg5O1EcxoFkreC0U3mt564NiJWDzhOuB/RgRKhYBSt1L6jQRYd9Eplt+LOQJaJl5My5Kj3Sl/dfszSiCtkkhrT8dwE/QnVKJjk02I3NTyhbEQHvGOpohE3/mS275ScWqVPwljbp5DM1N+JCY2MGUeBnYwoDs2il4n/eZ0Uw2t/IlSSIlds/lGYSoIxyY4nfaE5Qzm2hDIt7K6EDammDG1FRVuCt3jyMmlWK95l5eK+Wq6d53UU4BhO4Aw8uIIa3EIdGsBAwjO8wpvz6Lw4787HfHTFyTNH8AfO5w/1SI/i</latexit> Labeling <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"ipQ+JKlINPDcPjrbUYUkqyyzp40=\">AAAB+nicbVC7TsMwFHXKq5RXCiOLRYXEQpVUvMZKLIxF0IfURpXj3LRWHSeyHVBV+iksDCDEypew8Te4aQZoOZKlo3Puy8dPOFPacb6twsrq2vpGcbO0tb2zu2eX91sqTiWFJo15LDs+UcCZgKZmmkMnkUAin0PbH13P/PYDSMVica/HCXgRGQgWMkq0kfp2+S6bdNqQoCUxQ4K+XXGqTga8TNycVFCORt/+6gUxTSMQmnKiVNd1Eu1NiNSMcpiWeqmChNARGUDXUEEiUN4kO32Kj40S4DCW5gmNM/V3x4RESo0j31RGRA/VojcT//O6qQ6vvAkTSapB0PmiMOVYx3iWAw6YBKr52BBCJTO3YjokklBt0iqZENzFLy+TVq3qXlTPb2uV+lkeRxEdoiN0glx0ieroBjVQE1H0iJ7RK3qznqwX6936mJcWrLznAP2B9fkDSAyT+w==</latexit> Source-Pretrained <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model <latexit sha1_base64=\"5LNAmmVR/AN9Lc2T+FRV/is2yz8=\">AAAB8nicbVDLSgNBEJyNrxhfUY9eBoPgKewGX8eACB48RDAP2CxhdjKbDJmdWWZ6lbDkM7x4UMSrX+PNv3GS7EETCxqKqm66u8JEcAOu++0UVlbX1jeKm6Wt7Z3dvfL+QcuoVFPWpEoo3QmJYYJL1gQOgnUSzUgcCtYOR9dTv/3ItOFKPsA4YUFMBpJHnBKwkn+nnvCNBK2Sca9ccavuDHiZeDmpoByNXvmr21c0jZkEKogxvucmEGREA6eCTUrd1LCE0BEZMN9SSWJmgmx28gSfWKWPI6VtScAz9fdERmJjxnFoO2MCQ7PoTcX/PD+F6CrIuExSYJLOF0WpwKDw9H/c55pREGNLCNXc3orpkGhCwaZUsiF4iy8vk1at6l1Uz+9rlfpZHkcRHaFjdIo8dInq6BY1UBNRpNAzekVvDjgvzrvzMW8tOPnMIfoD5/MHKbiRJQ==</latexit> Low Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"wuZucU3JbeEJSquG2WgqGdYMCR8=\">AAAB83icbVDLSgMxFL3js9ZX1aWbYBFclZnia1kQocsK9gHtUDJppg3NJCHJCGXob7hxoYhbf8adf2PazkJbD1w4nHMv994TKc6M9f1vb219Y3Nru7BT3N3bPzgsHR23jEw1oU0iudSdCBvKmaBNyyynHaUpTiJO29H4bua3n6g2TIpHO1E0TPBQsJgRbJ3Uq7PhCN0Lq6Wa9Etlv+LPgVZJkJMy5Gj0S1+9gSRpQoUlHBvTDXxlwwxrywin02IvNVRhMsZD2nVU4ISaMJvfPEXnThmgWGpXwqK5+nsiw4kxkyRynQm2I7PszcT/vG5q49swY0KllgqyWBSnHFmJZgGgAdOUWD5xBBPN3K2IjLDGxLqYii6EYPnlVdKqVoLrytVDtVy7zOMowCmcwQUEcAM1qEMDmkBAwTO8wpuXei/eu/exaF3z8pkT+APv8wfIYpF9</latexit> High Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"1BO6D/gzkeZNQ7HNIaph5NqELCI=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPF17LgRncV7AOmQ8mkd9rQTDIkGaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3kHtPmHCmjet+O6W19Y3NrfJ2ZWd3b/+genjU0TJVFNpUcql6IdHAmYC2YYZDL1FA4pBDN5zc5n73CZRmUjyaaQJBTEaCRYwSYyX/XlAFMQhD+KBac+vuHHiVeAWpoQKtQfWrP5Q0zdOUE619z01MkBFlGOUwq/RTDQmhEzIC31JBYtBBNl95hs+sMsSRVPYJg+fq70RGYq2ncWgnY2LGetnLxf88PzXRTZAxkaQGBF18FKUcG4nz+/GQKaCGTy0hVDG7K6Zjogg1tqWKLcFbPnmVdBp176p++dCoNS+KOsroBJ2ic+Sha9REd6iF2ogiiZ7RK3pzjPPivDsfi9GSU2SO0R84nz9y2ZFU</latexit> Incremental <latexit sha1_base64=\"Jmobmj50NeE6y3ftB4xt5xZD5Eg=\">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKewGX8dALh4jmAcmS5id9CZDZmeXmVkhLP6FFw+KePVvvPk3TpI9aGJBQ1HVTXdXkAiujet+O4W19Y3NreJ2aWd3b/+gfHjU1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfM7j6g0j+W9mSboR3QkecgZNVZ6aIhUG1Rcjgblilt15yCrxMtJBXI0B+Wv/jBmaYTSMEG17nluYvyMKsOZwKdSP9WYUDahI+xZKmmE2s/mFz+RM6sMSRgrW9KQufp7IqOR1tMosJ0RNWO97M3E/7xeasIbP+MySQ1KtlgUpoKYmMzeJ0OukBkxtYQyxe2thI2posymoEs2BG/55VXSrlW9q+rlXa1Sv8jjKMIJnMI5eHANdbiFJrSAgYRneIU3RzsvzrvzsWgtOPnMMfyB8/kDzgaQ+A==</latexit> Clustering <latexit sha1_base64=\"c4xrXg0yZYBSSDLHCxlf45OWNzg=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBA8hd2Aj2PAi8eI5gHJEmYnnWTIzOwyMyuEJR/hxYMiXv0eb/6Nk2QPmljQUFR1090VJYIb6/vf3tr6xubWdmGnuLu3f3BYOjpumjjVDBssFrFuR9Sg4AoblluB7UQjlZHAVjS+nfmtJ9SGx+rRThIMJR0qPuCMWie1HqhMBJpeqexX/DnIKglyUoYc9V7pq9uPWSpRWSaoMZ3AT2yYUW05EzgtdlODCWVjOsSOo4pKNGE2P3dKzp3SJ4NYu1KWzNXfExmVxkxk5DoltSOz7M3E/7xOagc3YcZVklpUbLFokApiYzL7nfS5RmbFxBHKNHe3EjaimjLrEiq6EILll1dJs1oJriqX99VyrZrHUYBTOIMLCOAaanAHdWgAgzE8wyu8eYn34r17H4vWNS+fOYE/8D5/AF7Wj40=</latexit> Samples <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"KzBZ8R84UC9mpPFQBWeRHFxcqjw=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKVI8FLx4rmLbQhrLZbNq1m92wuxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzwpQzbVz32yltbG5t75R3K3v7B4dH1eOTjpaZItQnkkvVC7GmnAnqG2Y47aWK4iTktBtObud+94kqzaR4MNOUBgkeCRYzgo2VOn4aYUOH1ZpbdxdA68QrSA0KtIfVr0EkSZZQYQjHWvc9NzVBjpVhhNNZZZBpmmIywSPat1TghOogX1w7QxdWiVAslS1h0EL9PZHjROtpEtrOBJuxXvXm4n9ePzPxTZAzkWaGCrJcFGccGYnmr6OIKUoMn1qCiWL2VkTGWGFibEAVG4K3+vI66TTqXrPevG/UWldFHGU4g3O4BA+uoQV30AYfCDzCM7zCmyOdF+fd+Vi2lpxi5hT+wPn8AYuwjxQ=</latexit> Update <latexit sha1_base64=\"y2NH6tDs2GygUDqZYglGwvR4SpA=\">AAAB+nicbVBNSwMxEJ2tX7V+bfXoJVgEQSi7PVSPFS8eK9oPaEvJptk2NMkuSVYpa3+KFw+KePWXePPfmLZ70NYHA4/3ZpiZF8ScaeN5305ubX1jcyu/XdjZ3ds/cIuHTR0litAGiXik2gHWlDNJG4YZTtuxolgEnLaC8fXMbz1QpVkk780kpj2Bh5KFjGBjpb5bvMMi5lSjc3QlyShSuu+WvLI3B1olfkZKkKHed7+6g4gkgkpDONa643ux6aVYGUY4nRa6iaYxJmM8pB1LJRZU99L56VN0apUBCiNlSxo0V39PpFhoPRGB7RTYjPSyNxP/8zqJCS97KZNxYqgki0VhwpGJ0CwHNGCKEsMnlmCimL0VkRFWmBibVsGG4C+/vEqalbJfLVdvK6Wal8WRh2M4gTPw4QJqcAN1aACBR3iGV3hznpwX5935WLTmnGzmCP7A+fwBUnKTWg==</latexit> Samples + Anchors <latexit sha1_base64=\"u0BDOcH87PXd3DsT+o414+7cHnI=\">AAAB7XicbZC7SgNBFIbPxluMt6ilIINBsAq7FjGdARvLBMwFkhBmZ2eTMbMzy8ysEJaU9jYWitj6Cql8CDufwZdwcik0+sPAx/+fw5xz/JgzbVz308msrK6tb2Q3c1vbO7t7+f2DhpaJIrROJJeq5WNNORO0bpjhtBUriiOf06Y/vJrmzTuqNJPixoxi2o1wX7CQEWys1eiQQBrdyxfcojsT+gveAgqX75Pa1/3xpNrLf3QCSZKICkM41rrtubHpplgZRjgd5zqJpjEmQ9ynbYsCR1R309m0Y3RqnQCFUtknDJq5PztSHGk9inxbGWEz0MvZ1PwvaycmLHdTJuLEUEHmH4UJR0ai6eooYIoSw0cWMFHMzorIACtMjD1Qzh7BW175LzTOi16pWKq5hUoZ5srCEZzAGXhwARW4hirUgcAtPMATPDvSeXRenNd5acZZ9BzCLzlv33Yvk3g=</latexit> ··· <latexit sha1_base64=\"+7L/8ObZcl+JIZaSFhVO3t+lUUE=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2bvizjWb9YcituBrJMvDkp1Y6CDPV+8as7iFgScoVMUmM6nhtjL6UaBZN8UugmhseUPdIh71iqaMhNL82unZBTqwxIEGlbCkmm/p5IaWjMOPRtZ0hxZBa9qfif10kwuOqlQsUJcsVmi4JEEozI9HUyEJozlGNLKNPC3krYiGrK0AZUsCF4iy8vk+Z5xatWqnc2jQuYIQ/HcAJl8OASanALdWgAgwd4hld4cyLnxXl3PmatOWc+cwh/4Hz+AFjYkTs=</latexit> D l ( t ) <latexit sha1_base64=\"9C0bB8PYImk9DX0HLfGvGd44PFA=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2b/qiMZ/1iya24Gcgy8eakVDsKMtT7xa/uIGJJyBUySY3peG6MvZRqFEzySaGbGB5T9kiHvGOpoiE3vTS7dkJOrTIgQaRtKSSZ+nsipaEx49C3nSHFkVn0puJ/XifB4KqXChUnyBWbLQoSSTAi09fJQGjOUI4toUwLeythI6opQxtQwYbgLb68TJrnFa9aqd7ZNC5ghjwcwwmUwYNLqMEt1KEBDB7gGV7hzYmcF+fd+Zi15pz5zCH8gfP5A1K8kTc=</latexit> D h ( t ) <latexit sha1_base64=\"eNrtnhPGeU8n4BRDMStm5cjQ4ts=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDbbTbt0s4m7E6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSKQw6Lrfzsbm1vbObmGvuH9weHRcOjltmzjVjLdYLGPdDajhUijeQoGSdxPNaRRI3gkmjbnfeeLaiFg94DThfkRHSoSCUbRS1zNIGlTKQansVtwFyDrxclKGHM1B6as/jFkacYVMUmN6npugn1GNgkk+K/ZTwxPKJnTEe5YqGnHjZ4t7Z+TSKkMSxtqWQrJQf09kNDJmGgW2M6I4NqveXPzP66UY3vqZUEmKXLHlojCVBGMyf54MheYM5dQSyrSwtxI2ppoytBEVbQje6svrpF2teLVK7b5arl/ncRTgHC7gCjy4gTrcQRNawEDCM7zCm/PovDjvzseydcPJZ87gD5zPH1Naj3k=</latexit> 1st Call <latexit sha1_base64=\"mxsL+XuWb2hqFND+pzTctrB1rcY=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDababt0s4m7G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfc7T6g0j+WDmSboR3Qk+ZAzaqzUrcqQNKgQg1LZrbgLkHXi5aQMOZqD0lc/jFkaoTRMUK17npsYP6PKcCZwVuynGhPKJnSEPUsljVD72eLeGbm0SkiGsbIlDVmovycyGmk9jQLbGVEz1qveXPzP66VmeOtnXCapQcmWi4apICYm8+dJyBUyI6aWUKa4vZWwMVWUGRtR0Ybgrb68TtrViler1O6r5fp1HkcBzuECrsCDG6jDHTShBQwEPMMrvDmPzovz7nwsWzecfOYM/sD5/AE0o49l</latexit> 2nd Call <latexit sha1_base64=\"oSA1OFmXXL9y3PJtqoVxTIG9mto=\">AAAB8HicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaElCY2UwkQ8DF7K3zMGGvb3L7p6REH6FjYXG2Ppz7Pw3LnCFgi+Z5OW9mczMCxLBtXHdbye3sbm1vZPfLeztHxweFY9PWjpOFcMmi0WsOgHVKLjEpuFGYCdRSKNAYDsY1+d++xGV5rG8N5ME/YgOJQ85o8ZKD7f4ZEidCtEvltyyuwBZJ15GSpCh0S9+9QYxSyOUhgmqdddzE+NPqTKcCZwVeqnGhLIxHWLXUkkj1P50cfCMXFhlQMJY2ZKGLNTfE1MaaT2JAtsZUTPSq95c/M/rpia89qdcJqlByZaLwlQQE5P592TAFTIjJpZQpri9lbARVZQZm1HBhuCtvrxOWpWyVy1X7yqlWiWLIw9ncA6X4MEV1OAGGtAEBhE8wyu8Ocp5cd6dj2VrzslmTuEPnM8fSFeQCA==</latexit> Next Call Figure 2: Overview of the SimATTA framework. and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (8) Corollary 4 validates that the selected low-entropy samples can mitigate the CF problem under the assumption that these samples are source-like, which is also empirically validated in Fig. 1. Note that our strategy employs entropy minimization in a selective manner, aiming to solve CF rather than the main adaptation issue. While many FTTA works use entropy minimization to adapt across domains without guarantees, our use is more theoretically-sound. 4 A N ATTA ALGORITHM Building on our theoretical findings, we introduce a simple yet effective ATTA method, known as SimATTA, that innovatively integrates incremental clustering and selective entropy minimization techniques, as illustrated in Fig. 2. We start with an overview of our methodology, including the learning framework and the comprehensive sample selection strategies. We then proceed to discuss the details of the incremental clustering technique designed for real-time sample selections. 4.1 A LGORITHM OVERVIEW Let (x, y) be a labeled sample and f(·; θ) be our neural network, where ˆy = f(x; θ) and θ represents the parameters. We have a model pre-trained on source domains with the pre-trained parameters ϕ. We initialize model parameters as θ(0) = ϕ and aim to adapt the model f(·; θ) in real-time. During the test phase, the model continuously predicts labels for streaming-in test data and concurrently gets fine-tuned. We perform sample selection to enable active learning. As discussed in Sec. 3.2, we empirically consider informative high-entropy samples for addressing distribution shifts and source-like low-entropy samples to mitigate CF. As shown in Alg. 1, at each time step t, we first partition unlabeled test samples Ute(t) into high entropy and low entropy datasets, Uh(t) and Ul(t), using an entropy threshold. The source-pretrained model f(·; ϕ) is frozen to predict pseudo labels for low entropy data. We obtain labeled low-entropy data Dl(t) by labeling Ul(t) with f(·; ϕ) and combining it with Dl(t − 1). In contrast, the selection of high-entropy samples for active labeling is less straightforward. Since the complete test dataset is inaccessible for analyzing the target domain distribution, real-time sample selection is required. We design an incremental clustering sample selection technique to reduce sample redundancy and increase distribution coverage, detailed in Sec. 4.2. The incremental clustering algorithm outputs the labeled test samples Dh(t), also referred to as anchors, given Dh(t −1) and Uh(t). After sample selection, the model undergoes test-time training using the labeled test anchors Dh(t) and pseudo-labeled source-like anchors Dl(t). Following the analyses in Sec. 3.1, the training weights and sample numbers should satisfy w(t) ≈ λ(t) for Dh(t) and Dl(t) for optimal results. The analyses and results in Sec. 3.2 further indicate that balancing the source and target ratio is the key to mitigating CF. However, when source-like samples significantly outnumber test samples, the optimal w(t) for test domains can deviate from λ(t) according to Eq. (4). 4.2 I NCREMENTAL CLUSTERING We propose incremental clustering, a novel continual clustering technique designed to select informa- tive samples in unsupervised settings under the ATTA framework. The primary goal of this strategy is to store representative samples for distributions seen so far. Intuitively, we apply clusters to cover all seen distributions while adding new clusters to cover newly seen distributions. During this process with new clusters added, old clusters may be merged due to the limit of the cluster budget. Since 6Published as a conference paper at ICLR 2024 Algorithm 1 SIMATTA: A SIMPLE ATTA ALGORITHM Require: A fixed source pre-trained model f(·; ϕ) and a real-time adapting model f(·; θ(t)) with θ(0) = ϕ. Streaming test data Ute(t) at time step t. Entropy of predictions H(ˆy) = −P c p(ˆyc) logp(ˆyc). Low entropy and high entropy thresholds el and eh. The number of cluster centroid budget NC (t) at time step t. Centroid increase number k. Learning step size η. 1: for t = 1, . . . , Tdo 2: Model inference on Ute(t) using f(·; θ(t − 1)). 3: Dl(t) ← Dl(t − 1) ∪ {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el} 4: Uh(t) ← {x|x ∈ Ute(t), H(f(x; θ)) > eh} 5: Dh(t) ← Dh(t − 1) ∪ {(x, y)|∀x ∈ IC(Dh(t − 1), Uh(t), NC(t)), y= Oracle(x)} 6: λ(t) ← |Dl(t)|/(|Dl(t)| + |Dh(t)|), |Dh(t)|/(|Dl(t)| + |Dh(t)|) 7: w(t) ← GetW(λ(t)) ▷ Generally, GetW(λ(t)) = λ(t) is a fair choice. 8: θ(t) ← θ(t − 1) 9: for (xl, yl) in Dl and (xh, yh) in Dh do 10: θ(t) ← θ(t) − ηw0∇ℓCE (f(xl; θ(t)), yl) − η(1 − w0)∇ℓCE (f(xh; θ(t)), yh) 11: end for 12: NC (t + 1) ← UpdateCentroidNum(NC (t)) ▷ Naive choice: NC (t + 1) ← NC (t) + k. 13: end for clusters cannot be stored efficiently, we store the representative samples of clusters, named anchors, instead. In this work, we adopt weighted K-means (Krishna and Murty, 1999) as our base clustering method due to its popularity and suitability for new setting explorations. When we apply clustering with new samples, a previously selected anchor should not weigh the same as new samples since the anchor is a representation of a cluster,i.e., a representation of many samples. Instead, the anchor should be considered as a barycenter with a weight of the sum of its cluster’s sample weights. For a newly added cluster, its new anchor has the weight of the whole cluster. For clusters containing multiple old anchors, i.e., old clusters, the increased weights are distributed equally among these anchors. These increased weights are contributed by new samples that are close to these old anchors. Intuitively, this process of clustering is analogous to the process of planet formation. Where there are no planets, new planets (anchors) will be formed by the aggregation of the surrounding material (samples). Where there are planets, the matter is absorbed by the surrounding planets. This example is only for better understanding without specific technical meanings. Specifically, we provide the detailed Alg. 2 for incremental clustering. In each iteration, we apply weighted K-Means for previously selected anchors Danc and the new streaming-in unlabeled data Unew. We first extract all sample features using the model from the previous step f(·; θ(t − 1)), and then cluster these weighted features. The initial weights of the new unlabeled samples are 1, while anchors inherit weights from previous iterations. After clustering, clusters including old anchors are old clusters, while clusters only containing new samples are newly formed ones. For each new cluster, we select the centroid-closest sample as the new anchor to store. As shown in line 10 of Alg. 2, for both old and new clusters, we distribute the sample weights in this cluster as its anchors’ weights. With incremental clustering, although we can control the number of clusters in each iteration, we cannot control the number of new clusters/new anchors. This indirect control makes the increase of new anchors adaptive to the change of distributions, but it also leads to indirect budget control. Therefore, in experimental studies, we set the budget limit, but the actual anchor budget will not reach this limit. The overall extra storage requirement is O(B) since the number of saved unlabeled samples is proportional to the number of saved labeled samples (anchors). 5 E XPERIMENTAL STUDIES In this study, we aim to validate the effectiveness of our proposed method, as well as explore the various facets of the ATTA setting. Specifically, we design experiments around the following research questions: RQ1: Can TTA methods address domain distribution shifts? RQ2: Is ATTA as efficient as TTA? RQ3: How do the components of SimATTA perform? RQ4: Can ATTA perform on par with stronger Active Domain Adaptation (ADA) methods? We compare ATTA with three settings, TTA (Tab. 2), enhanced TTA (Tab. 3 and 5), and ADA (Tab. 4). Datasets. To assess the OOD performance of the TTA methods, we benchmark them using datasets from DomainBed (Gulrajani and Lopez-Paz, 2020) and Hendrycks and Dietterich (2019a). We employ PACS (Li et al., 2017), VLCS (Fang et al., 2013), Office-Home (Venkateswara et al., 2017), and Tiny-ImageNet-C datasets for our evaluations. For each dataset, we designate one domain as 7Published as a conference paper at ICLR 2024 Table 2: TTA comparisons on PACS and VLCS.This table includes the two data stream mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. PACS Domain-wise data stream Post-adaptation Random data stream Post-adaptation P →A→ →C→ →S P A C S →1→ →2→ →3→ →4 P A C S BN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 Tent (steps=1) N/A 67.29 64.59 44.67 97.60 66.85 64.08 42.58 56.35 54.09 51.83 48.58 97.19 63.53 60.75 41.56Tent (steps=10) N/A 67.38 57.85 20.23 62.63 34.52 40.57 13.59 47.36 31.01 22.84 20.33 50.78 23.68 20.95 19.62EATA N/A 67.04 64.72 50.27 98.62 66.50 62.46 48.18 57.31 56.06 58.17 59.78 98.62 69.63 65.70 54.26CoTTA N/A 65.48 62.12 53.17 98.62 65.48 63.10 53.78 56.06 54.33 57.16 57.42 98.62 65.97 62.97 54.62SAR (steps=1) N/A 66.75 63.82 49.58 98.32 66.94 62.93 45.74 56.78 56.35 56.68 56.70 98.44 68.16 64.38 52.53SAR (steps=10) N/A 69.38 68.26 49.02 96.47 62.16 56.19 54.62 53.51 51.15 51.78 45.60 94.13 56.64 56.02 36.37 SimATTA (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00SimATTA (B ≤500) N/A 77.93 76.02 76.30 98.62 88.33 83.49 83.74 68.46 78.22 80.91 85.49 99.16 86.67 84.77 87.71 VLCS Domain-wise data stream Post-adaptation Random data stream Post-adaptation C →L→ →S→ →V C L S V →1→ →2→ →3→ →4 C L S V BN w/o adapt 100.00 33.55 41.10 49.05 100.00 33.55 41.10 49.05 41.23 41.23 41.23 41.23 100.00 33.55 41.10 49.05BN w/ adapt 85.16 37.31 33.27 52.16 85.16 37.31 33.27 52.16 40.91 40.91 40.91 40.91 85.16 37.31 33.27 52.16 Tent (steps=1) N/A 38.55 34.40 53.88 84.73 43.86 33.61 53.11 44.85 44.29 47.38 44.98 85.30 43.49 37.81 53.35Tent (steps=10) N/A 45.41 31.44 32.32 42.54 37.65 27.79 33.12 46.13 42.31 43.51 39.48 52.01 40.32 33.64 40.37EATA N/A 37.24 33.15 52.58 84.10 37.69 32.39 52.49 43.77 42.48 43.34 41.55 83.32 36.67 31.47 52.55CoTTA N/A 37.39 32.54 52.25 82.12 37.65 33.12 52.90 43.69 42.14 43.21 42.32 81.98 37.99 33.52 53.23SAR (steps=1) N/A 36.18 34.43 52.46 83.96 39.72 36.53 52.37 43.64 43.04 44.20 41.93 85.09 40.70 36.44 53.02SAR (steps=10) N/A 35.32 34.10 51.66 82.12 41.49 33.94 53.08 43.56 42.05 42.53 41.16 85.09 37.58 33.12 52.01 SimATTA (B ≤300) N/A 62.61 65.08 74.38 99.93 69.50 66.67 77.34 62.33 69.33 73.20 71.93 99.93 69.43 72.46 80.39SimATTA (B ≤500) N/A 63.52 68.01 76.13 99.51 70.56 73.10 78.35 62.29 70.45 73.50 72.02 99.43 70.29 72.55 80.18 the source domain and arrange the samples from the other domains to form the test data stream. For DomainBed datasets, we adopt two stream order strategies. The first order uses a domain-wise data stream, i.e., we finish streaming samples from one domain before starting streaming another domain. The second order is random, where we shuffle samples from all target domains and partition them into four splits 1, 2, 3, and 4, as shown in Tab. 2. More dataset details are provided in Appx. G.1. Baselines. For baseline models, we start with the common source-only models, which either utilize pre-calculated batch statistics (BN w/o adapt) or test batch statistics (BN w/ adapt). For comparison with other TTA methods, we consider four state-of-the-art TTA methods: Tent (Wang et al., 2021), EATA (Niu et al., 2022), CoTTA (Wang et al., 2022a), and SAR (Niu et al., 2023). The three of them except Tent provide extra design to avoid CF. To compare with ADA methods, we select algorithms that are partially comparable with our method, i.e., they should be efficient (e.g., uncertainty-based) without the requirements of additional networks. Therefore, we adopt random, entropy (Wang and Shang, 2014), k-means (Krishna and Murty, 1999), and CLUE (Prabhu et al., 2021) for comparisons. Settings. For TTA, we compare with general TTA baselines in streaming adaptation using the two aforementioned data streaming orders, domain-wise and random. We choose P in PACS and C in VLCS as source domains. For domain-wise data stream, we use order A → C → S for PACS and L → S → V for VLCS. We report the real-time adaptation accuracy results for each split of the data stream, as well as the accuracy on each domain after all adaptations through the data stream (under “post-adaptation” columns). Enhanced TTA is built on TTA with access to extra random sample labels. TTA baselines are further fine-tuned with these random samples. To further improve enhanced TTA, we use long-term label storage and larger unlabeled sample pools. To its extreme where the model can access the whole test set samples, the setting becomes similar to ADA, thus we also use ADA methods for comparisons. ADA baselines have access to all samples in the pre-collected target datasets but not source domain data, whereas our method can only access the streaming test data. 5.1 T HE FAILURE OF TEST-TIME ADAPTATION The failure of TTA methods on domain distribution shifts is one of the main motivations of the ATTA setting. As shown in Tab. 2, TTA methods cannot consistently outperform eventhe simplest baseline \"BN w/ adapt\" which uses test time batch statistics to make predictions, evidencing that current TTA methods cannot solve domain distribution shifts (RQ1). Additionally, Tent (step=10) exhibits significant CF issues, where \"step=10\" indicates 10 test-time training updates, i.e., 10 gradient backpropagation iterations. This failure of TTA methods necessitates the position of ATTA. In contrast, SimATTA, with a budget B less than 300, outperforms all TTA methods on both source and target domains by substantial margins. Moreover, compared to the source-only baselines, our method improves the target domain performances significantly with negligible source performance loss, showing that ATTA is a more practically effective setting for real-world distribution shifts. 5.2 E FFICIENCY & ENHANCED TTA SETTING COMPARISONS To validate the efficiency of ATTA and broaden the dataset choice, we conduct this study on Tiny- ImageNet-C which, though does not focus on domain shifts, is much larger than PACS and VLCS. we 8Published as a conference paper at ICLR 2024 Table 3: Comparisons with Enhanced TTA on Tiny-ImageNet-C (severity level 5). Tiny-ImageNet-C Time (sec)Noise Blur Weather Digital Gauss. Shot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Contr. Elastic Pixel JPEG Avg. Tent (step=1) 68.83 9.32 11.97 8.86 10.43 7.00 12.20 14.34 13.58 15.46 13.55 3.99 13.31 17.79 18.61 12.17Tent (step=10) 426.90 0.86 0.63 0.52 0.52 0.55 0.54 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.54EATA 93.14 3.98 3.33 2.18 4.80 2.37 11.02 11.41 14.06 15.26 9.65 1.36 9.88 14.24 12.12 8.26CoTTA 538.78 5.63 7.12 6.31 8.05 5.74 9.68 10.55 11.75 12.00 11.15 4.17 5.35 7.82 8.90 8.16SAR (step=1) 113.76 8.90 3.11 1.67 1.55 1.47 1.35 1.19 1.03 1.04 0.93 0.83 1.00 0.74 0.77 1.83SAR (step=10) 774.11 2.67 3.26 2.38 1.64 1.85 2.49 3.16 3.81 2.72 3.12 0.81 3.47 4.04 1.76 2.66 SimATTA (step=10) 736.289.68 19.40 12.14 30.28 17.03 42.36 43.10 31.96 40.08 29.243.21 34.56 45.24 45.74 28.86 enhance the TTA setting by fine-tuning baselines on randomly selected labeled samples. Specifically, the classifier of ResNet18-BN is pre-adapted to the brightness corruption (source domain) before test-time adapting. SimATTA’s label budget is around 4,000, while all other TTA methods have budget 4,500 for randomly selected labeled samples. The data stream order is shown in Tab. 3. Time is measured across all corrupted images in the Noise and Blur noise types, and the values represent the average time cost for adapting 10,000 images. The results clearly evidence the efficiency of ATTA (RQ2), while substantially outperforming all enhanced TTA baselines. Simply accessing labeled samples cannot benefit TTA methods to match ATTA. With 10 training updates (step=10) for each batch, FTTA methods would suffer from severe CF problem. In contrast, ATTA covers a statistically significant distribution, achieving stronger performances with 10 training updates or even more steps till approximate convergences. In fact, longer training on Tent (step=10) leads to worse results (compared to step=1), which further motivates the design of the ATTA setting. The reason for higher absolute time cost in Tab. 3 is due to differences in training steps. In this experiment, SimATTA has a training step of 10, and similar time cost as SAR per step. Note that if the enhanced TTA setting is further improved to maintain distributions with a balanced CF mitigation strategy and an incremental clustering design, the design approaches ATTA. Specifically, we compare SimATTA with its variants as the ablation study (RQ3) in Appx. I.2. 5.3 C OMPARISONS TO A STRONGER SETTING : ACTIVE DOMAIN ADAPTATION Table 4: Comparisons to ADA baselines. Source domains are denoted as \"(S)\". Results are average accuracies (with standard deviations). PACS P (S) A C S Random (B= 300) 96.21 (0.80) 81.19 (0.48) 80.75 (1.27) 84.34 (0.18)Entropy (B= 300) 96.31 (0.64)88.00 (1.46)82.48 (1.71) 80.55 (1.01)Kmeans (B= 300) 93.71 (1.50) 79.31 (4.01) 79.64 (1.44) 83.92 (0.65)CLUE (B= 300) 96.69 (0.17)83.97 (0.57)84.77 (0.88) 86.91 (0.26) SimATTA (B ≤300) 98.89 (0.09)84.69 (0.22)83.09 (0.83)83.76 (2.24) VLCS C (S) L S V Random (B= 300) 96.21 (1.65) 66.67 (1.70) 70.72 (0.30) 72.14 (1.71)Entropy (B= 300) 97.74 (1.56) 69.29 (2.26)69.25 (4.77) 75.26 (3.07)Kmeans (B= 300) 98.61 (0.27)67.57 (1.64)70.77 (0.01)74.49 (0.97)CLUE (B= 300) 85.70 (10.09) 65.29 (1.49) 69.42 (2.64) 69.09 (6.05) SimATTA (B ≤300) 99.93 (0.00) 69.47 (0.03)69.57 (2.90)78.87 (1.53) In addtion to the above comparisons with (en- hanced) TTA, which necessitate the requirement of extra information in the ATTA setting, we com- pare ATTA with a stronger setting Active Domain Adaptation (ADA) to demonstrate another supe- riority of ATTA, i.e., weaker requirements for comparable performances (RQ4). ADA baselines are able to choose the global best active samples, while ATTA has to choose samples from a small sample buffer (e.g., a size of 100) and discard the rest. Tab. 4 presents the post-adaptation model per- formance results. All ADA results are averaged from 3 random runs, while ATTA results are the post-adaptation performances averaged from the two data stream orders. As can be observed, despite the lack of a pre-collected target dataset, SimATTA produces better or competitive results against ADA methods. Moreover, without source data access, SimATTA’s design for CF allows it to maintain superior source domain performances over ADA methods. Further experimental studies including the Office-Home dataset are provided in Appx. I. In conclusion, the significant improvement compared to weaker settings (TTA, enhanced TTA) and the comparable performance with the stronger setting, ADA, rendering ATTA a setting that is as efficient as TTA and as effective as ADA. This implies its potential is worthy of future explorations. 6 C ONCLUSION AND DISCUSSION There’s no denying that OOD generalization can be extremely challenging without certain information, often relying on various assumptions easily compromised by different circumstances. Thus, it’s prudent to seek methods to achieve significant improvements with minimal cost, e.g., DG methods leveraging environment partitions and ATTA methods using budgeted annotations. As justified in our theoretical and experimental studies, ATTA stands as a robust approach to achieve real-time OOD generalization. Although SimATTA sets a strong baseline for ATTA, there’s considerable scope for further investigation within the ATTA setting. One potential direction involves developing alternatives to prevent CF in ATTA scenarios. While selective entropy minimization on low-entropy samples has prove to be empirically effective, it relies on the quality of the pre-trained model and training on incorrectly predicted low-entropy samples may reinforce the errors. It might not be cost-effective to expend annotation budgets on low-entropy samples, but correcting them could be a viable alternative solution. We anticipate that our work will spur numerous further explorations in this field. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGMENTS This work was supported in part by National Science Foundation grant IIS-2006861 and National Institutes of Health grant U01AG070112. REFERENCES Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marchand. Domain- adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014. Lucas Baier, Tim Schlör, Jakob Schöffer, and Niklas Kühl. Detecting concept drift with neural network model uncertainty. In Hawaii International Conference on System Sciences, 2021. URL https://api.semanticscholar.org/CorpusID:235731947. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79:151–175, 2010. Davide Cacciarelli and Murat Kulahci. A survey on online active learning, 2023. Cheng Chen, Quande Liu, Yueming Jin, Qi Dou, and Pheng-Ann Heng. Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pages 225–235. Springer, 2021. Li Chen, Tutian Tang, Zhitian Cai, Yang Li, Penghao Wu, Hongyang Li, Jianping Shi, Junchi Yan, and Yu Qiao. Level 2 autonomous driving on a single device: Diving into the devils of openpilot. arXiv preprint arXiv:2206.08176, 2022a. Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu, and Yueting Zhuang. Self-supervised noisy label learning for source-free unsupervised domain adaptation. In 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS) , pages 10185–10192. IEEE, 2022b. Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. Advances in Neural Information Processing Systems, 33:21061–21071, 2020. David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129–145, 1996. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. Yuhe Ding, Lijun Sheng, Jian Liang, Aihua Zheng, and Ran He. Proxymix: Proxy-based mixup training with label refinery for source-free domain adaptation. arXiv preprint arXiv:2205.14566, 2022. Cian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Schölkopf. Source-free adaptation to measurement shift via bottom-up feature restoration. arXiv preprint arXiv:2107.05446, 2021. Jiahao Fan, Hangyu Zhu, Xinyu Jiang, Long Meng, Chen Chen, Cong Fu, Huan Yu, Chenyun Dai, and Wei Chen. Unsupervised domain adaptation by statistics alignment for deep sleep staging networks. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30:205–216, 2022. Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE International Conference on Computer Vision, pages 1657–1664, 2013. Yuqi Fang, Pew-Thian Yap, Weili Lin, Hongtu Zhu, and Mingxia Liu. Source-free unsupervised domain adaptation: A survey. arXiv preprint arXiv:2301.00265, 2022. Francois Fleuret et al. Uncertainty reduction for model adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9613–9623, 2021. 10Published as a conference paper at ICLR 2024 Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180–1189. PMLR, 2015. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342, 2021. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. Shurui Gui, Chaoyue Wang, Qihua Chen, and Dacheng Tao. Featureflow: Robust video interpolation via structure-to-texture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14004–14013, 2020. Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=8hHg-zs_p-h. Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. March 2019a. doi: 10.48550/ARXIV .1903.12261. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019b. Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Semisupervised svm batch mode active learning with applications to image retrieval. ACM Transactions on Information Systems (TOIS), 27(3):1–29, 2009. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853–17862, 2023. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. Advances in Neural Information Processing Systems, 34:3635–3649, 2021. Masato Ishii and Masashi Sugiyama. Source-free domain adaptation via distributional alignment by matching batch normalization statistics. arXiv preprint arXiv:2101.10842, 2021. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021. Suyog Dutt Jain and Kristen Grauman. Active image segmentation propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2864–2873, 2016. Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4893–4902, 2019. Ashish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. Active learning with gaussian processes for object categorization. In 2007 IEEE 11th international conference on computer vision, pages 1–8. IEEE, 2007. Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu. Test-time adaptable neural networks for robust medical image segmentation. Medical Image Analysis, 68:101907, 2021. 11Published as a conference paper at ICLR 2024 Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In VLDB, volume 4, pages 180–191. Toronto, Canada, 2004. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521–3526, 2017. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal- subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637–5664. PMLR, 2021. Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, and Dinesh Manocha. Salad: Source-free active label-agnostic domain adaptation for classification, segmentation and detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 382–391, 2023. K Krishna and M Narasimha Murty. Genetic k-means algorithm. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 29(3):433–439, 1999. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu- tional neural networks. Communications of the ACM, 60(6):84–90, 2017. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrap- olation (REx). In International Conference on Machine Learning , pages 5815–5826. PMLR, 2021. Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 615–625, 2021. David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148–156. Elsevier, 1994. Aodong Li, Alex Boyd, Padhraic Smyth, and Stephan Mandt. Detecting and adapting to irregular distribution shifts in bayesian online learning. Advances in neural information processing systems, 34:6816–6828, 2021a. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542–5550, 2017. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9641–9650, 2020. Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsupervised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8474–8481, 2021b. Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Jian Liang, Dapeng Hu, Ran He, and Jiashi Feng. Distill and fine-tune: Effective adaptation from a black-box source model. arXiv preprint arXiv:2104.01539, 1(3), 2021. Jian Liang, Dapeng Hu, Jiashi Feng, and Ran He. Dine: Domain adaptation from single and multiple black-box predictors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8003–8013, 2022. 12Published as a conference paper at ICLR 2024 Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without environment partition? Advances in Neural Information Processing Systems, 35:24529–24542, 2022. Xiaofeng Liu, Fangxu Xing, Chao Yang, Georges El Fakhri, and Jonghye Woo. Adapting off-the- shelf source segmenter for target medical image segmentation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24, pages 549–559. Springer, 2021a. Xinyu Liu and Yixuan Yuan. A source-free domain adaptive polyp detection framework with style diversification flow. IEEE Transactions on Medical Imaging, 41(7):1897–1908, 2022. Yuang Liu, Wei Zhang, Jun Wang, and Jianyong Wang. Data-free knowledge transfer: A survey. arXiv preprint arXiv:2112.15278, 2021b. Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021c. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97–105. PMLR, 2015. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017. Chaochao Lu, Yuhuai Wu, José Miguel Hernández-Lobato, and Bernhard Schölkopf. Invariant causal representation learning for out-of-distribution generalization. In International Conference on Learning Representations, 2021. Xinhong Ma, Junyu Gao, and Changsheng Xu. Active universal domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8968–8977, 2021. Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, and Dongmei Zhang. Source free unsupervised graph domain adaptation. arXiv preprint arXiv:2112.00955, 2021. Christoforos Mavrogiannis, Francesca Baldini, Allan Wang, Dapeng Zhao, Pete Trautman, Aaron Steinfeld, and Jean Oh. Core challenges of social robot navigation: A survey. ACM Transactions on Human-Robot Interaction, 12(3):1–39, 2023. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Munan Ning, Donghuan Lu, Dong Wei, Cheng Bian, Chenglang Yuan, Shuang Yu, Kai Ma, and Yefeng Zheng. Multi-anchor active domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9112–9122, 2021. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In International conference on machine learning, pages 16888–16905. PMLR, 2022. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. InThe Eleventh International Con- ference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE transactions on neural networks, 22(2):199–210, 2010. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 13Published as a conference paper at ICLR 2024 Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53–69, 2015. Judea Pearl. Causality. Cambridge university press, 2009. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947–1012, 2016. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017. Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, and Judy Hoffman. Active domain adaptation via clustering uncertainty-weighted embeddings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8505–8514, 2021. Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. arXiv preprint arXiv:2010.05761, 2020. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal- ization help optimization? Advances in neural information processing systems, 31, 2018. Akanksha Saran, Safoora Yousefi, Akshay Krishnamurthy, John Langford, and Jordan T. Ash. Streaming active learning with deep neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 30005–30021. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr. press/v202/saran23a.html. Harald Schafer, Eder Santana, Andrew Haden, and Riccardo Biasini. A commute in data: The comma2k19 dataset, 2018. Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for informa- tion extraction. In Advances in Intelligent Data Analysis: 4th International Conference, IDA 2001 Cascais, Portugal, September 13–15, 2001 Proceedings 4, pages 309–318. Springer, 2001. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. Burr Settles. Active learning literature survey. 2009. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji, and Manmohan Chandraker. Active adversarial domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 739–748, 2020. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pages 443–450. Springer, 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229–9248. PMLR, 2020. 14Published as a conference paper at ICLR 2024 Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7472–7481, 2018. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pages 4068–4076, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167–7176, 2017. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018–5027, 2017. Sudheendra Vijayanarasimhan and Ashish Kapoor. Visual recognition and detection under bounded computational resources. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1006–1013. IEEE, 2010. Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint conference on neural networks (IJCNN), pages 112–119. IEEE, 2014. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test- time adaptation by entropy minimization. InInternational Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312: 135–153, 2018. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022a. Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. Cross-domain contrastive learning for unsupervised domain adaptation. IEEE Transactions on Multimedia , 2022b. Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 11(5):1–46, 2020. Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, Xinjing Cheng, and Guoren Wang. Active learning for domain adaptation: An energy-based approach. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8708–8716, 2022. Zhao Xu, Kai Yu, V olker Tresp, Xiaowei Xu, and Jizhi Wang. Representative sampling for text classification using support vector machines. In Advances in Information Retrieval: 25th European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14–16, 2003. Proceedings 25, pages 393–407. Springer, 2003. Baoyao Yang, Hao-Wei Yeh, Tatsuya Harada, and Pong C Yuen. Model-induced generalization error bound for information-theoretic representation learning in source-data-free unsupervised domain adaptation. IEEE Transactions on Image Processing, 31:419–432, 2021a. Guanglei Yang, Hao Tang, Zhun Zhong, Mingli Ding, Ling Shao, Nicu Sebe, and Elisa Ricci. Transformer-based source-free domain adaptation. arXiv preprint arXiv:2105.14138, 2021b. Jianfei Yang, Xiangyu Peng, Kai Wang, Zheng Zhu, Jiashi Feng, Lihua Xie, and Yang You. Divide to adapt: Mitigating confirmation bias for domain adaptation of black-box predictors. arXiv preprint arXiv:2205.14467, 2022. H Yao, Yuhong Guo, and Chunsheng Yang. Source-free unsupervised domain adaptation with surrogate data generation. In Proceedings of NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021. 15Published as a conference paper at ICLR 2024 Hao-Wei Yeh, Baoyao Yang, Pong C Yuen, and Tatsuya Harada. Sofa: Source-data-free feature alignment for unsupervised domain adaptation. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 474–483, 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Hu Yu, Jie Huang, Yajing Liu, Qi Zhu, Man Zhou, and Feng Zhao. Source-free domain adaptation for real-world image dehazing. In Proceedings of the 30th ACM International Conference on Multimedia, pages 6645–6654, 2022. Haojian Zhang, Yabin Zhang, Kui Jia, and Lei Zhang. Unsupervised domain adaptation of black-box source models. arXiv preprint arXiv:2101.02839, 2021. Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems, 35:38629–38642, 2022a. Yifan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Adanpc: Exploring non-parametric classifier for test-time adaptation. In International Conference on Machine Learning, pages 41647–41676. PMLR, 2023. Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2339–2348, 2022b. Bowen Zhao, Chen Chen, and Shu-Tao Xia. Delta: degradation-free fully test-time adaptation. arXiv preprint arXiv:2301.13018, 2023a. Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In International Conference on Machine Learning (ICML), 2023b. Chunting Zhou, Xuezhe Ma, Paul Michel, and Graham Neubig. Examining and combating spurious features under distribution shift. In International Conference on Machine Learning, pages 12857– 12867. PMLR, 2021. 16Published as a conference paper at ICLR 2024 Active Test-Time Adaptation: Foundational Analyses and An Algorithm Supplementary Material A B ROADER IMPACTS The field of domain generalization primarily concentrates on enhancing a model’s generalization abilities by preparing it thoroughly before deployment. However, it is equally important for deep learning applications to have the capacity for real-time adaptation, as no amount of preparation can account for all possible scenarios. Consequently, domain generalization and test-time adaptation are complementary strategies: the former is more weighty and extensive, while the latter is more agile, lightweight and privacy-friendly. This work delves into the development of a real-time model adaptation strategy that can be applied to any pre-trained models, including large language models, to enhance their adaptive capabilities. Our research does not involve any human subjects or dataset releases, nor does it raise any ethical concerns. Since this work does not directly tie to specific applications, we do not foresee any immediate negative societal impacts. Nonetheless, we acknowledge that any technological advancement may carry potential risks, and we encourage the continued assessment of the broader impacts of real-time adaptation methodologies in various contexts. B FAQ & D ISCUSSIONS To facilitate the reviewing process, we summarize the answers to the questions that arose during the discussion of an earlier version of this paper. The major updates of this version are reorganized theoretical studies, incremental clustering details, experimental reorganization, and additional datasets and settings . We include more related field comparisons to distinguish different settings. We also cover the position of this paper in literature and the main claims of this paper. Finally, we will frankly acknowledge the limitations of this paper, explain and justify the scope of coverage, and provide possible future directions. Q1: What is the relationship between the proposed ATTA protocol and stream based active learning (Saran et al., 2023)? A: We would like to discuss the difference between our work and the referenced work. 1. Real-time Training Distinction: Saran et al. (2023) doesn’t operate in real-time capacity. This is evident from their experiments, where their model is trained only after completing a round. In contrast, our work involves training the model post each batch. This positions Saran et al. (2023)’s work as an intrinsic active learning technique, while our approach leans towards TTA methods. 2. Continual Training Nuance: Following the point above, Saran et al. (2023) stands out of the scope of continual training. As they mentioned ‘each time new data are acquired, the ResNet is reset to the ImageNet pre-trained weights before being updated‘, Saran et al. (2023) starts afresh with each iteration and is out of scope for CF discussions. Contrarily, our model is continuously trained on varying distributions, compelling us to address the CF issue while preserving advantages derived from various stored distributions. 3. Comparative Complexity: Given the aforementioned distinctions, it’s evident that our task presents a greater challenge compared to theirs. In addition, we have included comparisons with stronger active learning settings in Sec. 5.3. Q2: What are the insights from the theoretically foundational analysis? A: 1. It sets a well-defined formulation and grounded theoretical framework for the ATTA setting. 2. While entropy minimizations can cause CF, balancing the learning rate and number of high/low entropy samples is conversely the key solution to both distribution shifts and 17Published as a conference paper at ICLR 2024 CF by corresponding benefits. Though adding low-entropy data is intuitive, it is crucial in that this simple operation can make methods either too conservative or too aggressive without the correct balancing conditions. 3. The studies in Sec. 3.1 directly present a feasible and guaranteed solution for imple- menting ATTA to tackle shifts while avoiding CF. The aligned empirical validations of Sec. 3.2 also instruct the implementation of SimATTA. Q3: In test-time adaptation, one important issue is that the number of testing samples in a batch may be small, which means the sample size m will also be very small. May it affect the theorem and make them become very loose? A: We consider this issue jointly from theoretical and empirical validations. 1. It is true that the theoretical bounds can be loose given a small size of m unlabeled test samples. This situation of the error bound is mathematically ascribed to the quotient between the VC-dimension d of the hypothesis class and m. Under the VC-dimension theory, the ResNet18 model we adopt should have d ≫ m. However, practically we perform fine-tuning on pre-trained models instead of training from scratch, which significantly reduces the scale of parameter update. In this case, an assumption can be established that fine-tuning a model is roughly equivalent to learning a model with a relatively small d (Appx. H). This assumption is potentially underpinned by the empirical alignment of our validation experiments with the theoretical framework (Fig. 1). To this end, experiments indicate thatd and m are practically of similar scale for our settings. This prevents our theoretical bounds from being very loose and meaningless in reality. 2. Regarding cases that our assumption does not apply, this issue would appear inevitable, since it is rigorously inherent in the estimation error of our streaming and varying test distributions. The distribution of a test stream can be hardly monitored when only a limited batch is allowed, which we consider as a limitation of TTA settings. Moreover, this issue directly implies the necessity of using a buffer for unlabeled samples. A good practice is to maintain a relatively comparable sample buffer scale. Q4: What distribution shifts can ATTA solve? A: We would like to follow (but not limited to) the work (Zhao et al., 2023b) to discuss the distribution shifts ATTA can solve. 1. As elucidated in Sec. 3.1 and Sec. 5, ATTA can solve domain generalization shifts. Domain generalization shifts include complex shifts on the joint data distribution P(X, Y), given X as the covariates and Y as the label variable. Since P(X, Y) = P(X)P(Y |X), ATTA can handle covariate shift (P(X)), label shift (P(Y )), and conditional shift (P(Y |X)). The shifts on both covariate and conditional distributions can cover the shift on labels, but they (covariate + conditional shifts) are more complicated than pure label shifts, where only the marginal label distribution changes while the conditional distribution remains. Note that the conditional shifts are generally caused by spurious correlations, where the independent causal mechanism assumption (Pearl, 2009) holds or no concept drifts exist. 2. In our framework, the distribution support of X at different time steps can be different, but we don’t cover the situation where the support of Y changes, i.e., class-incremental problems. Q5: It is unclear how many samples are selected in each minibatch of testing samples. How the total budget is distributed across the whole testing data stream? A: The number of selected samples for each minibatch is decided jointly by the incremental clustering and the cluster centroid number NC (t). Intuitively, this sample selection is a dynamic process, with NC (t) restricting the budget and incremental clustering performing sample selection. For each batch, we increase applicable clustering centroids as a maximum limit, while the exact number of the selected samples is given by the incremental clustering by how many clusters are located in the scope of new distributions. e.g., if the incoming batch does not introduce new data distributions, then we select zero samples even with increased NC (t). In contrast, if the incoming batch contains data located in multiple new distributions, the incremental clustering tends to select more samples than the NC (t) limit, thus forcing to merging of multiple previous clusters into one new cluster. 18Published as a conference paper at ICLR 2024 The incremental clustering is detailed in Sec. 4.2, and NC (t) is naively increased by a constant hyper-parameter k. Therefore, the budget is adaptively distributed according to the data streaming distribution with budgets controlled by k, which is also the reason why we compare methods under a budget limit. Q6: Could compared methods have access to a few ground-truth labels as well? Making other algorithms be able to use the same amount of ground-truth labels randomly will produce fairer comparisons. A: 1. The enhanced TTA setting is exactly the setup we provide to produce fairer comparisons. See Tab. 3 and Tab. 5 for comparison results. 2. ATTA also compares to a stronger setting ADA which can access the whole test datasets multiple times. Table 5: The table demonstrates the comparisons on PACS where all enhanced TTA baselines have 300 budgets to randomly select labeled samples. The training steps of these labeled samples are the same as the original TTA method training steps. For accumulated sample selection, please refer to our ablation studies. Method Domain-wise data stream A VG Random data stream A VG P→ →A→ →C→ →S P A C S 1 2 3 4 P A C S Source onlyBN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 TTA Tent (steps=1) N/A 70.07 68.43 64.42 97.72 74.17 72.61 68.92 61.20 62.36 66.59 67.32 98.14 74.37 70.26 66.07Tent (steps=10) N/A 76.27 63.78 49.35 59.46 38.62 48.46 55.03 56.20 53.22 52.55 55.55 58.32 47.56 60.75 58.00EATA N/A 69.53 66.94 61.42 98.56 69.38 66.60 64.83 60.34 59.81 64.38 65.02 98.68 73.78 68.30 59.74CoTTA N/A 66.55 63.14 59.91 90.12 61.67 66.68 67.68 57.26 57.36 63.46 65.64 92.22 71.53 70.44 62.41SAR (steps=1) N/A 66.60 63.78 50.34 98.38 67.87 64.04 49.48 57.21 56.06 56.78 57.14 98.38 68.80 64.59 53.02SAR (steps=10) N/A 69.09 66.55 49.07 96.23 62.50 59.34 46.53 49.76 52.74 48.51 49.06 95.39 57.13 54.61 38.76 Ours (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00 Q7: What is the position of ATTA? A: Comparisons with different settings are challenging. In this work, the design of our experiments (Sec. 5) is to overcome this challenge by comparing both weaker settings and stronger settings. While the significant performance over weaker settings renders the necessity of extra information, the comparable performance with stronger settings provides the potential to relax restricted requirements. Intuitively, ATTA is the most cost-effective option in the consideration of both efficiency and effectiveness. We further provide the following ATTA summary: ATTA, which incorporates active learning in FTTA, is the light, real-time, source-free, widely applicable setting to achieve high generalization performances for test-time adaptation. 1. Necessity: From the causality perspective, new information is necessary (Lin et al., 2022; Pearl, 2009; Peters et al., 2017) to attain generalizable over distribution shifts which are insurmountable within the current TTA framework. 2. Effectiveness: Compared to FTTA methods, ATTA produces substantially better perfor- mances, on-par with the costly active domain adaptation (ADA) methods as shown in Table 3 in the paper. 3. Efficiency: Relative to ADA methods, ATTA possesses superior efficiency, similar to general FTTA methods, as shown in Tab. 3. 4. Applicability: ATTA is a model-agnostic setting. (1) Compared to domain generalization methods, ATTA do not require re-training and has the potential to apply to any pre-trained models. One interesting future direction is designing ATTA methods for large language models (LLMs), where re-trainings are extremely expensive and source data may be in- accessible. (2) Compared to FTTA methods, ATTA can protect model parameters from corrupting while learning new distributions by fine-tuning pre-trained models, rendering it more feasible and practical. In comparison with existing works, ATTA is motivated to mitigate the limitations of previous settings: 1. FTTA: Limited generalization performance. 19Published as a conference paper at ICLR 2024 2. TTT: Not source-free; limited generalization performance. 3. ADA & domain adaptation/generalization: Expensive re-trainings; limited applicability to pre-trained models. 4. Online active learning: It does not maintain and protect adaptation performances for multiple distributions in one model and does not consider the CF problem. Q8: What is the potential practical utility of ATTA? A: 1. Empirically, our method can generally finish a round of sample selection/training of 100 frames in 5s, i.e., 20 frames per sec, which is more than enough to handle multiple practical situations. Experiments on time complexity are provided in Tab. 3, where SimATTA has comparable time efficiency. 2. As a case analysis, the autopilot system (Hu et al., 2023; Chen et al., 2022a) presents an application scenario requiring high-speed low-latency adaptations, while these adaptations are largely underexplored. When entering an unknown environment, e.g., a construction section, a system of ATTA setting can require the driver to take over the wheel. During the period of manual operation when the driver is handling the wheel, steering signals are generated, and the in-car system quickly adaptations. The system doesn’t need to record 60 frames per second, since only the key steering operations and the corresponding dash cam frames are necessary, which can be handled by ATTA algorithms processing at 20 frames per sec. In this case, the human annotations are necessary and indirect. ATTA makes use of this information and adapts in the short term instead of collecting videos and having a long-round fine-tuning (Schafer et al., 2018). 3. In addition, many scenarios applicable for ATTA are less speed-demanding than the case above. One example is a personalized chatbot that subtly prompts and gathers user labels during user interaction. In a home decoration setting, applications can request that users scan a few crucial areas to ensure effective adaptation. Social robots (Mavrogiannis et al., 2023), e.g., vacuum robots, often require users to label critical obstacles they’ve encountered. 4. Compared with ADA, ATTA stands out as the tailored solution for the above scenarios. It does not require intensive retraining or server-dependent fine-tuning, offering both speed and computational efficiency. Meanwhile, akin to other TTA methods, ATTA also ensures user privacy. While it might marginally exceed the cost of standard TTA methods, the superior generalization ability makes it a compelling choice and justifies the additional expense. Q9: What can be covered by this paper? A: This paper endeavors to establish the foundational framework for a novel setting referred to as ATTA. We target (1) positioning the ATTA setting, (2) solving the two major and basic challenges of ATTA,i.e., the mitigation of distribution shifts and the avoidance of catastrophic forgetting (CF). We achieve the first goal by building the problem formulation and analyses, and further providing extensive qualitative and well-organized experimental comparisons with TTA, enhanced TTA, and ADA settings. These efforts position ATTA as the most cost-effective option between TTA and ADA, where ATTA inherits the efficiency of TTA and the effectiveness of ADA. With our theoretical analyses and the consistent algorithm design, we validate the success of our second goal through significant empirical performances. Q10: What are not covered by this paper? A: Constructing a new setting involves multifaceted complexities. Although there are various potential applications discussed above including scaling this setting up for large models and datasets, we cannot cover them in this single piece of work. There are three main reasons. First, the topics covered by a single paper are limited. Formally establishing ATTA setting and addressing its major challenges of ATTA takes precedence over exploring practical applications. Secondly, given the interrelations between ATTA and other settings, our experimental investigations are predominantly comparative, utilizing the most representative datasets from TTA and domain adaptation to showcase persuasive results. Thirdly, many practical applications necessitate task-specific configurations, rendering them unsuitable for establishing a universal learning setting. While the current focus is on laying down the foundational aspects of ATTA, the exploration of more specialized applications remains a prospective avenue for future work in the ATTA domain. 20Published as a conference paper at ICLR 2024 C R ELATED WORKS The development of deep learning witnesses various applications (He et al., 2016; Gui et al., 2020). To tackle OOD problem, various domain generalization works emerge (Krueger et al., 2021; Sagawa et al., 2019). C.1 U NSUPERVISED DOMAIN ADAPTATION Unsupervised Domain Adaptation (UDA) (Pan et al., 2010; Patel et al., 2015; Wilson and Cook, 2020; Wang and Deng, 2018) aims at mitigating distribution shifts between a source domain and a target domain, given labeled source domain samples and unlabeled target samples. UDA methods generally rely on feature alignment techniques to eliminate distribution shifts by aligning feature distributions between source and target domains. Typical feature alignment techniques include discrepancy minimization (Long et al., 2015; Sun and Saenko, 2016; Kang et al., 2019) and adversarial training (Ganin and Lempitsky, 2015; Tsai et al., 2018; Ajakan et al., 2014; Ganin et al., 2016; Tzeng et al., 2015; 2017). Nevertheless, alignments are normally not guaranteed to be correct, leading to the alignment distortion problem as noted by Ning et al. (2021). Source-free Unsupervised Domain Adaptation (SFUDA) (Fang et al., 2022; Liu et al., 2021b) algorithms aim to adapt a pre-trained model to unlabeled target domain samples without access to source samples. Based on whether the algorithm can access model parameters, these algorithms are categorized into white-box and black-box methods. White-box SFUDA typically considers data recovery (generation) and fine-tuning methods. The former focuses on recovering source- like data (Ding et al., 2022; Yao et al., 2021), e.g., training a Generative Adversarial Network (GAN) (Kurmi et al., 2021; Li et al., 2020), while the latter employs various techniques (Mao et al., 2021), such as knowledge distillation (Chen et al., 2022b; Liu and Yuan, 2022; Yang et al., 2021b; Yu et al., 2022), statistics-based domain alignment (Ishii and Sugiyama, 2021; Liu et al., 2021a; Fan et al., 2022; Eastwood et al., 2021), contrastive learning (Huang et al., 2021; Wang et al., 2022b), and uncertainty-based adaptation (Gawlikowski et al., 2021; Fleuret et al., 2021; Chen et al., 2021; Li et al., 2021b). Black-box SFUDA cannot access model parameters and often relies on self-supervised knowledge distillation (Liang et al., 2022; 2021), pseudo-label denoising (Zhang et al., 2021; Yang et al., 2022), or generative distribution alignment (Yeh et al., 2021; Yang et al., 2021a). C.2 T EST-TIME ADAPTATION Test-time Adaptation (TTA), especially Fully Test-time Adaptation (FTTA) algorithms (Wang et al., 2021; Iwasawa and Matsuo, 2021; Karani et al., 2021; Nado et al., 2020; Schneider et al., 2020; Wang et al., 2022a; Zhao et al., 2023a; Niu et al., 2022; Zhang et al., 2022a; Niu et al., 2023; You et al., 2021; Zhang et al., 2022b), can be considered as realistic and lightweight methods for domain adaptation. Built upon black-box SFUDA, FTTA algorithms eliminate the requirement of a pre-collected target dataset and the corresponding training phase. Instead, they can only access an unlabeled data stream and apply real-time adaptation and training. In addition to FTTA, Test-time Training (TTT) (Sun et al., 2020; Liu et al., 2021c) often relies on appending the original network with a self-supervised task. TTT methods require retraining on the source dataset to transfer information through the self-supervised task. Although they do not access the source dataset during the test-time adaptation phase, TTT algorithms are not off-the-shelf source-free methods. TTA is a promising and critical direction for real-world applications, but current entropy minimization-based methods can be primarily considered as feature calibrations that require high-quality pseudo-labels. This requirement, however, can be easily violated under larger distribution shifts. Current TTA algorithms, inheriting UDA drawbacks, cannot promise good feature calibration results, which can be detrimental in real-world deployments. For instance, entropy minimization on wrongly predicted target domain samples with relatively low entropy can only exacerbate spurious correla- tions (Chen et al., 2020). Without extra information, this problem may be analogous to applying causal inference without intervened distributions, which is intrinsically unsolvable (Peters et al., 2016; Pearl, 2009). This paper aims to mitigate this issue with minimal labeled target domain samples. To minimize the cost, we tailor active learning techniques for TTA settings. It is worth noting that a recent work AdaNPC (Zhang et al., 2023) is essentially a domain gener- alization method with a TTA phase attached, while our ATTA is built based on the FTTA setting. Specifically, Current FTTA methods and our work cannot access the source domain. In contrast, 21Published as a conference paper at ICLR 2024 AdaNPC accesses source data to build its memory bank, circumventing the catastrophic forgetting problem. Furthermore, AdaNPC requires multiple source domains and training before performing TTA. Thus AdaNPC uses additional information on domain labels and retraining resources for its memory bank, undermining the merits of FTTA. Regarding theoretical bounds, their target domain is bounded by source domain error and model estimations (in big-O expression), while we consider active sample learning and time variables for varying test distributions. C.3 C ONTINUAL DOMAIN ADAPTATION Many domain adaptation methods focus on improving target domain performance, neglecting the performance on the source domain, which leads to the CF problem (Kemker et al., 2018; Kirkpatrick et al., 2017; Li and Hoiem, 2017; Lopez-Paz and Ranzato, 2017; De Lange et al., 2021; Wang et al., 2022a; Niu et al., 2022). This issue arises when a neural network, after being trained on a sequence of domains, experiences a significant degradation in its performance on previously learned domains as it continues to learn new domains. Continual learning, also known as lifelong learning, addresses this problem. Recent continual domain adaptation methods have made significant progress by employing gradient regularization, random parameter restoration, buffer sample mixture, and more. Although the CF problem is proposed in the continual learning field, it can occur in any source-free OOD settings since the degradation caused by CF is attributed to the network’s parameters being updated to optimize performance on new domains, which may interfere with the representations learned for previous domains. C.4 A CTIVE DOMAIN ADAPTATION Active Domain Adaptation (ADA) (Prabhu et al., 2021; Ning et al., 2021; Su et al., 2020; Ma et al., 2021; Xie et al., 2022) extends semi-supervised domain adaptation with active learning strate- gies (Cohn et al., 1996; Settles, 2009), aiming to maximize target domain performance with a limited annotation budget. Therefore, the key challenge of active learning algorithms is selecting the most informative unlabeled data in target domains (Kapoor et al., 2007). Sample selection strategies are of- ten based on uncertainty (Lewis and Catlett, 1994; Scheffer et al., 2001), diversity (Jain and Grauman, 2016; Hoi et al., 2009), representativeness (Xu et al., 2003), expected error minimization (Vijaya- narasimhan and Kapoor, 2010), etc. Among these methods, uncertainty and diversity-based methods are simple and computationally efficient, making them the most suitable choices to tailor for TTA settings. Adapting these strategies is non-trivial because, compared to typical active domain adaptation, our proposed Active Test-time Adaptation (ATTA) setting does not provide access to source data, model parameters, or pre-collected target samples. This requirement demands that our active sample selection algorithm select samples for annotation during data streaming. Consequently, this active sampling selection process is non-regrettable, i.e., we can only meet every sample once in a short period. To avoid possible confusion, compared to the recent Source-free Active Domain Adaptation (SFADA) method SALAD (Kothandaraman et al., 2023), we do not require access to model parameter gradients, training additional neural networks, or pre-collected target datasets. Therefore, our ATTA setting is quite different, much lighter, and more realistic than ADA and SFADA. C.5 A CTIVE ONLINE LEARNING The most related branch of active online learning (AOL) (Cacciarelli and Kulahci, 2023) is active online learning on drifting data stream (Zhou et al., 2021; Baier et al., 2021; Li et al., 2021a). Generally, these methods include two components, namely, detection and adaptation. Compared with ATTA, there are several distinctions. First, this line of studies largely focuses on the distribution shift detection problem, while ATTA focuses on multi-domain adaptations. Second, AOL on drifting data stream aims to detect and adapt to one current distribution in the stream, without considering preserving the adaptation abilities of multiple past distributions by maintaining and fine-tuning the original pre-trained models. In contrast, ATTA’s goal is to achieve the OOD generalization optimums adaptable across multiple source and target distributions, leading to the consideration of CF problems. Third, while AOL requires one-by-one data input and discard, ATTA maintains a buffer for incoming data before selection decisions. This is because ATTA targets maintaining the original model without corrupting and replacing it, such that making statistically meaningful and high-quality decisions is 22Published as a conference paper at ICLR 2024 critical for ATTA. In contrast, AOL allows resetting and retraining new models, whose target is more lean to cost saving and one-by-one manner. D F URTHER THEORETICAL STUDIES In this section, we refine the theoretical studies with supplement analysis and further results. We use the H-divergence and H∆H-distance definitions following (Ben-David et al., 2010). Definition 2 (H-divergence). For a function class H and two distributions D1 and D2 over a domain X, the H-divergence between D1 and D2 is defined as dH(D1, D2) = sup h∈H |Px∼D1 [h(x) = 1] − Px∼D2 [h(x) = 1]|. The H∆H-distance is defined base on H-divergence. We use the H∆H-distance definition follow- ing (Ben-David et al., 2010). Definition 3 (H∆H-distance). For two distributions D1 and D2 over a domain X and a hypothesis class H, the H∆H-distance between D1 and D2 w.r.t. H is defined as dH∆H(D1, D2) = sup h,h′∈H Px∼D1 [h(x) ̸= h′(x)] + Px∼D2 [h(x) ̸= h′(x)]. (9) The H∆H-distance essentially provides a measure to quantify the distribution shift between two distributions. It measures the maximum difference of the disagreement between two hypotheses in H for two distributions, providing a metrics to quantify the distribution shift between D1 and D2. H-divergence and H∆H-distance have the advantage that they can be applied between datasets, i.e., estimated from finite samples. Specifically, let S1, S2 be unlabeled samples of size m sampled from D1 and D2; then we have estimated H∆H-distance ˆdH(S1, S2). This estimation can be bounded based on Theorem 3.4 of Kifer et al. (2004), which we state here for completeness. Theorem 5. Let A be a collection of subsets of some domain measure space, and assume that the VC-dimension is some finite d. Let P1 and P2 be probability distributions over that domain and S1, S2 finite samples of sizes m1, m2 drawn i.i.d. according P1, P2 respectively. Then Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16, (10) where Pm1+m2 is the m1 + m2’th power of P - the probability that P induces over the choice of samples. Theorem 5 bounds the probability for relativized discrepancy, and its applications in below lemmas and Theorem 1 help us bound the quantified distribution shifts between domains. The probability, according to a distribution D, that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source domain dataset is inaccessible under ATTA settings, we consider the existence of the source dataset DS for the purpose of accurate theoretical analysis. Thus, we initialize Dtr(0) as DS, i.e., Dtr(0) = DS. For every time step t, the test and training data can be expressed as Ute(t) and Dtr(t) = DS ∪ Dte(1) ∪ Dte(2) ∪ ··· ∪Dte(t). (11) We use N to denote the total number of samples in Dtr(t) and λ = (λ0, λ1, ··· , λt) to represent the ratio of sample numbers in each component subset. In particular, we have |DS| |Dtr(t)| = λ0, |Dte(1)| |Dtr(t)| = λ1, ··· , |Dte(t)| |Dtr(t)| = λt, (12) where Pt i=0 λi = 1. Therefore, at time step t, the model has been trained on labeled data Dtr(t), which contains t + 1 components consisting of a combination of data from the source domain and multiple test-time domains. For each domain the model encounters, DS, Ute(1), Ute(2), ··· , Ute(t), let ϵj(h(t)) denote the error of hypothesis h at time t on the jth domain. Specifically, ϵ0(h(t)) = ϵS(h(t)) represents the error of h(t) on the source data DS, and ϵj(h(t)) for j ≥ 1 denotes the error of h(t) on test data Ute(j). Our optimization minimizes a convex combination of training error over the labeled samples from all domains. Formally, given the vector w = (w0, w1, ··· , wt) of domain error 23Published as a conference paper at ICLR 2024 weights with Pt j=0 wj = 1 and the sample number from each component Nj = λjN, we minimize the empirical weighted error of h(t) as ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj Nj X Nj |h(x, t) − g(x)|. (13) Note that w, λ and N are also functions of t, which we omit for simplicity. We now establish two lemmas as the preliminary for Theorem 1. In the following lemma, we bound the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)). Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. In the following lemma, we provide an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. The proofs for both lemmas are provided in Appx. E. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Lemma 6 bounds the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)), which is majorly influenced by the estimatedH∆H-distance and the quality of discrepancy estimation. During the ATTA process, the streaming test data can form multiple domains and distributions. However, if we consider all data during the test phase as a single test domain,i.e., St i=1 Ute(i), we can simplify Lemma 6 to obtain an upper bound for the test error ϵT as |ϵw(h(t)) − ϵT (h(t))| ≤w0  1 2 ˆdH∆H(S0, ST ) + 2 s 2d log(2m) + log 2 δ m + γ  , (14) where γ = min h∈H{ϵ0(h(t)) + ϵT (h(t))}, and ST is sampled from St i=1 Ute(i). To understand Lamma 7, we need to understand Hoeffding’s Inequality, which we state below as a Proposition for completeness. Proposition 8 (Hoeffding’s Inequality). Let X be a set, D1, . . . , Dt be probability distributions on X, and f1, . . . , ft be real-valued functions on X such that fi : X → [ai, bi] for i = 1, . . . , t. Then for any ϵ >0, P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! (15) where E[fi(x)] is the expected value of fi(x). Lamma 7 provides an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Theorem 1 essentially bounds the performance of ATTA on the source and each test domains. The adaptation performance on a test domain is majorly 24Published as a conference paper at ICLR 2024 bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. If we consider the multiple data distributions during the test phase as a single test domain, i.e., St i=1 Ute(i), Theorem 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . With the optimal test/source hypothesis h∗ T (t) = arg min h∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (16.a), with approximately B = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (17) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. When the budget is sufficiently large or the source data amount is not sufficiently large compared to the distribution shift A, the optimal w∗ 0 for the test error bound is w∗ 0 = 0, i.e., using no source data since possible error reduction from the data addition is always less than the error increase caused by large divergence between the source data and the test data. Theorem 2 offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Following Theorem 1, when no active learning is included during TTA,i.e., w0 = λ0 = 1, the upper boundw0A+ q w2 0 λ0 + (1−w0)2 1−λ0 B ≥ A+B; when enabling ATTA, withw0 = λ0 ̸= 1, we can easily achieve an upper bound w0A + B < A+ B. Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. Entropy quantifies the amount of information contained in a probability distribution. In the context of a classification model, lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction. When a model assigns low entropy to a sample, this high confidence can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model “recognizes” the sample as being similar to those it was trained on, hence the high confidence in its prediction. While entropy is not a direct measure of distributional distance, it can be used as an indicator of how closely a sample aligns with the model’s learned distribution. This interpretation is more about model confidence and the implied proximity rather than a strict mathematical measure of distributional distance. The pre-trained model is well-trained on abundant source domain data, and thus the model distribution is approximately the source distribution. Selecting low-entropy samples using essentially provides an estimate of sampling from the source dataset. Thus, Dϕ,S(t), based on well-aligned with the model’s learned distribution is an approximation of DS. When we consider the CF problem and feasibly include the source-like dataset Dϕ,S(t) into the ATTA training data in place of the inaccessible DS in Eq. (11), we can also derive bounds on the domain errors under this practical ATTA setting when minimizing the empirical weighted errorϵ′ w(h(t)) using the hypothesis h at time t, similar to Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domainsDϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is 25Published as a conference paper at ICLR 2024 N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵ′ w(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Other derived results following Theorem 1 also apply for this practical ATTA setting. Further empirical validations for our theoretical results are provided in Appx. H. E P ROOFS This section presents comprehensive proofs for all the lemmas, theorems, and corollaries mentioned in this paper, along with the derivation of key intermediate results. Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Proof. First we prove that given unlabeled samples of size m S1, S2 sampled from two distributions D1 and D2, we have dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (18) We start with Theorem 3.4 of Kifer et al. (2004): Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16. (19) In Eq. 19, ’d’ is the VC-dimension of a collection of subsets of some domain measure space A, while in our case, d is the VC-dimension of hypothesis space H. Following (Ben-David et al., 2010), the H∆H space is the set of disagreements between every two hypotheses inH, which can be represented as a linear threshold network of depth 2 with 2 hidden units. Therefore, the VC-dimension of H∆H is at most twice the VC-dimension of H, and the VC-dimension of our domain measure space is 2d for Eq. 19 to hold. Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2m)2de−m1ϵ2/16 + (2m)2de−m2ϵ2/16. We rewrite the inequality as δ (2m)2d = e−m1ϵ2/16 + e−m2ϵ2/16; taking the logarithm of both sides, we get log δ (2m)2d = −m1 ϵ2 16 + log(1 +e−(m1−m2) ϵ2 16 ). 26Published as a conference paper at ICLR 2024 Assuming m1 = m2 = m and defining a = ϵ2 16 , we have log δ (2m)2d = −ma + log 2; rearranging the equation, we then get ma + log(δ/2) = 2d log(2m). Now, we can solve for a: a = 2d log(2m) + log 2 δ m . Recall that a = ϵ2 16 , so we get: ϵ = 4√a ϵ = 4 s 2d log(2m) + log 2 δ m . With probability of at least 1 − δ, we have |ϕA(S1, S2) − ϕA(P1, P2)| ≤4 s 2d log(2m) + log 2 δ m ; therefore, dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (20) Now we prove Lemma 6. We use the triangle inequality for classification error in the derivation. For the domain error of hypothesis h at time t on the jth domain ϵj(h(t)), given the definition of ϵw(h(t)), |ϵw(h(t)) − ϵj(h(t))| = | tX i=0 wiϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi|ϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(|ϵi(h(t)) − ϵi(h(t), h∗ i (t))| + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + |ϵj(h(t), h∗ i (t)) − ϵj(h(t))|) ≤ tX i=0 wi(ϵi(h∗ i (t)) + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + ϵj(h∗ i (t))) ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|), where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. By the definition of H∆H-distance and our proved Eq. 20, |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| ≤sup h,h′∈H |ϵi(h(t), h′(t)) − ϵj(h(t), h′(t))| = sup h,h′∈H Px∼Di[h(x) ̸= h′(x)] + Px∼Dj [h(x) ̸= h′(x)] = 1 2dH∆H(Di, Dj) ≤ 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m , 27Published as a conference paper at ICLR 2024 where Di, Dj denote the ith and jth domain. Therefore, |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|) ≤ tX i=0 wi(γi + 1 2dH∆H(Di, Dj)) ≤ tX i=0 wi(γi + 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m ). Since ϵi(h(t)) − ϵj(h(t)) = 0 when i = j, we derive |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. This completes the proof. Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Proof. We apply Hoeffding’s Inequality in our proof: P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! . (21) In the jth domain, there are λjN samples. With the true labeling function g(x), for each of the λjN samples x, let there be a real-valued function fi(x) fi(x) = wj λj |h(x, t) − g(x)|, where fi(x) ∈ [0, wj λj ]. Incorporating all the domains, we get ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj λjN X λjN |h(x, t) − g(x)| = 1 N tX j=0 λjNX i=1 fi(x), which corresponds to the 1 t Pt i=1 fi(x) part in Hoeffding’s Inequality. Due to the linearity of expectations, we can calculate the sum of expectations as 1 N tX j=0 λjNX i=1 E[fi(x)] = 1 N ( tX j=0 λjN wj λj ϵj(h(t))) = tX j=0 wjϵj(h(t)) = ϵw(h(t)), which corresponds to the 1 t Pt i=1 Ex∼Di[fi(x)] part in Hoeffding’s Inequality. Therefore, we can apply Hoeffding’s Inequality as P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2N2ϵ2/( NX i=0 range2(fi(x))) ! = 2 exp   −2N2ϵ2/( tX j=0 λjN(wj λj )2) ! = 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . This completes the proof. 28Published as a conference paper at ICLR 2024 Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. Proof. First we prove that for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (22) We apply Theorem 3.2 of Kifer et al. (2004) and Lemma 7, P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . We rewrite the inequality as δ (2N)d = e −2Nϵ2/(Pt j=0 w2 j λj ) , taking the logarithm of both sides, we get log δ (2N)d = −2Nϵ2/( tX j=0 w2 j λj ). Rearranging the equation, we then get ϵ2 = ( tX j=0 w2 j λj )d log(2N) − log(δ) 2N . Therefore, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (23) 29Published as a conference paper at ICLR 2024 Based on Eq. 23, we now prove Theorem 1. For the empirical domain error of hypothesis h at time t on the jth domain ϵj(ˆh(t)), applying Lemma 6, Eq. 23, and the definition of h∗ j (t), we get ϵj(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ j (t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵj(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   = ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k where k > 0, we have the assumption that k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H(D(k′), Ute(t + k)) ≤ δD. Here, we slightly abuse the notation D(k′) to represent Ds if k′ = 0 and Ute(k′) if k′ > 0. Then we get ϵt+k(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, St+k) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2( ˆdH∆H(Si, Sk′ ) + ˆdH∆H(Sk′ , St+k)) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   30Published as a conference paper at ICLR 2024 ≤ ˆϵw(h∗ t+k(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵt+k(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   = ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. with probability of at least 1−δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 , γi = minh∈H{ϵi(h(t))+ ϵt+k(h(t))}, and 0 ≤ δD ≪ +∞. This completes the proof. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (24) Proof. From Theorem 1, we can derive the bound for the test error where the test-time data are considered as a single test domain: |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t) = w0( ˆdH∆H(S0, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N ; and we simplify the above equation as |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (25) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Since we have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (26) 31Published as a conference paper at ICLR 2024 where Formula 26 obtains the minimum value if and only if w0 = λ0; when enabling ATTA with any λ0 ̸= 1, we can get EBT (w, λ, N, t) = w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≥ w0A + B, (27) where the minimum value EBT (w, λ, N, t)min = w0A + B can be obtained with condition w0 = λ0 ̸= 1. When no active learning is included, i.e., for weight and sample ratio vectors w′ and λ′, w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, we have EBT (w′, λ′, N, t) = w′ 0A + s w′2 0 λ′ 0 + (1 − w′ 0)2 1 − λ′ 0 B = A + B. (28) Since for EBT (w, λ, N, t)min = w0A + B, w0 < 1 and A, B >0 hold, we derive EBT (w, λ, N, t)min = w0A + B < A+ B = EBT (w′, λ′, N, t). (29) This completes the proof. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Theorem 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Theorem 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Proof. For the empirical source error on DS of hypothesis h at time t, similar to Theorem 1, we apply Lemma 6, Eq. 23 to get ϵS(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ S(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵS(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   32Published as a conference paper at ICLR 2024 = ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. This completes the proof. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Theorem 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (30) Proof. From Theorem 1, considering the test-time data as a single test domain, we can derive the bound for the source error on DS: |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N , where ST is sampled fromSt i=1 Ute(i), γ = minh∈H{ϵ0(h(t))+ϵS(h(t))}, and γ′ = minh∈H{ϵT (h(t))+ ϵS(h(t))}. We have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (31) where the equality and the minimum value are obtained if and only if w0 = λ0. When Dϕ,S(t) is not included,i.e., with the weight and sample ratio vectorsw′ and λ′ s.t. w′ 0 = λ′ 0 = 0, using the empirical gap term B = 2 q d log(2N)−log(δ) 2N , we have EBS(w′, λ′, N, t) = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + s w2 0 λ0 + (1 − w0)2 1 − λ0 B = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B. When Dϕ,S(t) is included with λ0 ̸= 0, EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≤ w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B, 33Published as a conference paper at ICLR 2024 Algorithm 2 INCREMENTAL CLUSTERING (IC) Require: Given previously selected anchors, new unlabeled samples, and the cluster budget as Danc, Unew, and NC . Global anchor weights wanc = (wanc 1 , . . . , wanc |Danc|)⊤. 1: For simplicity, we consider anchor weights wanc as a global vector. 2: function IC(Danc, Unew, NC ) 3: wsp ← Concat(wanc, 1⊤ |Unew|) ▷ Assign all new samples with weight 1. 4: Φ ← Extract the features from the penultimate layer of model f on x ∈ Danc ∪ Unew in order. 5: clusters ← Weighted-K-Means(Φ, wsp, NC) 6: new_clusters ← {clusteri | ∀clusteri ∈ clusters, ∀x ∈ Danc, x /∈ clustersi} 7: Xnew_anchors ← {the closest sample x to the centroid of clusteri | ∀clusteri ∈ new_clusters} 8: Xanchors ← {x ∈ Danc} ∪Xnew_anchors 9: wanc ← Concat(wanc, 0⊤ |Xnew_anchors|) ▷ Initialize new anchor weights. 10: for wanc i ∈ wanc, wanc i ← wanc i + # sample of clusterj # anchor in clusterj , wanc i ∈ clusterj ▷ Weight accumulation. 11: Return Xanchors 12: end function where the minimum value can be obtained with condition w0 = λ0 ̸= 0. In practical learning scenarios, we generally assume adaptation tasks are solvable; therefore, there should be a prediction function that performs well on two distinct domains. In this case, γ and γ′ should be relatively small, so we can assume γ ≈ γ′. If ˆdH∆H(S0, SS) < ˆdH∆H(SS, ST ), then we have EBS(w, λ, N, t)min = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B < ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B = EBS(w′, λ′, N, t). Therefore, we derive EBS(w, λ, N, t)min < EBS(w′, λ′, N, t). (32) This completes the proof. F I NCREMENTAL CLUSTERING F.1 A LGORITHM DETAILS We provide the detailed algorithm for incremental clustering as Alg. 2. F.2 V ISUALIZATION To better illustrate the incremental clustering algorithm, we provide visualization results on PACS to demonstrate the process. As shown in Fig. 3, the initial step of IC is a normal K-Means clustering step, and ten anchors denoted as \"X\" are selected. The weights of all samples in a clusters is aggregated into the corresponding anchor’s weight. Therefore, these ten samples (anchors) are given larger sizes visually (i.e., larger weights) than that of other new test samples in the first IC step (Fig. 4). During the first IC step, several distributions are far away from the existed anchors and form clusters 1,7,9 and 10, which leads to 4 new selected anchors. While the number of cluster centroid is only increased by 1, 4 of the existing anchors are clustered into the same cluster 8 (purple). Thus IC produces 4 new anchors instead of 1. Similarly, in the second IC step (Fig. 5), the new streaming-in test samples introduce a new distribution; IC produces 3 new clusters (4, 8, and 11) and the corresponding number of anchors to cover them. The number of centroid is only increased by 1, which implies that there are two original-cluster-merging events. More IC step visualization results are provided in Fig. 6 and 7. 34Published as a conference paper at ICLR 2024 Figure 3: Initial IC step: normal clustering. Left: Clustering results. Right: Selecting new anchors. Figure 4: The first IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 5: The second IC step. Left: Weighted clustering results. Right: Selecting new anchors. 35Published as a conference paper at ICLR 2024 Figure 6: The third IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 7: The fourth IC step. Left: Weighted clustering results. Right: Selecting new anchors. 36Published as a conference paper at ICLR 2024 G E XPERIMENT DETAILS In this section, we provide more experimental details including the details of the datasets and training settings. G.1 D ETAILS ABOUT THE DATASETS We adopt datasets PACS, VLCS, and Office-Home from DomainBed (Gulrajani and Lopez-Paz, 2020) with the same domain splits. All available licenses are mentioned below. • PACS (Li et al., 2017) includes four domains: art, cartoons, photos, and sketches. PACS is a 7-class classification dataset with 9,991 images of dimension (3, 224, 224). • VLCS (Fang et al., 2013) contains photographic domains: Caltech101, LabelMe, SUN09, and VOC2007. This dataset includes 10,729 images of dimension (3, 224, 224) with 5 classes. • Office-Home (Venkateswara et al., 2017) is a 65-class dataset, including domains: art, clipart, product, and real. VLCS includes 10,729 images of dimension (3, 224, 244). (License) • Tiny-ImageNet-C is a 200-class dataset, including 15 corrupt types. Tiny-ImageNet-C includes 150,000 images of dimension (3, 224, 244). Since the class number 200 is less than ImageNet (1000), the model’s last layer classifier needs to be adapted. In this work, we use the brightness corruption domain to adapt. In the source pretraining phase, we adopt the most ImageNet-like domain as our source domain. For PACS and Office-Home, we use domains \"photos\" and \"real\" as the source domains, respectively, while for VLCS, Caltech101 is assigned to apply the source pretraining. We freeze the random seeds to generate the sample indices order for the two test data streams, namely, the domain-wise data stream and the random data stream. For PACS, the domain-wise data stream inputs samples from domain art, cartoons, to sketches, while we shuffle all samples from these three domains in the random data stream. For VLCS, we stream the domains in the order: LabelMe, SUN09, and VOC2007, as the domain-wise data stream. For Office-Home, the domain-wise data stream order becomes art, clipart, and product. G.2 T RAINING AND OPTIMIZATION SETTINGS In this section, we extensively discuss the model architectures, optimization settings, and method settings. G.2.1 A RCHITECTURES PACS & VLCS. We adopt ResNet-18 as our model encoder followed by a linear classifier. The initial parameters of ResNet-18 are ImageNet pre-trained weights. In our experiment, we remove the Dropout layer since we empirically found that using the Dropout layer might degrade the optimization process when the sample number is small. The specific implementation of the network is closely aligned with the implementation in DomainBed (Gulrajani and Lopez-Paz, 2020). Office-Home. We employ ResNet-50 as our model encoder for Office-Home. Except for the architecture, the other model settings are aligned with the ResNet-18. Tiny-ImageNet-C ResNet-18 is adapted from ImageNet to Tiny-ImageNet-C by training the last linear layer. G.2.2 T RAINING & OPTIMIZATION In this section, we describe the training configurations for both the source domain pre-training and test-time adaptation procedures. Source domain pre-training. For the PACS and VLCS datasets, models are fine-tuned on the selected source domains for 3,000 iterations. The Adam optimizer is utilized with a learning rate 37Published as a conference paper at ICLR 2024 of 10−4. In contrast, for the Office-Home dataset, the model is fine-tuned for a longer duration of 10,000 iterations with a slightly adjusted learning rate of 5 × 10−5. Test-time adaptation. For test-time adaptation across PACS and VLCS, the pre-trained source model is further fine-tuned using the SGD optimizer with a learning rate of 10−3. While on Office-Home and Tiny-ImageNet-C, a learning rate of 10−4 is adopted. For all TTA baselines, barring specific exceptions, we faithfully adhere to the original implementation settings. A noteworthy exception is the EATA method, which requires a cosine similarity threshold. The default threshold of the original EATA implementation was not suitable for the three datasets used in our study, necessitating an adjustment. We empirically set this threshold to 0.5 for training. Unlike Tent and SAR, which only require the optimization of batch normalization layers (Santurkar et al., 2018), SimATTA allows the training of all parameters in the networks. In experiments, we use a tolerance count (tol) to control the training process. SimATTA will stop updating once the loss does not descrease for more than 5 steps. However, for Tiny-ImageNet-C, SimATTA uses ‘steps=10‘ for time comparisons since other methods apply at most 10 steps. G.2.3 M ETHOD SETTINGS Tent. In our experiments, we apply the official implementation of Tent1. Specifically, we evaluate Tent with 1 test-time training step and 10 steps, respectively. EATA.Our EATA implementation follows its official code2. In our experiments, EATA has 2000 fisher training samples, E0 = 0.4 × log(# class), ϵ <0.5. CoTTA. For CoTTA, we strictly follow all the code and settings from its official implementation3. SAR. With SAR’s official implementation4, we set E0 = 0 .4 × log(# class) and e0 = 0 .1 in our experiments. ADA baselines. For ADA baselines, we follow the architecture of the official implementation of CLUE (Prabhu et al., 2021)5. SimATTA Implementation. Our implementation largely involves straightforward hyperparameter settings. The higher entropy bound eh = 10−2 should exceed the lower entropy bound el, but equal values are acceptable. Empirically, the lower entropy bound el can be set to 10−3 for VLCS and Office-Home, or 10−4 for PACS. The choice of el is largely dependent on the number of source-like samples obtained. A lower el may yield higher-accuracy low-entropy samples, but this could lead to unstable training due to sample scarcity. Though experimentation with different hyperparameters is encouraged, our findings suggest that maintaining a non-trivial number of low-entropy samples and setting an appropriateλ0 are of primary importance. If λ0 < 0.5, CF may ensue, which may negate any potential improvement. Regarding the management of budgets, numerous strategies can be adopted. In our experiments, we utilized a simple hyperparameter k, varying from 1 to 3, to regulate the increasing rate of budget consumption. This strategy is fairly elementary and can be substituted by any adaptive techniques. G.3 S OFTWARE AND HARDWARE We conduct our experiments with PyTorch (Paszke et al., 2019) and scikit-learn (Pedregosa et al., 2011) on Ubuntu 20.04. The Ubuntu server includes 112 Intel(R) Xeon(R) Gold 6258R CPU @2.70GHz, 1.47TB memory, and NVIDIA A100 80GB PCIe graphics cards. The training process costs graphics memory less than 10GB, and it requires CPU computational resources for scikit-learn K-Means clustering calculations. Our implementation also includes a GPU-based PyTorch K-Means method for transferring calculation loads from CPUs to GPUs. However, for consistency, the results of our experiments are obtained with the original scikit-learn K-Means implementation. 1https://github.com/DequanWang/tent 2https://github.com/mr-eggplant/EATA 3https://github.com/qinenergy/cotta 4https://github.com/mr-eggplant/SAR 5https://github.com/virajprabhu/CLUE 38Published as a conference paper at ICLR 2024 Figure 8: Target loss surface on 2000 samples without source pre-training. The red points denote the loss minimum for a fixed λ0. The orange line denote the place where w0 = λ0. Figure 9: Target loss surface on 2000 samples with source pre-training. H E MPIRICAL VALIDATIONS FOR THEORETICAL ANALYSIS In this section, we undertake empirical validation of our learning theory, which encompasses multiple facets awaiting verification. In contemporary computer vision fields, pre-trained models play a pivotal role, and performance would significantly decline without the use of pre-trained features. The learning theory suggests that given the vast VC-dimension of complete ResNets, without substantial data samples, the training error cannot be theoretically tight-bounded. However, we show empirically in the following experiments that fine-tuning pre-trained models is behaviorally akin to training a model with a low VC-dimension. Training on 2000 Samples Without Source Domain Pre-training. For an ImageNet pre-trained ResNet-18 model, we trained it using 2000 samples from the PACS dataset. To ascertain the optimal value w∗ 0 in Equation 4, we trained multiple models for different w0 and λ0 pairings. For each pair, we derived the target domain loss (from art, cartoons, and sketches) post-training and plotted this loss on the z-axis. With w0 and λ0 serving as the xy-axes, we drafted the target domain loss ϵT surface in Figure 8. As the results show, given a λ0, the optimal w∗ 0 typically aligns with the line λ0 = w0, with a slight downward shift, which aligns with Equation 4. 39Published as a conference paper at ICLR 2024 Figure 10: Target loss surface on 500 samples with source pre-training. Figure 11: Source loss surface on 500 samples with source pre-training. 40Published as a conference paper at ICLR 2024 Figure 12: Target and source loss surface on 500 samples with source pre-training. Table 6: TTA comparisons on Office-Home. This table includes the two data stream settings mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. Office-Home Domain-wise data stream Post-adaptation Random data stream Post-adaptation R →A→ →C→ →P R A C P 1 2 3 4 R A C P BN w/o adapt 93.78 42.93 37.62 59.90 93.78 42.93 37.62 59.90 46.82 46.82 46.82 46.82 93.78 42.93 37.62 59.90BN w/ adapt 92.38 49.69 39.43 63.53 92.38 49.69 39.43 63.53 50.88 50.88 50.88 50.88 92.38 49.69 39.43 63.53 Tent (steps=1) N/A 49.61 39.31 63.87 92.47 49.57 39.89 63.89 49.95 50.27 50.23 52.06 92.40 49.24 39.68 63.98Tent (steps=10) N/A 49.61 39.04 61.41 87.08 44.79 38.37 60.49 50.05 49.31 48.74 47.79 85.31 42.85 37.89 58.71EATA N/A 49.65 39.04 63.53 91.60 49.61 38.65 63.48 49.73 50.27 49.45 51.07 91.05 49.11 38.26 62.99CoTTA N/A 49.61 38.76 61.84 87.81 44.95 35.92 59.04 49.84 49.84 48.95 50.43 86.99 43.68 34.73 57.56SAR (steps=1) N/A 49.65 39.24 63.53 92.45 49.73 39.36 63.69 49.84 50.05 49.91 51.67 92.38 49.57 39.50 63.87SAR (steps=10) N/A 49.53 38.81 61.50 88.94 46.15 37.04 59.41 50.09 50.30 49.77 49.22 89.14 46.23 36.31 59.45 SimATTA (B ≤300) N/A 56.20 48.38 71.66 95.75 60.07 52.62 74.70 58.57 60.88 62.91 63.67 95.89 62.01 54.98 74.70SimATTA (B ≤500) N/A 58.71 51.11 74.36 96.03 62.05 57.41 76.98 58.85 62.63 63.41 64.31 95.91 63.78 57.87 77.09 Training on 2000 Samples with Source Domain Pre-training. To further assess the effects of source pre-training, we repeated the same experiment on a source pre-trained ResNet-18. The results are depicted in Figure 9. This experiment provides empirical guidance on selecting w0 in source domain pre-trained situations. The findings suggest that the optimal w∗ 0 non-trivially shifts away from the line λ0 = w0 towards lower-value regions. Considering the source pre-training process as using a greater quantity of source domain samples, it implies that when the number of source samples greatly exceeds target samples, a lower w0 can enhance target domain results. Training on 500 Samples with Source Domain Pre-training. We proceed to fine-tune the source domain pre-trained ResNet-18 using only 500 samples, thereby simulating active TTA settings. We train models with various w0 and λ0 pairings, then graph the target domain losses, source domain losses, and the combined losses. As shown in Figure 10, the target losses still comply with our theoretical deductions where the local minima are close to the line λ0 = w0 and marginally shift towards lower values. Considering the challenge of CF, the source domain results in Figure 11 suggest a reverse trend compared to the target domain, where lower λ0 and w0 values yield superior target domain results but inferior source domain results. Thus, to curb CF, the primary strategy is to maintain a relatively higher λ0. When considering both target and source domains, a balance emerges as depicted in Figure 12. The global minimum is located in the middle region, demonstrating the trade-off between the target domain and source domain performance. I A DDITIONAL EXPERIMENT RESULTS In this section, we provide additional experiment results. The Office-Home results and ablation studies will be presented in a similar way as the main paper. In the full results Sec. I.3, we will post more detailed experimental results with specific budget numbers and intermediate performance during the test-time adaptation. 41Published as a conference paper at ICLR 2024 Table 7: Comparisons to ADA baselines on Office-Home. The source domain is denoted as \"(S)\" in the table. Results are average accuracies with standard deviations). Office-Home R (S) A C P Random (B = 300) 95.04 (0.20) 57.54 (1.16) 53.43 (1.17) 73.46 (0.97) Entropy (B = 300) 94.39 (0.49) 61.21 (0.71) 56.53 (0.71) 72.31 (0.28) Kmeans (B = 300) 95.09 (0.14) 57.37 (0.90) 51.74 (1.34) 71.81 (0.39) CLUE (B = 300) 95.20 (0.23) 60.18 (0.98) 58.05 (0.43) 73.72 (0.70) Ours (B ≤300) 95.82 (0.07) 61.04 (0.97) 53.80 (1.18) 74.70 (0.00) I.1 R ESULTS ON OFFICE -HOME We conduct experiments on Office-Home and get the test-time performances and post-adaptation performances for two data streams. As shown in Tab. 6, SimATTA can outperform all TTA baselines with huge margins. Compared to ADA baselines under the source-free settings, as shown in Tab. 7, SimATTA obtains comparable results. I.2 A BLATION STUDIES Figure 13: Ablation study on PACS and VLCS.\"IC=0\" denotes removing incremental clustering (IC) selection. \"LE=0\" denotes removing the low-entropy (LE) sample training. Domain-wise stream and random stream are applied on first and second rows, respectively. The accuracy values are averaged across all splits/domains. In this section, we explore three variations of our method to examine the individual impacts of its components. The first variant replaces the incremental clustering selection with entropy selection, 42Published as a conference paper at ICLR 2024 where only the samples with the highest entropy are chosen. The second variant eliminates low- entropy sample training. The third variation combines the first and second variants. We perform this ablation study on the PACS and VLCS as outlined in Fig. 13. We denote the use of incremental clustering (IC) and low-entropy training (LE) respectively as IC=1 and LE=1. The experiments essentially reveals the effectiveness of incremental clustering and low-entropy- sample training. As we have detailed in Sec. 3.2, these techniques are designed to to select informative samples, increase distribution coverage, and mitigate catastrophic forgetting. These designs appositely serve the ATTA setting where the oracle has costs and the budget is limited. Therefore, their effectiveness is prominent particularly when the budget is small. As the results show, when the budget B ≤100 or B ≤300, removing the components observably impairs performances. When B gets large, more active samples cover a larger distribution; thus the performance gap from random selection and informative selection gets smaller. In the extreme case where B → ∞, all samples are selected and thus the superiority of our meticulously-designed techniques are not manifested. Specifically, our analysis yields several insights. First, SimATTA (LE=1, IC=1) comprehensively outperforms other variants on both datasets, different streams, and different budgets. Second, variants without low-entropy training (LE=0, IC=0/1) easily fail to produce stable results (e.g., domain-wise stream in VLCS). Third, SimATTA’s performance surpasses this variant on PACS’s domain-wise stream clearly especially when the budgets are low. This indicates these variants fail to retrieve the most informative style shift (PACS’s shifts) samples, which implies the advantage of incremental clustering when the budget is tight. In addition, these results show that IC has its unique advantage on domain-wise streams where distributions change abruptly instead of random streams. Therefore, compared to PACS’s domain- wise stream results, the reason for the smaller performance improvement of SimATTA over the variant (LE=1, IC=0) on VLCS’s domain-wise stream is that images in VLCS are all photos that do not include those severe style shifts in PACS (i.e., art, cartoons, and sketches). That is, when the shift is not severe, we don’t need IC to cover very different distributions, and selecting samples using entropy can produce good results. In brief, IC is extraordinary for severe distribution shifts and quick adaptation. It is worth mentioning that low budget comparison is essential to show the informative sample retrieval ability, since as the budget increases, all AL techniques will tend to perform closely. I.3 C OMPLETE EXPERIMENT RESULTS We provide complete experimental results in this section. As shown in Tab. 8, we present the full results for two data streams. The test-time adaptation accuracies are shown in the \"Current domain\" row, while the \"Budgets\" row denotes the used budget by the end of the domain. The rest four rows denote the four domain test results by the end of the real-time adaptation of the current domain, where the first column results are the test accuracy before the test-time adaptation phase. N/A represents \"do not apply\". Table 8: Tent (steps=1) on PACS. Tent (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.29 64.59 44.67 56.35 54.09 51.83 48.58 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.38 97.60 98.56 98.08 97.72 97.19 A 59.38 69.09 68.95 66.85 68.07 67.33 65.58 63.53 C 28.03 64.04 65.19 64.08 64.85 65.19 62.97 60.75 S 42.91 53.65 47.39 42.58 54.57 49.83 44.13 41.56 J C HALLENGES AND PERSPECTIVES Despite advancements, test-time adaptation continues to pose considerable challenges. As previously discussed, without supplementary information and assumptions, the ability to guarantee model generalization capabilities is limited. However, this is not unexpected given that recent progress 43Published as a conference paper at ICLR 2024 Table 9: Tent (steps=10) on PACS. Tent (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.38 57.85 20.23 47.36 31.01 22.84 20.33 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 95.45 87.43 62.63 93.83 81.32 65.39 50.78 A 59.38 64.94 55.03 34.52 55.32 40.28 28.27 23.68 C 28.03 55.89 56.70 40.57 54.52 39.68 27.22 20.95 S 42.91 36.96 26.27 13.59 32.25 23.16 20.95 19.62 Table 10: EATA on PACS. EATA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.04 64.72 50.27 57.31 56.06 58.17 59.78 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.62 98.50 98.62 98.68 98.62 98.50 98.62 A 59.38 68.90 68.16 66.50 68.65 68.95 69.34 69.63 C 28.03 63.74 65.36 62.46 65.19 66.00 65.57 65.70 S 42.91 54.01 52.89 48.18 55.71 55.64 54.09 54.26 Table 11: CoTTA on PACS. CoTTA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 65.48 62.12 53.17 56.06 54.33 57.16 57.42 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.62 98.62 98.62 98.62 98.56 98.62 A 59.38 65.82 65.87 65.48 66.02 65.87 66.31 65.97 C 28.03 62.63 63.05 63.10 63.01 62.88 63.01 62.97 S 42.91 53.88 54.03 53.78 54.67 55.31 55.10 54.62 Table 12: SAR (steps=1) on PACS. SAR (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 66.75 63.82 49.58 56.78 56.35 56.68 56.70 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.50 98.32 98.74 98.56 98.50 98.44 A 59.38 68.02 68.07 66.94 67.87 68.65 68.55 68.16 C 28.03 62.84 64.97 62.93 63.82 64.89 64.46 64.38 S 42.91 53.47 52.07 45.74 54.92 55.46 53.68 52.53 Table 13: SAR (steps=10) on PACS. SAR (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 69.38 68.26 49.02 53.51 51.15 51.78 45.60 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.20 95.39 96.47 97.13 97.78 97.72 94.13 A 59.38 72.36 66.60 62.16 62.74 64.94 66.11 56.64 C 28.03 63.44 68.30 56.19 59.77 61.73 62.03 56.02 S 42.91 53.37 44.59 54.62 41.00 49.66 48.79 36.37 44Published as a conference paper at ICLR 2024 Table 14: SimATTA (B ≤300) on PACS. SimATTA (B ≤300) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 76.86 70.90 75.39 69.47 76.49 82.45 82.22 Budgets N/A 75 145 223 66 142 203 267 P 99.70 98.44 98.86 98.80 97.96 98.68 99.04 98.98 A 59.38 80.71 82.32 84.47 73.97 80.52 81.10 84.91 C 28.03 48.12 82.00 82.25 72.35 81.06 83.36 83.92 S 42.91 32.78 56.25 81.52 79.49 83.10 84.78 86.00 Table 15: SimATTA (B ≤500) on PACS. SimATTA (B ≤500) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 77.93 76.02 76.30 68.46 78.22 80.91 85.49 Budgets N/A 121 230 358 102 221 343 425 P 99.70 98.92 98.86 98.62 98.20 99.46 99.10 99.16 A 59.38 87.01 87.60 88.33 73.39 79.20 84.91 86.67 C 28.03 54.78 83.96 83.49 68.43 74.40 84.22 84.77 S 42.91 46.37 63.53 83.74 81.34 81.04 86.66 87.71 Table 16: Tent (steps=1) on VLCS. Tent (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 38.55 34.40 53.88 44.85 44.29 47.38 44.98 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.81 85.44 84.73 84.95 85.16 85.80 85.30 L 33.55 40.02 43.11 43.86 39.68 41.98 43.11 43.49 S 41.10 33.39 35.41 33.61 36.29 37.90 38.27 37.81 V 49.08 53.20 54.06 53.11 53.76 54.18 53.76 53.35 Table 17: Tent (steps=10) on VLCS. Tent (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 45.41 31.44 32.32 46.13 42.31 43.51 39.48 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 73.07 48.34 42.54 74.13 62.19 56.54 52.01 L 33.55 46.61 38.44 37.65 44.88 45.93 43.41 40.32 S 41.10 31.75 28.82 27.79 35.37 36.14 35.28 33.64 V 49.08 48.05 40.14 33.12 50.50 44.49 42.48 40.37 Table 18: EATA on VLCS. EATA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.24 33.15 52.58 43.77 42.48 43.34 41.55 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 85.16 85.02 84.10 84.73 84.52 84.10 83.32 L 33.55 37.16 37.24 37.69 37.09 36.78 36.90 36.67 S 41.10 33.39 33.49 32.39 33.33 32.54 31.84 31.47 V 49.08 51.87 52.16 52.49 52.07 52.43 52.64 52.55 45Published as a conference paper at ICLR 2024 Table 19: CoTTA on VLCS. CoTTA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.39 32.54 52.25 43.69 42.14 43.21 42.32 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 81.55 81.98 82.12 82.61 82.47 82.12 81.98 L 33.55 37.20 37.91 37.65 38.48 38.22 38.40 37.99 S 41.10 30.71 32.78 33.12 34.00 33.70 33.97 33.52 V 49.08 52.01 52.64 52.90 53.64 53.14 53.08 53.23 Table 20: SAR (steps=1) on VLCS. SAR (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 36.18 34.43 52.46 43.64 43.04 44.20 41.93 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.31 84.17 83.96 85.09 85.23 85.23 85.09 L 33.55 35.62 38.29 39.72 38.55 39.34 40.21 40.70 S 41.10 33.24 36.41 36.53 34.37 35.62 36.29 36.44 V 49.08 51.75 52.61 52.37 52.90 52.75 53.05 53.02 Table 21: SAR (steps=10) on VLCS. SAR (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 35.32 34.10 51.66 43.56 42.05 42.53 41.16 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 83.96 83.04 82.12 84.03 84.24 85.23 85.09 L 33.55 34.07 35.92 41.49 39.53 38.37 37.65 37.58 S 41.10 31.93 34.89 33.94 35.19 32.94 33.88 33.12 V 49.08 51.33 51.51 53.08 52.78 52.34 51.78 52.01 Table 22: SimATTA (B ≤300) on VLCS. SimATTA (B ≤300) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 62.61 65.08 74.38 62.33 69.33 73.20 71.93 Budgets N/A 79 175 272 71 135 208 262 C 100.00 99.51 98.52 99.93 99.86 99.79 100.00 99.93 L 33.55 68.11 69.92 69.50 62.61 66.64 68.45 69.43 S 41.10 55.24 68.89 66.67 65.54 69.29 71.79 72.46 V 49.08 66.08 70.94 77.34 73.79 76.87 78.82 80.39 Table 23: SimATTA (B ≤500) on VLCS. SimATTA (B ≤500) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 63.52 68.01 76.13 62.29 70.45 73.50 72.02 Budgets N/A 113 266 446 107 203 283 356 C 100.00 99.29 98.59 99.51 99.93 99.86 99.86 99.43 L 33.55 62.95 70.63 70.56 66.57 67.09 67.24 70.29 S 41.10 51.31 73.83 73.10 65.33 71.79 72.91 72.55 V 49.08 59.36 71.65 78.35 73.58 77.84 80.01 80.18 46Published as a conference paper at ICLR 2024 Table 24: Tent (steps=1) on Office-Home. Tent (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.31 63.87 49.95 50.27 50.23 52.06 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.33 92.36 92.47 92.38 92.45 92.45 92.40 A 57.07 49.73 49.73 49.57 49.69 49.73 49.57 49.24 C 44.97 39.27 39.54 39.89 39.45 39.68 39.73 39.68 P 73.15 63.60 63.66 63.89 63.60 63.82 63.93 63.98 Table 25: Tent (steps=10) on Office-Home. Tent (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.04 61.41 50.05 49.31 48.74 47.79 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 91.99 89.14 87.08 92.08 90.80 88.59 85.31 A 57.07 49.94 46.77 44.79 49.44 48.21 45.69 42.85 C 44.97 38.58 39.11 38.37 40.18 40.02 38.63 37.89 P 73.15 63.28 61.03 60.49 64.36 63.64 61.12 58.71 Table 26: EATA on Office-Home. EATA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.04 63.53 49.73 50.27 49.45 51.07 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.36 92.17 91.60 92.38 92.22 91.71 91.05 A 57.07 49.57 49.53 49.61 49.69 49.40 49.36 49.11 C 44.97 39.08 39.01 38.65 39.27 39.01 38.42 38.26 P 73.15 63.42 63.42 63.48 63.51 63.37 63.33 62.99 Table 27: CoTTA on Office-Home. CoTTA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 38.76 61.84 49.84 49.84 48.95 50.43 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 90.38 88.02 87.81 90.48 89.37 88.00 86.99 A 57.07 48.58 45.53 44.95 47.34 46.35 44.62 43.68 C 44.97 36.66 35.58 35.92 37.55 36.40 35.44 34.73 P 73.15 60.40 57.74 59.04 61.12 59.63 58.35 57.56 Table 28: SAR (steps=1) on Office-Home. SAR (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.24 63.53 49.84 50.05 49.91 51.67 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.38 92.31 92.45 92.40 92.36 92.36 92.38 A 57.07 49.65 49.57 49.73 49.69 49.61 49.57 49.57 C 44.97 39.34 39.22 39.36 39.34 39.56 39.47 39.50 P 73.15 63.51 63.51 63.69 63.60 63.71 63.71 63.87 47Published as a conference paper at ICLR 2024 Table 29: SAR (steps=10) on Office-Home. SAR (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.53 38.81 61.50 50.09 50.30 49.77 49.22 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.20 92.06 88.94 92.40 92.47 91.53 89.14 A 57.07 49.40 49.77 46.15 49.81 50.02 48.91 46.23 C 44.97 39.20 38.63 37.04 39.50 39.29 38.65 36.31 P 73.15 63.53 62.69 59.41 64.18 64.18 62.83 59.45 Table 30: SimATTA (B ≤300) on Office-Home. SimATTA (B ≤300) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 56.20 48.38 71.66 58.57 60.88 62.91 63.67 Budgets N/A 75 187 277 79 147 216 278 R 96.44 95.43 95.43 95.75 95.91 95.96 96.01 95.89 A 57.07 57.56 59.50 60.07 58.34 59.91 61.15 62.01 C 44.97 42.25 52.46 52.62 51.66 52.30 54.75 54.98 P 73.15 68.84 70.13 74.70 72.45 73.10 74.50 74.70 Table 31: SimATTA (B ≤500) on Office-Home. SimATTA (B ≤500) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 58.71 51.11 74.36 58.85 62.63 63.41 64.31 Budgets N/A 107 284 440 126 248 361 467 R 96.44 95.69 95.71 96.03 96.26 96.19 95.87 95.91 A 57.07 61.43 61.43 62.05 58.18 61.15 61.52 63.78 C 44.97 46.41 57.73 57.41 53.17 55.14 56.79 57.87 P 73.15 70.74 71.98 76.98 73.51 74.18 75.78 77.09 48Published as a conference paper at ICLR 2024 in deep learning heavily relies on large-scale data. Consequently, two promising paths emerge: establishing credible assumptions and leveraging additional information. Firstly, developing credible assumptions can lead to comprehensive comparisons across various stud- ies. Given that theoretical guarantees highlight the inherent differences between methods primarily based on the application limits of their assumptions, comparing these assumptions becomes critical. Without such comparative studies, empirical evaluations may lack precise guidance and explanation. Secondly, while we acknowledge the value of real-world data (observations), discussions surrounding the use of extra information remain pertinent. Considerations include the strategies to acquire this supplementary information and the nature of the additional data needed. Despite the myriad of works on domain generalization, domain adaptation, and test-time adaptation, a comprehensive survey or benchmark encapsulating the aforementioned comparisons remains an unmet need. Moreover, potential future directions for out-of-distribution generalization extend beyond domain generalization and test-time adaptation. One promising avenue is bridging the gap between causal inference and deep learning, for instance, through causal representation learning. In conclusion, our hope is that this work not only offers a novel practical setting and algorithm but also illuminates meaningful future directions and research methodologies that can benefit the broader scientific community. 49",
      "meta_data": {
        "arxiv_id": "2404.05094v1",
        "authors": [
          "Shurui Gui",
          "Xiner Li",
          "Shuiwang Ji"
        ],
        "published_date": "2024-04-07T22:31:34Z",
        "pdf_url": "https://arxiv.org/pdf/2404.05094v1.pdf",
        "github_url": "https://github.com/divelab/ATTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes Active Test-Time Adaptation (ATTA) as a novel problem setting to address significant domain distribution shifts for streaming test data, integrating active learning into fully test-time adaptation (FTTA). It provides a learning theory analysis demonstrating that incorporating limited labeled test instances enhances performance and prevents catastrophic forgetting (CF) with theoretical guarantees. A simple yet effective algorithm, SimATTA, is introduced, utilizing real-time sample selection techniques, incremental clustering, and selective entropy minimization. Extensive experiments confirm SimATTA's substantial performance improvements over TTA methods while maintaining efficiency and achieving effectiveness comparable to more demanding Active Domain Adaptation (ADA) methods.",
        "methodology": "The ATTA framework formally defines an optimization objective that minimizes a weighted empirical error over labeled training data (including actively selected instances) and an unsupervised learning loss over unlabeled streaming data. The theoretical analysis employs learning bounds based on VC-dimension theory and H∆H-distance to quantify distribution shifts and derive conditions for optimal adaptation performance. To mitigate catastrophic forgetting, a selective entropy minimization strategy is used: low-entropy samples from the test stream are pseudo-labeled by a frozen source-pretrained model, approximating source-like data, while high-entropy samples are selected for active labeling. SimATTA implements this methodology by partitioning streaming test data into high-entropy and low-entropy sets. Low-entropy data are pseudo-labeled by the frozen source model, and an incremental clustering technique (weighted K-means) is applied to high-entropy data to select representative \"anchors\" for active labeling. The model is then fine-tuned using a balanced combination of actively labeled high-entropy anchors and pseudo-labeled low-entropy samples with appropriate weights.",
        "experimental_setup": "The study uses datasets from DomainBed (PACS, VLCS, Office-Home) and Hendrycks and Dietterich (Tiny-ImageNet-C) for benchmarking. Specific source domains include \"photos\" for PACS, \"Caltech101\" for VLCS, \"real\" for Office-Home, and \"brightness corruption\" for Tiny-ImageNet-C. Two data stream order strategies are employed: domain-wise (sequential target domain processing) and random (shuffled samples from all target domains). Models used are ResNet-18 (for PACS, VLCS, Tiny-ImageNet-C) and ResNet-50 (for Office-Home), initialized with ImageNet pre-trained weights. Baselines include source-only models (BN w/o adapt, BN w/ adapt), state-of-the-art TTA methods (Tent, EATA, CoTTA, SAR), and Active Domain Adaptation (ADA) methods (Random, Entropy, K-means, CLUE) for stronger setting comparisons. Optimization involves Adam for pre-training and SGD for test-time adaptation, with specific learning rates. Evaluation metrics include real-time and post-adaptation accuracy, and time efficiency. Ablation studies examine the impact of incremental clustering and low-entropy sample training.",
        "limitations": "The selective entropy minimization strategy relies on the quality of the pre-trained model, and training on incorrectly predicted low-entropy samples may reinforce errors. It might not always be cost-effective to expend annotation budgets on low-entropy samples. Theoretical bounds can be loose with small numbers of unlabeled test samples, an inherent challenge given the streaming nature and limited batch sizes, although the practical assumption is that fine-tuning is equivalent to learning with a relatively small VC-dimension. The current framework does not cover class-incremental problems (situations where the support of labels changes). Finally, the exact number of new anchors selected by incremental clustering is adaptive, leading to indirect budget control for clusters.",
        "future_research_directions": "Future research directions include developing alternative methods to prevent catastrophic forgetting in ATTA scenarios. Investigating whether correcting incorrectly predicted low-entropy samples could be a more viable and cost-effective solution is another avenue. Exploring the application and scaling of ATTA methods for large models, such as Large Language Models (LLMs), where retraining is extremely expensive and source data may be inaccessible, is also suggested. Bridging the gap between causal inference and deep learning, for instance, through causal representation learning, is a promising direction for out-of-distribution generalization. Finally, exploring more specialized applications requiring task-specific configurations, building upon the foundational ATTA setting, is anticipated.",
        "experimental_code": "File Path: ATTA/kernel/algorithms/Base.py\nContent:\nimport os\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import ConcatDataset\n# import models for resnet18\n\nfrom ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader\nfrom ATTA.utils.config_reader import Conf\nfrom ATTA.utils.initial import reset_random_seed\nfrom ATTA.utils.register import register\n\n\n# import torch.multiprocessing\n# torch.multiprocessing.set_sharing_strategy('file_system')\n\n@register.alg_register\nclass AlgBase:\n    def __init__(self, config: Conf):\n        super(AlgBase, self).__init__()\n\n        if not os.path.exists(config.ckpt_dir):\n            os.makedirs(config.ckpt_dir)\n\n        reset_random_seed(config)\n        self.dataset = register.datasets[config.dataset.name](config.dataset.dataset_root, config.dataset.test_envs,\n                                                              config)\n        config.dataset.dataset_type = 'image'\n        config.dataset.input_shape = self.dataset.input_shape\n        config.dataset.num_classes = 1 if self.dataset.num_classes == 2 else self.dataset.num_classes\n        config.model.model_level = 'image'\n        config.metric.set_score_func(self.dataset.metric)\n        config.metric.set_loss_func(self.dataset.task)\n\n        self.config = config\n\n        self.inf_loader = [InfiniteDataLoader(env, weights=None, batch_size=self.config.train.train_bs,\n                                              num_workers=self.config.num_workers) for env in self.dataset]\n        reset_random_seed(config)\n        self.train_split = [np.random.choice(len(env), size=int(len(env) * 0.8), replace=False) for env in self.dataset]\n        print(self.train_split)\n        self.val_split = [np.setdiff1d(np.arange(len(env)), self.train_split[i]) for i, env in enumerate(self.dataset)]\n        self.train_loader = [InfiniteDataLoader(env, weights=None, batch_size=self.config.train.train_bs,\n                                                num_workers=self.config.num_workers, subset=self.train_split[i]) for\n                             i, env in enumerate(self.dataset)]\n        self.val_loader = [FastDataLoader(env, weights=None, batch_size=self.config.train.train_bs,\n                                          num_workers=self.config.num_workers,\n                                          subset=self.val_split[i], sequential=True) for i, env in\n                           enumerate(self.dataset)]\n        reset_random_seed(config)\n        fast_random = [np.random.permutation(len(env)) for env in self.dataset]\n        self.fast_loader = [FastDataLoader(env, weights=None,\n                                           batch_size=self.config.atta.batch_size,\n                                           num_workers=self.config.num_workers,\n                                           subset=fast_random[i], sequential=True) for i, env in\n                            enumerate(self.dataset)]\n\n        reset_random_seed(config)\n        self.target_dataset = ConcatDataset(\n            [env for i, env in enumerate(self.dataset) if i in config.dataset.test_envs[1:]])\n        len_target = len(self.target_dataset)\n        target_choices = np.random.permutation(len_target)\n        len_split = len_target // 4\n        self.target_splits = [target_choices[i * len_split: (i + 1) * len_split] for i in range(4)]\n        self.target_splits[-1] = target_choices[3 * len_split:]\n        self.target_loader = [FastDataLoader(self.target_dataset, weights=None,\n                                             batch_size=self.config.atta.batch_size,\n                                             num_workers=self.config.num_workers, subset=self.target_splits[i],\n                                             sequential=True) for i in range(4)]\n\n        self.encoder = register.models[config.model.name](config).to(self.config.device)\n\n        #     self.fc = self.encoder.fc\n        #     self.model = nn.Sequential(self.encoder, self.fc).to(self.config.device)\n        # else:\n        self.fc = nn.Linear(self.encoder.n_outputs, config.dataset.num_classes).to(self.config.device)\n        self.model = nn.Sequential(self.encoder, self.fc).to(self.config.device)\n\n        if 'ImageNet' in config.dataset.name or 'CIFAR' in config.dataset.name:\n            self.train_on_env(self.config.dataset.test_envs[0], train_only_fc=True, train_or_load='load')\n        else:\n            self.train_on_env(self.config.dataset.test_envs[0], train_only_fc=False, train_or_load='load')\n\n    def __call__(self, *args, **kwargs):\n        for env_id in self.config.dataset.test_envs:\n            self.test_on_env(env_id)\n\n    @torch.no_grad()\n    def test_on_env(self, env_id):\n        # self.encoder.eval()\n        # self.fc.eval()\n        self.model.eval()\n        test_loss = 0\n        test_acc = 0\n        for data, target in self.fast_loader[env_id]:\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            output = self.fc(self.encoder(data))\n            test_loss += self.config.metric.loss_func(output, target, reduction='sum').item()\n            test_acc += self.config.metric.score_func(target, output) * len(data)\n        test_loss /= len(self.fast_loader[env_id].dataset)\n        test_acc /= len(self.fast_loader[env_id].dataset)\n        print(f'#I#Env {env_id} Test set: Average loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}')\n        return test_loss, test_acc\n\n    @torch.no_grad()\n    def val_on_env(self, env_id):\n        self.model.eval()\n        val_loss = 0\n        val_acc = 0\n        for data, target in self.val_loader[env_id]:\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            output = self.fc(self.encoder(data))\n            val_loss += self.config.metric.loss_func(output, target, reduction='sum').item()\n            val_acc += self.config.metric.score_func(target, output) * len(data)\n        val_loss /= len(self.val_split[env_id])\n        val_acc /= len(self.val_split[env_id])\n        return val_loss, val_acc\n\n    @torch.enable_grad()\n    def train_on_env(self, env_id, train_only_fc=True, train_or_load='train'):\n        if train_or_load == 'train' or not os.path.exists(self.config.ckpt_dir + f'/encoder_{env_id}.pth'):\n            # if train_only_fc:\n            #     self.encoder.train()\n            #     self.fc.train()\n            #     optimizer = torch.optim.Adam(self.fc.parameters(), lr=self.config.train.lr)\n            #     inf_loader = self.extract_pretrained_feat(self.fast_loader[env_id], self.config.train.train_bs)\n            #     for batch_idx, (data, target) in enumerate(inf_loader):\n            #         optimizer.zero_grad()\n            #         data, target = data.to(self.config.device), target.to(self.config.device)\n            #         output = self.fc(data)\n            #         loss = self.config.metric.loss_func(output, target)\n            #         acc = self.config.metric.score_func(target, output)\n            #         loss.backward()\n            #         optimizer.step()\n            #         if batch_idx % self.config.train.log_interval == 0:\n            #             print(f'Iteration: {batch_idx} Loss: {loss.item():.4f} Acc: {acc:.4f}')\n            #         if batch_idx > self.config.train.max_iters:\n            #             break\n            # else:\n            # best_val_loss = float('inf')\n            best_val_acc = 0\n            if train_only_fc:\n                self.model.eval()\n                self.fc.train()\n                optimizer = torch.optim.Adam(self.fc.parameters(), lr=self.config.train.lr)\n            else:\n                self.model.train()\n                optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.train.lr)\n            for batch_idx, (data, target) in enumerate(self.train_loader[env_id]):\n                optimizer.zero_grad()\n                data, target = data.to(self.config.device), target.to(self.config.device)\n                output = self.fc(self.encoder(data))\n                loss = self.config.metric.loss_func(output, target)\n                acc = self.config.metric.score_func(target, output)\n                loss.backward()\n                optimizer.step()\n                if batch_idx % self.config.train.log_interval == 0:\n                    print(f'Iteration: {batch_idx} Loss: {loss.item():.4f} Acc: {acc:.4f}')\n                    val_loss, val_acc = self.val_on_env(env_id)\n                    if val_acc > best_val_acc:\n                        print(f'New best val acc: {val_acc:.4f}')\n                        best_val_acc = val_acc\n                        torch.save(self.encoder.state_dict(), self.config.ckpt_dir + f'/encoder_{env_id}.pth')\n                        torch.save(self.fc.state_dict(), self.config.ckpt_dir + f'/fc_{env_id}.pth')\n                    self.model.train()\n                if batch_idx > self.config.train.max_iters:\n                    break\n        else:\n            self.encoder.load_state_dict(\n                torch.load(self.config.ckpt_dir + f'/encoder_{env_id}.pth', map_location=self.config.device), strict=False)\n            self.fc.load_state_dict(\n                torch.load(self.config.ckpt_dir + f'/fc_{env_id}.pth', map_location=self.config.device))\n\n    # @torch.no_grad()\n    # def extract_pretrained_feat(self, loader, batch_size):\n    #     print('Extracting features from the loader')\n    #     feats, targets = [], []\n    #     for data, target in loader:\n    #         data, target = data.to(self.config.device), target.to(self.config.device)\n    #         feat = self.encoder(data)\n    #         feats.append(feat.cpu())\n    #         targets.append(target.cpu())\n    #     feats = torch.cat(feats, dim=0)\n    #     targets = torch.cat(targets, dim=0)\n    #     feat_dataset = TensorDataset(feats, targets)\n    #     weights = misc.make_weights_for_balanced_classes(\n    #         feat_dataset) if self.config.dataset.class_balanced else None\n    #     inf_loader = InfiniteDataLoader(dataset=feat_dataset, weights=weights,\n    #                                     batch_size=batch_size, num_workers=self.config.num_workers)\n    #     return inf_loader\n\nFile Path: ATTA/kernel/algorithms/SimATTA.py\nContent:\nimport copy\nimport pathlib\nimport time\nfrom typing import Union\n\nimport numpy as np\n# from sklearnex import patch_sklearn, config_context\n# patch_sklearn()\n\n# from sklearn.cluster import KMeans\n# from ATTA.utils.fast_pytorch_kmeans import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nfrom typing import Literal\n\nfrom torch import nn\nimport torch\n# import models for resnet18\nfrom munch import Munch\nfrom ATTA import register\nfrom ATTA.utils.config_reader import Conf\nfrom ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader\nfrom torch.utils.data import TensorDataset\nfrom tqdm import tqdm\nfrom .Base import AlgBase\nimport pandas as pd\nfrom ATTA.definitions import STORAGE_DIR\n\n\n\n@register.alg_register\nclass SimATTA(AlgBase):\n    def __init__(self, config: Conf):\n        super(SimATTA, self).__init__(config)\n\n        self.teacher = copy.deepcopy(self.model.to('cpu'))\n\n        self.model.to(config.device)\n        self.teacher.to(config.device)\n        self.update_teacher(0)  # copy student to teacher\n\n        self.budgets = 0\n        self.anchors = None\n        self.source_anchors = None\n        self.buffer = []\n        self.n_clusters = 10\n        self.nc_increase = self.config.atta.SimATTA.nc_increase\n        self.source_n_clusters = 100\n\n        self.cold_start = self.config.atta.SimATTA.cold_start\n\n        self.consistency_weight = 0\n        self.alpha_teacher = 0\n        self.accumulate_weight = True\n        self.weighted_entropy: Union[Literal['low', 'high', 'both'], None] = 'both'\n        self.aggressive = True\n        self.beta = self.config.atta.SimATTA.beta\n        self.alpha = 0.2\n\n        self.target_cluster = True if self.config.atta.SimATTA.target_cluster else False\n        self.LE = True if self.config.atta.SimATTA.LE else False\n        self.vis_round = 0\n\n\n    def __call__(self, *args, **kwargs):\n        # super(SimATTA, self).__call__()\n        self.continue_result_df = pd.DataFrame(\n            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)\n        self.random_result_df = pd.DataFrame(\n            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)\n\n        self.enable_bn(self.model)\n        if 'ImageNet' not in self.config.dataset.name:\n            for env_id in self.config.dataset.test_envs:\n                acc = self.test_on_env(env_id)[1]\n                self.continue_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n                self.random_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n\n        for adapt_id in self.config.dataset.test_envs[1:]:\n            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)\n            self.continue_result_df.loc['Budgets', adapt_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]\n\n        self.__init__(self.config)\n        for target_split_id in range(4):\n            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader, target_split_id)\n            self.random_result_df.loc['Budgets', target_split_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]\n\n        print(f'#IM#\\n{self.continue_result_df.round(4).to_markdown()}\\n'\n              f'{self.random_result_df.round(4).to_markdown()}')\n        # print(self.random_result_df.round(4).to_markdown(), '\\n')\n        self.continue_result_df.round(4).to_csv(f'{self.config.log_file}.csv')\n        self.random_result_df.round(4).to_csv(f'{self.config.log_file}.csv', mode='a')\n\n\n    @torch.no_grad()\n    def val_anchor(self, loader):\n        self.model.eval()\n        val_loss = 0\n        val_acc = 0\n        for data, target in loader:\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            output = self.fc(self.encoder(data))\n            val_loss += self.config.metric.loss_func(output, target, reduction='sum').item()\n            val_acc += self.config.metric.score_func(target, output) * len(data)\n        val_loss /= len(loader.sampler)\n        val_acc /= len(loader.sampler)\n        return val_loss, val_acc\n\n    def update_teacher(self, alpha_teacher):  # , iteration):\n        for t_param, s_param in zip(self.teacher.parameters(), self.model.parameters()):\n            t_param.data[:] = alpha_teacher * t_param[:].data[:] + (1 - alpha_teacher) * s_param[:].data[:]\n        if not self.config.model.freeze_bn:\n            for tm, m in zip(self.teacher.modules(), self.model.modules()):\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    tm.running_mean = alpha_teacher * tm.running_mean + (1 - alpha_teacher) * m.running_mean\n                    tm.running_var = alpha_teacher * tm.running_var + (1 - alpha_teacher) * m.running_var\n\n    @torch.enable_grad()\n    def cluster_train(self, target_anchors, source_anchors):\n        self.model.train()\n\n        source_loader = InfiniteDataLoader(TensorDataset(source_anchors.data, source_anchors.target), weights=None,\n                                           batch_size=self.config.train.train_bs,\n                                           num_workers=self.config.num_workers)\n        target_loader = InfiniteDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                             batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        alpha = target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())\n        if source_anchors.num_elem() < self.cold_start:\n            alpha = min(0.2, alpha)\n\n        ST_loader = iter(zip(source_loader, target_loader))\n        val_loader = FastDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                    batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)\n        # print('Cluster train')\n        delay_break = False\n        loss_window = []\n        tol = 0\n        lowest_loss = float('inf')\n        for i, ((S_data, S_targets), (T_data, T_targets)) in enumerate(ST_loader):\n            S_data, S_targets = S_data.to(self.config.device), S_targets.to(self.config.device)\n            T_data, T_targets = T_data.to(self.config.device), T_targets.to(self.config.device)\n            L_T = self.one_step_train(S_data, S_targets, T_data, T_targets, alpha, optimizer)\n            # self.update_teacher(self.alpha_teacher)\n            if len(loss_window) < self.config.atta.SimATTA.stop_tol:\n                loss_window.append(L_T.item())\n            else:\n                mean_loss = np.mean(loss_window)\n                tol += 1\n                if mean_loss < lowest_loss:\n                    lowest_loss = mean_loss\n                    tol = 0\n                if tol > 5:\n                    break\n                loss_window = []\n            if 'ImageNet' in self.config.dataset.name or 'CIFAR' in self.config.dataset.name:\n                if i > self.config.atta.SimATTA.steps:\n                    break\n\n\n    def one_step_train(self, S_data, S_targets, T_data, T_targets, alpha, optimizer):\n        # print('one step train')\n        L_S = self.config.metric.loss_func(self.model(S_data), S_targets)\n        L_T = self.config.metric.loss_func(self.model(T_data), T_targets)\n        loss = (1 - alpha) * L_S + alpha * L_T\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        return L_T\n\n    def softmax_entropy(self, x: torch.Tensor, y: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"Entropy of softmax distribution from logits.\"\"\"\n        if y is None:\n            if x.shape[1] == 1:\n                x = torch.cat([x, -x], dim=1)\n            return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n        else:\n            return - 0.5 * (x.softmax(1) * y.log_softmax(1)).sum(1) - 0.5 * (y.softmax(1) * x.log_softmax(1)).sum(1)\n\n    def update_anchors(self, anchors, data, target, feats, weight):\n        if anchors is None:\n            anchors = Munch()\n            anchors.data = data\n            anchors.target = target\n            anchors.feats = feats\n            anchors.weight = weight\n            anchors.num_elem = lambda: len(anchors.data)\n        else:\n            anchors.data = torch.cat([anchors.data, data])\n            anchors.target = torch.cat([anchors.target, target])\n            anchors.feats = torch.cat([anchors.feats, feats])\n            anchors.weight = torch.cat([anchors.weight, weight])\n        return anchors\n\n    def update_anchors_feats(self, anchors):\n        # sequential_data = torch.arange(200)[:, None]\n        anchors_loader = FastDataLoader(TensorDataset(anchors.data), weights=None,\n                                        batch_size=32, num_workers=self.config.num_workers, sequential=True)\n\n        anchors.feats = None\n        self.model.eval()\n        for data in anchors_loader:\n            # print(data)\n            data = data[0].to(self.config.device)\n            if anchors.feats is None:\n                anchors.feats = self.model[0](data).cpu().detach()\n            else:\n                anchors.feats = torch.cat([anchors.feats, self.model[0](data).cpu().detach()])\n\n        return anchors\n\n    @torch.no_grad()\n    def sample_select(self, model, data, target, anchors, n_clusters, ent_beta, use_pseudo_label=False, ent_bound=1e-2, incremental_cluster=False):\n        model.eval()\n        feats = model[0](data)\n        outputs = model[1](feats)\n        pseudo_label = outputs.argmax(1).cpu().detach()\n        data = data.cpu().detach()\n        feats = feats.cpu().detach()\n        target = target.cpu().detach()\n        entropy = self.softmax_entropy(outputs).cpu()\n        if not incremental_cluster:\n            entropy = entropy.numpy()\n            if ent_beta == 0:\n                closest = np.argsort(entropy)[: n_clusters]\n                closest = closest[entropy[closest] < ent_bound]\n            elif ent_beta == 1:\n                closest = np.argsort(entropy)[- n_clusters:]\n                closest = closest[entropy[closest] >= ent_bound]\n            else:\n                raise NotImplementedError\n            weights = torch.zeros(len(closest), dtype=torch.float)\n        else:\n            if ent_beta == 0:\n                sample_choice = entropy < ent_bound\n            elif ent_beta == 1:\n                sample_choice = entropy >= ent_bound\n            else:\n                raise NotImplementedError\n\n            data = data[sample_choice]\n            target = target[sample_choice]\n            feats = feats[sample_choice]\n            pseudo_label = pseudo_label[sample_choice]\n\n            if anchors:\n                feats4cluster = torch.cat([anchors.feats, feats])\n                sample_weight = torch.cat([anchors.weight, torch.ones(len(feats), dtype=torch.float)])\n            else:\n                feats4cluster = feats\n                sample_weight = torch.ones(len(feats), dtype=torch.float)\n\n            if self.config.atta.gpu_clustering:\n                from ATTA.utils.fast_pytorch_kmeans import KMeans\n                from joblib import parallel_backend\n                kmeans = KMeans(n_clusters=n_clusters, n_init=10, device=self.config.device).fit(\n                    feats4cluster.to(self.config.device),\n                    sample_weight=sample_weight.to(self.config.device))\n                with parallel_backend('threading', n_jobs=8):\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n            # elif self.config.atta.gpu_clustering == 'jax':\n            #     from ott.tools.k_means import k_means as KMeans\n            #     import jax\n            #     import jax.numpy as jnp\n            #     tik = time.time()\n            #     kmeans = KMeans(jnp.array(feats4cluster.numpy()), k=n_clusters, weights=jnp.array(sample_weight.numpy()), n_init=10)\n            #     mit = time.time()\n            #     print(f'#IN#Kmeans time: {mit - tik}')\n            #     @jax.jit\n            #     def jax_pairwise_distances_argmin(c, feats):\n            #         dis = lambda x, y: jnp.sqrt(((x - y) ** 2).sum())\n            #         argmin_dis = lambda x, y: jnp.argmin(jax.vmap(dis, in_axes=(None, 0))(x, y))\n            #         return jax.vmap(argmin_dis, in_axes=(0, None))(c, feats)\n            #     raw_closest = np.array(jax_pairwise_distances_argmin(kmeans.centroids, jnp.array(feats4cluster.numpy())))\n            #     print(f'#IN#Pairwise distance time: {time.time() - mit}')\n            #     kmeans_labels = np.array(kmeans.assignment)\n            else:\n                from joblib import parallel_backend\n                from sklearn.cluster import KMeans\n                with parallel_backend('threading', n_jobs=8):\n                    kmeans = KMeans(n_clusters=n_clusters, n_init=10, algorithm='elkan').fit(feats4cluster,\n                                                                                                  sample_weight=sample_weight)\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n\n\n\n            if anchors:\n                num_anchors = anchors.num_elem()\n                prev_anchor_cluster = torch.tensor(kmeans_labels[:num_anchors], dtype=torch.long)\n\n                if self.accumulate_weight:\n                    # previous anchor weight accumulation\n                    # Average the weight of the previous anchor if sharing the same cluster\n                    num_prev_anchors_per_cluster = prev_anchor_cluster.unique(return_counts=True)\n                    num_prev_anchors_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_prev_anchors_per_cluster_dict[num_prev_anchors_per_cluster[0].long()] = \\\n                    num_prev_anchors_per_cluster[1]\n\n                    num_newsample_per_cluster = torch.tensor(kmeans_labels).unique(return_counts=True)\n                    num_newsample_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_newsample_per_cluster_dict[num_newsample_per_cluster[0].long()] = num_newsample_per_cluster[1]\n                    assert (num_prev_anchors_per_cluster_dict[prev_anchor_cluster] == 0).sum() == 0\n                    # accumulate the weight of the previous anchor\n                    anchors.weight = anchors.weight + num_newsample_per_cluster_dict[prev_anchor_cluster] / \\\n                                          num_prev_anchors_per_cluster_dict[prev_anchor_cluster].float()\n\n                anchored_cluster_mask = torch.zeros(len(raw_closest), dtype=torch.bool).index_fill_(0,\n                                                                                                    prev_anchor_cluster.unique().long(),\n                                                                                                    True)\n                new_cluster_mask = ~ anchored_cluster_mask\n\n                closest = raw_closest[new_cluster_mask] - num_anchors\n                if (closest < 0).sum() != 0:\n                    # The cluster's closest sample may not belong to the cluster. It makes sense to eliminate them.\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    new_cluster_mask = torch.where(new_cluster_mask)[0]\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    print(closest)\n                    print(closest >= 0)\n                    new_cluster_mask = new_cluster_mask[closest >= 0]\n                    closest = closest[closest >= 0]\n\n\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1][new_cluster_mask]\n            else:\n                num_anchors = 0\n                closest = raw_closest\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1]\n\n        if use_pseudo_label:\n            anchors = self.update_anchors(anchors, data[closest], pseudo_label[closest], feats[closest], weights)\n        else:\n            anchors = self.update_anchors(anchors, data[closest], target[closest], feats[closest], weights)\n\n        return outputs, closest, anchors\n\n    def enable_bn(self, model):\n        if not self.config.model.freeze_bn:\n            for m in model.modules():\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    m.momentum = 0.1\n\nFile Path: ATTA/data/loaders/fast_data_loader.py\nContent:\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\nimport torch\nfrom torch.utils.data import DataLoader\n\n\nclass _InfiniteSampler(torch.utils.data.Sampler):\n    \"\"\"Wraps another Sampler to yield an infinite stream.\"\"\"\n    def __init__(self, sampler):\n        self.sampler = sampler\n\n    def __iter__(self):\n        while True:\n            for batch in self.sampler:\n                yield batch\n\nclass InfiniteDataLoader:\n    def __init__(self, dataset, weights, batch_size, num_workers, sequential=False, subset=None):\n        super().__init__()\n        self.dataset = dataset\n        if weights is not None:\n            sampler = torch.utils.data.WeightedRandomSampler(weights,\n                replacement=True,\n                num_samples=batch_size)\n        elif sequential:\n            if subset is None:\n                sampler = torch.utils.data.SequentialSampler(dataset)\n            else:\n                sampler = ActualSequentialSampler(subset)\n        elif subset is not None:\n            sampler = torch.utils.data.SubsetRandomSampler(subset)\n        else:\n            sampler = torch.utils.data.RandomSampler(dataset,\n                replacement=True)\n        self.sampler = sampler\n\n        if weights == None:\n            weights = torch.ones(len(dataset))\n\n        batch_sampler = torch.utils.data.BatchSampler(\n            sampler,\n            batch_size=batch_size,\n            drop_last=False)\n\n        self._infinite_iterator = iter(torch.utils.data.DataLoader(\n            dataset,\n            num_workers=num_workers,\n            batch_sampler=_InfiniteSampler(batch_sampler),\n            pin_memory=False,\n            persistent_workers=True if num_workers > 0 else False\n        ))\n\n    def __iter__(self):\n        while True:\n            yield next(self._infinite_iterator)\n\n    def __len__(self): \n        raise ValueError\n\nclass FastDataLoader:\n    \"\"\"DataLoader wrapper with slightly improved speed by not respawning worker\n    processes at every epoch.\"\"\"\n    def __init__(self, dataset, weights, batch_size, num_workers, sequential=False, subset=None):\n        super().__init__()\n        self.dataset = dataset\n        if weights is not None:\n            sampler = torch.utils.data.WeightedRandomSampler(weights,\n                replacement=False,\n                num_samples=batch_size)\n        elif sequential:\n            if subset is None:\n                sampler = torch.utils.data.SequentialSampler(dataset)\n            else:\n                sampler = ActualSequentialSampler(subset)\n        elif subset is not None:\n            sampler = torch.utils.data.SubsetRandomSampler(subset)\n        else:\n            sampler = torch.utils.data.RandomSampler(dataset,\n                replacement=False)\n        self.sampler = sampler\n\n        batch_sampler = torch.utils.data.BatchSampler(\n            sampler,\n            batch_size=batch_size,\n            drop_last=False\n        )\n\n        self._infinite_iterator = iter(torch.utils.data.DataLoader(\n            dataset,\n            num_workers=num_workers,\n            batch_sampler=_InfiniteSampler(batch_sampler),\n            pin_memory=False,\n            persistent_workers=True if num_workers > 0 else False\n        ))\n\n        self._length = len(batch_sampler)\n\n    def __iter__(self):\n        for _ in range(len(self)):\n            yield next(self._infinite_iterator)\n\n    def __len__(self):\n        return self._length\n\n\nclass ActualSequentialSampler(torch.utils.data.Sampler):\n    r\"\"\"Samples elements sequentially, always in the same order.\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    \"\"\"\n\n    def __init__(self, data_source):\n        self.data_source = data_source\n\n    def __iter__(self):\n        return iter(self.data_source)\n\n    def __len__(self):\n        return len(self.data_source)\nFile Path: ATTA/utils/fast_pytorch_kmeans/kmeans.py\nContent:\nimport warnings\n\nimport math\nimport torch\nfrom time import time\nimport numpy as np\nimport pynvml\nfrom .init_methods import init_methods\n\n\nclass KMeans:\n    '''\n    Kmeans clustering algorithm implemented with PyTorch\n\n    Parameters:\n      n_clusters: int,\n        Number of clusters\n\n      max_iter: int, default: 100\n        Maximum number of iterations\n\n      tol: float, default: 0.0001\n        Tolerance\n\n      verbose: int, default: 0\n        Verbosity\n\n      mode: {'euclidean', 'cosine'}, default: 'euclidean'\n        Type of distance measure\n\n      init_method: {'random', 'point', '++'}\n        Type of initialization\n\n      minibatch: {None, int}, default: None\n        Batch size of MinibatchKmeans algorithm\n        if None perform full KMeans algorithm\n\n    Attributes:\n      centroids: torch.Tensor, shape: [n_clusters, n_features]\n        cluster centroids\n    '''\n\n    def __init__(self, n_clusters, max_iter=300, tol=0.0001, verbose=0, mode=\"euclidean\", init_method=\"kmeans++\",\n                 minibatch=None, n_init=None, algorithm=None, device=None):\n        self.n_clusters = n_clusters\n        self.max_iter = max_iter\n        self.tol = tol\n        self.verbose = verbose\n        self.mode = mode\n        self.init_method = init_method\n        self.minibatch = minibatch\n        self._loop = False\n        self._show = False\n\n        self.n_init = n_init\n\n        if algorithm is not None:\n            warnings.warn(\"The parameter algorithm is not valid in this implementation of KMeans. Default: 'lloyd'\")\n\n        try:\n            import pynvml\n            self._pynvml_exist = True\n        except ModuleNotFoundError:\n            self._pynvml_exist = False\n\n        self.device = device\n        self.cluster_centers_ = None\n        self.labels_ = None\n\n    @staticmethod\n    # @torch.compile\n    def cos_sim(a, b):\n        \"\"\"\n          Compute cosine similarity of 2 sets of vectors\n\n          Parameters:\n          a: torch.Tensor, shape: [m, n_features]\n\n          b: torch.Tensor, shape: [n, n_features]\n        \"\"\"\n        a_norm = a.norm(dim=-1, keepdim=True)\n        b_norm = b.norm(dim=-1, keepdim=True)\n        a = a / (a_norm + 1e-8)\n        b = b / (b_norm + 1e-8)\n        return a @ b.transpose(-2, -1)\n\n    @staticmethod\n    # @torch.compile\n    def euc_sim(a, b):\n        \"\"\"\n          Compute euclidean similarity of 2 sets of vectors\n\n          Parameters:\n          a: torch.Tensor, shape: [m, n_features]\n\n          b: torch.Tensor, shape: [n, n_features]\n        \"\"\"\n        return 2 * a @ b.transpose(-2, -1) - (a ** 2).sum(dim=1)[..., :, None] - (b ** 2).sum(dim=1)[..., None, :]\n\n    def remaining_memory(self):\n        \"\"\"\n          Get remaining memory in gpu\n        \"\"\"\n        with torch.cuda.device(self.device):\n            torch.cuda.synchronize()\n            torch.cuda.empty_cache()\n        if self._pynvml_exist:\n            pynvml.nvmlInit()\n            gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(self.device.index)\n            info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n            remaining = info.free\n        else:\n            remaining = torch.cuda.memory_allocated()\n        return remaining\n\n    # @torch.compile\n    def max_sim(self, a, b):\n        \"\"\"\n          Compute maximum similarity (or minimum distance) of each vector\n          in a with all of the vectors in b\n\n          Parameters:\n          a: torch.Tensor, shape: [m, n_features]\n\n          b: torch.Tensor, shape: [n, n_features]\n        \"\"\"\n        batch_size = a.shape[0]\n        if self.mode == 'cosine':\n            sim_func = self.cos_sim\n        elif self.mode == 'euclidean':\n            sim_func = self.euc_sim\n\n        if self.device == 'cpu':\n            sim = sim_func(a, b)\n            max_sim_v, max_sim_i = sim.max(dim=-1)\n            return max_sim_v, max_sim_i\n        else:\n            if a.dtype == torch.double:\n                expected = a.shape[0] * a.shape[1] * b.shape[0] * 8\n            if a.dtype == torch.float:\n                expected = a.shape[0] * a.shape[1] * b.shape[0] * 4\n            elif a.dtype == torch.half:\n                expected = a.shape[0] * a.shape[1] * b.shape[0] * 2\n            ratio = math.ceil(expected / self.remaining_memory())\n            subbatch_size = math.ceil(batch_size / ratio)\n            msv, msi = [], []\n            for i in range(ratio):\n                if i * subbatch_size >= batch_size:\n                    continue\n                sub_x = a[i * subbatch_size: (i + 1) * subbatch_size]\n                sub_sim = sim_func(sub_x, b)\n                sub_max_sim_v, sub_max_sim_i = sub_sim.max(dim=-1)\n                del sub_sim\n                msv.append(sub_max_sim_v)\n                msi.append(sub_max_sim_i)\n            if ratio == 1:\n                max_sim_v, max_sim_i = msv[0], msi[0]\n            else:\n                max_sim_v = torch.cat(msv, dim=0)\n                max_sim_i = torch.cat(msi, dim=0)\n            return max_sim_v, max_sim_i\n\n    # @torch.compile\n    def fit_predict(self, X, sample_weight=None, centroids=None):\n        \"\"\"\n          Combination of fit() and predict() methods.\n          This is faster than calling fit() and predict() seperately.\n\n          Parameters:\n          X: torch.Tensor, shape: [n_samples, n_features]\n\n          centroids: {torch.Tensor, None}, default: None\n            if given, centroids will be initialized with given tensor\n            if None, centroids will be randomly chosen from X\n\n          Return:\n          labels: torch.Tensor, shape: [n_samples]\n        \"\"\"\n        assert isinstance(X, torch.Tensor), \"input must be torch.Tensor\"\n        assert X.dtype in [torch.half, torch.float, torch.double], \"input must be floating point\"\n        assert X.ndim == 2, \"input must be a 2d tensor with shape: [n_samples, n_features] \"\n\n        batch_size, emb_dim = X.shape\n        X = X.to(self.device)\n        if sample_weight is None:\n            sample_weight = torch.ones(batch_size, device=self.device, dtype=X.dtype)\n        else:\n            sample_weight = sample_weight.to(self.device)\n        start_time = time()\n        cluster_centers_ = centroids\n        num_points_in_clusters = torch.ones(self.n_clusters, device=self.device, dtype=X.dtype)\n        closest = None\n        closest, cluster_centers_, i, sample_weight, sim_score = self.fit_loop(X, batch_size, closest, cluster_centers_,\n                                                                               num_points_in_clusters, sample_weight)\n\n        if self.verbose >= 1:\n            print(\n                f'used {i + 1} iterations ({round(time() - start_time, 4)}s) to cluster {batch_size} items into {self.n_clusters} clusters')\n\n        inertia = (sim_score * sample_weight).sum().neg()\n        return cluster_centers_.detach(), closest.detach(), inertia.detach()\n\n    @torch.compile\n    def fit_loop(self, X, batch_size, closest, cluster_centers_, num_points_in_clusters, sample_weight):\n        for i in range(self.max_iter):\n            iter_time = time()\n            if self.minibatch is not None:\n                minibatch_idx = np.random.choice(batch_size, size=[self.minibatch], replace=False)\n                x = X[minibatch_idx]\n                sample_weight = sample_weight[minibatch_idx]\n            else:\n                x = X\n\n            sim_score, closest = self.max_sim(a=x, b=cluster_centers_)\n            matched_clusters, counts = closest.unique(return_counts=True)\n            unmatched_clusters = torch.where(\n                torch.ones(len(cluster_centers_), dtype=torch.bool, device=self.device).index_fill_(0,\n                                                                                                    matched_clusters.long(),\n                                                                                                    False) == True)[0]\n            # reallocate unmatched clusters according to the machanism described\n            # in https://github.com/scikit-learn/scikit-learn/blob/4af30870b0a09bf0a04d704bea4c5d861eae7c83/sklearn/cluster/_k_means_lloyd.pyx#L156\n            while unmatched_clusters.shape[0] > 0:\n                worst_x = x[sim_score.argmin(dim=0)]\n                cluster_centers_[unmatched_clusters[0]] = worst_x\n                sim_score, closest = self.max_sim(a=x, b=cluster_centers_)\n                matched_clusters, counts = closest.unique(return_counts=True)\n                unmatched_clusters = torch.where(\n                    torch.ones(len(cluster_centers_), dtype=torch.bool, device=self.device).index_fill_(0,\n                                                                                                        matched_clusters.long(),\n                                                                                                        False) == True)[\n                    0]\n\n            c_grad = torch.zeros_like(cluster_centers_)\n            expanded_closest = closest[None].expand(self.n_clusters, -1)\n            mask = (expanded_closest == torch.arange(self.n_clusters, device=self.device)[:, None]).to(\n                X.dtype)  # [n_clusters, minibatch] one-hot sample masks for each cluster\n            mask = mask * sample_weight[None, :]\n            c_grad = mask @ x / mask.sum(-1)[..., :, None]\n            c_grad[c_grad != c_grad] = 0  # remove NaNs\n\n            error = (c_grad - cluster_centers_).pow(2).sum()\n            if self.minibatch is not None:\n                lr = 1 / num_points_in_clusters[:, None] * 0.9 + 0.1\n                # lr = 1/num_points_in_clusters[:,None]**0.1\n            else:\n                lr = 1\n            num_points_in_clusters[matched_clusters] += counts\n            cluster_centers_ = cluster_centers_ * (1 - lr) + c_grad * lr\n            if self.verbose >= 2:\n                print('iter:', i, 'error:', error.item(), 'time spent:', round(time() - iter_time, 4))\n            if error <= self.tol:\n                break\n        return closest, cluster_centers_, i, sample_weight, sim_score\n\n    def predict(self, X):\n        \"\"\"\n          Predict the closest cluster each sample in X belongs to\n\n          Parameters:\n          X: torch.Tensor, shape: [n_samples, n_features]\n\n          Return:\n          labels: torch.Tensor, shape: [n_samples]\n        \"\"\"\n        assert isinstance(X, torch.Tensor), \"input must be torch.Tensor\"\n        assert X.dtype in [torch.half, torch.float, torch.double], \"input must be floating point\"\n        assert X.ndim == 2, \"input must be a 2d tensor with shape: [n_samples, n_features] \"\n\n        return self.max_sim(a=X, b=self.cluster_centers_)[1]\n\n    def fit(self, X, sample_weight=None, centroids=None):\n        \"\"\"\n          Perform kmeans clustering\n\n          Parameters:\n          X: torch.Tensor, shape: [n_samples, n_features]\n        \"\"\"\n        assert isinstance(X, torch.Tensor), \"input must be torch.Tensor\"\n        assert X.dtype in [torch.half, torch.float, torch.double], \"input must be floating point\"\n        assert X.ndim == 2, \"input must be a 2d tensor with shape: [n_samples, n_features] \"\n\n        self.cluster_centers_, self.labels_, self.inertia_ = [], [], []\n\n        if centroids is None:\n            cluster_centers_ = [init_methods[self.init_method](X, self.n_clusters, self.minibatch) for _ in range(self.n_init)]\n        else:\n            cluster_centers_ = centroids\n        cluster_centers_ = torch.stack(cluster_centers_)\n\n        # self.cluster_centers_, self.labels, self.inertia = torch.compile(torch.vmap(self.fit_predict, in_dims=(None, None, 0)))(X, sample_weight, cluster_centers_)\n\n        for i in range(self.n_init):\n            cluster_centers, labels, inertia = self.fit_predict(X, sample_weight, cluster_centers_[i])\n            self.cluster_centers_.append(cluster_centers)\n            self.labels_.append(labels)\n            self.inertia_.append(inertia)\n        best_cluster_idx = torch.argmin(torch.stack(self.inertia_))\n        self.cluster_centers_, self.labels_, self.inertia_ = self.cluster_centers_[best_cluster_idx].cpu().numpy(), self.labels_[best_cluster_idx].cpu().numpy(), self.inertia_[best_cluster_idx].cpu().numpy()\n        return self",
        "experimental_info": "The SimATTA method is implemented as `SimATTA(AlgBase)` and inherits data loading and model initialization from `AlgBase`. The core adaptation logic resides in `SimATTA.__call__` which orchestrates the adaptation process over different target domains/splits.\n\nKey experimental settings and their implementation in SimATTA:\n\n1.  **Frozen Source-Pretrained Model (Teacher Model)**: The method uses a 'teacher' model (`self.teacher`) which is a deep copy of the 'student' model (`self.model`) at initialization. This teacher model is intended to remain frozen or updated with a very low alpha, approximating a frozen source model. `self.update_teacher(0)` at init copies the student state, then subsequent calls to `update_teacher` with a small `alpha_teacher` can be used for EMA updates if `alpha_teacher` is not 0 (though in the provided `SimATTA.py` code, `self.alpha_teacher` is initialized to 0 and not explicitly updated elsewhere, implying a fully frozen teacher).\n\n2.  **Streaming Data Partitioning based on Entropy**: The `sample_select` method is central to partitioning streaming data. It calculates `entropy = self.softmax_entropy(outputs)`. Data samples are then chosen based on this entropy and the `ent_beta` parameter:\n    *   `ent_beta == 0`: selects low-entropy samples (`entropy < ent_bound`). This is used for pseudo-labeling. The `ent_bound` for low-entropy samples is controlled by `config.atta.SimATTA.el`.\n    *   `ent_beta == 1`: selects high-entropy samples (`entropy >= ent_bound`). This is used for active labeling. The `ent_bound` for high-entropy samples is controlled by `config.atta.SimATTA.eh`.\n\n3.  **Pseudo-Labeling Low-Entropy Samples**: When `use_pseudo_label=True` (which is the case for `source_anchors` through `self.LE` flag), the `sample_select` function assigns `pseudo_label = outputs.argmax(1).cpu().detach()` to low-entropy samples, effectively pseudo-labeling them.\n\n4.  **Incremental Clustering (Weighted K-means) for High-Entropy Samples**: When `incremental_cluster=True` (controlled by `self.target_cluster` config parameter) and `ent_beta == 1`, `sample_select` performs clustering:\n    *   It concatenates features of existing anchors (`anchors.feats`) and new high-entropy samples (`feats`) to form `feats4cluster`.\n    *   It assigns `sample_weight` to both existing anchors (accumulated if `self.accumulate_weight` is true) and new samples (weight of 1).\n    *   `KMeans` (either `ATTA.utils.fast_pytorch_kmeans.KMeans` or `sklearn.cluster.KMeans` depending on `config.atta.gpu_clustering`) is applied to `feats4cluster` using these `sample_weight`s. The number of clusters `n_clusters` for target data increases incrementally by `self.nc_increase` (e.g., `self.n_clusters += self.nc_increase`). For source data, `self.source_n_clusters` is used.\n    *   Representative 'anchors' are selected using `pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)`.\n\n5.  **Model Fine-tuning with Balanced Combination**: The `cluster_train` method fine-tunes the model:\n    *   It takes `target_anchors` (high-entropy, actively labeled) and `source_anchors` (low-entropy, pseudo-labeled) as input.\n    *   An `alpha` weight is calculated as `target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())`, balancing the contribution of these two sets. During a 'cold start' phase (`source_anchors.num_elem() < self.cold_start`), `alpha` is capped at `min(0.2, alpha)`.\n    *   The model is trained for `self.config.atta.SimATTA.steps` using `torch.optim.SGD` with learning rate `self.config.atta.SimATTA.lr`. The loss is a weighted sum: `loss = (1 - alpha) * L_S + alpha * L_T`, where `L_S` is loss from source anchors and `L_T` is loss from target anchors. This directly reflects the "
      }
    },
    {
      "title": "Diffusion-TTA: Test-time Adaptation of Discriminative Models via Generative Feedback",
      "abstract": "The advancements in generative modeling, particularly the advent of diffusion\nmodels, have sparked a fundamental question: how can these models be\neffectively used for discriminative tasks? In this work, we find that\ngenerative models can be great test-time adapters for discriminative models.\nOur method, Diffusion-TTA, adapts pre-trained discriminative models such as\nimage classifiers, segmenters and depth predictors, to each unlabelled example\nin the test set using generative feedback from a diffusion model. We achieve\nthis by modulating the conditioning of the diffusion model using the output of\nthe discriminative model. We then maximize the image likelihood objective by\nbackpropagating the gradients to discriminative model's parameters. We show\nDiffusion-TTA significantly enhances the accuracy of various large-scale\npre-trained discriminative models, such as, ImageNet classifiers, CLIP models,\nimage pixel labellers and image depth predictors. Diffusion-TTA outperforms\nexisting test-time adaptation methods, including TTT-MAE and TENT, and\nparticularly shines in online adaptation setups, where the discriminative model\nis continually adapted to each example in the test set. We provide access to\ncode, results, and visualizations on our website:\nhttps://diffusion-tta.github.io/.",
      "full_text": "Diffusion-TTA: Test-time Adaptation of Discriminative Models via Generative Feedback Mihir Prabhudesai∗ Tsung-Wei Ke∗ Alexander C. Li Deepak Pathak Katerina Fragkiadaki {mprabhud,tsungwek,acl2,dpathak,katef}@cs.cmu.edu Carnegie Mellon University 15 26 37 48 59 70 Top-1 Accuracy+14.0 points ResNet50 ViT-B/32 ConvNext-Large +11.9 points +22.8 points 18 22 26 30 34 +3.2 points +1.8 points +1.4 points CLIP: ViT-B/32 CLIP: ViT-B/16 CLIP: ViT-L/14 Top-1 Accuracy (a) ImageNet-C (online) (b) FGVC Aircraft (single-sample) Figure 1: Diffusion-TTA improves state-of-the-art pre-trained image classifiers and CLIP models across various benchmarks. Our model adapts pre-trained image discriminative models using feedback from pre-trained image generative diffusion models. Left: Image classification performance of pre-trained image classifiers improves after online adaptation. The image classifiers are pre-trained on ImageNet and adapted on ImageNet-C in an unsupervised manner using generative feedback. As can be seen, we get a significant boost across various model architectures. Right: Accuracy of open-vocabulary CLIP classifiers improves after single-sample adaptation, where we adapt to each unlabelled sample in the test set independently. CLIP is trained on millions of image-text pairs collected from the Internet [38], here we test it on the FGVC dataset [30]. Abstract The advancements in generative modeling, particularly the advent of diffusion mod- els, have sparked a fundamental question: how can these models be effectively used for discriminative tasks? In this work, we find that generative models can be great test-time adapters for discriminative models. Our method, Diffusion-TTA, adapts pre-trained discriminative models such as image classifiers, segmenters and depth predictors, to each unlabelled example in the test set using generative feedback from a diffusion model. We achieve this by modulating the conditioning of the diffusion model using the output of the discriminative model. We then maximize the image likelihood objective by backpropagating the gradients to discriminative model’s parameters. We show Diffusion-TTA significantly enhances the accuracy of vari- ous large-scale pre-trained discriminative models, such as, ImageNet classifiers, CLIP models, image pixel labellers and image depth predictors. Diffusion-TTA outperforms existing test-time adaptation methods, including TTT-MAE and TENT, and particularly shines in online adaptation setups, where the discriminative model is continually adapted to each example in the test set. We provide access to code, results, and visualizations on our website: diffusion-tta.github.io/. ∗Equal Technical Contribution 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2311.16102v2  [cs.CV]  29 Nov 20231 Introduction Currently, all state-of-the-art predictive models in machine learning, that care to predict an output y from an input x, are discriminative in nature, that means, they are functions trained to map directly x to (a distribution of) y. For example, existing successful image understanding models, whether classifiers, object detectors, segmentors, or image captioners, are trained to encode an image into features relevant to the downstream task through end-to-end optimization of the end objective, and thus ignore irrelevant details. Though they shine within the training distribution, they often dramatically fail outside of it [14, 25], mainly because they learn shortcut pathways to better fitp(y|x). Generative models on the other hand are trained for a much harder task of modeling p(x|y). At test-time, these models can be inverted using the Bayes rule to infer the hypothesis ˆy that maximizes the data likelihood of the example at hand, p(x|ˆy). Unlike discriminative models that take an input and predict an output in a feed-forward manner, generative models work backwards, by searching over hypotheses y that fit the data x. This iterative process, also known as analysis-by-synthesis [52], is slower than feed-forward inference. However, the ability to generate leads to a richer and a more nuanced understanding of the data, and hence enhances its discriminative potential [ 22]. Recent prior works, such as Diffusion Classifier [27], indeed find that inverting generative models generalizes better at classifying out-of-distribution images than popular discriminative models such as ResNet [17] and ViT [11]. However, it is also found that pure discriminative methods still outperform inversion of generative methods across almost all popular benchmarks, where test sets are within the training distribution. This is not too surprising since generative models aim to solve a much more difficult problem, and they were never directly trained using the end task objective. In this paper, instead of considering generative and discriminative models as competitive alternatives, we argue that they should be coupled during inference in a way that leverages the benefits of both, namely, the iterative reasoning of generative inversion and the better fitting ability of discriminative models. A naive way of doing so would be to ensemble them, that is pθ(y|x)+pϕ(y|x) 2 , where θ and ϕ stand for discriminative and generative model parameters, respectively. However, empirically we find that this does not result in much performance boost, as the fusion between the two models happens late, at the very end of their individual predictions. Instead, we propose to leverage generative models to adapt discriminative models at test time through iterative optimization, where both models are optimized for maximizing a sample’s likelihood. We present Diffusion-based Test Time Adaptation (TTA) (Diffusion-TTA), a method that adapts discriminative models, such as image classifiers, segmenters and depth predictors, to individual unlabelled images by using their outputs to modulate the conditioning of an image diffusion model and maximize the image diffusion likelihood. This operates as an inversion of the generative model, to infer the discriminative weights that result in the discriminative hypothesis with the highest conditional image likelihood. Our model is reminiscent of an encoder-decoder architecture, where a pre-trained discriminative model encodes the image into a hypothesis, such as an object category label, segmentation map, or depth map, which is used as conditioning to a pre-trained generative model to generate back the image. We show that Diffusion-TTA effectively adapts image classifiers for both in- and out-of-distribution examples across established benchmarks, including ImageNet and its variants, as well as image segmenters and depth predictors in ADE20K and NYU Depth dataset. Generative models have previously been used for test time adaptation of image classifiers and segmentors, by co-training the model under a joint discriminative task loss and a self-supervised image reconstruction loss, e.g., TTT-MAE [12] or Slot-TTA [37]. At test time, the discriminative model is finetuned using the image reconstruction loss. While these models show that adaptation boosts performance, this boost often stems from the subpar performance of their initial feed-forward discriminative model. This can be clearly seen in our TTT-MAE baseline [ 12], where the before- adaptation results are significantly lower than a pre-trained feed-forward classifier. In contrast, our approach refrains from any joint training, and instead directly adapts pre-trained discriminative models at test time using pre-trained generative diffusion models. We test our approach on multiple tasks, datasets and model architectures. For classification, we test pre-trained ImageNet clasisifers on ImageNet [9], and its out-of-distribution variants (C, R, A, v2, S). Further we test, large-scale open-vocabulary CLIP-based classifiers on CIFAR100, Food101, FGVC, Oxford Pets, and Flowers102 datasets. For adapting ImageNet classifiers we use DiT [36] as our generative model, which is a diffusion model trained on ImageNet from scratch. For adapting open-vocabulary CLIP-based classifiers, we use Stable Diffusion [40] as our generative model. We 2show consistent improvements over the initially employed classifier as shown in Figure 1. We also test on the adaptation of semantic segmentation and depth estimation tasks, where segmenters of SegFormer [49] and depth predictors of DenseDepth [ 1] performance are greatly improved on ADE20K and NYU Depth v2 dataset. For segmentation and depth prediction, we use conditional latent diffusion models [ 40] that are trained from scratch on their respective datasets. We show extensive ablations of different components of our Diffusion-TTA method, and present analyses on how diffusion generative feedback enhances discriminative models. We hope our work to stimulate research on combining discriminative encoders and generative decoder models, to better handle images outside of the training distribution. Our code and trained models are publicly available in our project’s website: diffusion-tta.github.io/. 2 Related Work Generative models for discriminative tasks Recently there have been many works exploring the application of generative models for discriminative tasks. We group them into the following three categories: (i) Inversion-Based Methods: Given a test input x and a conditional generative model pϕ(x|c), these methods make a prediction by finding the conditional representation c that maximizes the estimated likelihood pϕ(x|c) of the test input. Depending on the architecture of the conditional generative model, this approach can infer the text caption [ 8, 27], class label [27], or semantic map [7] of the input image. For instance, Li et al. [27] showed that finding the text prompt that maximizes the likelihood of an image under a text-to-image model like Stable Diffusion [40] is a strong zero-shot classifier, and finding the class index that maximizes the likelihood of an image under a class-conditional diffusion model is a strong standard classifier. (ii) Generative Models for Data Augmentation: This category of methods involves using a generative model to enhance the training data distribution [2, 45, 51]. These methods create synthetic data using the generative model and train a discriminative classifier on the augmented training dataset. For instance, Azizi et al. [2] showed that augmenting the real training data from ImageNet with synthetic data from Imagen [ 41], their large-scale diffusion model, improves classification accuracy. (iii) Generative Models as feature extractors: These techniques learn features with a generative model during pretraining and then fine-tune them on downstream discriminative tasks [3, 10, 18, 35, 42, 50]. Generative pre-training is typically a strong initialization for downstream tasks. However, these methods require supervised data for model fine-tuning and cannot be directly applied in a zero-shot manner. Our work builds upon the concept of Inversion-Based Methods (i). We propose that instead of inverting the conditioning representation, one should directly adapt a pre-trained discriminative model using the likelihood loss. This strategy enables us to effectively combine the best aspects of both generative and discriminative models. Our method is complementary to other methods of harnessing generative models, such as generative pre-training or generating additional synthetic labeled images Test-time adaptation Test-time adaptation (TTA) also referred as unsupervised domain adaptation (UDA) in this context, is a technique that improves the accuracy of a model on the target domain by updating its parameters without using any labelled examples. Methods such as pseudo labeling and entropy minimization [46] demonstrate that model accuracy can be enhanced by using the model’s own confident predictions as a form of supervision. These methods require batches of examples from the test distribution for adaptation. In contrast, TTA approaches based on self-supervised learning (SSL) have demonstrated empirical data efficiency. SSL methods jointly train using both the task and SSL loss, and solely use the SSL loss during test-time adaptation [5, 12, 16, 31, 37, 44]. These methods do not require batch of examples and can adapt to each example in the test set independently. Our contribution falls within the SSL TTA framework, where we use diffusion generative loss for adaptation. We refrain from any form of joint training and directly employ a pre-trained conditional diffusion model to perform test-time adaptation of a pre-trained discriminative model. 3 Test-Time Adaptation with Diffusion Models The architecture of Diffusion-TTA method is shown in Figure 2 and its pseudocode in shown in Algorithm 1. We discuss relevant diffusion model preliminaries in Section 3.1 and describe Diffusion-TTA in detail in Section 3.2. 3Pre-trained Discriminative  Model  fθ Input x Pre-trained Diffusion   Model ϵϕ Noise  𝜖 Noisy x Predicted  Noise ϵ Task Output y Condition c 2 Actual  𝜖 transform Backprop Backprop Backprop Diffusion Loss Figure 2: Architecture of Diffusion-TTA. Our method consists of discriminative and generative modules. Given an image x, the discriminative model fθ predicts task output y. The task output y is transformed into condition c. Finally, we use the generative diffusion model ϵϕ to measure the likelihood of the input image, conditioned on c. This consists of using the diffusion model ϵϕ to predict the added noise ϵ from the noisy image xt and condition c. We maximize the image likelihood using the diffusion loss by updating the discriminative and generative model weights via backpropagation. For a task-specific version of this figure, please refer to Figure 7 in the Appendix. 3.1 Diffusion Models A diffusion model learns to model a probability distributionp(x) by inverting a process that gradually adds noise to the image x. The diffusion process is associated with a variance schedule {βt ∈ (0, 1)}T t=1, which defines how much noise is added at each time step. The noisy version of sample x at time t can then be written xt = √¯αtx + √1 − ¯αtϵ where ϵ ∼ N(0, 1), is a sample from a Gaussian distribution (with the same dimensionality as x), αt = 1 − βt, and ¯αt = Qt i=1 αi. One then learns a denoising neural network ϵϕ(xt; t) that takes as input the noisy image xt and the noise level t and tries to predict the noise component ϵ. Diffusion models can be easily extended to draw samples from a distribution p(x|c) conditioned on a condition c, where c can be an image category, image caption, semantic map, a depth map or other information [ 28, 40, 54]. Conditioning on c can be done by adding c as an additional input of the network ϵϕ. Modern conditional image diffusion models are trained on large collections D = {(xi, ci)}N i=1 of N images paired with conditionings by minimizing the loss: Ldiff(ϕ; D) = 1 |D| X xi,ci∈D ||ϵϕ(√¯αtxi + √ 1 − ¯αtϵ, ci, t) − ϵ||2. (1) 3.2 Test-time Adaptation with Diffusion Models In Diffusion-TTA, the condition c of the diffusion model depends on the output of the discriminative model. Let fθ denote the discriminative model parameterized by parameters θ, that takes as input an image x and outputs y = fθ(x). We consider image classifiers, image pixel labellers or image depth predictors as candidates for discriminative models to adapt. For image classifiers, y represents a probability distribution overL categories, y ∈ [0, 1]L, y⊤1L = 1. Given the learnt text embeddings of a text-conditional diffusion model for the L categories ℓj ∈ Rd, j∈ {1..L}, we write the diffusion condition as c = PL j=1 yj · ℓj. For pixel labellers, y represents the set of per-pixel probability distributions over L categories, y = {yu ∈ [0, 1]L, yu⊤1L = 1, u∈ x}. The diffusion condition then is c = {PL j=1 yu j · ℓj, u∈ x}, where ℓj is the embedding of category j. 4For depth predictors y represents a depth map y ∈ R+w×h, where w, hthe width and height of the image, and c = y. As c is differentiable with respect to the discriminative model’s weights θ, we can now update them via gradient descent by minimizing the following diffusion loss: L(θ, ϕ) = Et,ϵ∥ϵϕ(√¯αtx + √ 1 − ¯αtϵ, c, t) − ϵ∥2. (2) We minimize this loss by sampling random timesteps t coupled with random noise variables ϵ. For online adaptation, we continually update the model weights across test images that the model encounters in a streaming manner. For single-sample adaptation, the model parameters are updated to each sample in the test set independently. Algorithm 1 Diffusion-TTA 1: Input: Test image x, discriminative model weights θ, diffusion model weights ϕ, adaptation steps N, batch size B, learning rate η. 2: for adaptation step s ∈ (1, . . . , N) do 3: Compute current discriminative output y = fθ(x) 4: Compute condition c based on output y 5: Sample timesteps {ti}B i=1 and noises {ϵi}B i=1 6: Loss L(θ, ϕ) = 1 N PB i=1 ∥ϵϕ(√¯αtix + √1 − ¯αtiϵi, c, ti) − ϵi∥2 7: Update classifier weights θ ← θ − η∇θL(θ, ϕ) 8: Optional: update diffusion weights ϕ ← ϕ − η∇ϕL(θ, ϕ) 9: end for 10: return updated output y = fθ(x) Implementation Details. Our experiments reveal that minimizing the variance in the diffusion loss significantly enhances our test-time adaptation performance. We achieve this by crafting a larger batch size through randomly sampling timesteps from a uniform distribution, coupled with randomly sampling noise vectors from a standard Gaussian distribution. We conduct our experiments on a single NVIDIA-A100 40GB VRAM GPU, with a batch size of approximately 180. Since our GPU fits only a batch size of 20, we utilize a gradient accumulation of nine steps to expand our batch size to 180. Consequently, this setup results in an adaptation time of roughly 55 seconds per example. We use Stochastic Gradient Descent (SGD) with a learning rate of 0.005 or Adam optimizer [24] with a learning rate of 0.00001. We set momentum to 0.9. For all experiments, we adjust all the parameters of the discriminative model. We freeze the diffusion model parameters for the open-vocabulary experiments in Section 4.2 with CLIP and Stable Diffusion 2.0, and otherwise, update all diffusion model parameters. For CLIP, instead of adapting the whole network we only adapt the LoRA adapter weights [23] added to the query and value projection layers of the CLIP model. For our experiments on adapting to ImageNet distribution shift in Section 4.1, we observe a significant performance boost when adapting both the discriminative and generative model. This finding suggests that our method effectively combines the distinct knowledge encoded by these two models. We present a more detailed analyses in Section 4.4. 4 Experiments We test Diffusion-TTA in adapting ImageNet classifiers [11, 17, 29], CLIP models [38], image pixel labellers [ 49], and depth predictors [ 1] across multiple image classification, semantic segmentation, and depth estimation datasets, for both in-distribution and out-of-distribution test images. Our experiments aim to answer the following questions: 1. How well does Diffusion-TTAtest-time adapt ImageNet and CLIP classifiers in comparison to other TTA methods under online and single-sample adaptation settings? 2. How well does Diffusion-TTA test-time adapt semantic segmentation and depth estimation models? 53. How does performance of our model vary for in- and out-of-distribution images? 4. How does performance of our model vary with varying hyperparameters, such as layers to adapt, batchsize to use, or diffusion timesteps to consider? Evaluation metrics. We report the top-1 accuracy, mean pixel accuracy, and Structure Similar- ity index [48] on classification, semantic segmentation, and depth estimation benchmark datasets respectively. 4.1 Test-time Adaptation of Pre-trained ImageNet Classifiers We use Diffusion-TTA to adapt multiple ImageNet classifiers with varying backbone architectures and sizes: ResNet-18 [ 17], ResNet-50, ViT-B/32 [11], and ConvNext-Tiny/Large [ 29]. For fair comparisions, we use Diffusion Transformer (DiT-XL/2) [36] as our class-conditional generative model, which is trained on ImageNet from scratch. Datasets We consider the following datasets for TTA of ImageNet classifiers: ImageNet [9] and its out-of-distribution counterparts: ImageNet-C [ 19] (level-5 gaussian noise), ImageNet-A [ 21], ImageNet-R [20], ImageNetV2 [39], and Stylized ImageNet [13]. Baselines We compare our model against the following state-of-the-art TTA approaches. We use their official codebases for comparision: • TTT-MAE [12] is a per-image test-time adaptation model trained under a joint classification and masked autoencoding loss, and test-time adapted using only the (self-supervised) masked autoencoding loss. For comparison, we use the numbers reported in the paper when possible, else we test the publicly available model. • TENT [46] is a TTA method that adapts the batchnorm normalization parameters by minimizing the entropy (maximizing the confidence) of the classifier at test time per example. • COTTA [47] test-time adapts a (student) classifier using pseudo labels, that are the weighted- averaged classifications across multiple image augmentations predicted by a teacher model. ImageNet ImageNet-A ImageNet-R ImageNet-C ImageNet-V2 ImageNet-S Customized ViT-L/16 classifier [12] 82.1 14.4 33.0 17.5 72.5 11.9 + TTT-MAE (single-sample) 82.0 (-0.1) 21.3 (+6.9) 39.2 (+6.2) 27.5 (+10.0) 72.3 (-0.2) 20.2 (+0.3) ResNet18 69.5 1.4 34.6 2.6 57.1 7.7 + TENT (online) 63.0 (-6.5) 0.6 (-0.8) 34.7 (+0.1) 12.1 (+9.5) 52.0 (-5.1) 9.8 (+2.1) + CoTTA (online) 63.0 (-6.5) 0.7 (-0.7) 34.7 (+0.1) 11.7 (+9.1) 52.1 (-5.0) 9.7 (+2) + Diffusion-TTA (single-sample)77.2 (+7.7) 6.1 (+4.7) 39.7 (+5.1) 4.5 (+1.9) 63.8 (+6.7) 12.3 (+4.6) ViT-B/32 75.7 9.0 45.2 39.5 61.0 15.8 + TENT (online) 75.7 (0.0) 9.0 (0.0) 45.3 (+0.1) 38.9 (-0.6) 61.1 (+0.1) 10.4 (-5.4) + CoTTA (online) 75.8 (+0.1) 8.6 (-0.4) 45.0 (-0.2) 40.0 (+0.5) 60.9 (-0.1) 1.1 (-14.7) + Diffusion-TTA (single-sample)77.6 (+1.9) 11.2 (+2.2) 46.5 (+1.3) 41.4 (+1.9) 64.4 (+3.4) 21.3 (+5.5) ConvNext-Tiny 81.9 22.7 47.8 16.4 70.9 20.2 + TENT (online) 79.3 (-2.6) 10.6 (-12.1) 42.7 (-5.1) 2.7 (-13.7) 69.0 (-1.9) 19.9 (-0.3) + CoTTA (online) 80.5 (-1.4) 13.2 (-9.5) 47.2 (-0.6) 13.7 (-2.7) 68.9 (-2.0) 19.3 (-0.9) + Diffusion-TTA (single-sample)83.1 (+1.2) 25.8 (+3.1) 49.7 (+1.9) 21.0 (+4.6) 71.5 (+0.6) 22.6 (+2.4) Table 1: Singe-sample test-time adaptation of ImageNet-trained classifiers. We observe consistent and significant performance gain across all types of classifiers and distribution drifts. We present classification results on in-distribution (ImageNet) and out-of-distribution (ImageNet-C, R, A, V2, and S) for single-sample adaptation in Table 1. Further we present online adaptation results in Table 2. We evaluate on ImageNet-C, which imposes different types of corruption on ImageNet. For both COTTA and TENT we always report online-adaptation results as they are not applicable in the single-sample setting. 6ImageNet Corruption: Gaussian Noise Fog Pixelate Snow Contrast Customized ViT-L/16 classifier [12] 17.1 38.7 47.1 35.6 6.9 + TTT-MAE (online) 37.9 (+20.8) 51.1 (+12.4) 65.7 (+18.6) 56.5 (+20.9) 10.0 (+3.1) ResNet50 6.3 25.2 26.5 16.7 3.6 + TENT (online) 12.3 (+6.0) 43.2 (+18.0)41.8 (+15.3) 28.4 (+11.7)12 (+8.4) + CoTTA (online) 12.2 (+5.9) 42.4 (+17.2) 41.7 (+15.2) 28.6 (+11.9) 11.9 (+8.3) + Diffusion-TTA (online) 19.0 (+12.7) 43.2 (+18.0) 50.2 (+23.7) 33.6 (+16.9)2.7 (-0.9) ViT-B/32 39.5 35.9 55.0 30.0 31.5 + TENT (online) 38.9 (-0.6) 35.8 (-0.1) 55.5 (+0.5) 30.7 (+0.7) 32.1 (+0.6) + CoTTA (online) 40.0 (+0.5) 34.6 (-1.3) 54.5 (-0.5) 29.7 (-0.3) 32.0 (+0.5) + Diffusion-TTA (online) 46.5 (+7.0) 56.2 (+20.3) 64.7 (+9.7) 50.4 (+20.4) 33.6 (+2.1) ConvNext-Tiny 16.4 32.3 37.2 38.3 32.0 + TENT (online) 2.7 (-13.7) 5.0 (-27.3) 43.9 (+6.7) 15.2 (-23.1) 40.7 (-18.7) + CoTTA (online) 13.7 (-2.7) 29.8 (-2.5) 37.3 (+0.1) 26.6 (-11.7) 32.6 (+0.6) + Diffusion-TTA (online) 47.4 (+31.0) 65.9 (+33.6) 69.0 (+31.8) 62.6 (+24.3) 46.2 (+14.2) ConvNext-Large 33.0 34.4 49.3 44.5 39.8 + TENT (online) 30.8 (-2.2) 53.5 (+19.1) 51.1 (+1.8) 44.6 (+0.1) 52.4 (+12.6) + CoTTA (online) 33.3 (+0.3) 15.1 (-18.7) 34.6 (-15.3) 7.7 (-36.8) 10.7 (-29.1) + Diffusion-TTA (online) 54.9 (+21.9) 67.7 (+33.3) 71.7 (+22.4) 64.8 (+20.3) 55.7 (+15.9) Table 2: Online test-time adaptation of ImageNet-trained classifiers on ImageNet-C. Our model achieves significant performance gain using online adaptation across all types of classifiers and distribution drifts. Our conclusions are as follows: (i) Our method consistently improves classifiers on in-distribution (ImageNet) and OOD test images across different image classifier architectures. For all ResNet, ViT, and ConvNext-Tiny, we observe significant performance gains. (ii) Diffusion-TTA outperforms TENT and COTTA across various classifier architectures. Our results are consistent with the analysis in [55]: methods that primarily work under online settings (TENT or CoTTA) are not robust to different types of architectures and distribution shifts (see Table 4 in [55]). (iii) Diffusion-TTA outperforms TTT-MAEeven with a smaller-size classifier. ConvNext-tiny (28.5M params) optimized by Diffusion-TTA (online) achieves better performance than a much bigger custom backbone of ViT-L + ViT-B (392M params) optimized by TTT-MAE (online). (iv) Diffusion-TTA significantly improves ConvNext-Large, in online setting. ConvNext-Large is the largest available ConvNext model with 197M parameters and achieves state-of-the-art performance on ImageNet. 4.2 Test-time Adaptation of Open-Vocabulary CLIP Classifiers CLIP models are trained across millions of image-caption pairs using contrastive matching of the language and image feature embeddings. We test Diffusion-TTA for adapting three different CLIP models with different backbone sizes: ViT-B/32, ViT-B/16, and ViT-L/14. Experiments are conducted under single-sample adaptation setup. We convert CLIP into a classifier following [38], where we compute the latent embedding for each text category label in the dataset using the CLIP text encoder, and then compute its similarity with the image latent to predict the classification logits across all class labels. Note that the CLIP models have not seen test images from these datasets during training. Datasets We consider the following datasets for TTA of CLIP classifiers: ImageNet [9], CIFAR- 100 [26], Food101 [6], Flowers102 [32], FGVC Aircraft [30], and Oxford-IIIT Pets [34] annotated with 1000,100,101, 102, 100, and 37 classes, respectively. 7Food101 CIFAR-100 Aircraft Oxford Pets Flowers102 ImageNet CLIP-ViT-B/32 82.6 61.2 17.8 82.2 66.7 57.5 + Diffusion-TTA (Ours)86.2 (+3.6) 62.8 (+1.6) 21.0 (+3.2) 84.9 (+2.7) 67.7 (+1.0) 60.8 (+3.3) CLIP-ViT-B/16 88.1 69.4 22.8 85.5 69.3 62.3 + Diffusion-TTA (Ours)88.8 (+0.7) 69.0 (-0.4) 24.6 (+1.8) 86.1 (+0.6) 71.5 (+2.2) 63.8 (+1.5) CLIP-ViT-L/14 93.1 79.6 32.0 91.9 78.8 70.0 + Diffusion-TTA (Ours)93.1(0.0) 80.6 (+1.0) 33.4 (+1.4) 92.3 (+0.4) 79.2 (+0.4) 71.2 (+1.2) Table 3: Single-sample Test-time adaptation of CLIP classifiers. Our evaluation is performed across multiple model sizes and a variety of test datasets. We show test-time adaptation results in Table 3.Our method improves CLIP classifiers of different sizes consistently over all datasets, including small-scale (CIFAR-100), large-scale (ImageNet), and fine-grained (Food101, Aircraft, Pets, and Flowers102) datasets. Corruption: Clean Gaussian-Noise Fog Frost Snow Contrast Shot Segmentation: SegFormer 66.1 65.3 63.0 58.0 55.2 65.3 63.3 + Diffusion-TTA 66.1 (0.0) 66.4 (+1.1) 65.1 (+2.1) 58.9 (+0.9) 56.6 (+1.4) 66.4 (+1.1) 63.7 (+0.4) Depth: DenseDepth 92.4 79.1 81.6 72.2 72.7 77.4 81.3 + Diffusion-TTA 92.6 (+0.3) 82.1 (+3.0) 84.4 (+2.8) 73.0 (+0.8) 74.1 (+1.4)77.4 82.1 (+0.8) Table 4: Single-sample test-time adaptation for semantic segmentation on ADE20K and depth estimation on NYU Depth v2. We observe consistent and significant performance gain across all types of distribution drifts. Image ground-truth Before-TTA After-TTA Figure 3: Semantic segmentation before and after TTA. We colorize ground-truth / predicted segmentation based on discrete class labels. Our method improves segmentation after adaptation. 4.3 Test-time Adaptation on Semantic Segmentation and Depth Estimation We use Diffusion-TTA to adapt semantic segmentors of SegFormer [ 49] and depth predictors of DenseDepth [1]. We use a latent diffusion model [40] which is pre-trained on ADE20K and NYU Depth v2 dataset for the respective task. The diffusion model concatenates the segmentation/depth map with the noisy image during conditioning. Datasets We consider ADE20K and NYU Depth v2 test sets for evaluating semantic segmentation and depth estimation. Note that both discriminative and generative diffusion models are trained on the same dataset. We evaluate with different types of image corruption under single-sample settings. We show quantitative semantic segmentation and depth estimation results in Table 4. Further we show some qualitative semantic segmentation results before and after TTA in Figure 3. Our Diffusion-TTA improves both tasks consistently across all types of distribution drifts. 84.4 Ablations We present ablative analysis of various design choices. We study how Diffusion-TTA varies with hyperparameters such as diffusion timesteps, number of samples per timestep and batchsize. We also study the effect of adapting different model parameters. Additionally we visualize before and after TTA adaptation results in Figure 5. Figure 4: Diffusion loss of class labels at each diffusion timestep. The ground-truth class does not al- ways have lower loss across all timesteps, compared to incorrect classes. Thus using only single dif- fusion timestep is a bad approxima- tion of the likelihood. Ablation of hyperparameters In Table 5, we ablate hyperpa- rameters of our model on ImageNet-A dataset with ConvNext as our pre-trained classifier and DiT as our diffusion model. If we perform test-time adaptation using a single randomly sampled timestep and noise latent (+diffusion TTA), we find a significant reduction in the classification accuracy ( −2.9%). Increasing the batch size from 1 to 180 by sampling random timesteps from an uniform distribution (+ timestep aug BS=180) gives a significant boost in accuracy (+4.8%). As shown in Figure 4, correct class labels do not always have lower loss across all timesteps, and thus using only single diffusion step is a bad approximation of the likelihood. Further sampling random noise latents per timestep (+ noise aug) gives an added boost of (+0.2%). Finally, adapting the diffusion weights of DiT (+ adapting diffusion weights ) in Section 4.1, gives further (+1.1%) boost. Ablation of parameters/layers to adapt In Table 6, we ablate different paramters to adapt. We conduct our ablations on on ImageNet and ImageNet-R while using ResNet18 and DiT as our discriminative and generative models. We compare against following baselines: 1. we randomly initialize the classifier instead of using the pre-trained weights 2. we do not use any pre-trained classifier, instead we initialize the logits using zeros and optimize them per example using diffusion objective (adapt logits), 3. we average the probabilities of the adapt logits baseline with the probabilities predicted by ResNet18 ( adapt logits + ensemble), 4. we adapt only the batch normalization layers in ResNet18 ( adapt BN), 5. we adapt only the last fully connected layer in ResNet18 ( adapt last FC layer ), 6. we adapt the whole ResNet18 (adapt classifier), and 7. we adapt the whole ResNet18 and DiT, which refers to our method Diffusion-TTA. Adapting the whole classifier and generative model achieves the best performance. We randomly sample one image per category for the experiment. ImageNet-A ConvNext-Tiny [29] 22.7 + diffusion loss TTA 19.8 (-2.9) + timestep aug 24.5 (+4.8) + noise aug 24.7 (+0.2) + adapting diffusion weights25.8 (+1.1) Table 5: Ablative analysis of Diffusion-TTA components for Con- vNext classifier and DiT diffusion model. We evaluate single-sample adptation for classification on ImageNet-A. ImageNet ImageNet-R ResNet18 68.4 37.0random init 0.7 4.0adapt logits 71.8 31.0adapt logits + ensemble 72.6 36.5adapt BN 69.0 37.0adapt last FC layer 68.6 37.5adapt classifier 73.0 38.0adapt classifier + adapt diffusion (Ours)78.2 42.6 Table 6: Ablative analysis of model / layer adaptation. We evaluate single-sample adaptation for classifica- tion on ImageNet and ImageNet-R. Our Diffusion-TTA adapts both the classifier (ResNet18) and diffusion model (DiT), achieving best adaptation improvement. 4.5 Discussion - Limitations Diffusion-TTA optimizes pre-trained discriminative and generative models using a generative dif- fusion loss. A trivial solution to this optimization could be the discriminative model fθ overfitting 9Ground truth RGB Before-TTA RGB Prediction  After-TTA top-3 classificationBefore-TTA top-3 classification After-TTA RGB Prediction Figure 5: Visualizing Diffusion-TTA improvement across adaptation steps. From left to right: We show input image, predicted class probabilities before adaptation (green bars indicate the ground truth category), predicted RGB image via DDIM inversion when the diffusion model is conditioned on predicted class probabilities, the predicted class probabilities after adaptation and predicted RGB image after adaptation. As can be seen, Classification ability of the classifier improves as diffusion model’s ability to reconstruct the input image improves, thus indicating that there is strong correlation between the diffusion loss and the classification accuracy. to the generative loss and thus predicting precisely pϕ(c|x). Empirically we find that this does not happen: Diffusion-TTA consistently outperforms Diffusion Classifier [27], as shown in Table 7. We conjecture that the reason why the discriminative model does not overfit to the generative loss is because of the weight initialization of the (pre-trained) discriminative model, which prevents it from converging to this trivial solution. For instance, if the predictions of the generative and discriminative model are too far from one another that is: distance(pϕ(c|x), pθ(c|x)) is too high, then it becomes very difficult to optimize θ to reach a location such that it predicts pϕ(c|x). To verify our conjecture, we run a few ablations in Table 6, i) We randomly initializeθ, ii) Instead of optimizing θ we directly optimize the conditioning variable c. In both cases, we find that results are significantly worse than when optimizing the pre-trained discriminative model. Diffusion-TTA modulates the task conditioning of an image diffusion model with the output of the discriminative model. While this allows us to use readily available pre-trained models it also prevents us from having a tighter integration between the generative and the discriminative parts. Exploring this integration is a direct avenue of future work. Currently the generative model learnsp(x|y), where x is the input image. Training different conditional generative model for different layers such as p(l1|y) or p(l1|l2), where l1 and l2 are features at different layers, could provide diverse learning signals and it is worth studying. Finally, Diffusion-TTA is significantly slow as we have to sample multiple timesteps to get a good estimate of the diffusion loss. Using Consistency Models [ 43] to improve the test-time-adaptation speed is a direct avenue for future work. 5 Conclusion We introduced Diffusion-TTA, a test time adaptation method that uses generative feedback from a pre-trained diffusion model to improve pre-trained discriminative models, such as classifiers, segmenters and depth predictors. Adaptation is carried out for each example in the test-set by backpropagating diffusion likelihood gradients to the discriminative model weights. We show that our model outperforms previous state-of-the-art TTA methods, and that it is effective across multiple discriminative and generative diffusion model variants. The hypothesis of visual perception as inversion of a generative model has been pursued from early years in the field [33, 53].The formidable performance of today’s generative models and the presence of feed-forward and top-down pathways in the visual cortex [15] urges us to revisit this paradigm. We hope that our work stimulates further research efforts in this direction. 10Acknowledgements We thank Shagun Uppal, Yulong Li and Shivam Duggal for helpful discussions. This work is supported by DARPA Machine Common Sense, an NSF CAREER award, an AFOSR Young Investigator Award, and ONR MURI N00014-22-1-2773. AL is supported by the NSF GRFP DGE1745016 and DGE2140739. References [1] I. Alhashim and P. Wonka. High quality monocular depth estimation via transfer learning.arXiv e-prints, abs/1812.11941:arXiv:1812.11941, 2018. URL https://arxiv.org/abs/1812. 11941. [2] S. Azizi, S. Kornblith, C. Saharia, M. Norouzi, and D. J. Fleet. Synthetic data from diffusion models improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023. [3] D. Baranchuk, I. Rubachev, A. V oynov, V . Khrulkov, and A. Babenko. Label-efficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021. [4] A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019. [5] A. Bartler, A. Bühler, F. Wiewel, M. Döbler, and B. Yang. Mt3: Meta test-time training for self-supervised test-time adaption. In International Conference on Artificial Intelligence and Statistics, pages 3080–3090. PMLR, 2022. [6] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101 – mining discriminative components with random forests. In European Conference on Computer Vision, 2014. [7] R. Burgert, K. Ranasinghe, X. Li, and M. S. Ryoo. Peekaboo: Text to image diffusion models are zero-shot segmentors. arXiv preprint arXiv:2211.13224, 2022. [8] K. Clark and P. Jaini. Text-to-image diffusion models are zero-shot classifiers. arXiv preprint arXiv:2303.15233, 2023. [9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. [10] J. Donahue, P. Krähenbühl, and T. Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016. [11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [12] Y . Gandelsman, Y . Sun, X. Chen, and A. Efros. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 35:29374–29385, 2022. [13] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel. Imagenet- trained CNNs are biased towards texture; increasing shape bias improves accuracy and ro- bustness. In International Conference on Learning Representations , 2019. URL https: //openreview.net/forum?id=Bygh9j09KX. [14] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665–673, 2020. [15] C. D. Gilbert and M. Sigman. Brain states: Top-down influences in sensory processing. Neuron, 54(5):677–696, 2007. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2007.05.019. URL https://www.sciencedirect.com/science/article/pii/S0896627307003765. [16] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new ap- proach to self-supervised learning. Advances in neural information processing systems , 33: 21271–21284, 2020. 11[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778, 2016. [18] K. He, X. Chen, S. Xie, Y . Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000–16009, 2022. [19] D. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common corrup- tions and perturbations. arXiv preprint arXiv:1903.12261, 2019. [20] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Para- juli, M. Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340–8349, 2021. [21] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15262–15271, 2021. [22] G. E. Hinton. To recognize shapes, first learn to generate images. Progress in brain research, 165:535–547, 2007. [23] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [24] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [25] P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Ya- sunaga, R. L. Phillips, S. Beery, J. Leskovec, A. Kundaje, E. Pierson, S. Levine, C. Finn, and P. Liang. WILDS: A benchmark of in-the-wild distribution shifts. CoRR, abs/2012.07421, 2020. URL https://arxiv.org/abs/2012.07421. [26] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009. [27] A. C. Li, M. Prabhudesai, S. Duggal, E. Brown, and D. Pathak. Your diffusion model is secretly a zero-shot classifier. arXiv preprint arXiv:2303.16203, 2023. [28] Y . Li, H. Liu, Q. Wu, F. Mu, J. Yang, J. Gao, C. Li, and Y . J. Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint arXiv:2301.07093, 2023. [29] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976–11986, 2022. [30] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013. [31] S. Manli, N. Weili, H. De-An, Y . Zhiding, G. Tom, A. Anima, and X. Chaowei. Test-time prompt tuning for zero-shot generalization in vision-language models. In NeurIPS, 2022. [32] M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages 722–729. IEEE, 2008. [33] B. A. Olshausen. Perception as an inference problem. 2013. URL https://api. semanticscholar.org/CorpusID:1194136. [34] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V . Jawahar. Cats and dogs. InIEEE Conference on Computer Vision and Pattern Recognition, 2012. [35] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2536–2544, 2016. 12[36] W. Peebles and S. Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. [37] M. Prabhudesai, A. Goyal, S. Paul, S. Van Steenkiste, M. S. Sajjadi, G. Aggarwal, T. Kipf, D. Pathak, and K. Fragkiadaki. Test-time adaptation with slot-centric models. In International Conference on Machine Learning, pages 28151–28166. PMLR, 2023. [38] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. [39] B. Recht, R. Roelofs, L. Schmidt, and V . Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 5389–5400. PMLR, 2019. [40] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684–10695, 2022. [41] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon- tijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35: 36479–36494, 2022. [42] M. Singh, Q. Duval, K. V . Alwala, H. Fan, V . Aggarwal, A. Adcock, A. Joulin, P. Dollár, C. Feichtenhofer, R. Girshick, et al. The effectiveness of mae pre-pretraining for billion-scale pretraining. arXiv preprint arXiv:2303.13496, 2023. [43] Y . Song, P. Dhariwal, M. Chen, and I. Sutskever. Consistency models. 2023. [44] Y . Sun, X. Wang, Z. Liu, J. Miller, A. Efros, and M. Hardt. Test-time training with self- supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229–9248. PMLR, 2020. [45] B. Trabucco, K. Doherty, M. Gurinas, and R. Salakhutdinov. Effective data augmentation with diffusion models. arXiv preprint arXiv:2302.07944, 2023. [46] D. Wang, E. Shelhamer, S. Liu, B. Olshausen, and T. Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020. [47] Q. Wang, O. Fink, L. Van Gool, and D. Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022. [48] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600–612, 2004. [49] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077–12090, 2021. [50] J. Xu, S. Liu, A. Vahdat, W. Byeon, X. Wang, and S. De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. arXiv preprint arXiv:2303.04803, 2023. [51] T. Yu, T. Xiao, A. Stone, J. Tompson, A. Brohan, S. Wang, J. Singh, C. Tan, J. Peralta, B. Ichter, et al. Scaling robot learning with semantically imagined experience. arXiv preprint arXiv:2302.11550, 2023. [52] A. Yuille and D. Kersten. Vision as bayesian inference: analysis by synthesis? Trends in cognitive sciences, 10:301–8, 08 2006. doi: 10.1016/j.tics.2006.05.002. [53] A. Yuille and D. Kersten. Vision as Bayesian inference: analysis by synthesis? Trends in Cognitive Sciences, 10:301–308, 2006. [54] L. Zhang and M. Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023. [55] H. Zhao, Y . Liu, A. Alahi, and T. Lin. On pitfalls of test-time adaptation. arXiv preprint arXiv:2306.03536, 2023. 13A Appendix We present Diffusion-TTA, a test-time adaptation approach that modulates the text conditioning of a text conditional pre-trained image diffusion model to adapt pre-trained image classifiers, large-scale CLIP models, image pixel labellers, and image depth predictors to individual unlabelled images. We show improvements on multiple datasets including ImageNet and its out-of-distribution variants (C, R, A, V2, and S), CIFAR-100, Food101, FGVC, Oxford Pets, and Flowers102 over the initially employed classifiers. We also show performance gain on ADE20K and NYU Depth v2 under various distribution drifts over the pre-trained pixel labellers and depth predictors. In the Supplementary, we include further details on our work: 1. We provide comparison to Diffusion Classifier on open-V ocabulary classification in Sec- tion A.1. 2. We provide additional results with DiT on ObjectNet dataset in Section A.2. 3. We present architecture diagrams for the tasks of image classification and semantic segmen- tation in Section A.3 4. We provide a detailed analysis of the computational speed ofDiffusion-TTAin Section A.4. 5. We detail the hyperparameters and input pre-processing in Section A.5. 6. We provide details of the datasets used for our experiments in Section A.6. Food101 CIFAR-100 FGVC Oxford Pets Flowers102 ImageNet Diffusion Classifier [27] 77.9 42.7 24.3 85.7 56.8 58.4 CLIP-ViT-L/14 93.1 79.6 32.0 91.9 78.8 70.0 + Diffusion-TTA (Ours)93.1(0.0) 80.6 (+1.0) 33.4 (+1.4) 92.3 (+0.4) 79.2 (+0.4) 71.2 (+1.2) Table 7: Comparison to Diffusion Classifier on open-vocabulary classification. Our model outperforms Diffusion Classifier consistently over all benchmark datasets. A.1 Comparison to Diffusion Classifier on Open-Vocabulary Classification We perform single-sample test-tiem adaptation of CLIP classifiers and compare to Diffusion Clas- sifier [27] on open-vocabulary classification. We test on Food101, CIFAR-100, FGVC, Oxford Pets, Flowers102, and ImageNet datasets. As shown in Table 7, Diffusion-TTA consistent improves CLIP-ViT-L/14 and outperforms Diffusion Classifier over all benchmark datasets. ObjectNet Customized ViT-L/16 classifier [12] 37.8 + TTT-MAE 38.9 ResNet18 23.3 + Diffusion-TTA 26.0 (+2.7) ViT-B/32 26.6 + Diffusion-TTA 31.3 (+4.7) ConvNext-Tiny 41.0 + Diffusion-TTA 43.7 (+2.7) Table 8: Single-sample test-time adap- tation on ObjectNet. We use ImageNet pre-trained ResNet18, ViT-B/32, and Con- vNext as our classifiers. Our method im- proves pre-trained image classifiers on out- of-distribution images of ObjectNet. ImageNet ObjectNet (a) pose (b) scene (c) viewpoint Figure 6: Comparison of image samples from Ima- geNet and ObjectNet. ObjectNet images have more (a) different poses, (b) complex background scenes, and (c) camera viewpoints. 14A.2 Test-time Adaptation of Pre-trained ImageNet Classifiers on ObjectNet We extend Table 1 in our paper to ObjectNet [4] dataset. ObjectNet is a variant of ImageNet, created by keeping ImageNet objects in uncommon configurations and poses as shown in Figure 6. ObjectNet has 113 common classes with ImageNet, a single class in ObjectNet can correspond to multiple classes in ImageNet. If the predicted class belongs in it’s respective set of ImageNet classes, then it’s considered as ‘correct’ if not ‘incorrect’. We report the results for ObjectNet in Table 8, following the setup of Section 4.1 in the main paper. Our Diffusion-TTA consistently improves pre-trained ImageNet classifiers with different backbone architectures. (a) Diffusion-TTA for image classification (b) Diffusion-TTA for semantic segmentation Figure 7: Architecture diagrams for the tasks of (a) image classification and (b) semantic segmentation. For image classification, we generate a text prompt vector (condition c) by doing a weighted average over the probability distribution predicted by the pretrained classifier. We then condition c on a pre-trained text-conditioned diffusion model via adaptive layer normalization or cross attentions. For semantic segmentation, we generate per-pixel text prompt (condition c) using a pre-trained segmentation model. We then condition c on a pre-trained segementation conditioned Diffusion models via channel-wise concatenation. In the Figure on © denotes channel-wise concate- nation. A.3 Architecture Diagrams for the Tasks of Image Classification and Semantic Segmentation We show architectural details for the tasks of image classification and semantic segmentation in Figure 7. Both tasks share the same architectural design, but differ in the way to condition on c. For image classification, we generate a text prompt vector (condition c) by weighted averaging over the probability distribution predicted by the pre-trained classifier. We condition c on a pre-trained text-conditioned diffusion model via adaptive layer normalization or cross attentions. For semantic segmentation, we generate per-pixel text prompt (condition c) using a pre-trained image pixel labeller. We condition c on a pre-trained segementation conditioned Diffusion models via channel-wise concatenation. A.4 Analysis of Computation Speed We compare the computational latency between Diffusion-TTA and Diffusion Classifier [ 27] on image classification. Diffusion Classifier [27] is a discrete label search method that requires a forward pass through the diffusion model for each category in the dataset. The computational time increases linearly with the number of categories. In contrast, Diffusion-TTA is a gradient-based optimization method that updates model weights to maximize the image likelihood. The computational speed depends on the number of adaptation steps and the model size, not the number of categories. As shown in Figure 8, out method is less efficient than Diffusion Classifier with fewer categories, but more efficient with more categories. Notably, discrete label search methods, like Diffusion Classifier, do not apply to online adaptation and would be infeasible for dense labeling tasks (searching over pixel labellings is exponential wrt the number of pixels in the image). Computation compared to Diffusion Classifier. Diffusion Classifier [27] inverts a diffusion model by performing discrete optimization over categorical text prompts, instead we obtain continuous gradients to search much more effectively over a pre-trained classifier’s parameter space. This design choice makes significant trade-offs in-terms of computation costs. For instance, Diffusion Classifier requires to do a forward pass for each category seperately and thus the computation increases linearly 15Inference time per Image (in Seconds) 1.0 100.0 10000.0 CIFAR10 Pets Food101 ImageNetA ImageNet-1K ImageNet-21K Diﬀusion Classiﬁer (Li et al.)  Ours Linear wrt Number of Categories Constant wrt Number of Categories (10 Categories) (37 Categories) (101 Categories) (200 Categories) (1000 Categories) (21,000 Categories) Figure 8: Comparison of computation speed between Diffusion-TTA and Diffusion Classifier on image classification. We plot the required computation time with increasing numbers of image categories. Diffusion Classifier [27] requires a forward pass through the diffusion model for each category in the dataset and the computation increases linearly with the number of categories. On the other hand, our method adapts pre-trained classifiers and thus the computation is instead dependent on the throughput of the classifier. Our method becomes more computationally efficient than Diffusion Classifier as the number of categories in the dataset increases. with the number of categories, however for us it’s independent of the number of categories. We compare the per-example inference speed in Figure 8, across various datasets. A.5 Hyper-parameters and Input Pre-Processing For test-time-adaptation of individual images, we randomly sample 180 different pairs of noise ϵ and timestep t for each adaptation step, composing a mini-batch of size 180. Timestep t is sampled over an uniform distribution from the range 1 to 1000 and epsilon ϵ is sampled from an unit gaussian. We apply 5 test-time adaptation steps for each input image. We adopt Stochastic Gradient Descent (SGD) (or Adam optimizer [24]), and set learning rate, weight decay and momentum to 0.005 (or 0.00001), 0, and 0.9, respectively. To isolate the contribution of the improvement due to the diffusion model we do not use any form of data augmentation during test-time adaptation. We use Stable Diffusion v2.0 [40] to adapt CLIP models. For the CLIP classifier, we follow their standard preprocessing step where we resize and center-crop to a resolution of 224 × 224 pixels. For Stable Diffusion, we process the images by resizing them to a resolution of 512 × 512 pixels which is the standard image size in Stable Diffusion models. For the CLIP text encoder in Stable Diffusion and the classifier we use the text prompt of \"a photo of a <class_name>\", where <class_name> is the name of the class label as mentioned in the dataset. For the adaptation of ImageNet classifiers, we use pre-trained Diffusion Transformers (DiT) [ 36] specifically their XL/2 model of resolution256×256, which is trained on ImageNet1K. For ImageNet classifiers we follow the standard data pre-processing pipeline where we first resize the image to 232 × 232 and then do center crop to a resolution of 224 × 224 pixels. For DiT we resize the image to 256 × 256, before passing it as input to their model. A.6 Datasets In the following section, we provide further details on the datasets used in our experiments. We adapt CLIP classifiers to the following datasets incuding ImageNet: 1. CIFAR-100 [26] is a compact, generic image classification dataset featuring 100 unique object classes, each with 100 corresponding test images. The resolution of these images is relatively low, standing at a size of 32 × 32 pixels. 2. Food101 [6] is an image dataset for fine-grained food recognition. The dataset annotates 101 food categories and includes 250 test images for each category. 3. FGVC Airplane [30] is an image dataset for fine-grained categorization containing 100 different aircraft model variants, each with 33 or 34 test images. 4. Oxford Pets [34] includes 37 pet categories, each with roughly 100 images for 16Food101 CIFAR-100 FGVC Oxford Pets Flowers102 ImageNet Figure 9: Image datasets for zero-shot classification. From left to right: we show an image example obtained from Food101, CIFAR-100, FGVC, Oxford Pets, Flowers102, and ImageNet dataset. CLIP classifiers are not trained but tested on these datasets. testing. 5. Flower102 [32] hosts 6149 test images, annotated in 102 flower categories commonly found in the United Kingdom. For all of these datasets, we sample 5 images per category for testing. ImageNet ImageNet-C ImageNet-A ImageNet-R ImageNetV2 ImageNet-S Figure 10: Image samples from ImageNet and its out-of-distribution variants. From left to right: we show images of the puffer category in the ImageNet, ImageNet-C, ImageNet-A, ImageNet-R, ImageNetV2, and Stylized ImageNet dataset. ImageNet-C applies different corruptions to images. ImageNet-A consists of real-world image examples that are misclassified by existing classifiers. ImageNet-R renders images in artistic styles, e.g. cartoon, sketch, graphic, etc. ImageNetV2 attempts to collect images from the same distribution as ImageNet, but still suffers from minor distribution shift. ImageNet-S transforms ImageNet images into different art styles, while preserving the global contents of original images. Though these images all correspond to the same category, they are visually dissimilar and can easily confuse ImageNet-trained classifiers. We consider ImageNet and its out-of-distribution variants for test-time adaptation. 1. ImageNet [9] is a large-scale generic object classification dataset, featuring 1000 object categories. We test on the validation set that consists of 50 images for each category. 2. ImageNet-C [19] applies various visual corruptions to the same validation set as ImageNet. We consider the shift due to gaussian noise (level-5) in our single-sample experiments. We consider 5 types of shift (gaussian noise, fog, pixelate, contrast, and snow) in our online adaptation experiments. 3. ImageNet-A[21] consists of real-world image examples that are misclassified by existing classifiers. The dataset convers 200 categories in ImageNet and 7450 images for testing. 4. ImageNet-R [20] is a synthetic dataset that renders 200 ImageNet classes in art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game styles. The dataset consists of 30K test images. 5. ImageNetV2 collects images from the same distribution as ImageNet, composed of 1000 categories, each with 10 test images. 6. ImageNet-S transforms ImageNet images into different artistic styles, while preserving the global contents of original images. For ImageNet and its C, R, V2, and S variants except A, we sample 3 images per category, as these datasets contains a lot more classes than the other datasets. For ImageNet-A we evaluate on it’s whole test-set. 17",
      "meta_data": {
        "arxiv_id": "2311.16102v2",
        "authors": [
          "Mihir Prabhudesai",
          "Tsung-Wei Ke",
          "Alexander C. Li",
          "Deepak Pathak",
          "Katerina Fragkiadaki"
        ],
        "published_date": "2023-11-27T18:59:53Z",
        "pdf_url": "https://arxiv.org/pdf/2311.16102v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "Diffusion-TTA is a novel test-time adaptation (TTA) method that leverages generative feedback from pre-trained diffusion models to enhance pre-trained discriminative models, such as image classifiers, segmenters, and depth predictors. The core contribution is modulating the conditioning of a diffusion model using the output of a discriminative model, and then maximizing the image likelihood objective by backpropagating gradients to the discriminative model's parameters. Diffusion-TTA significantly improves the accuracy of large-scale pre-trained models (e.g., ImageNet classifiers, CLIP models) across various benchmarks, outperforming existing TTA methods like TTT-MAE and TENT, particularly in online adaptation scenarios.",
        "methodology": "The Diffusion-TTA method integrates a pre-trained discriminative model `fθ` and a pre-trained generative diffusion model `ϵϕ`. For a given test image `x`, `fθ` predicts a task output `y` (e.g., class probabilities, pixel-wise distributions, or depth map). This output `y` is transformed into a condition `c` for the diffusion model (e.g., a weighted sum of text embeddings for classification, or directly the depth map). The diffusion model `ϵϕ` then uses `c` to measure the likelihood of `x` by predicting the noise `ϵ` from a noisy version of `xt`. The discriminative model's weights `θ` are updated via gradient descent to minimize a diffusion loss `L(θ, ϕ) = Et,ϵ∥ϵϕ(√¯αtx + √ 1 − ¯αtϵ, c, t) − ϵ∥2`. Adaptation can be performed in single-sample or online modes. Implementation details include minimizing variance by sampling multiple timesteps and noise vectors (creating a large effective batch size), using SGD or Adam optimizers, and adapting all discriminative model parameters. In some cases, like with CLIP and Stable Diffusion, diffusion model parameters are frozen, while for ImageNet classifiers with DiT, both discriminative and generative model parameters are adapted for optimal performance.",
        "experimental_setup": "Experiments were conducted on image classification, semantic segmentation, and depth estimation tasks. For classification, ImageNet-trained classifiers (ResNet, ViT, ConvNext) were adapted using DiT-XL/2, and open-vocabulary CLIP models (ViT-B/32, ViT-B/16, ViT-L/14) were adapted using Stable Diffusion. Semantic segmenters (SegFormer) and depth predictors (DenseDepth) were adapted using conditional latent diffusion models. Datasets included ImageNet, its out-of-distribution variants (ImageNet-C, -A, -R, -V2, -S), ObjectNet, CIFAR-100, Food101, Flowers102, FGVC Aircraft, Oxford-IIIT Pets, ADE20K, and NYU Depth v2. Baselines for ImageNet TTA included TTT-MAE, TENT, and COTTA. Performance was evaluated using top-1 accuracy for classification, mean pixel accuracy for segmentation, and Structure Similarity Index for depth estimation. Adaptation involved 5 steps per image, with an effective batch size of 180 through gradient accumulation, using SGD (0.005 LR) or Adam (0.00001 LR), and no data augmentation during TTA.",
        "limitations": "The paper identifies several limitations. A potential concern is the discriminative model overfitting to the generative loss, converging to a trivial solution (precisely predicting the generative model's output); however, empirical results suggest this is mitigated by the strong initialization of the pre-trained discriminative model. Another limitation is the indirect integration between the generative and discriminative models, which relies on modulating the diffusion model's conditioning rather than a tighter coupling. Furthermore, Diffusion-TTA is computationally intensive and slow, with adaptation taking approximately 55 seconds per example, largely due to the necessity of sampling multiple timesteps to obtain a reliable estimate of the diffusion loss.",
        "future_research_directions": "Future work could explore tighter integration mechanisms between generative and discriminative models, moving beyond modulating conditioning to achieve more profound synergy. Investigating the training of different conditional generative models for various feature layers (e.g., `p(l1|y)` or `p(l1|l2)`) is suggested to provide diverse learning signals. A significant avenue for future research is to improve the computational speed of test-time adaptation, potentially by incorporating more efficient generative models like Consistency Models. The authors also express hope that their work will stimulate broader research into the paradigm of visual perception as an inversion of a generative model, especially given advancements in generative models and insights from neural science on visual processing."
      }
    },
    {
      "title": "TinyTTA: Efficient Test-time Adaptation via Early-exit Ensembles on Edge Devices"
    },
    {
      "title": "A simpler approach to accelerated optimization: iterative averaging meets optimism"
    },
    {
      "title": "A simple but strong baseline for online continual learning: Repeated Augmented Rehearsal",
      "abstract": "Online continual learning (OCL) aims to train neural networks incrementally\nfrom a non-stationary data stream with a single pass through data.\nRehearsal-based methods attempt to approximate the observed input distributions\nover time with a small memory and revisit them later to avoid forgetting.\nDespite its strong empirical performance, rehearsal methods still suffer from a\npoor approximation of the loss landscape of past data with memory samples. This\npaper revisits the rehearsal dynamics in online settings. We provide\ntheoretical insights on the inherent memory overfitting risk from the viewpoint\nof biased and dynamic empirical risk minimization, and examine the merits and\nlimits of repeated rehearsal. Inspired by our analysis, a simple and intuitive\nbaseline, Repeated Augmented Rehearsal (RAR), is designed to address the\nunderfitting-overfitting dilemma of online rehearsal. Surprisingly, across four\nrather different OCL benchmarks, this simple baseline outperforms vanilla\nrehearsal by 9%-17% and also significantly improves state-of-the-art\nrehearsal-based methods MIR, ASER, and SCR. We also demonstrate that RAR\nsuccessfully achieves an accurate approximation of the loss landscape of past\ndata and high-loss ridge aversion in its learning trajectory. Extensive\nablation studies are conducted to study the interplay between repeated and\naugmented rehearsal and reinforcement learning (RL) is applied to dynamically\nadjust the hyperparameters of RAR to balance the stability-plasticity trade-off\nonline. Code is available at\nhttps://github.com/YaqianZhang/RepeatedAugmentedRehearsal",
      "full_text": "Repeated Augmented Rehearsal: A Simple but Strong Baseline for Online Continual Learning Yaqian Zhang University of Waikato New Zealand yaqianz@waikato.ac.nz Bernhard Pfahringer University of Waikato New Zealand bernhard@waikato.ac.nz Eibe Frank University of Waikato New Zealand eibe@waikato.ac.nz Albert Bifet University of Waikato New Zealand LTCI, Télécom Paris, France abifet@waikato.ac.nz Nick Jin Sean Lim University of Waikato New Zealand nick.lim@waikato.ac.nz Yunzhe Jia University of Waikato New Zealand alvin.jia@waikato.ac.nz Abstract Online continual learning (OCL) aims to train neural networks incrementally from a non-stationary data stream with a single pass through data. Rehearsal-based meth- ods attempt to approximate the observed input distributions over time with a small memory and revisit them later to avoid forgetting. Despite their strong empirical performance, rehearsal methods still suffer from a poor approximation of past data’s loss landscape with memory samples. This paper revisits the rehearsal dynamics in online settings. We provide theoretical insights on the inherent memory overﬁtting risk from the viewpoint of biased and dynamic empirical risk minimization, and examine the merits and limits of repeated rehearsal. Inspired by our analysis, a simple and intuitive baseline, repeated augmented rehearsal (RAR), is designed to address the underﬁtting-overﬁtting dilemma of online rehearsal. Surprisingly, across four rather different OCL benchmarks, this simple baseline outperforms vanilla rehearsal by 9%-17% and also signiﬁcantly improves the state-of-the-art rehearsal-based methods MIR, ASER, and SCR. We also demonstrate that RAR successfully achieves an accurate approximation of the loss landscape of past data and high-loss ridge aversion in its learning trajectory. Extensive ablation studies are conducted to study the interplay between repeated and augmented rehearsal, and reinforcement learning (RL) is applied to dynamically adjust the hyperparameters of RAR to balance the stability-plasticity trade-off online. Code is available at https://github.com/YaqianZhang/RepeatedAugmentedRehearsal. 1 Introduction Despite its recent success, deep learning largely relies on the assumption of independent and identi- cally distributed (i.i.d.) data that can be repeatedly revisited during training. Non-i.i.d settings are challenging for neural networks due to catastrophic forgetting: previously learned knowledge can easily be overwritten when training on new data because this data may follow a different distribu- tion [Li and Hoiem, 2017, Rebufﬁ et al., 2017, Delange et al., 2021]. Online continual learning (OCL or Online CL) studies how to enable deep learning in an online manner from a non-stationary data stream. As the data stream can be vast or even inﬁnite, it is infeasible to store and shufﬂe the dataset for multiple epochs of training. Therefore, a fundamental assumption is that the data stream can only 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2209.13917v2  [cs.LG]  13 Nov 2022be accessed one batch at a time and training is performed with a single pass over the data [Aljundi et al., 2019]. Experience replay (ER), also known as rehearsal [Chaudhry et al., 2019, Delange et al., 2021], is a key idea in OCL. It stores a subset of previously seen data Din a ﬁxed-size memory Mand revisits the memorized samples during training to mitigate forgetting of previous tasks. To update the model, a batch sampled from the memory is combined with the incoming batch from the stream to compute the gradient [Chaudhry et al., 2019]. Different variants of ER have been developed to improve memory management policies and representation learning, achieving state-of-the-art performance in a number of standard OCL benchmarks [Aljundi et al., 2019, Mai et al., 2021, Shim et al., 2021]. However, whether rehearsal is appropriate for continual learning, considering the risk of overﬁtting the memory when using data from the memory to directly contribute to the gradient computation, has been debated vigorously [Lopez-Paz and Ranzato, 2017, Chaudhry et al., 2019, Verwimp et al., 2021]. The potential for overﬁtting has motivated the development of constraint-based replay methods [Lopez- Paz and Ranzato, 2017], e.g., GEM and A-GEM, which use memory samples solely to constrain the gradient direction. However, there is empirical evidence suggesting that rehearsal-based methods consistently outperform methods that do not train directly on the memory [Chaudhry et al., 2019]. This indicates rehearsal on the memory does not necessarily prevent effective generalization, possibly due to the implicit regularization effect of incoming data [Chaudhry et al., 2019]. Nevertheless, recent work analyzing the loss landscape when applying rehearsal to ofﬂine continual learning ﬁnds that memory samples indeed provide a poor approximation of the loss landscape of past tasks, especially near a high-loss ridge. As a result, “instead of ending up near the high-loss ridge in perspective of the rehearsal memory, the solution in reality resideson the high-loss ridge for the training data” [Verwimp et al., 2021]. This latest ﬁnding poses the question of how to better approximate the loss surface of past data L(D; θ) with memory samples’ lossL(M; θ). To better approximate past data’s loss surface, previous work studies which samples should be memorized for rehearsal. Instead, we examine the optimization process during rehearsal and study how to effectively perform rehearsal with the memorized samples. Focused on online CL, our study extends the previous understanding of rehearsal along two directions. First, we provide theoretical considerations that reveal two insights regarding the extent of overﬁtting to memory: it is a) related to the inherent attributes of the OCL problem concerned and b) varies across the different stages of continual learning. Second, we highlight the limits of applying rehearsal with multiple iterations—a trick used to maximally utilize the incoming batch [Aljundi et al., 2019]—and identify an underﬁtting-overﬁtting dilemma for online rehearsal. Based on our analysis, we design a simple baseline to deal with the underﬁtting-overﬁtting dilemma in online CL problems, dubbed repeated augmented rehearsal (RAR), that can be easily integrated into existing rehearsal-based methods. Surprisingly, this simple baseline leads to a large performance boost for ER, as well as state-of-the-art ER-based approaches, across four OCL benchmarks. More importantly, the loss landscape analysis shows that RAR can help memory samples reliably approxi- mate the distribution of past data and successfully avoids the high-loss ridge of past tasks. To better understand the behavior of RAR, we further investigate the interplay between repeated rehearsal and augmented rehearsal via an ablation study. We also propose a reinforcement learning-based method to dynamically adjust the hyperparameters of RAR and balance the stability-plasticity trade-off in an online manner. 2 Related Work Online continual learning: We consider the online continual learning setting with a non-stationary (potentially inﬁnite) stream of data Dt: at each time step t, the continual learning agent receives an incoming batch of data samples Bt = {xi,yi}i=1,..,|Bt|that are drawn from the current data distribution P(Dt). The period of time where the data distribution stays the same is often called atask or experience in the continual learning literature. An abrupt change in the data distribution occurs when the task changes. The standard objective during training is to minimize the empirical risk on all the data seen so far: min θ R(θ) = min θ 1∑ t|Bt| ∑ t ∑ x,y∈Bt L(fθ(x),y) , (1) with loss function L, the CL network function f, and its associated parameters θ. 2Metrics: A common metric is the end accuracy after training on T tasks, deﬁned as AT = 1 T ∑j=T j=1 aT,j, where ai,j denotes the model’s accuracy on the held-out test set of task j after training on task i. Other metrics are “forgetting” [Chaudhry et al., 2018], which is deﬁned as FT = − 1 T−1 ∑T−1 i=1 (aT,i −maxl∈1...T−1 al,i) and the related metric “backward transfer” [Lopez- Paz and Ranzato, 2017]: BT = 1 T−1 ∑T−1 i=1 aT,i −ai,i. Online setting: A key difference between online and ofﬂine CL is that the latter assumes full access to the whole training data for the task that is currently being processed. Therefore, it allows training on each single task with multiple epochs (e.g., 70-200 epochs) [Rebufﬁ et al., 2017, Wu et al., 2019, Cha et al., 2021]. The online CL setting is more challenging because the agent can only access the current batch of incoming data and performs training with a single pass through the data. Experience Replay (Rehearsal): Chaudhry et al. (2019) propose experience replay (ER), which performs joint training on memory samples and incoming samples. A simple but strong baseline approach to sampling in ER is reservoir sampling Vitter [1985]. Aljundi et al. (2019) propose Maximally Interfered Retrieval (MIR), which retrieves the samples that will be most negatively impacted by the foreseen parameter updates. Shim et al. (2021) propose ASER, which selects samples to best preserve existing memory-based class boundaries. In terms of model training, Mai et al. (2021) propose to replace the cross-entropy loss with the supervised contrastive loss to learn a better representation. We consider all these variants of ER in our experiments. Augmentation: In the standard i.i.d. setting, data augmentation is a widely used method to improve deep learning [Cubuk et al., 2020]. In the ofﬂine CL setting, Mai et al. [2021] use augmentation to construct a supervised contrastive loss and Bang et al. [2021] employ it together with an uncertainty- based memory management strategy. However, these papers apply augmentation together with other advanced techniques, and there is no ablation study on the effect of augmentation per se. Thus, it is unclear whether augmentation itself is beneﬁcial to rehearsal or not, especially in the online setting. Repeated Rehearsal (Multiple Iterations): Vanilla online continual learning employs a single gradient update given an incoming batch of data. To maximally utilize the current incoming batch, Aljundi et al. [2019] propose to perform multiple gradient updates instead. Their experiment with CIFAR10 shows using MIR with ﬁve iterations leads to a 1.7% improvement in accuracy. In this paper, we systematically analyze the effect of multiple iterations on online rehearsal and provide theoretical and empirical insights on when this trick may improve or harm performance. Hyperparameter Tuning for OCL: Hyperparameter tuning is a particular challenge in OCL due to the lack of a dedicated validation set and the constraint of a single pass through the data. Chaudhry et al. [2018] and Mai et al. [2022] employ a hyperparameter tuning protocol that uses an external validation data stream with a small number of tasks. Ofﬂine hyperparameter tuning is applied to this validation data with multiple passes to identify optimal values, which are then used for the actual online continual learning tasks. A limitation of this method is that it relies on external validation data. 3 Revisiting Online Rehearsal: Is Repeated Rehearsal a Good Idea? We revisit rehearsal from two directions. First, while previous work demonstrates its strong empirical performance [Chaudhry et al., 2019] and provides conceptual analysis for ofﬂine CL [Verwimp et al., 2021], we focus on the online setting and provide theoretical insights through the lens of empirical risk minimization (ERM). Second, we examine the dynamics of employing rehearsal with multiple iterations. This has been proposed as a trick for online CL to maximally utilize the incoming batch already [Mai et al., 2022, Aljundi et al., 2019]; we investigate whether it is always better than rehearsal with a single iteration. 3.1 Empirical Risk Minimization in Online Rehearsal: a Biased and Dynamic Objective For rehearsal in OCL, at each iteration t, a batch of data Bt is obtained from the incoming task, where Bt ∼DT and Bt = {xi,yi}i=1,...|Bt|, and a batch BM t is sampled from memory, where BM t ∼Dt M and BM t = {xi,yi}i=1,...|BM t |. The gradient-based update rule of ER is: θt+1 = θt − η |Bt| ∑ x,y∈Bt ∇L(fθ(x),y) − η⏐⏐BM t ⏐⏐ ∑ x,y∈BM t ∇L(fθ(x),y) . (2) 3Given this update rule, we would like to establish the corresponding objective function, but this is not straightforward to derive because the memory is immediately updated after each incoming batch, which means the memory data samples Dt Mchanges all the time. For a widely used memory management policy, where the memory is updated using reservoir sampling [Vitter, 1985, Chaudhry et al., 2019], we prove in the appendix that the empirical risk for online ER follows Proposition 1. Proposition 1 (ERM for online rehearsal) : Assume an incoming task stream DT and an initial memory set D0 Mwith different data distribution P(DT) ̸= P(D0 M). Assume further that the memory is updated at the end of each iteration using reservoir sampling. Then, Eq 2 implements unbiased stochastic gradient descent for the following loss function: min θ Rt(θ) = min θ ∑ x,y∈DT L(fθ(x),y) + βtλ ∑ x,y∈D0 M L(fθ(x),y), (3) where λ:= |DT| |D0 M|and |D0 M|and |DT|are the memory size and incoming task data size respectively; βt := 1/(1 + 2Nt cur NT past ), and Nt cur = ∑i=t i=1 |Bi|denotes the number of samples of the current task that have been seen so far and NT past = ∑j=T j=1 |Dj|denotes the number of samples pertaining to the tasks so far, excluding the current task. We immediately have βt ∈(0,1]. The proposition reveals several interesting properties: • Bias: compared with the true objective in continual learning in Eq 1, the loss of online ER in Eq 3 is actually a biased approximation of the former, as it puts a different weight (βtλ) on memory samples while the true objective treats all samples with equal weight. This bias, introduced by ER’s objective function, can contribute to the risk of memory overﬁtting. • Problem-dependence: given that βt ∈(0,1], the biased weight on the memory samples is mostly inﬂuenced by λ. In other words, the memory overﬁtting risk is related to an inherent property of the CL problem concerned: the ratio λbetween the current task data size and the memory data size.1. While previous works extensively report empirical results on the inﬂuence of memory size, to our knowledge, we are the ﬁrst to point out that the relative data size of an incoming task also plays an important role. Empirical evidence is provided in Section 6.3 to support this claim. With a 2k memory, performing rehearsal on the CORE50 dataset with a larger task data size (λ= 6) faces a high level of memory overﬁtting while the CLRS dataset with a smaller task data size (λ= 1.12) enjoys a lower risk of memory overﬁtting. • Dynamic: ER in online CL optimizes towards a dynamic objective, which varies with each incoming batch t, as the weight on the memory sample depends on Nt cur, and NT past. This analysis shows that memory overﬁtting may be relatively slight for the ﬁrst few tasks when NT past is small. As more tasks arrive, memory overﬁtting worsens as βt increases with NT past. In the case of an inﬁnite data stream, limNT past→∞βt = 1, the memory weight is solely determined by λ. 3.2 Repeated Rehearsal: The Decaying Regularization Effect of Incoming Data We now investigate whether performing multiple iterations is beneﬁcial to rehearsal or not. We refer to applying multiple iterations in ER as “repeated experience replay” (Repeated ER) or “repeated rehearsal” and formalize it as follows: for each incoming data batch Bt ∼DT from the data stream, we perform multiple gradient updates (Kin total) using stochastic gradient descent (SGD) or variants thereof. At each gradient update k = 1,...K, a data batch BM t,k is chosen from memory Mand concatenated with the current incoming batch Bt to perform a replay iteration as follows: θt,k+1 = θt,k − η |Bt| ∑ x,y∈Bt ∇L ( fθt,k (x),y ) − η⏐⏐⏐BM t,k ⏐⏐⏐ ∑ x,y∈BM t,k ∇L ( fθt,k (x),y ) . (4) 1The experiments in this paper mainly consider a balanced CL setting where incoming tasks have the same data size. For imbalanced CL cases, λis also dependent on different tasks and should be expressed as λT. The ﬁnding remains the same. 4(a) RER (K = 10)  (b) RAR (K = 10) Figure 1: Loss contours of RER and RAR. Memory data overﬁtting can be observed in (a) for RER but not in (b) for RAR. Note how the shape and position of the loss contour of “T1 test” differs from the “T1 memory” loss contour in (a). At the CL solution point, the test loss is 7.9 (left blue arrow) while the memory loss is only 2.1 (right blue arrow). Note that when k= 1, this update rule provides an unbiased stochastic gradient for Eq 3; in the case of k> 1, it provides a biased gradient estimate as the same incoming batch is used for consecutive gradient updates. To study the inﬂuence of this biased gradient update in repeated rehearsal, we examine the internal dynamics of repeated rehearsal by studying the loss landscape. Memory Overﬁtting: We compare the loss surfaces regarding the memory samples and the test data of past tasks during repeated rehearsal. Following the visualization method used in Verwimp et al. [2021], Mirzadeh et al. [2020], we examine the learning process on the ﬁrst two tasks in the Split Mini-ImageNet dataset and plot the loss landscape in the 2D plane deﬁned by three model parameter vectors2: the model w1 obtained by training on the ﬁrst task until convergence, the modelw2 obtained on the second task using experience replay, and the model w2,ft obtained after training on the second task using ﬁnetuning without replay. Verwimp et al. (2021) use this method to demonstrate the memory overﬁtting in ER for ofﬂine continual learning setting with 10 epochs. Our results show that applying ER in the online setting yields severe memory overﬁtting with 10 iterations (see Fig 1 (a)). In other words, increasing the number of iterations means the loss landscape of the rehearsal memory provides a poorer approximation of the loss landscape of previous tasks, as shown by the differences in the positions and the shapes between the left and right red contours in Fig 1 (a). As a result, the learning trajectory of RER avoids the high-loss ridge region for the memory data but goes right into the high-loss ridge region for the past tasks’ test data. Regularization Effect of Incoming Data: To investigate why repeated rehearsal may suffer from even more memory overﬁtting than vanilla rehearsal, we analyze the regularization effects of incoming data during repeated ER. To this end, we examine the training process given an incoming batch and compare the training loss on the memory batch and incoming batch with respect to memory iteration k(see Fig 2 ). An interesting observation is that during the training session of a given incoming data batch, the decrease in the training loss on the incoming batch is much faster than the decrease in loss on the memory batches. One intuitive explanation is that the former is computed over a ﬁxed batch during multiple iterations and the latter is computed over different memory batch samples. As a result, even though the incoming loss is larger than the memory loss at the start of a training session (k = 1), at later iterations (i.e., k >5) the training loss of the incoming batch becomes 10 −102 times lower than that of the memory batch. This means that at this stage the regularization effect of the incoming data batch is greatly undermined: the joint training on the memory batch and the incoming batch becomes similar to training on memory only. In summary, our ﬁndings imply that the performance of online rehearsal is constrained by the dilemma between overﬁtting locally and underﬁtting globally. Speciﬁcally, online rehearsal faces the challenge of underﬁtting of the large data stream but overﬁtting of a small memorized data subset. Applying repeated rehearsal ameliorates the former problem but aggravates the latter problem. Therefore, the performance gain from repeated rehearsal is quite limited. 2When training w2 and w2,ft , the model is initialized from w1. The memory contains 100 samples/task (see Fig 1 (b) left and right). The 2-d coordinate system is built by orthogonalizing w2 −w1 and w2,ft −w1. 54 Repeated Augmented Rehearsal To deal with the overﬁtting-underﬁtting dilemma, we explore a simple strategy, “repeated augmented rehearsal” (RAR), which combines repeated rehearsal with data augmentation. Consider a group of transforms G that acts on the input space Xand is invariant under function f, i.e., f(gx) = f(x),g ∈G,x ∈X. Given an incoming batch from the data stream, Bt, multiple replay iterations are conducted using this batch. At each replay iteration k, a random memory batch BM t,k is sampled and concatenated with the incoming batch. Then, a random transform gt,k ∈Gis sampled and applied to each data point xi in the concatenated minibatch. The model parameters are updated as : grar = 1 |Bt| ∑ x,y∈Bt ∇L(fθ(gt,kx),y) + 1⏐⏐⏐BM t,k ⏐⏐⏐ ∑ x,y∈BM t,k ∇L(fθ(gt,kx),y) . Intuitively, the augmentation can help alleviate memory overﬁtting in two ways. First, we observe that applying augmentation on the incoming batch helps strengthen the regularization effect. As shown in Fig 2(b), the decaying regularization effect of incoming data is alleviated in RAR, as the loss of the incoming batch stays comparable to the loss of the memory batch during multiple iterations. Second, rehearsal on augmented memory batches can help to more accurately reﬂect past tasks’ data distributions. With RAR, the loss landscapes of memory data and past tasks’ test data become very similar (see Fig 1 (b)), which suggests that the model ends up in a part of the parameter space where the rehearsal memory approximates the past tasks’ distribution well. Moreover, the continual learning solution identiﬁed with RAR avoids the high-loss ridge not only in the memory data loss landscape but also in the test data loss landscape. Theoretically, we prove that augmented rehearsal reduces the generalization error in OCL. Speciﬁ- cally, assume the augmentation group Gis a compact topological group and follows a probability distribution Q. Similar to Proposition 1, it can be easily proven that the augmented rehearsal gradient corresponds to unbiased SGD on an augmented empirical risk3 (see Proposition 3 in Appendix A): ¯Rt(θ) = ∑ x,y∈DT ∫ G L(fθ(gx),y)dQ(g) + βtλ ∑ x,y∈DM ∫ G L(fθ(gx),y)dQ(g). (5) This result shows that applying augmented rehearsal is equivalent to performing an averaging operation of the loss of rehearsal in Eq 3 over the orbits of a certain group that keeps the data distribution approximately invariant. In the standard i.i.d. learning setting, Chen et al. [2020] found that such an orbit-averaging operation can reduce both the variance and generalization error. Based on Eq 3 and Eq 5, we show that this theoretical beneﬁt of using augmentation to boost model invariance is also applicable to rehearsal in continual learning. In fact, as discussed in Section 3.1, rehearsal in online CL has a biased empirical risk Eq 3, which leads to inherent memory overﬁtting and poor generalization ability. Thus, this beneﬁt of augmentation in reducing generalization error is particularly important when applying rehearsal in online CL. The modiﬁcations required for the RAR procedure are summarized in Lines 3 and 5 of Algorithm 1. It uses a general framework for ER-based continual learning that consists of three key components: sampling from memory, joint training on memory data and incoming data, and updating of the memory. As mentioned in the related work section, different ER variants have been proposed to improve these components. RAR can ﬂexibly be combined with any of these ER variants, and we investigate the effectiveness of RAR on these different ER variants in the experiment section. 5 Reinforcement Learning-based Adaptive Repeated Augmented Rehearsal There are two key components in RAR: repeated rehearsal and augmented rehearsal. The interplay of the two is determined by the number of memory iterations and the strength of augmentation. A key question is how to choose these hyperparameters. In general, hyperparameter tuning (HPT) still remains an unsolved challenge for online CL due to the single-pass assumption [Chaudhry et al., 2018]. Finding suitable RAR hyperparameters needs to account for the severity of memory overﬁtting 3Note that the theoretical analysis of the loss functions in Eq 3 and Eq 5 is also applicable to ofﬂine continual learning, which may be of independent interest. More discussion of the inﬂuence of augmentation in ofﬂine continual learning can be found in Appendix D.6. 6Algorithm 1: RL-based RAR Mis the memory with ﬁxed size, Bt is the incoming batch from the current task, θare the parameters of the CL network, ware the parameters of the RL agent, Kis the number of memory iterations, P,Q are the augmentation hyperparameters 1: procedure RAR( Mt, Bt, θt, wt ) 2: Kt,Pt,Qt = SampleAction(wt) 3: for k= 1,...,K t do 4: BM t,k ∼MemRetrieval(Mt) 5: Baug ←aug(BM t,k ∪Bt,Pt,Qt) 6: rt ←ComputeReward(Baug,θt,k) 7: θt,k+1 ←SGD(Baug,θt,k) 8: end for 9: Mt+1 ←MemUpdate(Mt,Bt) 10: wt+1 ←UpdateRL(rt) 11: end procedure Figure 2: Memory loss vs. incoming loss. and poses extra challenges. In particular, as shown in the ERM analysis, the extent of memory overﬁtting is related to the CL problem features (e.g., task data size and memory size) and also varies at different training stages of the continual learning process. To automatically select suitable RAR hyperparameters for the different CL problems and different training stages, we propose to use reinforcement learning to adaptively adjust the hyperparameters (see Algorithm 1). In particular, we design the hyperparameters of RAR as the action space and use the training statistics as the reward (see lines 2, 6, and 10 in Algorithm 1). A major challenge of applying RL in online HPT is sample efﬁciency. The exploration horizon (i.e, training steps) in the OCL environment is quite limited due to the constraint of a single pass through the data and poor action choices (undesirable hyperparameters) may lead to a bad gradient update step and hurt the OCL training process. To address the sample efﬁciency issue, we employ the multi-armed bandit framework and apply bootstrapped policy gradient (BPG) [Zhang and Goh, 2019]. The key idea of BPG is to incorporate prior knowledge to bootstrap the policy gradient to achieve stable and fast convergence with limited samples. To obtain prior knowledge in OCL problems, we use the training accuracy on the memory batch as the overﬁtting feedback, as a higher training memory accuracy suggests a higher chance of memory overﬁtting. Reward is deﬁned as the distance between the current memory accuracy and target memory accuracy. Compared against a target memory accuracy (e.g., 0.9), the current memory accuracy is used to indicate whether the current choice of rehearsal iteration or augmentation causes too much memory overﬁtting and then the action selection probability is adjusted following BPG (see Appendix C.2 for more RL design and implementation details). The algorithm details of applying BPG as a speciﬁc RL method for hyperparameter tuning is summarized in Algorithm 2 of Appendix C.2). 6 Experiments 6.1 Experiment Setup Baseline: We apply RAR to four ER-based continual learning algorithms: ER [Chaudhry et al., 2019], MIR [Aljundi et al., 2019], ASER [Shim et al., 2021], and SCR [Mai et al., 2021]. We also compare it with other continual learning methods, including the regularization-based method LWF [Li and Hoiem, 2017] and the constrained optimization-based method A-GEM [Chaudhry et al., 2018]. Dataset: Four CL benchmarks are used in the experiments: Seq-CIFAR100 (20 tasks), Seq- MiniImageNet (10 tasks) Vinyals et al. [2016], CORE50-NC (9 tasks) Lomonaco and Maltoni [2017] and CLRS25-NC (5 tasks) Li et al. [2020](see Appendix B for more details). Additionally, we also investigate ER and RAR on the large-scale ImageNet-1k dataset in Appendix D.3. 7Table 1: Accuracy on four OCL benchmarks with 2k and 5k memory. The performance boost of RAR over ER and ER variants is shown. SEQ-CIFAR100 SEQ-MINI-IMAGENET CORE50-NC CLRS25-NC 2K 5K 2K 5K 2K 5K 2K 5K FINETUNE 3.2±0.1 4.3±0.8 7.7±0.2 6.5±0.9 LWF 8.7±0.5 10.9±0.5 9.6±0.3 12.4±2.2 AGEM 8.5±0.4 9.2 ±0.2 11.6±0.1 13.1 ±0.3 18.6±0.4 19.4 ±1.8 14.6±1.4 14.4 ±0.3 ER 19.0±0.6 26.2 ±0.2 20.0±0.8 23.0 ±0.6 24.0±2.0 27.8 ±0.2 18.7±1.6 19.2 ±0.3 ER-RAR 27.8±0.5 36.2 ±0.7 30.0±0.9 36.5 ±0.4 39.3±1.4 45.0 ±2.7 28.6±2.7 28.9 ±1.5 GAINS 8.8↑ 10.0↑ 10.0↑ 13.5↑ 15.3↑ 17.2↑ 9.9↑ 9.7↑ MIR 18.4±0.8 25.7 ±1.8 19.4±0.6 22.3 ±0.2 25.2±1.3 26.9 ±0.9 14.3±3.6 15.2 ±3.0 MIR-RAR 27.5±0.2 36.1 ±0.3 29.5±0.6 34.9 ±0.7 39.1±1.0 44.6 ±1.7 27.8±1.6 29.2 ±2.6 GAINS 9.1↑ 10.4↑ 10.1↑ 12.6↑ 13.9↑ 17.7↑ 13.5↑ 14.0↑ ASER 20.9±0.3 24.3 ±2.0 15.7±0.1 17.5 ±0.7 16.4±1.4 16.7 ±2.3 19.4±1.3 19.7 ±1.4 ASER-RAR 28.1±0.3 35.8 ±1.0 27.0±0.3 32.2 ±0.6 24.2±0.4 30.0 ±1.6 28.7±0.2 29.5 ±0.2 GAINS 7.2↑ 11.5↑ 11.3↑ 14.7↑ 7.8↑ 13.3↑ 9.3↑ 9.8↑ SCR 32.0±1.1 37.4 ±0.2 29.7±1.0 33.1 ±1.9 45.1±0.1 50.3 ±1.9 23.5±2.2 23.6 ±3.0 SCR-RAR 37.1±0.7 45.8 ±0.2 35.4±0.7 43.7 ±0.4 53.4±0.9 61.1 ±1.1 37.4±1.0 41.5 ±0.9 GAINS 5.1↑ 8.4↑ 5.7↑ 10.6↑ 8.3↑ 10.8↑ 14.9↑ 17.9↑ ERRW 21.0±1.0 26.2 ±0.2 20.1±0.8 23.0 ±0.6 24.6±0.6 27.8 ±0.8 19.2±0.6 19.2 ±0.3 ERRW-RAR 30.8±0.1 36.5 ±0.4 30.4±1.3 36.5 ±0.4 45.3±2.2 50.8 ±0.9 28.6±2.7 28.9 ±1.5 GAINS 9.8↑ 10.3↑ 10.3↑ 13.5↑ 20.7↑ 23.0↑ 9.4↑ 9.7↑ DER 8.4±0.6 9.1 ±0.3 11.8±0.5 12.3 ±1.7 23.8±0.6 23.4 ±2.5 11.8±2.6 12.6 ±1.1 DER-RAR 30.0±1.2 41.9 ±0.5 26.2±0.4 35.5 ±1.5 37.7±1.4 42.0 ±3.7 28.4±3.2 27.4 ±3.8 GAINS 21.6↑ 32.8↑ 14.4↑ 23.2↑ 11.9↑ 18.6↑ 16.6↑ 14.8↑ Figure 3: Stability and plasticity trade-off: CIFAR100 (left) and Mini-ImageNet(right) Implementation: We use a reduced ResNet-18 for all datasets following Mai et al. [2021], Aljundi et al. [2019]. Single-head evaluation is employed with a shared ﬁnal layer trained for all the tasks. RandAugmentation Cubuk et al. [2020] is used for auto augmentation. Given a set of augmentation operations, it randomly selects P augmentation operations and exerts an augmentation magnitude of Qfor all the selected augmentation operations on each image. All the experimental results we present are averages of three runs. We summarize all hyperparameter details in Appendix C. The running time of different algorithms is shown in Appendix D.5. 6.2 Main Results RAR with ER and its variants We ﬁrst analyze RAR’s performance with a pre-deﬁned hyperparam- eter set (K = 10,P = 1,Q = 14). As shown in Table 1, RAR greatly improves the ER method on the four datasets, by +8.8% ∼+17.2%. Moreover, RAR also leads to substantial gains for the other algorithmic variants of ER for all datasets (MIR: +9.1% ∼+17.7%, ASER: +7.2% ∼+14.7%, SCR: +5.1% ∼+17.9%). These results suggest that even with advanced memory management strategies, such as MIR or ASER, or representation learning techniques, e.g., SCR, OCL still beneﬁts substantially from repeated augmented rehearsal. RAR with Modiﬁed Rehearsal Loss Besides using the vanilla online rehearsal loss in Eq 3, we investigate the effectiveness of RAR with another two, more advanced, rehearsal loss designs: 1) Reweighted memory loss: ER-rw introduces a reweighting hyperparameter αin the gradient of Eq 2 to deal with the biased ER loss by balancing the weight of the memory loss and incoming loss; 2) Distillation-based memory loss : DER [Buzzega et al., 2020] employs the logits-based distillation loss for memory samples, instead of the cross entropy loss. The results in Table 1 show 8Table 2: Accuracy of variants of RAR and different hyperparameter tuning methods. SEQ-CIFAR100 S EQ-MINI-IMAGENET CORE50-NC CLRS25-NC ER 19.0±0.6 20.0±0.8 24.0±2.0 18.7±1.6 RAR-MEM 25.4±0.7 27.4±0.8 38.6±0.7 28.8±1.0 RAR-INC 21.6±0.2 24.5±0.1 35.7±1.1 29.1±1.1 RAR-BOTH 27.8±0.5 30.0±0.9 39.3±1.4 28.6±2.7 RAR-HTOCL 23.4±0.2 26.0±0.2 40.8±0.7 26.9±0.5 RAR-RL 29.6±0.4 32.1±1.0 44.4±0.8 35.0±0.7 (a) CIFAR100  (b) MINI-IMAGENET  (c) CORE50  (d) CLRS25 Figure 4: Effects of augmentation and rehearsal iterations (red stars: accuracy in Table 1). RAR leads to large performance gains for ER-rw (for the bestαchoice; further results can be found in Appendix D.2) and DER for all four datasets (ER-rw:+9.4% ∼+23.0%, DER:+11.9% ∼+32.8%). This suggests that even with more advanced rehearsal loss designs, repeated augmented rehearsal is important for online rehearsal. Stability and Plasticity Trade-off Based on the deﬁnition of accuracy AT, forgetting FT and backward transfer BT in Section 2, we ﬁnd that these three metrics have the following relationship: AT = 1 TΣT i=1ai,i    Plasticity + T −1 T BT    Stability ≥ 1 TΣT i=1ai,i −T −1 T FT. Interestingly, this ﬁnding shows that accuracy is related to the ability to learn new tasks, quantiﬁed by 1 T ∑T i=1 ai,i, and the ability to avoid forgetting past tasks, quantiﬁed by T−1 T BT, which draws a connection to the more general problem of the stability-plasticity trade-off in neural networks and continual learning [Grossberg, 2012, Delange et al., 2021]. Plasticity refers to the ability to integrate new knowledge and stability refers to the ability to retain old knowledge. Fig 3 presents the stability and plasticity trade-off in RAR. Generally, we observe increasing the repeated rehearsal iterations (K) leads to a higher level of plasticity. However, this may also cause a decrease of stability, i.e., introduce forgetting. On the other hand, the use of augmentation generally improves stability. More importantly, the use of augmentation in repeated rehearsal shifts the stability-plasticity trade-off curve towards the upper right, thus creating a better stability-plasticity trade-off frontier. Hyperparameter Tuning for RARWe compare the RL-based hyperparameter tuning method with the hyperparameter tuning framework for continual learning (HTOCL) method used in Chaudhry et al. [2018], Mai et al. [2022] (see Section 2). The results in Table 2 show that RL-based RAR signiﬁcantly outperforms using HTOCL to select hyperparameters for RAR. One reason is that the extent of memory overﬁtting varies at the different training stages of CL. HTOCL only uses the ﬁrst few tasks to select hyperparameters for RAR, which may not optimal for later stages of CL. In fact, we observe HTOCL tends to select a large number of repeats and small augmentation strength. This selection strategy may be desirable for problems with short task sequences but problematic for long task sequences with increased memory overﬁtting risk. In contrast, the RL-based method can take into account the latest feedback (e.g., train memory accuracy) to adjust hyperparameter choices. The selected iteration numbers and augmentation are shown in Appendix D.4. 6.3 Ablation Studies Interplay between Repeated and Augmented Rehearsal We investigate the interaction of the number of replay iterations with the augmentation strength in Fig 4. For three out of four datasets, using augmentation alone without repeated rehearsal leads to even worse performances than rehearsal 9without augmentation (see K = 1 in Fig 4). One explanation is the underﬁtting challenge of online CL. Training on augmented samples can make model underﬁtting even worse. The only exception is the CORE50 dataset, which inherently has a high memory overﬁtting risk with λ= 6 making it beneﬁt more from augmentation. Similarly, employing repeated rehearsal alone (see the blue solid line in Fig 4) also harms performance in three out of four datasets, with the CLRS dataset as the only exception, which enjoys a low memory overﬁtting risk with λ= 1.12. An important takeaway message is that repeated rehearsal or augmented rehearsal is not always helpful in OCL settings and whether they will beneﬁt or harm the performance is dependent on the structure of the OCL problem at hand (e.g., the task data size and memory data size). RAR’s Robustness to Large Numbers of Repeats Although the performance curve ﬂattens out around 10 iterations, there is no evident drop in performance even with 20 iterations. This result reinforces how RAR can help with the underﬁtting-overﬁtting dilemma: it can support the use of more training in OCL without having to worry about the performance drop introduced by memory overﬁtting. In comparison, for repeated rehearsal without augmentation (solid lines), the accuracy starts to drop quickly on CIFAR100 and MiniImageNet when using more than two iterations. Augmenting the Memory vs. Augmenting the Incoming Data RAR applies augmentation to both the memory batch and the incoming batch. We examine the effectiveness of the two separately. Table 2 presents the performance of RAR applied (a) solely with memory augmentation (RAR- mem) and (b) solely with incoming data augmentation (RAR-inc). We ﬁnd that RAR achieves the best performance compared to RAR-mem and RAR-inc. This shows that it is beneﬁcial to apply augmentation to both the memory batch and the incoming batch. Interestingly, RAR-inc itself also achieves consistent performance improvements over RER, e.g., a gain of 7.7% in CORE50. As discussed in Section 3, adding augmentation on the incoming batch can strengthen the regularization effect of the incoming task and indirectly alleviate memory overﬁtting. RAR with MIR, ASER, SCR We also perform ablation studies to investigate RAR’s strong perfor- mance with the ER variants (see Appendix D.1). Similar to ER, the performance gains for RAR-MIR and RAR-ASER come from the combination of repeated rehearsal and augmented rehearsal. However, for SCR, the repeated rehearsal itself also leads to a consistent performance boost. One reason is that SCR already includes a strong augmentation procedure with four augmentation operations to construct the supervised contrastive loss used in this method. It also works poorly without augmentation. Augmentation for Ofﬂine Rehearsal Although this paper focuses on online CL, the (augmented) empirical risk analysis is also relevant to ofﬂine CL (see Propositions 2 and 3 in Appendix A). This suggests the memory overﬁtting risk in ofﬂine rehearsal is related to the ratio of task-to-memory sizeλ and can be alleviated by augmented rehearsal (see more empirical results in Table 8 of Appendix D.6). 7 Discussion and Conclusion Rehearsal-based methods play a central role in ﬁghting catastrophic forgetting when learning from non-stationary data streams. Compared to ofﬂine rehearsal, online rehearsal has faced particular challenges in tackling complex CL datasets due to the single-pass-through data constraint. This work tries to analyze the internal workings of online rehearsal from a theoretical and conceptual perspective and identiﬁes the fundamental challenge that it faces as the dilemma between overﬁtting locally and underﬁtting globally. To deal with this challenge, we propose a simple baseline: repeated and augmented rehearsal (RAR). Surprisingly, despite its simplicity, RAR achieves a large performance boost for a set of different rehearsal-based methods. Additionally, we propose an RL-based method to tune the hyperparameters of RAR to balance the stability-plasticity trade-off in an online manner. It achieves promising results compared to hyperparameter tuning based on validation data. This work is focused on continual learning for classiﬁcation problems with image data. An interesting future research direction is to look at other CL domains, e.g., text/audio inputs or RL problems. Acknowledgments and Disclosure of Funding This research is funded by the New Zealand MBIE TAIAO data science programme. 10References R. Aljundi, E. Belilovsky, T. Tuytelaars, L. Charlin, M. Caccia, M. Lin, and L. Page-Caccia. Online continual learning with maximal interfered retrieval. Advances in Neural Information Processing Systems, 32:11849–11860, 2019. J. Bang, H. Kim, Y . Yoo, J.-W. Ha, and J. Choi. Rainbow memory: Continual learning with a memory of diverse samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8218–8227, 2021. P. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920–15930, 2020. H. Cha, J. Lee, and J. Shin. Co2L: Contrastive continual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9516–9525, 2021. A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny. Efﬁcient lifelong learning with a-gem. In International Conference on Learning Representations, 2018. A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. Torr, and M. Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019. S. Chen, E. Dobriban, and J. H. Lee. A group-theoretic framework for data augmentation. Journal of Machine Learning Research, 21(245):1–71, 2020. E. D. Cubuk, B. Zoph, J. Shlens, and Q. V . Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702–703, 2020. M. Delange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, and T. Tuytelaars. A continual learning survey: Defying forgetting in classiﬁcation tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. S. T. Grossberg. Studies of mind and brain: Neural principles of learning, perception, development, cognition, and motor control, volume 70. Springer Science & Business Media, 2012. H. Li, H. Jiang, X. Gu, J. Peng, W. Li, L. Hong, and C. Tao. CLRS: Continual learning benchmark for remote sensing image scene classiﬁcation. Sensors, 20(4):1226, 2020. Z. Li and D. Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935–2947, 2017. V . Lomonaco and D. Maltoni. CORe50: a new dataset and benchmark for continuous object recognition. In Conference on Robot Learning, pages 17–26. PMLR, 2017. D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017. Z. Mai, R. Li, H. Kim, and S. Sanner. Supervised contrastive replay: Revisiting the nearest class mean classiﬁer in online class-incremental continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3589–3599, 2021. Z. Mai, R. Li, J. Jeong, D. Quispe, H. Kim, and S. Sanner. Online continual learning in image classiﬁcation: An empirical survey. Neurocomputing, 469:28–51, 2022. S. I. Mirzadeh, M. Farajtabar, D. Gorur, R. Pascanu, and H. Ghasemzadeh. Linear mode connectivity in multitask and continual learning. In International Conference on Learning Representations, 2020. S.-A. Rebufﬁ, A. Kolesnikov, G. Sperl, and C. H. Lampert. iCaRL: Incremental classiﬁer and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001–2010, 2017. 11D. Shim, Z. Mai, J. Jeong, S. Sanner, H. Kim, and J. Jang. Online class-incremental continual learning with adversarial shapley value. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pages 9630–9638, 2021. E. Verwimp, M. De Lange, and T. Tuytelaars. Rehearsal revealed: The limits and merits of revisiting samples in continual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 9385–9394, 2021. O. Vinyals, C. Blundell, T. Lillicrap, and D. Wierstra. Matching networks for one shot learning. Advances in neural information processing systems, 29:3630–3638, 2016. J. S. Vitter. Random sampling with a reservoir.ACM Transactions on Mathematical Software (TOMS), 11(1):37–57, 1985. Y . Wu, Y . Chen, L. Wang, Y . Ye, Z. Liu, Y . Guo, and Y . Fu. Large scale incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 374–382, 2019. Y . Zhang and W.-B. Goh. Bootstrapped policy gradient for difﬁculty adaptation in intelligent tutoring systems. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pages 711–719, 2019. Checklist The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or [N/A] . You are strongly encouraged to include a justiﬁcation to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example: • Did you include the license to the code and datasets? [Yes] Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below. 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] (c) Did you discuss any potential negative societal impacts of your work? [N/A] (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] 12(b) Did you mention the license of the assets? [Yes] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 13A Theoretical Analysis This section contains the theoretical analysis of the loss functions of ofﬂine experience replay (Proposition 2), augmented experience replay (Proposition 3), and online experience replay with reservoir sampling (Proposition 1). Proposition 2 (Empirical risk minimization for experience replay) : We assume a memory set DMand an incoming task stream DT with different data distribution P(DT) ̸= P(DM). At each iteration t, a batch of data is sampled from memory BM t ∼DMwith BM t = {xi,yi}i=1,...|BM t |, and a batch of data is sampled from incoming task Bt ∼DT with Bt = {xi,yi}i=1,...|Bt|. To update the parameter θof a function fθ: fθ ×X→Y based on loss function L, consider the parameter update rule deﬁned by θ= θ− η |Bt| ∑ xi,yi∈Bt ∇L(fθ(xi),yi) − η |BM t | ∑ xi,yi∈BM t ∇L(fθ(xi),yi), then it is an unbiased stochastic gradient descent update rule for the following empirical risk: R(θ) = ∑ xi,yi∈DT L(fθ(xi),yi) + |DT| |DM| ∑ xi,yi∈DM L(fθ(xi),yi). Proof: Given the empirical gradient ˆgER = 1 |Bt| ∑ xi,yi∈Bt ∇L(fθ(xi),yi) + 1 |BM t | ∑ xi,yi∈BM t ∇L(fθ(xi),yi) during stochastic optimization, we can derive the gradient expectation as follows: EBM t ∼DM,Bt∼DT[ˆgER] = ∇(EBM t ∼DM[ 1⏐⏐BM t ⏐⏐ ∑ BM t L(fθ(xi),yi)] + EBt∼DT[ 1 |Bt| ∑ Bt L(fθ(xi),yi)]) = ∇(Exi,yi∼DM[L(fθ(xi),yi)] + Exi,yi∼DT[L(fθ(xi),yi)]) = ∇( 1 |DM| ∑ DM L(fθ(xi),yi) + 1 |DT| ∑ DT L(fθ(xi),yi)) = 1 |DT|∇ ( |DT| |DM| ∑ DM L(fθ(xi),yi) + ∑ DT L(fθ(xi),yi) ) Proposition 3 (Augmented empirical risk minimization for experience replay): Assume a mem- ory set DM = {xi,yi}i=1,..,|DM| and an incoming task stream DT = {xi,yi}i=1,..,|DT| with different data distribution P(DT) ̸= P(DM), and a compact topological group of transform G with a probability distribution Q that acts on the input space Xand is invariant under function f, i.e., f(gx) = f(x),g ∈G,x ∈X . At each iteration t, a batch of data is sampled from memory, BM t ∼DMand BM t = {xi,yi}i=1,...|BM t |, a batch of data is sampled from the incoming task, Bt ∼DT and Bt = {xi,yi}i=1,...|Bt|, and a group operation gt ∼Gis randomly selected. To update the parameter θ of the function fθ based on loss function L, consider the parameter update rule deﬁned by θ= θ− η |Bt| ∑ xi,yi∈Bt ∇L(fθ(gtxi),yi) − η |BM t | ∑ xi,yi∈BM t ∇L(fθ(gtxi),yi), then it is an unbiased stochastic gradient descent update rule for the loss function ¯R(θ) = ∑ xi,yi∈DT ∫ G L(fθ(gxi),yi)dQ(g) + |DT| |DM| ∑ xi,yi∈DM ∫ G L(fθ(gxi),yi)dQ(g). Proof: Given the augmented empirical gradient during stochastic optimization, we can derive the gradient expectation as follows: EBM t ∼DM,Bt∼DT,g∼Q[ˆg] = ∇(EBM t ∼DM[ 1⏐⏐BM t ⏐⏐ ∑ BM t ∫ g L(fθ(gxi),yi) dQ(g)] + EBt∼DT[ 1 |Bt| ∑ Bt ∫ g L(fθ(xi),yi) dQ(g)]) = 1 |DT|∇   ∑ xi,yi∈DT ∫ G L(fθ(gxi),yi)dQ(g) + |DT| |DM| ∑ xi,yi∈DM ∫ G L(fθ(gxi),yi)dQ(g)   14The second equality is based on the results of Proposition 1, which we prove next. Proposition 1 (ERM for online rehearsal): Assume an initial memory set D0 Mand an incoming task stream DT with different data distribution P(DT) ̸= P(DM). At each iteration t, t = 1,..T, a batch of data is sampled from the incoming task, Bt ∼DT and Bt = {xi,yi}i=1,...|Bt|, and a batch of data is sampled from the memory, BM t ∼Dt Mand BM t = {xi,yi}i=1,...|BM t |. To update the parameter θof the function fθ: fθ ×X→Y based on loss function L, consider a parameter update rule deﬁned by θ= θ− η |Bt| ∑ xi,yi∈Bt ∇L(fθ(xi),yi)− η |BM t | ∑ xi,yi∈BM t ∇L(fθ(xi),yi). Assume the memory is updated at the end of each iteration using reservoir sampling Vitter [1985], Chaudhry et al. [2019] Dt+1 M ←RS(Dt M,Bt). Then the gradient update rule is an unbiased stochastic gradient descent update rule for the loss function Rt(θ) = ∑ xi,yi∈DT L(fθ(xi),yi) + βt |DT| |D0 M| ∑ xi,yi∈D0 M L(fθ(xi),yi) where βt = 1 1+2∗ Ntcur Npast , Nt cur = ∑i=t i=1 |Bi|denotes the number of samples of the current task that have been seen so far and Npast = ∑j=T j=1 |Dj|denotes the number of samples of past tasks. Note 1: The objective function Rt(θ) changes with respect to the batch number tdue to the changes in βt. Note 2: 1 1+2∗|DT| Npast ≤βt ≤1 and βt is decreasing with the batch number t. At the start of a task, βt=0 = βmax = 1. At the end of a task, βt=T = βmin = 1 1+2∗|DT| Npast . Note 3: Consider a balanced continual learning dataset (e.g., Split-CIFAR100, Split-Mini-ImageNet) where |Dj|= |DT|,j = 1,..T. Then, we have βmin = T−1 T+1 = (1 − 2 T+1 ) and limT→∞ βmin = 1. Note 4: Consider general continual learning datasets. As CL learns more tasks, Npast increases, and limNpast→∞βt = limNpast→∞ 1 1+2∗ Ntcur Npast = 1. Proof: Given the empirical gradient ˆgER = 1 |Bt| ∑ xi,yi∈Bt ∇L(fθ(xi),yi) + 1 |BM t | ∑ xi,yi∈BM t ∇L(fθ(xi),yi) during stochastic optimization, we can derive the gradient expectation as follows: EDt M [EBM t ∼Dt M,Bt∼DT[ˆgER]] = ∇EDt M  EBM t ∼Dt M [ 1⏐⏐BM t ⏐⏐ ∑ BM t L(fθ(xi),yi)] + EBt∼DT[ 1 |Bt| ∑ Bt L(fθ(xi),yi)]   = ∇(EDt M [ Exi,yi∼Dt M [L(fθ(xi),yi)] ] + Exi,yi∼DT[L(fθ(xi),yi)]) = ∇( Npast Npast + Ntcur Exi,yi∼D0 M [L(fθ(xi),yi)] + ( Nt cur Npast + Ntcur + 1)Exi,yi∼DT[L(fθ(xi),yi)]) = Npast + 2 ∗Nt cur (Npast + Ntcur) ∗|DT|∇   Npast Npast + 2 ∗Ntcur |DT| |D0 M| ∑ D0 M L(fθ(xi),yi) + ∑ DT L(fθ(xi),yi)   The third equality is based on the condition that Dt Mis updated using reservoir sampling so that all the data seen so far have an equal probability of being stored in the memory. In other words, at a given time t, the memory contains a fraction of Npast Npast+Ntcur samples coming from past tasks and a fraction of Nt curr Npast+Ntcur samples coming from the current task. B Dataset details Table 3 lists the image size, the number of classes, the number of tasks, and data size per task of the four CL benchmarks. 15Table 3: Dataset information for the four CL benchmarks. IMAGE SIZE #TASK # CLASS TRAIN PER TASK TEST PER TASK SEQ-CIFAR100 3X32X32 20 100 2,500 500 SEQ-MINI-I MAGE NET 3X84X84 10 100 5,000 1,000 CORE50-NC 3X128 X128 9 50 12,000-24,000 4,500-9,000 CLRS-NC 3X256 X256 5 25 2,250 750 C Implementation Details C.1 Continual Learning Implementation The hyperparameter settings are summarized in Table 4. All models are optimized using vanilla SGD. For all experiments, we use the learning rate of 0.1 following the same setting as in Aljundi et al. [2019], Shim et al. [2021], and the Nearest-Class-Mean (NCM) classiﬁer is used for evaluation, as Mai et al. reported (2021) considerable and consistent performance gains when replacing the Softmax classiﬁer with the NCM classiﬁer. Each mini batch during training consists of 10 new and 10 memory samples, except for the SCR method, which employs 100 memory samples and 10 new incoming samples Mai et al. [2021]. By default, the repeated rehearsal parameter for all the results is K = 10 and the augmented rehearsal parameters are P = 1,Q = 14. This paper uses Randaugment [Cubuk et al., 2020], which is an auto augmentation method. It randomly selects P augmentation operators from a set of 14 operators and applies them to the images. The augmentation operator set includes:’Identity’, ’AutoContrast’, ’Equalize’, ’Rotate’, ’Solarize’, ’Color’,’Posterize’, ’Contrast’, ’Brightness’, ’Sharpness’,’ShearX’, ’ShearY’,’TranslateX’, ’TranslateY’. Table 4: Hyperparameter setting. HYPERPARAMETER LWF LR=0.1 AGEM LR=0.1 ER BATCHSIZE =10, LR=0.1 MIR BATCHSIZE =10,C=50, LR=0.1 ASER K=3, N_SMP _CLS =1.5, BATCHSIZE =10, LR=0.1 SCR TEMP =0.07, BATCH SIZE = 100, LR=0.1 DER α= 0.3, AUGMENTATION : FLIP AND CROP , K = 50 C.2 RL-based hyperparameter tuning implementation The memory iteration choices are from 1 to 20 and the augmentation choices are (1,5),(1,14),(2,14),(3,14),(4,14). Action selection probabilities are modeled with softmax weight πw(ai) = ewi∑ k ewk . Bootstrapped policy gradient is used to adjust action weights: gBPG = Eai∼πw [ |rai| ( ∇wlog ˆπ+ w (ai) −∇wlog ˆπ− w (ai) )] where ˆπ+ w (ai) := Σak∈X+ ai πw(ak) and ˆπ− w (ai) := Σak∈X− ai πw(ak) Better/Worse action set The key of the BPG idea is to incorporate prior information into the construction of better and worse action sets. To apply BPG in the OCL environment, we propose to determine the better/worse action set based on the feedback in the form of current memory batch accuracy AM, which reﬂects the memory overﬁtting level of the CL agent. We want the training memory accuracy to be neither too high nor too low. The desirable training memory accuracy is deﬁned by A∗ M. We ﬁnd a higher repeated replay iteration leads to higher memory accuracy. 16Therefore, the better/worse action set is deﬁned as follows: X+ a :=    ∀ak |Iter(ak) <Iter (a), A M(a) >A∗ M ∀ak |Iter(ak) >Iter (a), A M(a) <A∗ M ∅, A M(a) = A∗ M X− a :=    ∀ak |Iter(ak) >Iter (a), A M(a) >A∗ M ∀ak |Iter(ak) <Iter (a), A M(a) <A∗ M ∀ak |Iter(ak) ̸= Iter(a), A M(a) = A∗ M X+ a :=    ∀ak |Aug(ak) >Aug(a), A M(a) >A∗ M ∀ak |Aug(ak) <Aug(a), A M(a) <A∗ M ∅, A M(a) = A∗ M X− a :=    ∀ak |Aug(ak) <Aug(a), A M(a) >A∗ M ∀ak |Aug(ak) >Aug(a), A M(a) <A∗ M ∀ak |Aug(ak) ̸= Aug(a), A M(a) = A∗ M Reward A challenge in the hyperparameter tuning for the OCL setting is that the CL agent may face new tasks with unseen data distribution. Therefore, it is often infeasible to assume the existence of an external validation data containing all the tasks in advance for hyperparameter tuning. To achieve online hyperparameter tuning without external validation data, we propose to deﬁne the reward based on the accuracy on the memory. Given a target memory accuracy A∗ M, the reward is deﬁned as r= |AM−A∗ M| Non-stationarity To address the non-stationary nature in the CL environment, we reset the weight of BPG to a uniform weight at the start of each task. The analysis of the selected action can be found in Figures 9 and 10 in Section D.4. Algorithm 2: BPG-based RAR Mis the memory with ﬁxed size, Bt is the incoming batch from the current task, θare the parameters of the CL network, ware the parameters of the RL agent, Kis the number of memory iterations, P,Q are the augmentation hyperparameters A∗ Mtarget memory accuracy 1: procedure RAR( Mt, Bt, θt, wt ) 2: Kt ∼πwiter t 3: Pt,Qt ∼πwaug t 4: for k= 1,...,K t do 5: BM t,k ∼MemRetrieval(Mt) 6: Baug ←aug(BM t,k ∪Bt,Pt,Qt) 7: AM←MemAcc(M,θt,k) 8: X+,X−←ActionSet(AM,A∗ M) 9: θt,k+1 ←SGD(Baug,θt,k) 10: end for 11: Mt+1 ←MemUpdate(Mt,Bt) 12: witer t+1 ,waug t+1 ←UpdateRL(AM,X+,X−) 13: end procedure D Supplementary Experimental Results D.1 Ablation studies for MIR-RAR, ASER-RAR and SCR-RAR As shown in Table 1, RAR improves ER and ER variants (MIR, ASER and SCR). The detailed results of the ablation studies of RAR with ER variants are presented in this section. In particular, Fig 5 and 6 show the comparison of only using repeated rehearsal or augmented rehearsal for MIR and ASER respectively. Neither of them alone consistently improves the performance of the baseline. This result suggests the inﬂuence of RAR for MIR/ASER is similar to ER. The performance gain comes from the combination of the repeated rehearsal and augmented rehearsal. Fig 7 shows the results for SCR. Repeated rehearsal leads to consistent performance gains because SCR already includes augmentation. D.2 Reweighted ER 17Figure 5: Ablation study: MIR-RAR on the four datasets, with a 2k memory Figure 6: Ablation study: ASER-RAR on the four datasets, with a 2k memory To deal with the biased ER loss, one straightforward way is to balance the weight of the memory loss and incoming loss by introducing a reweighting hyperparameter α in the gradient of Eq 2. Speciﬁcally, the gradient for reweighted ER is implemented as gER−rw t = (1 −α) 1 |Bt| ∑ x,y∈Bt ∇L(fθ(x),y) + α 1 |BM t | ∑ x,y∈BM t ∇L(fθ(x),y),α ∈(0,1). The performance of ER and RAR with respect to different reweighting hyperparameter values α∈[0.1,0.3,0.5,0.7,0.9] is shown in Fig 8, where α= 0.5 denotes vanilla ER loss with the equal weighting of memory loss and incoming loss. To keep the learning rate comparable to vanilla ER, the learning rate of ER-rw is twice that of ER. A key observation is that similar to vanilla ER, reweighted ER (ER-rw) also signiﬁcantly beneﬁts from repeated and augmented rehearsal, as ER-rw-rar greatly improves over ER-rw (see the red line and blue line in Fig 8) for all four datasets. D.3 Large-scale online CL To examine the effectiveness of RAR in a large-scale continual learning problem, we apply RAR to ImageNet-1k. ImageNet-1k is split into 10 tasks and each task contains 100 classes. The training dataset of 10 tasks contains 1,281,167 images in total. Considering the task size, we evaluate with a memory size of M= 20k (with the task to memory size ratio λ ≈6.4) and a memory size of M=100k (with the task to memory size ratio λ≈1.28). These choices are similar to Seq-CIFAR100 (λ= 1.25) and CORE50 (λ= 6) with a 2k memory. We randomly crop the images to size 224x224, and use the ResNet-18 architecture for training on ImageNet-1k with a single epoch. An incoming batch size of 32 and a memory batch size of 32 are used. We employ a learning rate of 0.1 with 0.001 of weight decay. For the RAR method, we use 10 memory iterations with RandAugment and parameters P=1 and Q=14. The results are shown in the Table 5. Figure 7: Ablation study: SCR-RAR on the four datasets, with a 2k memory 18(a) CIFAR100  (b) MINI-IMAGENET (c) CORE50  (d) CLRS Figure 8: The performance of reweighting the memory loss of ER (ER-rw) and its effectiveness with RAR on the four datasets. (Star symbols denote the accuracy of ER and ER-RAR with a reweighting value of 0.5). We observe that repeated augmented rehearsal (RAR) is effective in this large-scale online continual learning problem and improves the vanilla rehearsal from 2.1% to 15.1% with a 20k memory and from 8.7% to 34.7% with a 100k memory. The discussion in Buzzega et al. [2020] suggests that to deal with complex continual learning datasets, it is necessary to employ multiple epochs of training with ofﬂine CL to avoid the underﬁtting problem present in online learning. Our results show that RAR can greatly improve online rehearsal for large- scale CL problems. Although several ofﬂine CL algorithms have been evaluated on ImageNet-1k, to our knowledge, our result is the ﬁrst attempt to apply online CL to this problem. We aim to investigate other online rehearsal-based methods on ImageNet-1k in future work. Table 5: Accuracy of ER and ER-RAR for ImageNet-1k with a 20k and 100k memory. IMAGE NET-1K M=20 K M=100 K ER 2.1 ±0.3 8.7 ±0.5 ER-RAR 15.1 ±0.4 34.7 ±0.1 GAINS 13.0 ↑ 26.0 ↑ D.4 Action Selection in Reinforcement Learning The selected hyperparameters for four datasets are shown in Figure 9. Interestingly, the RL-based method assigns a stronger augmentation and lower iteration for a dataset with a higher λattribute. As discussed in the ERM analysis, a dataset with a higher λsuggests a higher risk of overﬁtting. The RL-based method successfully takes this into account to tune the hyperparameters. In contrast, the OCL-HT method selects the weakest augmentation and highest iteration for three datasets. Figure 10 presents the selected hyperparameters at different CL training stages. Generally, as the continual learning proceeds, the RL-based method selects a stronger augmentation strength and lower iterations, to balance off the increasing risk of memory overﬁtting. 19(a) RAR-RL  (b) RAR-HTOCL Figure 9: The average of selected hyperparameters of RAR (iteration and augmentation values) for four datasets. Figure 10: The selected hyperparameters of RAR (iteration and augmentation values) of the RL-based method. D.5 Running time D.5.1 Running time of RAR The running time of ER and ER-RAR is shown in Table 6. Experiments are conducted using an Nvidia GeForce RTX 2080 TI Graphics Card on Mini-ImageNet. The running time of RAR grows linearly with respect to the number of replay iterations. Hence, compared to vanilla experience replay, RAR requires more running time due to the multiple iteration design. However, RAR is still much more computationally efﬁcient compared with ofﬂine CL with multiple epochs. An interesting future research direction is to study how to dynamically adjust the iteration number and augmentation strength of RAR to balance the trade-off of accuracy and running time. D.5.2 Running time of RL RL-based hyperparameter optimization is an online method that does not require repeated running over different hyperparameter choices. Therefore, RL-based HPO is much more computationally efﬁcient than ofﬂine hyperparameter selection methods. More speciﬁcally, the hyperparameter search space in our problem is 100 with 5 augmentation strength levels and 20 memory iteration numbers. Grid search would need to run the OCL algorithm 100 times while RL only needs to run it once. 20Table 6: Running time of ER and ER-RAR using an Nvidia GeForce RTX 2080 TI Graphics Card on Mini-ImageNet. The ofﬂine setting employs 50 epochs of training. RUNNING TIME (S) ACCURACY (%) ER 277 ±19 20.0 ±0.8 RAR ( K = 5) 1383 ±4 29.1 ±0.9 RAR ( K = 10) 2,345 ±4 30.4 ±1.3 R-ER ( K = 10) 2,236 ±1 17.8 ±0.6 ER- OFFLINE (E = 50) 10,878 ±31 20.4 ±0.6 RAR-RL ( K = 19.6) 18,037 ±71 32.1 ±1.0 Nevertheless, the training of RL agents indeed introduces extra computation. In practice, we observe the running time of RL-RAR is about two times slower than that of RAR, as shown in the Table 7. Table 7: Running time with and without RL-based hyperparameter optimization using an Nvidia GeForce RTX 2080 TI Graphics Card on CIFAR100. RUNNING TIME (S) ACCURACY (%) RAR ( K = 10) 1294 ±255 27.3 ±0.3 RAR ( K = 20) 3499 ±406 27.3 ±0.5 RL-RAR ( K = 19.8) 6137 ±34 29.2 ±0.3 D.6 Ofﬂine Continual Learning Although this paper is mostly focused on online continual learning, some of the analysis and discussion are also of independent interest to ofﬂine continual learning. Speciﬁcally, from a theoretical perspective, the empirical risk minimization of ofﬂine rehearsal is shown in Proposition 2 in Section A and its augmented risk is shown in Proposition 3. Two conclusions can be drawn from this analysis. First, the risk of memory overﬁtting in ofﬂine rehearsal is also related to the problem characteristic, the ratio λbetween task size and memory size. Second, augmentation can help with ofﬂine rehearsal since the orbit-averaging operation in the augmented empirical risk can reduce both the model variance and generalization error. From an empirical perspective, Table 8 shows the performance of ofﬂine ER with and without augmentation in four datasets. We use 50 epochs and a memory size of 2000. For all four datasets, ofﬂine ER with augmentation achieves a signiﬁcant performance gain over ofﬂine ER without augmentation. More interestingly, compared to datasets with a small λ(e.g., CLRS with λ= 1.125) we observe that the datasets with a higher task-to-memory size ratio (e.g., CORE50 with λ= 6) tend to beneﬁt more from augmentation, due to the increased risk of memory overﬁtting. 21Table 8: Performance of ofﬂine rehearsal with and without augmentation. SEQ-CIFAR100 S EQ-MINI -IMAGE NET CORE50-NC CLRS25-NC OFFLINE ER W/O AUG 17.0 ±0.6 20.4 ±0.6 30.2 ±1.4 33.5 ±1.6 OFFLINE ER W/ AUG 28.3 ±0.6 32.6 ±0.1 44.5 ±1.3 35.7 ±0.8 GAINS 11.3 ↑ 12.2 ↑ 14.3 ↑ 2.2 ↑ 22",
      "meta_data": {
        "arxiv_id": "2209.13917v2",
        "authors": [
          "Yaqian Zhang",
          "Bernhard Pfahringer",
          "Eibe Frank",
          "Albert Bifet",
          "Nick Jin Sean Lim",
          "Yunzhe Jia"
        ],
        "published_date": "2022-09-28T08:43:35Z",
        "pdf_url": "https://arxiv.org/pdf/2209.13917v2.pdf",
        "github_url": "https://github.com/YaqianZhang/RepeatedAugmentedRehearsal"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the dilemma of local overfitting and global underfitting in online continual learning (OCL) rehearsal-based methods, where memory samples poorly approximate past data's loss landscape. It provides theoretical insights into the inherent memory overfitting risk, showing its dependence on problem attributes (ratio of task to memory size) and its dynamic nature across learning stages. The main contribution is the design of Repeated Augmented Rehearsal (RAR), a simple yet strong baseline that combines repeated rehearsal with data augmentation. RAR significantly improves performance across various OCL benchmarks, outperforming vanilla rehearsal by 9-17% and boosting state-of-the-art rehearsal-based methods. RAR also achieves accurate approximation of past data's loss landscape and avoids high-loss ridges. Additionally, the work proposes a reinforcement learning-based method to dynamically adjust RAR's hyperparameters for balancing the stability-plasticity trade-off online.",
        "methodology": "The core methodology is Repeated Augmented Rehearsal (RAR), which combines repeated gradient updates (multiple iterations) with data augmentation for both memory and incoming data batches. Theoretically, RAR is shown to reduce generalization error by performing an orbit-averaging operation of the loss, which helps alleviate the biased empirical risk inherent in online rehearsal and strengthens the regularization effect of incoming data. To dynamically manage the interplay between repeated and augmented rehearsal, the paper introduces a reinforcement learning (RL)-based adaptive hyperparameter tuning method. This method uses a multi-armed bandit framework with Bootstrapped Policy Gradient (BPG), leveraging memory batch accuracy as feedback to adjust the number of memory iterations and augmentation strength online, thereby balancing the stability-plasticity trade-off.",
        "experimental_setup": "Experiments were conducted on four standard OCL benchmarks: Seq-CIFAR100 (20 tasks), Seq-MiniImageNet (10 tasks), CORE50-NC (9 tasks), and CLRS25-NC (5 tasks). A large-scale ImageNet-1k dataset (10 tasks) was also used for supplementary evaluation. Baselines included ER, MIR, ASER, SCR, LWF, A-GEM, ER-rw (reweighted memory loss), and DER (distillation-based memory loss). The model architecture used was a reduced ResNet-18, with a single-head evaluation and a shared final layer, and the Nearest-Class-Mean (NCM) classifier for evaluation. RandAugmentation was employed for data augmentation. Training parameters included a learning rate of 0.1, a mini-batch size of 10 new and 10 memory samples (except SCR), and a default of K=10 iterations with P=1, Q=14 for RandAugment. Performance was measured by end accuracy, and the stability-plasticity trade-off was analyzed. RL-based hyperparameter tuning was compared against a standard HTOCL method.",
        "limitations": "The current work focuses exclusively on continual learning for classification problems with image data. Although effective, RAR and its RL-based variant introduce increased computational cost; RAR's running time grows linearly with replay iterations, and RL-RAR is approximately two times slower than fixed-hyperparameter RAR. The theoretical analysis for online rehearsal, specifically Proposition 1, relies on the assumption that the memory is updated using reservoir sampling.",
        "future_research_directions": "Future research directions include extending the application of Repeated Augmented Rehearsal (RAR) to other continual learning domains beyond image classification, such as text and audio inputs, or reinforcement learning problems. Another suggested area is to investigate how to dynamically adjust the number of iterations and augmentation strength of RAR to effectively balance the trade-off between accuracy and running time, further optimizing its computational efficiency.",
        "experimental_code": "def softmax(vec, j=0):\n    para = 0\n    noise = np.random.rand(1) * 2 - 1\n    nn = (1 - para) * vec + (para) * noise\n    nn = nn - np.max(nn)\n    nn1 = np.exp(nn)\n    vec_prob = nn1 * 1.0 / np.sum(nn1)\n    return vec_prob\n\nclass ContinualLearner(torch.nn.Module, metaclass=abc.ABCMeta):\n    # ... (initializer and other methods)\n    def perform_augmentation(self,batch_x,batch_y,mem_batch_size):\n        if(self.task_seen >= self.params.aug_start):\n            if (self.params.randaug):\n                batch_x_aug2 = self.aug_agent.aug_data_old(batch_x, mem_batch_size, )\n                batch_x = batch_x_aug2\n            if (self.params.scraug):\n                batch_x = self.aug_agent.scr_aug_data(batch_x,mem_batch_size)\n        return batch_x,batch_y,mem_batch_size\n\nclass ExperienceReplay(ContinualLearner):\n    # ... (initializer and other methods)\n    def _batch_update(self,batch_x,batch_y,losses_batch,acc_batch,i,replay_para=None,mem_num=0):\n        self.model.train()\n        if(replay_para == None):\n            replay_para = self.replay_para\n        logits = self.model.forward(batch_x)\n        acc = (pred_label == batch_y)\n        ce_all = torch.nn.CrossEntropyLoss(reduction='none')\n        softmax_loss_full = ce_all(logits, batch_y)\n\n        acc_incoming = acc[mem_num:].sum().item() / (total_num - mem_num)\n        incoming_loss = torch.mean(softmax_loss_full[mem_num:])\n        self.train_loss_incoming.append(incoming_loss.item())\n        self.train_acc_incoming.append(acc_incoming)\n\n        if(mem_num>0):\n            acc_mem = acc[:mem_num].sum().item() / mem_num\n            mem_loss = torch.mean(softmax_loss_full[:mem_num])\n            self.train_acc_mem.append(acc_mem)\n            self.train_loss_mem.append(mem_loss.item())\n            loss = replay_para['mem_ratio'] * mem_loss + \\\n                   replay_para['incoming_ratio'] * incoming_loss\n            train_stats = {'acc_incoming': acc_incoming, 'acc_mem': acc_mem,\n                           'loss_incoming': incoming_loss.item(), 'loss_mem': mem_loss.item(), 'batch_num': i,}\n        else:\n            loss = replay_para['incoming_ratio'] * incoming_loss\n            train_stats=None\n        self.opt.zero_grad()\n        loss.backward()\n        self.opt.step()\n        self.loss_batch.append(loss.item())\n        return  train_stats\n\n    def concat_memory_batch(self, batch_x,batch_y,retrieve_num=None):\n        mem_x, mem_y = self.memory_manager.retrieve_from_mem(batch_x, batch_y, self.task_seen,retrieve_num=retrieve_num)\n        if mem_x.size(0) > 0:\n            mem_x = maybe_cuda(mem_x, self.cuda)\n            mem_y = maybe_cuda(mem_y, self.cuda)\n            batch_x = torch.cat([mem_x,batch_x,])\n            batch_y = torch.cat([mem_y,batch_y,])\n        return self.perform_augmentation(batch_x,batch_y,mem_x.size(0))\n\nclass aug_agent(object):\n    # ... (initializer and other methods)\n    def set_aug_NM(self, N,M):\n        self.transform_train_mem = transforms.Compose([transforms.ToTensor(),])\n        self.transform_train_mem.transforms.insert(0, RandAugment(N,M))\n        self.transform_train_incoming = transforms.Compose([transforms.ToTensor(),])\n        self.transform_train_incoming.transforms.insert(0, RandAugment(N,M))\n    def aug_data_old(self,concat_batch_x,mem_num):\n        n, c, w, h = concat_batch_x.shape\n        mem_images = [transforms.ToPILImage()(concat_batch_x[i]) for i in range(mem_num)]\n        incoming_images = [transforms.ToPILImage()(concat_batch_x[i]) for i in range(mem_num,n)]\n        if(self.mem_aug and mem_num>0):\n            aug_mem = [self.transform_train_mem(image).reshape([1, c, w, h]) for image in mem_images]\n            aug_mem = maybe_cuda(torch.cat(aug_mem,dim=0))\n        else:\n            aug_mem = concat_batch_x[:mem_num,:,:,:]\n        if(self.incoming_aug):\n            aug_incoming = [self.transform_train_incoming(image).reshape([1, c, w, h]) for image in incoming_images]\n            aug_incoming = maybe_cuda(torch.cat(aug_incoming,dim=0))\n        else:\n            aug_incoming = concat_batch_x[mem_num:,:,:,:]\n        if(mem_num>0):\n            aug_concat_batch_x = maybe_cuda(torch.cat((aug_mem,aug_incoming), dim=0))\n        else:\n            aug_concat_batch_x = maybe_cuda(aug_incoming)\n        return aug_concat_batch_x\n\nclass ER_dyna_iter_aug_dbpg_joint(ExperienceReplay):\n    def __init__(self, model, opt, params):\n        super(ER_dyna_iter_aug_dbpg_joint, self).__init__(model, opt, params)\n        if(params.aug_action_num == 8):\n            self.aug_map=[(1,5),(1,14),(2,5),(2,14),(3,5),(3,14),(4,5),(4,14)]\n        elif(params.aug_action_num == 10):\n            self.aug_map=[(1,5),(1,14),(2,5),(2,14),(3,5),(3,14),(4,5),(4,14),(5,14),(6,14)]\n        else:\n            self.aug_map = [(1, 5), (1, 14),  (2, 14), (3, 14),  (4, 14)]\n        self.aug_map.reverse()\n        self.iter_map= list(np.arange(1,21))\n        self.action_num_iter =len(self.iter_map)\n        self.weights_iter = 100*np.ones(self.action_num_iter)\n        self.action_prob_iter = softmax(self.weights_iter)\n        self.action_num_aug =len(self.aug_map)\n        self.weights_aug = 100*np.ones(self.action_num_aug)\n        self.action_prob_aug = softmax(self.weights_aug)\n        self.mem_iter_min = self.params.mem_iter_min\n        self.mem_iter_max = self.params.mem_iter_max\n        self.reward_list=[]\n        for i in range(self.action_num_iter):\n            self.reward_list.append([0.5])\n        self.N=None\n\n    def update_reward(self,reward,action):\n        self.reward_list[action].append(reward)\n\n    def update_weight_bpg(self,train_acc_list,STOP_FLAG,action_id,weights,prob,target_acc_start,target_acc_end):\n        current_iter = len(train_acc_list)\n        if(current_iter ==0 or current_iter == None):\n            return\n        last_acc = train_acc_list[-1]\n        alpha_too_large = self.params.bpg_lr_large\n        alpha_too_small = self.params.bpg_lr_small\n\n        if(last_acc>target_acc_end):\n            if np.sum(prob[action_id:])>0:\n                weights[action_id:] -= alpha_too_large * prob[action_id]/np.sum(prob[action_id:])\n            if np.sum(prob[:action_id+1])>0:\n                weights[:action_id] +=alpha_too_large * prob[action_id]/np.sum(prob[:action_id+1])\n        elif(last_acc <target_acc_start):\n            if np.sum(prob[action_id:])>0:\n                weights[action_id:] += alpha_too_small * prob[action_id]/np.sum(prob[action_id:])\n            if np.sum(prob[:action_id+1])>0:\n                weights[:action_id] -=alpha_too_small * prob[action_id]/np.sum(prob[:action_id+1])\n        else:\n            pass\n        prob[:] = softmax(weights)\n        return None\n\n    def restart_bpg(self):\n        self.weights_iter = 100 * np.ones(self.action_num_iter)\n        self.action_prob_iter = softmax(self.weights_iter)\n        self.weights_aug = 100 * np.ones(self.action_num_aug)\n        self.action_prob_aug = softmax(self.weights_aug)\n\n    def sample_action_bpg_aug(self):\n        action_idx = np.random.choice(range(0,self.action_num_aug), 1, replace=False, p=self.action_prob_aug)[0]\n        return self.aug_map[action_idx],action_idx\n\n    def sample_action_bpg(self):\n        action_idx = np.random.choice(range(0,self.action_num_iter), 1, replace=False, p=self.action_prob_iter)[0]\n        return self.iter_map[action_idx],action_idx\n\n    def train_learner(self, x_train, y_train):\n        if(self.params.bpg_restart):\n            self.restart_bpg()\n        self.before_train(x_train, y_train)\n        train_dataset = dataset_transform(x_train, y_train, transform=transforms_match[self.data])\n        train_loader = data.DataLoader(train_dataset, batch_size=self.batch, shuffle=True, num_workers=0, drop_last=True)\n        self.model = self.model.train()\n        losses_batch = AverageMeter()\n        acc_batch = AverageMeter()\n        STOP_FLAG = False\n        target_acc_iter_min = self.params.train_acc_min\n        target_acc_iter_max =self.params.train_acc_max\n        target_acc_aug_min = self.params.train_acc_min_aug\n        target_acc_aug_max =self.params.train_acc_max_aug\n        for ep in range(self.epoch):\n            for i, batch_data in enumerate(train_loader):\n                batch_x,batch_y = batch_data\n                batch_x = maybe_cuda(batch_x, self.cuda)\n                batch_y = maybe_cuda(batch_y, self.cuda)\n                memiter,action_iter = self.sample_action_bpg()\n                aug_para,action_aug= self.sample_action_bpg_aug()\n                [N,M]=aug_para\n                aug_strength=N*100+M\n                self.aug_agent.set_aug_NM(N, M)\n                self.aug_N_list.append(aug_strength)\n                train_acc_list = []\n                for j in range(memiter):\n                    concat_batch_x, concat_batch_y, mem_num = self.concat_memory_batch(batch_x, batch_y)\n                    train_stats = self._batch_update(concat_batch_x, concat_batch_y, losses_batch, acc_batch, i,mem_num=mem_num)\n                    if(train_stats != None):\n                        train_acc_list.append(train_stats['acc_mem'])\n                    STOP_FLAG = self.early_stop_check(train_stats)\n                self.mem_iter_list.append(memiter)\n                self.update_weight_bpg(train_acc_list,STOP_FLAG,action_iter,self.weights_iter,self.action_prob_iter,\n                                              target_acc_iter_min,target_acc_iter_max)\n                self.update_weight_bpg(train_acc_list,True,action_aug,self.weights_aug,self.action_prob_aug,\n                                              target_acc_aug_min,target_acc_aug_max)\n\n                if(train_stats!= None):\n                    reward = np.abs(train_stats['acc_mem']-0.9)\n                    self.update_reward(reward,action_iter)\n\n                self.memory_manager.current_performance=train_acc_list\n                self.memory_manager.update_memory(batch_x, batch_y)\n                if i % 100 == 1 and self.verbose:\n                    print('==>>> it: {}, avg. loss: {:.6f}, running train acc: {:.3f}'.format(i, losses_batch.avg(), acc_batch.avg()))\n                    print('==>>> it: {}, mem avg. loss: {:.6f}, running mem acc: {:.3f}'.format(i, losses_mem.avg(), acc_mem.avg()))\n                    print(\"memiter\",memiter,\"aug\",aug_strength,\"mem_acc\",train_acc_list[-1])\n        self.after_train()",
        "experimental_info": "The core methodology, Repeated Augmented Rehearsal (RAR), involves combining repeated gradient updates with data augmentation for both memory and incoming data batches. This implementation, `ER_dyna_iter_aug_dbpg_joint`, specifically integrates a reinforcement learning (RL)-based adaptive hyperparameter tuning method. It employs a multi-armed bandit framework with Bootstrapped Policy Gradient (BPG) to dynamically adjust two key hyperparameters: the number of memory iterations (`mem_iters`) and the data augmentation strength (defined by RandAugment parameters N and M). \n\n**Mechanism Breakdown:**\n1.  **Repeated Gradient Updates:** The `train_learner` method includes a loop (`for j in range(memiter):`) where the model performs multiple gradient updates for each incoming batch, as determined by the `memiter` variable.\n2.  **Data Augmentation:** The `perform_augmentation` method (inherited from `ContinualLearner` and further handled by `aug_agent`) applies RandAugment to both memory and incoming data batches. The strength of this augmentation is controlled by `N` and `M` parameters.\n3.  **RL-based Adaptive Tuning (BPG):**\n    *   **Action Space:** The agent jointly selects an action for `memiter` (from `iter_map`, e.g., 1 to 20 iterations) and for augmentation strength (from `aug_map`, which defines pairs of (N, M) for RandAugment).\n    *   **Policy (Action Probability):** `softmax(self.weights_iter)` and `softmax(self.weights_aug)` are used to derive probability distributions over possible `memiter` and augmentation actions, respectively. Actions are then sampled from these distributions using `sample_action_bpg()` and `sample_action_bpg_aug()`.\n    *   **Feedback/Reward:** The reward for updating the policy is implicitly derived from `np.abs(train_stats['acc_mem']-0.9)`, where `train_stats['acc_mem']` represents the accuracy of the memory batch. This feedback is used to drive the policy towards actions that help maintain memory accuracy around a target of 0.9.\n    *   **Policy Gradient Update (`update_weight_bpg`):** After each training step (or episode, effectively per batch), the `update_weight_bpg` function adjusts the `weights_iter` and `weights_aug` based on whether the memory accuracy is above (`target_acc_end`) or below (`target_acc_start`) desired thresholds. This update mechanism resembles a policy gradient step, where `alpha_too_large` and `alpha_too_small` act as learning rates to encourage or discourage certain actions.\n    *   **Online Adjustment:** The `memiter` and augmentation parameters (N, M) are dynamically adjusted online for each batch, aiming to balance the stability-plasticity trade-off by adapting to the model's current performance on the memory."
      }
    },
    {
      "title": "Knowledge-Adaptation Priors",
      "abstract": "Humans and animals have a natural ability to quickly adapt to their\nsurroundings, but machine-learning models, when subjected to changes, often\nrequire a complete retraining from scratch. We present Knowledge-adaptation\npriors (K-priors) to reduce the cost of retraining by enabling quick and\naccurate adaptation for a wide-variety of tasks and models. This is made\npossible by a combination of weight and function-space priors to reconstruct\nthe gradients of the past, which recovers and generalizes many existing, but\nseemingly-unrelated, adaptation strategies. Training with simple first-order\ngradient methods can often recover the exact retrained model to an arbitrary\naccuracy by choosing a sufficiently large memory of the past data. Empirical\nresults show that adaptation with K-priors achieves performance similar to full\nretraining, but only requires training on a handful of past examples.",
      "full_text": "Knowledge-Adaptation Priors Mohammad Emtiyaz Khan∗ RIKEN Center for AI Project Tokyo, Japan emtiyaz.khan@riken.jp Siddharth Swaroop∗ University of Cambridge Cambridge, UK ss2163@cam.ac.uk Abstract Humans and animals have a natural ability to quickly adapt to their surroundings, but machine-learning models, when subjected to changes, often require a complete retraining from scratch. We present Knowledge-adaptation priors (K-priors) to reduce the cost of retraining by enabling quick and accurate adaptation for a wide- variety of tasks and models. This is made possible by a combination of weight and function-space priors to reconstruct the gradients of the past, which recovers and generalizes many existing, but seemingly-unrelated, adaptation strategies. Training with simple ﬁrst-order gradient methods can often recover the exact retrained model to an arbitrary accuracy by choosing a sufﬁciently large memory of the past data. Empirical results show that adaptation with K-priors achieves performance similar to full retraining, but only requires training on a handful of past examples. 1 Introduction Machine-Learning (ML) at production often requires constant model updating which can have huge ﬁnancial and environmental costs [19, 44]. The production pipeline is continuously evolving, where new data are regularly pooled and labeled and old data become irrelevant. Regular tuning of hyperparameters is required to handle drifts [19], and sometimes even the model class/architecture may need to change. Due to this, the model is frequently retrained, retested, and redeployed, which can be extremely costly, especially when the data and model sizes are large. The cost can be reduced if, instead of repeated retraining, the system can quickly adapt to incremental changes. Humans and animals can naturally use their prior knowledge to handle a wide-variety of changes in their surroundings, but such quick, wide, and accurate adaptation has been difﬁcult to achieve in ML. In theory, this should be possible within a Bayesian framework where the posterior is used as the prior for the future, but exact Bayes is computationally challenging and the design of generic Bayesian priors has its own challenges [55, 39]. In ML, simpler mechanisms are more popular, for example, in Support Vector Machines (SVMs) for adding/removing data [12, 58, 60], and in deep learning for model compression [24]. Weight-priors are used in online learning [13], and more recently for continual learning [30, 40, 47, 33, 62], but they are not suited for many other tasks, such as model compression. In some settings, they also perform worse, for example, in continual learning when compared to memory-based strategies [ 34, 46, 45, 9]. All these previous works apply to narrow, speciﬁc settings, and designing generic adaptation-mechanisms remains an open challenge. We present Knowledge-adaptation priors (K-priors) for the design of generic adaptation-mechanisms. The general principle of adaptation is to combine the weight and function-space divergences to faithfully reconstruct the gradient of the past. K-priors can handle a wide variety of adaptation tasks (Fig. 1, left) and work for a range of models, such as generalized linear models, deep networks, and their Bayesian extensions. The principle uniﬁes and generalizes many seemingly-unrelated existing works, for example, weight-priors [ 13], knowledge distillation [ 24], SVMs [ 12, 35, 58], * Authors contributed equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. arXiv:2106.08769v2  [cs.LG]  27 Oct 2021Change Regularizer K-priors Past Model Past Memory Add new dataRemove  old dataChange RegularizerChange architecture Updated Model Adaptation tasks Past  data New  data Past  model Past memory Used in K-priors Add new dataRemove old data Batch  K-prior Validation acc (%)Validation acc (%)Validation acc (%) Memory size (% of past data) (a) Adult,logisticregression (b) USPS,logisticregression (c) USPS,neuralnetwork Add new data Remove old dataChange regularizerChange model class Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Validation acc (%)Validation acc (%)Validation acc (%) Memory size (% of past data) (a) Adult,logisticregression (b) USPS,logisticregression (c) USPS,neuralnetwork Add new data Remove old dataChange regularizerChange model class Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Validation acc (%)Validation acc (%)Validation acc (%) Memory size (% of past data) (a) Adult,logisticregression (b) USPS,logisticregression (c) USPS,neuralnetwork Add new data Remove old dataChange regularizerChange model class Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Validation acc (%)Validation acc (%)Validation acc (%) Memory size (% of past data) (a) Adult,logisticregression (b) USPS,logisticregression (c) USPS,neuralnetwork Add new data Remove old dataChange regularizerChange model class Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Change architecture (b) USPS Add Data(a)           over all (c) CIFAR-10 Add data Validation acc (%) Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) (d) CIFAR-10 KnowledgeDistillationwb Figure 3: (a) When compared at the Batch solution for the ‘Add Data’ task on USPS, weight priors give incorrect values of h 0 ( f i w ) (shown with black dots, each dot correspond to a data examples). Points on the diagonal means a perfect match which is the case for K-priors (show with red dots). (b) Due to this, weight-priors (green diamonds) perform worse than K-priors (red squares). (c) For the ’Add data’ task on CIFAR-10, K-priors outperform replay (blue circles), but performance can still be improved by using a temperature parameter (dark-red triangles). (d) The same is true for knowledge distillation [ 21 ], and we see that we can reduce memory size while still performing better than the student model. The dip in the middle, we believe, is due to suboptimal hyperparameter tuning. [ 29 ] with MLPs and 10-way classiﬁcation on CIFAR-10 with CifarNet [ 57 ], trained with the Adam319 optimizer [ 26 ]. In Figure 3(c) we show one representative result for the ‘Add data’ task with CIFAR-320 10, where we add a random 10% of CIFAR-10 training data to the other 90% (mean and standard321 deviation over 3 runs). Although vanilla K-priors outperform Replay, there is now a bigger gap322 between K-prior and Batch even with 50% past data stored. However, when we use a temperature323 (similar to knowledge distillation in (14) but with the weight term included), K-priors improves.324 A similar result is shown in Figure 3(d) for knowledge distillation ( \u0000 =0 but with a temperature325 parameter) where we are distill from a CifarNet teacher to a LeNet5-style student (details in Ap-326 pendix D). Here, K-priors with 100% data is equivalent to Knowledge Distillation, but when we327 reduce the memory size using our method, we still outperform Batch (which is trained from scratch328 on all data). There is a dip in performance at memory sizes of 10% past data, which we believe is due329 to the use of a suboptimal ⌧ or \u0000 . More empirical effort is require to tune various hyperparameters to330 get a consistent behavior at all memory sizes. Overall, our initial effort here suggests that K-priors331 can do better than Replay, and have potential to give better results with more hyperparameter tuning.332 6 Discussion333 In this paper, we proposed a class of new priors, called K-prior. We show general principles of334 obtaining accurate adaptation with K-priors which are based on accurate gradient reconstructions.335 These principles have many properties that a good prior is expected to have [ 50 ]. The prior applies336 to a wide-variety of adaptation tasks for a range of models, and helps us to connect many existing,337 seemingly-unrelated adaptation strategies in ML. Based on our adaptation principles, we derived338 practical methods to enable adaptation by tweaking models’ predictions at a few past examples. This339 is analogous to adaptation in humans and animals where past experiences is used for new situations.340 In practice, the amount of required past memory seems sufﬁciently low for many tasks. Overall,341 K-priors provide an intuitive yet practical mechanisms for generic adaptation in ML.342 The ﬁnancial and environmental costs of retraining are a huge concern for ML practitioners, which343 can be reduced with quick adaptations. During this work, we realized how little work has been done344 on this topic. The current pipelines and designs are specialized for an ofﬂine, static setting. Our345 approach here pushes towards a simpler design which will support a more dynamic setting. The346 approach can eventually lead to new systems that learn quickly and ﬂexibly, and also act sensibly347 across a wide range of tasks. This opens a path towards systems that learn incrementally in a continual348 fashion, with the potential to fundamentally change the way ML is used in scientiﬁc and industrial349 applications. We hope that this work will help others to do more work towards this goal in the future.350 We ourselves will continue to push this work in that direction.351 9 (b) USPS Add Data(a)           over all (c) CIFAR-10 Add data Validation acc (%) Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) (d) CIFAR-10 KnowledgeDistillationwb Figure 3: (a) When compared at the Batch solution for the ‘Add Data’ task on USPS, weight priors give incorrect values of h 0 ( f i w ) (shown with black dots, each dot correspond to a data examples). Points on the diagonal means a perfect match which is the case for K-priors (show with red dots). (b) Due to this, weight-priors (green diamonds) perform worse than K-priors (red squares). (c) For the ’Add data’ task on CIFAR-10, K-priors outperform replay (blue circles), but performance can still be improved by using a temperature parameter (dark-red triangles). (d) The same is true for knowledge distillation [ 21 ], and we see that we can reduce memory size while still performing better than the student model. The dip in the middle, we believe, is due to suboptimal hyperparameter tuning. [ 29 ] with MLPs and 10-way classiﬁcation on CIFAR-10 with CifarNet [ 57 ], trained with the Adam319 optimizer [ 26 ]. In Figure 3(c) we show one representative result for the ‘Add data’ task with CIFAR-320 10, where we add a random 10% of CIFAR-10 training data to the other 90% (mean and standard321 deviation over 3 runs). Although vanilla K-priors outperform Replay, there is now a bigger gap322 between K-prior and Batch even with 50% past data stored. However, when we use a temperature323 (similar to knowledge distillation in (14) but with the weight term included), K-priors improves.324 A similar result is shown in Figure 3(d) for knowledge distillation ( \u0000 =0 but with a temperature325 parameter) where we are distill from a CifarNet teacher to a LeNet5-style student (details in Ap-326 pendix D). Here, K-priors with 100% data is equivalent to Knowledge Distillation, but when we327 reduce the memory size using our method, we still outperform Batch (which is trained from scratch328 on all data). There is a dip in performance at memory sizes of 10% past data, which we believe is due329 to the use of a suboptimal ⌧ or \u0000 . More empirical effort is require to tune various hyperparameters to330 get a consistent behavior at all memory sizes. Overall, our initial effort here suggests that K-priors331 can do better than Replay, and have potential to give better results with more hyperparameter tuning.332 6 Discussion333 In this paper, we proposed a class of new priors, called K-prior. We show general principles of334 obtaining accurate adaptation with K-priors which are based on accurate gradient reconstructions.335 These principles have many properties that a good prior is expected to have [ 50 ]. The prior applies336 to a wide-variety of adaptation tasks for a range of models, and helps us to connect many existing,337 seemingly-unrelated adaptation strategies in ML. Based on our adaptation principles, we derived338 practical methods to enable adaptation by tweaking models’ predictions at a few past examples. This339 is analogous to adaptation in humans and animals where past experiences is used for new situations.340 In practice, the amount of required past memory seems sufﬁciently low for many tasks. Overall,341 K-priors provide an intuitive yet practical mechanisms for generic adaptation in ML.342 The ﬁnancial and environmental costs of retraining are a huge concern for ML practitioners, which343 can be reduced with quick adaptations. During this work, we realized how little work has been done344 on this topic. The current pipelines and designs are specialized for an ofﬂine, static setting. Our345 approach here pushes towards a simpler design which will support a more dynamic setting. The346 approach can eventually lead to new systems that learn quickly and ﬂexibly, and also act sensibly347 across a wide range of tasks. This opens a path towards systems that learn incrementally in a continual348 fashion, with the potential to fundamentally change the way ML is used in scientiﬁc and industrial349 applications. We hope that this work will help others to do more work towards this goal in the future.350 We ourselves will continue to push this work in that direction.351 9 Figure 1: Left: K-priors can handle a wide-variety of adaptation tasks by using the past model and a small memory of the past data. Middle: For adaptation, the model needs tweaking at only a handful of past examples (shown with dark purple markers). The adapted model (dashed red line) is very close to the retraining on the full batch (solid black line). Right: Results on binary classiﬁcation on ‘USPS’ digits with neural networks show that K-priors (red square) obtain solutions close to the batch training (gray line) but by using only a fraction (2-5%) of the past data. It also performs better than ‘Replay’ (blue circles) where the same memory is used for replay. See Sec. 5 for more details. online Gaussian Processes [17], and continual learning [ 36, 45, 9]. It leads to natural adaptation- mechanisms where models’ predictions need to be readjusted only at a handful of past experiences (Fig. 1, middle). This is quick and easy to train with ﬁrst-order optimization methods and, by choosing a sufﬁciently large memory, we obtain results similar to retraining-from-scratch (Fig. 1, right). Code is available at https://github.com/team-approx-bayes/kpriors. 2 Adaptation in Machine Learning 2.1 Knowledge-adaptation tasks Our goal is to quickly and accurately adapt an already trained model to incremental changes in its training framework. Throughout we refer to the trained model as the base model. We denote its outputs by fw∗(x) for inputs x ∈RD, and assume that its parameters w∗∈W⊂ RP are obtained by solving the following problem on the data D, w∗= arg min w∈W ¯ℓ(w), where ¯ℓ(w) = ∑ i∈D ℓi(w) + R(w). (1) Here, ℓi(w) denotes the loss function on the i’th data example, andR(w) is a regularizer. A good adaptation method should be able to handle many types of changes. The simplest and most common change is to add/remove data examples, as shown below at the left where the data example j /∈D is added to get w+, and at the right where an example k∈D is removed to get w−, w+ = arg min w∈W ∑ i∈D∪j ℓi(w) + R(w), w−= arg min w∈W ∑ i∈D\\k ℓi(w) + R(w). (2) We refer to these problems as ‘Add/Remove Data’ tasks. Other changes are the ‘Change Regularizer’ (shown on the left) task where the regularizer is replaced by a new one G(w), and the ‘Change Model Class/Architecture’ task (shown on the right) where the model class/architecture is changed leading to a change of the parameter space from Wto Θ, wG= arg min w∈W ∑ i∈D ℓi(w) + G(w), θ∗= arg min θ∈Θ ∑ i∈D ˜ℓi(θ) + ˜R(θ). (3) Here, we assume the loss ˜ℓi(θ) has the same form as ℓi(w) but using the new model ˜fθ(x) for prediction, with θ ∈Θ as the new parameter. θ can be of a different dimension, and the new regularizer can be chosen accordingly. The change in the model class could be a simple change in 2features, for example it is common in linear models to add/remove features, or the change could be similar to model compression or knowledge distillation [24], which may not have a regularizer. Another type of change is to add privileged information, originally proposed by Vapnik and Izmailov [60]. The goal is to include different types of data to improve the performance of the model. This is combined with knowledge distillation by Lopez-Paz et al. [37] and has recently been applied to domain adaptation in deep learning [4, 50, 51]. There could be several other kinds of changes in the training framework, for example, those involving a change in the loss function, or even a combination of the changes discussed above. Our goal is to develop a method that can handle such wide-variety of changes, or ‘adaptation tasks’ as we will call them throughout. Such adaptation can be useful to reduce the cost of model updating, for example in a continuously evolving ML pipeline. Consider k-fold cross-validation, where the model is retrained from scratch for every data-fold and hyperparameter setting. Such retraining can be made cheaper and faster by reusing the model trained in the previous folds and adapting them for new folds and hyperparameters. Model reuse can also be useful during active-learning for dataset curation, where a decision to include a new example can be made by using a quick Add Data adaptation. In summary, model adaptation can reduce the cost by avoiding the need to constantly retrain. 2.2 Challenges of knowledge adaptation Knowledge adaptation in ML has proven to be challenging. Currently there are no methods that can handle many types of adaptation tasks. Most existing works apply narrowly to speciﬁc models and mainly focuses on adaptation to Add/Remove Data only. This includes many early proposals for SVMs [12, 59, 25, 20, 49, 35, 31, 58], recent ones for machine-unlearning [11, 21, 41, 22, 53, 8], and methods that use weight and functional priors, for example, for online learning [13], continual deep learning [30, 40, 47, 33, 62, 34, 46, 7, 57, 45, 9], and Gaussian-Process (GP) models [ 17, 52, 56]. The methodologies of these methods are entirely different from each other, and they do not appear to have any common principles behind their adaptation mechanisms. Our goal is to ﬁll this gap and propose a single method that can handle a wide-variety of tasks for a range of models. We also note that there is no prior work on the Change Regularizer task. Adaptation has been used to boost hyperparameter tuning [61] but only Add/Remove Data adaptation is used. Warm-starts have been employed as well [18], but it is often not sufﬁcient and can even hurt performance [5]. 2.3 Problem setting and notation Throughout, we will use a supervised problem where the loss is speciﬁed by an exponential-family, ℓ(y,h(f)) = −log p(y|f) = −⟨y,f⟩+ A(f), (4) where y ∈Y denotes the scalar observation output, f ∈F is the canonical natural parameter, A(f) is the log-partition function, and h(f) = E(y) = ∇A(f) is the expectation parameter. A typical example is the cross-entropy loss for binary outcomes y∈{0,1}where A(f) = log(1 + ef) and h(f) = σ(f) is the Sigmoid function. It is straightforward to extend our method to a vector observation and model outputs. An extension to other types of learning frameworks is discussed in the next section (see the discussion around Eq. 14). Throughout the paper, we will use a shorthand for the model outputs, where we denote fi w = fw(xi). We will repeatedly make use of the following expression for the derivative of the loss, ∇ℓ(yi,h(fi w)) = ∇fi w [h(fi w) −yi]. (5) 3 Knowledge-Adaptation Priors (K-priors) We present Knowledge-adaptation priors (K-priors) to quickly and accurately adapt a model’s knowledge to a wide variety of changes in its training framework. K-priors, denoted below by K(w; w∗,M), refer to a class of priors that use both weight and function-space regularizers, K(w; w∗,M) = Df (f(w)∥f(w∗)) + τDw(w∥w∗) , (6) where f(w) is a vector of fw(ui), deﬁned at inputs in M= (u1,u2,..., uM). The divergence Df(·∥·) measures the discrepancies in the function space F, while Dw(·∥·) measures the same in the 3weight space W. Throughout, we will use Bregman divergences Bψ(p1,p2) = ψ(p1) −ψ(p2) − ∇ψ(p2)⊤(p1 −p2), speciﬁed using a strictly-convex Bregman function ψ(·). K-priors are deﬁned using the base model w∗, the memory set M, and a trade-off parameter τ >0. We keep τ = 1 unless otherwise speciﬁed. It might also use other parameters required to deﬁne the divergence functions. We will sometimes omit the dependency on parameters and refer to K(w). Our general principle of adaptation is to use K(w) to faithfully reconstruct the gradients of the past training objective. This is possible due to the combination of weight and function-space divergences. Below, we illustrate this point for supervised learning for Generalized Linear Models (GLMs). 3.1 K-priors for GLMs GLMs include models such as logistic and Poisson regression, and have a linear model fi w = φ⊤ i w, with feature vectors φi = φ(xi). The base model is obtained as follows, w∗= arg min w∈W ∑ i∈D ℓ(yi,h(fi w)) + R(w). (7) In what follows, for simplicity, we use an L2 regularizer R(w) = 1 2 δ∥w∥2, with δ >0. We will now discuss a K-prior thatexactly recovers the gradients of this objective. For this, we choose Dw(·∥·) to be the Bregman divergence with R(w) as the Bregman function, Dw(w∥w∗) = BR(w∥w∗) = 1 2 δ∥w −w∗∥2. We set memory M= X, where Xis the set of all inputs from D. We regularize each example using separate divergences whose Bregman function is equal to the log-partition A(f) (deﬁned in Eq. 4), Df (f(w)∥f(w∗)) = ∑ i∈X BA(fi w∥fi w∗) = ∑ i∈X ℓ ( h(fi w∗),h(fi w) ) + constant. Smaller memories are discussed later in this section. Setting τ = 1, we get the following K-prior, K(w; w∗,X) = ∑ i∈X ℓ ( h(fi w∗),h(fi w) ) + 1 2 δ∥w −w∗∥2, (8) which has a similar form to Eq. 7, but the outputs yi are now replaced by the predictions h(fi w∗), and the base model w∗serves as the mean of a Gaussian weight prior. We can now show that the gradient of the above K-prior is equal to that of the objective used in Eq. 7, ∇K(w; w∗,X) = ∑ i∈X φi [ h(fi w) −h(fi w∗) ] + δ(w −w∗), (9) = ∑ i∈D φi [ h(fi w) −yi ] + δw    =∇¯ℓ(w). − \u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018∑ i∈D φi [ h(fi w∗) −yi ] −δw∗    =0. , (10) where the ﬁrst line is obtained by using Eq. 5 and noting that ∇fi w = φi, and the second line is obtained by adding and subtracting outputs yi in the ﬁrst term. The second term there is equal to 0 because w∗is a minimizer and therefore ∇¯ℓ(w∗) = 0. In this case, the K-prior with M= Xexactly recovers the gradient of the past training objective. Why are we able to recover exact gradients? This is because the structure of the K-prior closely follows the structure of Eq. 7: the gradient of each term in Eq. 7 is recovered by a corresponding divergence in the K-prior. The gradient recovery is due to the property that the gradient of a Bregman divergence is the difference between the dual parameters ∇ψ(p): ∇p1 B(p1,p2) = ∇ψ(p1) −∇ψ(p2). This leads to Eq. 9. For the function-space divergence term, h(fi w) −h(fi w∗) are the differences in the (dual) expectation parameters. For the weight-space divergence term, we note that the dual space is equal to the original parameter space Wfor the L2 regularizer, leading to w −w∗. Lastly, we ﬁnd that terms cancel out by using the optimality of w∗, giving us the exact gradients. 43.2 K-priors with limited memory In practice, setting M= Xmight be as slow as full retraining, but for incremental changes, we may not need all of them (see Fig. 1, for example). Then, which inputs should we include? The answer lies in the gradient-reconstruction error e, shown below for M⊂X , e(w; M) = ∇¯ℓ(w) −∇K(w; w∗,M) = ∑ i∈X\\M φi [ h(fi w) −h(fi w∗) ] . (11) The error depends on the “leftover”φi for i∈X\\M, and their discrepancies h(fi w) −h(fi w∗). A simple idea could be to include the inputs where predictions disagree the most, but this is not feasible because the candidates w are not known beforehand. The following approximation is more practical, e(w; M) ≈G∗(X\\M)(w −w∗), where G∗(X\\M) = ∑ i∈X\\M φih′(fi w∗)φ⊤ i . (12) This is obtained by using the Taylor approximation h(fi w) −h(fi w∗) ≈h′(fi w∗)(∇fi w∗)⊤(w −w∗) in Eq. 11 (h′(fi) is the derivative). The approximation is conveniently expressed in terms of the Generalized Gauss-Newton (GGN) matrix [ 38], denoted by G∗(·). The approximation suggests that Mshould be chosen to keep the leftover GGN matrix G∗(X\\M) orthogonal to w −w∗. Since w changes during training, a reasonable approximation is to choose examples that keep the top-eigenvalues of the GGN matrix. This can be done by forming a low-rank approximation by using sketching methods, such as the leverage score [16, 1, 15, 10]. A cheaper alternative is to choose the examples with highest h′(fi w∗). The quantity is cheap to compute in general, for example, for deep networks it is obtained with just a forward pass. Such a set has been referred to as the ‘memorable past’ by Pan et al.[45], who found it to work well for classiﬁcation. Due to its simplicity, we will use this method in our experiments, and leave the application of sketching methods as future work. K-priors with limited memory can achieve low reconstruction error. This is due to an important feature of K-priors: they do not restrict inputs ui to lie within the training set X. The inputs can be arbitrary locations in the input space. This still works because a ground-truth label is not needed for ui, and only model predictions fw(ui) are used in K-priors. As long as the chosen ui represent X well, we can achieve a low gradient-reconstruction error, and sometimes even perfect reconstruction. Theoretical results regarding this point are discussed in App. A, where we present the optimal K-prior which can theoretically achieve perfect reconstruction error by using singular vectors of Φ⊤= [φ(x1),φ(x2),..., φ(xN)]. When only top-M singular vectors are chosen, the error grows according to the leftover singular values. The optimal K-prior is difﬁcult to realize in practice, but the result shows that it is theoretically possible to achieve low error with limited memory. 3.3 Adaptation using K-priors We now discuss how the K-prior of Eq. 8 can be used for the Add/Remove Data tasks. Other adaptation tasks and corresponding K-priors are discussed in App. B. Because K-priors can reconstruct the gradient of ¯ℓ(w), we can use them to adapt instead of retraining from scratch. For example, to add/remove data from the GLM solution in Eq. 7, we can use the following K-prior regularized objectives, ˆw+ = arg min w∈W ℓj(w) + K(w; w∗,M), ˆw−= arg min w∈W −ℓk(w) + K(w; w∗,M). (13) Using Eq. 10, it is easy to show that this recovers the exact solution when all the past data is used. App. B details analogous results for the Change Regularizer and Change Model Class tasks. Theorem 1. For M= X, we have w+ = ˆw+ and w−= ˆw−. For limited memory, we expect the solutions to be close when memory is large enough. This is because the error in the gradient is given by Eq. 11. The error can be reduced by choosing better M and/or by increasing its size to ultimately get perfect recovery. We stress that Eq. 13 is fundamentally different from replay methods that optimize an objective similar to Eq. 2 but use a small memory of past examples [ 48]. Unlike such methods, we use the predictions h(fi w∗), which we can think of as soft labels, with potentially more information than the true one-hot encoded labels yi. Given a ﬁxed memory budget, we expect K-prior regularization to perform better than such replay methods, and we observe this empirically in Sec. 5. 53.4 K-priors for Generic Learning Problems The main principle behind the design of K-priors is to construct it such that the gradients can faithfully be reconstructed. As discussed earlier, this is often possible by exploiting the structure of the learning problem. For example, to replace an old objective such as Eq. 1, with loss ℓold i (f) and regularizer Rold(w), with a new objective with loss ℓnew i (f) and regularizer Rnew(w), the divergences should be chosen such that they have the following gradients, ∇Dw(w∥w∗) = ∇Rnew(w) −∇Rold(w), ∇Df(f(w)∥f(w∗)) = ∇f(w)⊤B dm (14) where dm is an M-length vector with the discrepancy ∇ℓnew i (fi w) −∇ℓold i (fi w∗) as the i’th entry. The matrix B is added to counter the mismatch between Dand M. Similar constructions can be used for other learning objectives. For non-differentiable functions, a Bayesian version can be used with the Kullback-Leibler (KL) divergence (we discuss an example in the next section). We can use exponential-family distributions which implies a Bregman divergence through KL [ 6]. Since the gradient of such divergences is equal to the difference in the dual-parameters, the general principle is to use divergences with an appropriate dual space to swap the old information with new information. 4 K-priors: Extensions and Connections The general principle of adaptation used in K-priors connects many existing works in speciﬁc settings. We will now discuss these connections to show that K-priors provide a unifying and generalizing principle for these seemingly unrelated methods in ﬁelds such as online learning, deep learning, SVMs, Bayesian Learning, Gaussian Processes, and continual learning. 4.1 Weight-Priors Quadratic or Gaussian weight-priors [13, 30, 54] be seen as as specialized cases of K-priors, where restrictive approximations are used. For example, the following quadratic regularizer, Rquad(w; w∗) = (w −w∗)⊤[G∗(X) + δI] (w −w∗), which is often used in used in online and continual learning [ 13, 30], can be seen as a ﬁrst-order approximation of the K-prior in Eq. 8. This follows by approximating the K-prior gradient in Eq. 8 by using the Taylor approximation used in Eq. 12, to get ∇K(w; w∗,X) ≈ ∑ i∈X φi [ h′(φ⊤ i w∗)φ⊤ i (w −w∗) ] + δ(w −w∗) = ∇Rquad(w; w∗). K-priors can be more accurate than weight priors but may require larger storage for the memory points. However, we expect the memory requirements to grow according to the rank of the feature matrix (see App. A) which may still be manageable. If not, we can apply sketching methods. 4.2 K-priors for Deep Learning and Connections to Knowledge Distillation We now discuss the application to deep learning. It is clear that the functional term in K-priors is similar to Knowledge distillation (KD) [24], which is a popular approach for model compression in classiﬁcation problems using a softmax function (the following expression is from Lopez-Paz et al. [37], also see App. E), ℓKD(w) = λ ∑ i∈D ℓ ( yi,h(fi w) ) + (1 −λ) ∑ i∈D ℓ ( h(fi w∗/T), h(fi w) ) , (15) The base model predictions are often scaled with a temperature parameter T >0, and λ∈[0,1]. KD can be seen as a special case of K-priors without the weight-space term (τ = 0). K-priors extend KD in three ways, by (i) adding the weight-space term, (ii) allowing general link functions or divergence functions, and (iii) using a potentially small number of examples in Minstead of the whole dataset. With these extensions, K-priors can handle adaptation tasks other than compression. Due to their similarity, it is also possible to borrow tricks used in KD to improve the performance of K-priors. KD often yields solutions that are better than retraining from scratch. Theoretically the reasons behind this are not understood well, but we can view KD as a mechanism to reconstruct the past 6gradients, similarly to K-priors. As we now show, this gives a possible explanation behind KD’s success. Unlike GLMs, K-priors for deep learning do not recover the exact gradient of the past training objective, and there is an additional left-over term (a derivation is in App. C), ∇K(w) = ∑ i∈D ∇fi w [ h(fi w) −yi ] + δw    =∇¯ℓ(w) − (∑ i∈D ∇fi wri w∗+ δw∗ )    Additional term since ∇fiw̸=∇fiw∗ , (16) where ri w∗ := h(fi w∗) −yi is the residual of the base model. It turns out that the gradient of the KD objective in Eq. 15 has this exact same form when δ= 0, T = 1 (derivation in App. C), ∇ℓKD(w) = ∑ i∈D ∇fi w [ h(fi w) −yi ] −(1 −λ) ∑ i∈D ∇fi wri w∗. The additional term adds large gradients to push away from the high residual examples (the examples the teacher did not ﬁt well). This is similar to Similarity-Control for SVMs from Vapnik and Izmailov [60], where “slack”-variables are used in a dual formulation to improve the student, who could now be solving a simpler separable classiﬁcation problem. The residuals ri w∗ above play a similar role as the slack variables, but they do not require a dual formulation. Instead, they arise due to the K-prior regularization in a primal formulation. In this sense, K-priors can be seen as an easy-to-implement scheme for Similarity Control, that could potentially be useful for student-teacher learning. Lopez-Paz et al. [37] use this idea further to generalize distillation and interpret residuals from the teachers as corrections for the student (see Eq. 6 in their paper). In general, it is desirable to trust the knowledge of the base model and use it to improve the adapted model. These previous ideas are now uniﬁed in K-priors: we can provide the information about the decision boundary to the student in a more accessible form than the original data (with true labels) could. 4.3 Adding/removing data for SVMs K-prior regularized training yields equivalent solutions to the adaptation strategies used in SVM to add/remove data examples. K-priors can be shown to be equivalent to the primal formulation of such strategies [35]. The key trick to show the equivalence is to use the representer theorem which we will now illustrate for the ‘Add Data’ task in Eq. 13. LetΦ+ be the (N + 1) ×P feature matrix obtained on the dataset D∪j, then by the representer theorem we know that there exists a β ∈RN+1 such that w+ = Φ⊤ +β. Taking the gradient of Eq. 13, and multiplying by Φ+, we can write the optimality condition as, 0 = Φ⊤ +∇[ℓj(w+) + K(w+)] = ∑ i∈D∪j ( ∇fℓ(yi,h(f))|f=β⊤ i ki,+ ) ki,+ + δK+β, (17) where K+ = Φ+Φ⊤ + and its i’th column is denoted byki,+. This is exactly the gradient of the primal objective in the function-space deﬁned over the full batch D∪j; see Equation 3.6 in Chapelle [14]. The primal strategy is equivalent to the more common dual formulations [12, 59, 25, 20, 49, 31, 58]. The function-space formulations could be computationally expensive, but speed-ups can be obtained by using support vectors. This is similar to the idea of using limited memory in K-priors in Sec. 3. 4.4 K-priors for Bayesian Learning and Connections to GPs K-priors can be seamlessly used for adaptation within a Bayesian learning framework. Consider a Gaussian approximation q∗(w) trained on a variational counterpart of Eq. 1 with prior p(w) ∝ exp [−R(w)], and its adapted version where we add data, as shown below ( DKL[·∥·] is the KL divergence), q∗(w) = arg min q∈Q ∑ i∈D Eq[¯ℓi(w)] + DKL[p∥q], q+(w) = arg min q∈Q ∑ i∈D∪j Eq[ℓi(w)] + DKL[p∥q]. Assuming the same setup as Sec. 3, we can recover q+(w) by using qK(w) ∝exp [−K(w)] where we use the K-prior deﬁned in Eq. 8 (note that normalization constant of qKis not required), ˆq+(w) = arg min q∈Q Eq[ℓj(w)] + DKL[q∥qK], (18) 7This follows using Eq. 10. Details are in App. D. In fact, when this Bayesian extension is written in the function-space similarly to Eq. 17, it is related to the online updates used in GPs [17]. When qKis built with limited memory, as described in Sec. 3, the application is similar to sparse variational GPs, but now data examples are used as inducing inputs. These connections are discussed in more detail in App. D. Our K-prior formulations operates in the weight-space and can be easily trained with ﬁrst-order methods, however an equivalent formulation in the function space can also be employed, as is clear from these connections. The above extensions can be extended to handle arbitrary exponential- family approximations by appropriately deﬁning K-priors using KL divergences. We omit these details since this topic is more suitable for a Bayesian version of this paper. 4.5 Memory-Based Methods for Deep Continual Learning K-priors is closely related to recent functional regularization approaches proposed for deep continual learning [34, 46, 7, 54, 57, 45, 9]. The recent FROMP approach of Pan et al. [45] is closest to ours where the form of the functional divergence used is similar to our suggestion in Eq. 14. Speciﬁcally, comparing with Eq. 14, their functional divergence correspond to the vector dm with the i’th entry as h(fw(ui)) −h(fw∗(ui)) for ui ∈M, and the matrix B is (can be seen as Nystrom approximation), B = Λ(w) [ Λ(w∗)∇f(w∗)G(w∗)−1∇f(w∗)⊤Λ(w∗) ]−1 , where Λ(w) is a diagonal matrix with h′(fw(ui)) as the i’th diagonal entry, and G(w∗) = ∇f(w∗)⊤Λ(w∗)∇f(w∗) + δI is the GGN matrix. They also propose to use ‘memorable past’ examples obtained by sorting h′(fi w∗), which is consistent with our theory (see Eq. 11). Based on our work, we can interpret the approach of Pan et al. [45] as a mechanism to reconstruct the gradient of the past, which gives very good performance in practice. Another related approach is the gradient episodic memory (GEM) [36], where the goal is to ensure that the ∑ i∈M[ℓ(yi,fi w∗) −ℓ(yi,fi w)] <0. This is similar in spirit to the student-teacher transfer of Vapnik and Izmailov[60] where the loss of the student is regularized using the model output of the teacher (see Eq. 7 in Vapnik and Izmailov [60] for an example). Lopez-Paz and Ranzato [36] relax the optimization problem to write it in terms of the gradients, which is similar to K-priors, except that K-priors use a ﬁrst-order optimization method, which is simpler than the dual approach used in Lopez-Paz and Ranzato [36]. Most of these approaches do not employ a weight-space divergence, and sometimes even the function- space divergence is replaced by the Euclidean one [ 7, 9]. Often, the input locations are sampled randomly, or using a simple replay method [9] which could be suboptimal. Some approaches propose computationally-expensive methods for choosing examples to store in memory [2, 3], and some can be seen as related to choosing points with high leverage [3]. The approach in Titsias et al. [57] uses inducing inputs which is closely connected to the online GP update. The method we proposed does not contradict with these, but gives a more direct way to choose the points where the gradient errors are taken into consideration. 5 Experimental Results We compare the performance of K-priors to retraining with full-batch data (‘Batch’) and a retraining with replay from a small memory (‘Replay’), and useτ = 1. For fair comparisons, we use the same memory for Replay and K-priors obtained by choosing points with highest h′(fi w∗) (see Sec. 3). Memory chosen randomly often gives much worse results and we omit these results. Replay uses the true label while K-priors use model-predictions. We compare these three methods on the four adaptation tasks: ‘Add Data’, ‘Remove Data’, ‘Change Regularizer’, and ‘Change Architecture’. For the ‘Add Data’ task, we also compare to Weight-Priors with GGN. Our overall ﬁnding is that, for GLMs and deep learning on small problems, K-priors can achieve the same performance as Batch, but with a small fraction of data (often 2-10% of the full data (Fig. 1, right, and Fig. 2). Replay does much worse for small memory size, which clearly shows the advantage of using the model predictions (instead of true labels) in K-priors. Weight priors generally perform well, but they can do badly when the adaptation involves a drastic change for examples inM(see Fig. 3). Finally, for large deep-learning problems, results are promising but more investigations are required with extensive hyperparameter tuning. 8Validation acc (%)Validation acc (%) Memory size (% of past data) (a) Adult, logistic regression (b) USPS, logistic regression Add new data Remove old data Change regularizer Change model class Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Figure 2: K-priors (red squares) match Batch (grey) while mostly using 2-5% of the data (for only 3 tasks a larger fraction is required). K-priors always outperforms Replay which uses the true labels. K-priors replace the labels by the model predictions (see the discussion after Theorem 1). Several additional experiments are in App. E. In App. E.4, we study the effect of randomly initializa- tion and ﬁnd that it performs similarly to a warm start at w∗. In App. E.5, we ﬁnd that K-priors with limited memory are take much less time to reach a speciﬁed accuracy than both batch and replay. The low cost is due to a small memory size. Replay also uses small memory but performs poorly. Logistic Regression on the ‘UCI Adult’ dataset.This is a binary classiﬁcation problem consisting of 16,100 examples to predict income of individuals. We randomly sample 10% of the training data (1610 examples), and report mean and standard deviation over 10 such splits. For training, we use the L-BFGS optimizer for logistic regression with polynomial basis. Results are summarized in Fig. 2(a). For the ‘Add Data’ task, the base model uses 9% of the data and we add 1% new data. For ‘Remove Data’, we remove 100 data examples (6% of the training set) picked by sorting h′(fi w∗)). For the ‘Change Regularizer’ task, we change theL2 regularizer from δ= 50 to 5, and for ‘Change Model Class’, we reduce the polynomial degree from2 to 1. K-priors perform very well on the ﬁrst three tasks, remaining very close to Batch, even when the memory sizes are down to 2%. ‘Changing Model Class’ is slightly challenging, but K-priors still signiﬁcantly out-perform Replay. Logistic Regression on the ‘USPS odd vs even’ dataset.The USPS dataset consists of 10 classes (one for each digit), and has 7,291 training images of size 16 ×16. We split the digits into two classes: odd and even digits. Results are in Fig. 2(b). For the ‘Add Data’ task, we add all examples for the digit 9 to the rest of the dataset, and for ‘Remove Data’ we remove the digit 8 from the whole dataset. By adding/removing an entire digit, we enforce an inhomogeneous data split, making the tasks more challenging. The ‘Change Regularizer’ and ‘Change Model Class’ tasks are the same as the Adult dataset. K-priors perform very well on the ‘Add Data’ and ‘Change Regularizer’ tasks, always achieving close to Batch performance. For ‘Remove Data’, which is a challenging task due to inhomogenity, K-priors still only need to store 5% of past data to maintain close to 90% accuracy, whereas Replay requires 10% of the past data. Neural Networks on the ‘USPS odd vs even’ dataset.This is a repeat of the previous experiment but with a neural network (a 1-hidden-layer MLP with 100 units). Results are in Fig. 1 (right). The ‘Change Regularizer’ task now changesδ= 5 to 10, and the ‘Change Architecture’ task compresses the architecture from a 2-hidden-layer MLP (100 units per layer) to a 1-hidden-layer MLP with 100 units. We see that even with neural networks, K-priors perform very well, similarly out-performing Replay and remaining close to the Batch solution at small memory sizes. Weight-priors vs K-priors.As discussed in the main text, weight-priors can be seen as an approxi- mation of K-priors where h′(fi w) are replaced by ‘stale’h′(fi w∗), evaluated at the oldw∗. In Fig. 3(a), we visualize these ‘stale’h′(fi w∗) and compare them to K-priors which obtains values close to the ones found by Batch. Essentially, for the points at the diagonal the match is perfect, and we see that it is the case for K-priors but not for the weight-priors. We use logistic regression on the USPS data (the ‘Add Data’ task). This inhomogeneous data split is difﬁcult for weight-priors, and we show in Fig. 3(b) that weight-priors do not perform well. For homogeneous data splits, weight-priors do 9(b) USPS Add Data(a)           over all (c) CIFAR-10 Add data Validation acc (%) Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) (d) CIFAR-10 Knowledge Distillation Figure 3: (a) When compared at the Batch solution for the ‘Add Data’ task on USPS, weight priors give incorrect values of h′(fi w) (shown with black dots, each dot correspond to a data examples). Points on the diagonal means a perfect match which is the case for K-priors (show with red dots). (b) Due to this, weight-priors (green diamonds) perform worse than K-priors (red squares). (c) For the ’Add data’ task on CIFAR-10, K-priors outperform replay (blue circles), but performance can still be improved by using a temperature parameter (dark-red triangles). (d) The same is true for knowledge distillation [24], and we see that we can reduce memory size while still performing better than the student model. better than this, but this result goes to show that they do not have any mechanisms to ﬁx the mistakes made in the past. In K-priors, we can always change Mto improve the performance. We provide more plots and results in App. E, including results for weight-priors on all the ‘Add data’ tasks we considered in this paper. MNIST and CIFAR-10, neural networks.Finally, we discuss results on larger problems in deep learning. We show many adaptation tasks in App. E for 10-way classiﬁcation on MNIST [32] with MLPs and 10-way classiﬁcation on CIFAR-10 with CifarNet [62], trained with the Adam optimizer [29]. In Fig. 3(c) we show one representative result for the ‘Add data’ task with CIFAR-10, where we add a random 10% of CIFAR-10 training data to the other 90% (mean and standard deviation over 3 runs). Although vanilla K-priors outperform Replay, there is now a bigger gap between K-prior and Batch even with 50% past data stored. However, when we use a temperature (similar to knowledge distillation in (15) but with the weight term included), K-priors improves. A similar result is shown in Fig. 3(d) for knowledge distillation ( δ = 0 but with a temperature parameter) where we are distill from a CifarNet teacher to a LeNet5-style student (details in App. E). Here, K-priors with 100% data is equivalent to Knowledge Distillation, but when we reduce the memory size using our method, we still outperform Batch (which is trained from scratch on all data). Overall, our initial effort here suggests that K-priors can do better than Replay, and have potential to give better results with more hyperparameter tuning. 6 Discussion In this paper, we proposed a class of new priors, called K-prior. We show general principles of obtaining accurate adaptation with K-priors which are based on accurate gradient reconstructions. The prior applies to a wide-variety of adaptation tasks for a range of models, and helps us to connect many existing, seemingly-unrelated adaptation strategies in ML. Based on our adaptation principles, we derived practical methods to enable adaptation by tweaking models’ predictions at a few past examples. This is analogous to adaptation in humans and animals where past experiences is used for new situations. In practice, the amount of required past memory seems sufﬁciently low. The ﬁnancial and environmental costs of retraining are a huge concern for ML practitioners, which can be reduced with quick adaptations. The current pipelines and designs are specialized for an ofﬂine, static setting. Our approach here pushes towards a simpler design which will support a more dynamic setting. The approach can eventually lead to new systems that learn quickly and ﬂexibly, and also act sensibly across a wide range of tasks. This opens a path towards systems that learn incrementally in a continual fashion, with the potential to fundamentally change the way ML is used in scientiﬁc and industrial applications. We hope that this work will help others to do more towards this goal in the future. We ourselves will continue to push this work in that direction. 10Acknowledgements We would like to thank the members of the Approximate-Bayesian-Inference team at RIKEN- AIP. Special thanks to Dr. Thomas Möllenhoff (RIKEN-AIP), Dr. Gian Maria Marconi (RIKEN- AIP), Peter Nickl (RIKEN-AIP), and also to Prof. Richard E. Turner (University of Cambridge). Mohammad Emtiyaz Khan is partially supported by KAKENHI Grant-in-Aid for Scientiﬁc Research (B), Research Project Number 20H04247. Siddharth Swaroop is partially supported by a Microsoft Research EMEA PhD Award. Author Contributions Statement List of Authors: Mohammad Emtiyaz Khan (M.E.K.), Siddharth Swaroop (S.S.). Both the authors were involved in the idea conception. S.S. derived a version of theorem 1 with some help from M.E.K. This was then modiﬁed and generalized by M.E.K. for generic adaptation tasks. The general principle of adaptation described in the paper are due to M.E.K., who also derived connections to SVMs and GPs, and extensions to Bayesian settings (with regular feedback from S.S.). Both authors worked together on the connections to Knowledge Distillation and Deep Continual Learning. S.S. performed all the experiments (with regular feedback from M.E.K.). M.E.K. wrote the main sections with the help of S.S., and S.S. wrote the section about the experiments. Both authors proof-read and reviewed the paper. References [1] Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with statis- tical guarantees. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. [2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. InAdvances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [4] Shuang Ao, Xiang Li, and Charles Ling. Fast generalized distillation for semi-supervised domain adaptation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 31, 2017. [5] Jordan Ash and Ryan P Adams. On warm-starting neural network training. In Advances in Neural Information Processing Systems, volume 33, pages 3884–3894, 2020. [6] Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with Bregman divergences. Journal of machine learning research, 6(Oct):1705–1749, 2005. [7] Ari Benjamin, David Rolnick, and Konrad Kording. Measuring and regularizing networks in function space. In International Conference on Learning Representations, 2019. [8] Jonathan Brophy and Daniel Lowd. Machine unlearning for random forests. In International Conference on Machine Learning, 2021. [9] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. In Advances in Neural Information Processing Systems, volume 33, pages 15920–15930, 2020. [10] Daniele Calandriello, Luigi Carratino, Alessandro Lazaric, Michal Valko, and Lorenzo Rosasco. Gaussian process optimization with adaptive sketching: Scalable and no regret. In Conference on Learning Theory, pages 533–557. PMLR, 2019. [11] Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In2015 IEEE Symposium on Security and Privacy, page 463–480, May 2015. doi: 10.1109/SP.2015.35. [12] Gert Cauwenberghs and Tomaso Poggio. Incremental and decremental support vector machine learning. In Advances in Neural Information Processing Systems, volume 13. MIT Press, 2001. 11[13] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, learning, and games. Cambridge university press, 2006. [14] Olivier Chapelle. Training a support vector machine in the primal. Neural Computation, 19(5): 1155–1178, May 2007. ISSN 0899-7667. doi: 10.1162/neco.2007.19.5.1155. [15] Michael B Cohen, Cameron Musco, and Christopher Musco. Ridge leverage scores for low-rank approximation. arXiv preprint arXiv:1511.07263, 6, 2015. [16] R Dennis Cook. Detection of inﬂuential observation in linear regression. Technometrics, 19(1): 15–18, 1977. [17] Lehel Csató and Manfred Opper. Sparse on-line Gaussian processes. Neural computation, 14 (3):641–668, 2002. [18] Dennis DeCoste and Kiri Wagstaff. Alpha seeding for support vector machines. In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’00, page 345–349. ACM Press, 2000. ISBN 978-1-58113-233-5. doi: 10.1145/347090. 347165. [19] Tom Diethe, Tom Borchert, Eno Thereska, Borja Balle, and Neil Lawrence. Continual learning in practice. arXiv preprint arXiv:1903.05202, 2019. [20] Hua Duan, Hua Li, Guoping He, and Qingtian Zeng. Decremental learning algorithms for nonlinear langrangian and least squares support vector machines. In Proceedings of the First International Symposium on Optimization and Systems Biology (OSB’07) , pages 358–366. Citeseer, 2007. [21] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9304–9312, 2020. [22] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certiﬁed data removal from machine learning models. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3832–3842. PMLR, 13–18 Jul 2020. [23] Ralf Herbrich, Neil Lawrence, and Matthias Seeger. Fast sparse gaussian process methods: The informative vector machine. In Advances in Neural Information Processing Systems, volume 15. MIT Press, 2003. [24] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. [25] Masayuki Karasuyama and Ichiro Takeuchi. Multiple incremental decremental learning of support vector machines. IEEE Transactions on Neural Networks, 21(7):1048–1059, Jul 2010. ISSN 1045-9227, 1941-0093. doi: 10.1109/TNN.2010.2048039. [26] Mohammad Emtiyaz Khan. Decoupled variational Gaussian inference. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. [27] Mohammad Emtiyaz Khan and Haavard Rue. Learning-algorithms from Bayesian principles. 2020. https://emtiyaz.github.io/papers/learning_from_bayes.pdf. [28] Mohammad Emtiyaz Khan, Aleksandr Aravkin, Michael Friedlander, and Matthias Seeger. Fast dual variational inference for non-conjugate latent Gaussian models. In Proceedings of the 30th International Conference on Machine Learning, volume 28, pages 951–959, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. [29] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna- tional Conference on Learning Representations, 2015. [30] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. [31] Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller, Kristin P Bennett, and Emilio Parrado-Hernández. Incremental support vector learning: Analysis, implementation and applications. Journal of machine learning research, 7(9), 2006. 12[32] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http: //yann.lecun.com/exdb/mnist/. [33] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [34] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. [35] Zhizheng Liang and YouFu Li. Incremental support vector machine learning in the primal and applications. Neurocomputing, 72(10):2249–2258, Jun 2009. ISSN 0925-2312. doi: 10.1016/j.neucom.2009.01.001. [36] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learn- ing. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 6470–6479, Red Hook, NY , USA, 2017. Curran Associates Inc. ISBN 9781510860964. [37] David Lopez-Paz, L. Bottou, B. Schölkopf, and V . Vapnik. Unifying distillation and privileged information. arXiv preprint arXiv:1511.03643, 2016. [38] James Martens. New insights and perspectives on the natural gradient method. Journal of Machine Learning Research, 21(146):1–76, 2020. [39] Eric Nalisnick, Jonathan Gordon, and Jose Miguel Hernandez-Lobato. Predictive complexity priors. In Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130, pages 694–702. PMLR, 13–15 Apr 2021. [40] Cuong V . Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learning. In International Conference on Learning Representations, 2018. [41] Quoc Phong Nguyen, Bryan Kian Hsiang Low, and Patrick Jaillet. Variational bayesian unlearning. In Advances in Neural Information Processing Systems, volume 33, pages 16025– 16036. Curran Associates, Inc., 2020. [42] Frank Nielsen. On a variational deﬁnition for the jensen-shannon symmetrization of distances based on the information radius. Entropy, 23(4):464, 2021. [43] M. Opper and C. Archambeau. The variational Gaussian approximation revisited. Neural Computation, 21(3):786–792, 2009. [44] Andrei Paleyes, Raoul-Gabriel Urma, and Neil D. Lawrence. Challenges in deploying machine learning: a survey of case studies. arXiv:2011.09926 [cs], Jan 2021. arXiv: 2011.09926. [45] Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard Turner, and Mohammad Emtiyaz Khan. Continual deep learning by functional regularisation of memorable past. In Advances in Neural Information Processing Systems, volume 33, pages 4453–4464. Curran Associates, Inc., 2020. [46] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. iCaRL: Incremental classiﬁer and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001–2010, 2017. [47] Hippolyt Ritter, Aleksandar Botev, and David Barber. Online structured laplace approximations for overcoming catastrophic forgetting. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [48] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7 (2):123–146, 1995. [49] Enrique Romero, Ignacio Barrio, and Lluís Belanche. Incremental and decremental learning for linear support vector machines. In International Conference on Artiﬁcial Neural Networks, pages 209–218. Springer, 2007. [50] Sebastian Ruder, Parsa Ghaffari, and John G Breslin. Knowledge adaptation: Teaching to adapt. arXiv preprint arXiv:1702.02052, 2017. [51] Nikolaos Saraﬁanos, Michalis Vrigkas, and Ioannis A Kakadiaris. Adaptive SVM+: Learning with privileged information for domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 2637–2644, 2017. 13[52] Simo Särkkä, Arno Solin, and Jouni Hartikainen. Spatiotemporal learning via inﬁnite- dimensional Bayesian ﬁltering and smoothing: A look at Gaussian process regression through Kalman ﬁltering. IEEE Signal Processing Magazine, 30(4):51–61, 2013. [53] Sebastian Schelter. amnesia–towards machine learning models that can forget user data very fast. In 1st International Workshop on Applied AI for Database Systems and Applications (AIDB19), 2019. [54] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. In International Conference on Machine Learning, pages 4528–4537. PMLR, 2018. [55] Daniel Simpson, Håvard Rue, Andrea Riebler, Thiago G. Martins, and Sigrunn H. Sørbye. Penalising model component complexity: A principled, practical approach to constructing priors. Statistical Science, 32(1):1–28, 2017. ISSN 08834237, 21688745. [56] Arno Solin, James Hensman, and Richard E Turner. Inﬁnite-horizon Gaussian processes. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [57] Michalis K. Titsias, Jonathan Schwarz, Alexander G. de G. Matthews, Razvan Pascanu, and Yee Whye Teh. Functional regularisation for continual learning with gaussian processes. In International Conference on Machine Learning, 2020. [58] Cheng-Hao Tsai, Chieh-Yen Lin, and Chih-Jen Lin. Incremental and decremental training for linear classiﬁcation. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, page 343–352. ACM, Aug 2014. [59] Amund Tveit, Magnus Lie Hetland, and Håavard Engum. Incremental and decremental proximal support vector classiﬁcation using decay coefﬁcients. In International Conference on Data Warehousing and Knowledge Discovery, pages 422–429. Springer, 2003. [60] Vladimir Vapnik and Rauf Izmailov. Learning using privileged information: similarity control and knowledge transfer. Journal of Machine Learning Research, 16(1):2023–2049, 2015. [61] Zeyi Wen, Bin Li, Ramamohanarao Kotagiri, Jian Chen, Yawen Chen, and Rui Zhang. Improv- ing efﬁciency of SVM k-fold cross-validation by alpha seeding. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 31, 2017. [62] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International Conference on Machine Learning , pages 3987–3995. PMLR, 2017. 14A Optimal K-priors for GLMs We present theoretical results to show that K-priors with limited memory can achieve low gradient- reconstruction error. We will discuss the optimal K-prior which can theoretically achieve perfect reconstruction error. Note that the prior is difﬁcult to realize in practice since it requires all past training-data inputs X. Our goal here is to establish a theoretical limit, not to give practical choices. Our key idea is to choose a few input locations that provide a good representation of the training-data inputs X. We will make use of the singular-value decomposition (SVD) of the feature matrix, Φ⊤= U∗ 1:KS∗ 1:K(V∗ 1:K)⊤ where K ≤min(N,P ) is the rank, U∗ 1:K is P ×K matrix of left-singular vectors u∗ i, V∗ 1:K is N ×Kmatrix of right-singular vectors v∗ i, and S∗ 1:K is a diagonal matrix with singular values si as the i’th diagonal entry. We deﬁne M∗= {u∗ 1,u∗ 2,..., u∗ K}, and the following K-prior, Kopt(w; w∗,M∗) = K∑ j=1 β∗ jℓ ( h(fw∗(u∗ j)),h(fw(u∗ j)) ) + 1 2 δ∥w −w∗∥2. (19) Here, each functional divergence is weighted by β∗ j which refers to the elements of the following, β∗= D−1 u S∗ 1:KV⊤ 1:Kdx where dx is an N-length vector with entries h(fi w) −h(fi w∗) for all i∈X, while Du is a K×K diagonal matrix with diagonal entries h(fw(uj)) −h(fw∗(uj)) for all j = 1,2,...,K . The above deﬁnition departs slightly from the original deﬁnition where only a single τ is used. The weights β∗ j depend on X, so it is difﬁcult to compute them in practice when the memory is limited. However, it might be possible to estimate them for some problems. Nevertheless, with β∗ j, the above K-prior can be achieve perfect reconstruction. The proof is very similar to the one given in Equations 9 and 10, and is shown below, ∇Kopt(w; w∗,M∗) = K∑ j=1 β∗ ju∗ i [h(fw(ui)) −h(fw∗(ui))] + δ(w −w∗), = U∗ 1:KDuβ∗+ δ(w −w∗), = U∗ 1:KS∗ 1:KV⊤ 1:Kdx + δ(w −w∗), = Φ⊤dx + δ(w −w∗), = ∑ i∈X φi [ h(fi w) −h(fi w∗) ] + δ(w −w∗), = ∑ i∈D φi [ h(fi w) −yi ] + δw    =∇¯ℓ(w). − \u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018∑ i∈D φi [ h(fi w∗) −yi ] −δw∗    =0. . The ﬁrst line is simply the gradient, which is then rearranged in the matrix-vector product in the second line. The third line uses the deﬁnition of β∗, and the fourth line uses the SVD of Φ. In the ﬁfth line we expand it to show that it is the same as Eq. 9, and the rest follows as before. Due to their perfect gradient-reconstruction property, we call the prior in Eq. 19 the optimal prior. When only top-M singular vectors are chosen, the gradient reconstruction error grows according to the leftover singular values. We show this below where we have chosenM∗ M = {u∗ 1,u∗ 2,..., u∗ M} as the set of top-M singular vectors, eopt(w; w,M∗ M) = ∇¯ℓ(w) −∇Kopt(w; w∗,M∗ M) = ∇¯ℓ(w) −∇Kopt(w; w∗,M∗) + ∇Kopt(w; w∗,M∗) −∇Kopt(w; w∗,M∗ M) = ∇Kopt(w; w∗,M∗) −∇Kopt(w; w∗,M∗ M) = M∑ j=M+1 β∗ ju∗ i [h(fw(ui)) −h(fw∗(ui))] , = U∗ M+1:KS∗ M+1:KV⊤ M+1:Kdx. 15The ﬁrst line is simply the deﬁnition of the error, and in the second line we add and subtract the optimal K-prior with memory M∗. The next few lines use the deﬁnition of the optimal K-prior and rearrange terms. Using the above expression, we ﬁnd the following error, ∥eopt(w; w,M∗ M)∥= √ ΣK j=M+1s2 j(ax j)2 where ax j is the j’th entry of a vectora = V⊤ 1:Kdx. The error depends on the leftover singular values. The error is likely to be the optimal error achievable by any memory of size M, and establishes a theoretical bound on the best possible performance achievable by any K-prior. B Additional Examples of Adaptation with K-priors Here, we brieﬂy discuss the K-prior regularization for the other adaptation tasks. B.1 The Change Regularizer task For Change Regularizer task, we need to slightly modify the K-prior of Eq. 8. We replace the weight-space divergence in Eq. 8 with a Bregman divergence deﬁned using two different regularizers (see Proposition 5 in Nielsen [42]), BGR(w∥w∗) = G(w) + R∗(η∗) −w⊤η∗, (20) where η∗ = ∇R(w∗) is the dual parameter and R∗is the convex-conjugate of R. This is very similar to the standard Bregman divergence but uses two different (convex) generating functions. To get an intuition, consider hyperparameter-tuning for the L2 regularizer R(w) = 1 2 δ∥w∥2, where our new regularizer G(w) = 1 2 γ∥w∥2 uses a hyperparameter γ ̸= δ. Since the conjugate R∗(η) = 1 2 ∥η∥2/δand η∗= ∇R(w∗) = δw∗, we get BGR(w∥w∗) = 1 2 (γ∥w∥2 + δ∥w∗∥2 −2δw⊤w∗). When γ = δ, then this reduces to the divergence used in Eq. 8, but otherwise it enables us to reconstruct the gradient of the past objective but with the new regularizer. We deﬁne the following K-prior where the weight-divergence is replaced by Eq. 20, and use it to obtain ˆwG, K(w; w∗,M) = ∑ i∈M ℓ ( h(fi w∗),h(fi w) ) + BGR(w∥w∗), ˆwG= arg min w∈W K(w; w∗,M) (21) The following theorem states the recovery of the exact solution. Theorem 2. For M= Xand strictly-convex regularizers, we havewG= ˆwG. The derivation is very similar to Eq. 10, where δ(w −w∗) is replaced by ∇G(w) −∇R(w∗). B.2 The Change Model Class task We discuss the ‘Change Model Class’ task through an example. Suppose we want to remove the last feature from φi so that w ∈RP is replaced by a smaller weight-vector θ ∈RP−1. Assuming no change in the hyperparameter, we can simply use a weighting matrix to ‘kill’ the last element ofw∗. We deﬁne the matrix A = IP−1×P whose last column is 0 and the rest is the identity matrix of size P −1. With this, we can use the following training procedure over a smaller space ¯w, K(θ) = ∑ i∈M ℓ ( h(fi w∗),h(fθ(xi)) ) + BR(θ∥Aw∗), ˆθ∗= arg min θ∈Θ K(θ) (22) If the hyperparameters or regularizer are different for the new problem, then the Bregman divergence shown in Eq. 20 can be used, with an appropriate weighting matrix. Model compression is a speciﬁc instance of the ‘Change Model Class’ task, where the architecture is entirely changed. For neural networks, this also changes the meaning of the weights and the 16regularization term may not make sense. In such cases, we can simply use the functional-divergence term in K-priors, K(θ) = ∑ i∈M ℓ ( h(fi w∗),h(fθ(xi)) ) , ˆθ∗= arg min θ∈Θ K(θ) (23) This is equivalent to knowledge distillation (KD) in Eq. 15 with λ= 0 and T = 1. Since KD performs well in practice, it is possible to use a similar strategy to boost K-prior, e.g., we can deﬁne the following, ˆθ∗= arg min θ∈Θ λ ∑ i∈M ℓ(yi,h(fi θ)) + (1−λ)K(θ) (24) We could even use limited-memory in the ﬁrst term. The term λlets us trade-off teacher predictions with the actual data. We can construct K-priors to change multiple things at the same time, for example, changing the regularizer, the model class, and adding/removing data. A K-prior for such situations can be constructed using the same principles we have detailed. C Derivation of the K-priors Gradients for Deep Learning The gradient is obtained similarly to (10) where we add and subtract yi in the ﬁrst term in the ﬁrst line below, ∇K(w) = ∑ i∈X ∇fi w [ h(fi w) −h(fi w∗) ] + δ(w −w∗), = ∑ i∈D ∇fi w [ h(fi w) −yi ] + δw    =∇¯ℓ(w) − ∑ i∈D ∇fi w[h(fi w∗) −yi] −δw∗    ̸=∇¯ℓ(w∗),because ∇fiw̸=∇fiw∗ , The second term is not zero because ∇fi w ̸= ∇fi w∗ to get ∇¯ℓ(w∗) in the second term. The gradient of the KD objective can be obtained in a similar fashion, where we add and subtract yi in the second term in the ﬁrst line to get the second line, ∇ℓKD(w) = λ ∑ i∈D ∇fi w [ h(fi w) −yi ] + (1 −λ) ∑ i∈D ∇fi w [ h(fi w) −h(fi w∗) ] , = ∑ i∈D ∇fi w [ h(fi w) −yi ] −(1 −λ) ∑ i∈D ∇fi w [ h(fi w∗) −yi ] . D Proof for Adaptation for Bayesian Learning with K-priors To prove the equivalence of (18) to the full batch variational inference problem with a Gaussian q(w) = N(w|µ,Σ), we can use the following ﬁxed point of the variational objective (see Section 3 in [27] for the expression), 0 = ∇µEq[L(w)] |µ=µ+,Σ=Σ+ = Eq[∇wL(w)]|µ=µ+,Σ=Σ+ , (25) Σ−1 + = ∇ΣEq[L(w)]|µ=µ+,Σ=Σ+ = Eq[∇2 wL(w)] ⏐⏐ µ=µ+,Σ=Σ+ , (26) where L(w) = [ℓj(w) + ¯ℓ(w) + R(w)], µ+ and Σ+ are the mean and covariance of the optimal q+(w) for the ‘Add Data’ task. For GLMs, both the gradient and Hessian of¯ℓ(w) is equal to those of K(w) deﬁned in (8), which proves the equivalence. For equivalence to GPs, we ﬁrst note that, similarly to the representer theorem, the mean and covariance of q+(w) can be expressed in terms of the two N-length vectors α and λ [43, 26, 28], µ+ = Φ⊤ +α, Σ+ = (Φ⊤ +ΛΦ+ + δI)−1, 17where Λ is a diagonal matrix with λ as the diagonal. Using this, we can deﬁne a marginal q(fi) = N(fi|mi,vi), where fi = φ⊤ i w, with the mean and variance deﬁned as follows, mi = φ⊤ i µ+ = k⊤ i,+α, v i = φ⊤ i Σ+φi = kii,+ −k⊤ i,+ ( Λ−1 + δK+ )−1 ki,+, where kii,+ = φ⊤ i φi. Using these, we can now rewrite the optimality conditions in the function-space to show equivalence to GPs. We show this for the ﬁrst optimality condition (25), ∇µEq[L(w)]|µ=µ+,Σ=Σ+ = ∑ i∈D∪j EN(ϵi|0,1) [ ∇fℓ(yi,h(f))|f=φ⊤ i µ++(φ⊤ i Σ+φi) 1/2 ϵi ] φi + δµ+ Multiplying it by Φ+, we can rewrite the gradient in the function space, 0 = ∑ i∈D∪j EN(ϵi|0,1) [ ∇fℓ(yi,h(f))|f=mi+v1/2 i ϵi ] ki,+ + δK+α = ∑ i∈D∪j ∇miEq(fi) [ℓ(yi,h(fi))] ki,+ + δK+α where m is the vector of mi. Setting this to 0, gives us the ﬁrst-order condition for a GP with respect to the mean, e.g., see Equation 3.6 and 4.1 in Chapelle [14]. It is easy to check this for GP regression, where ℓ(yi,h(fi)) = (yi −fi)2, in which case, the equation becomes, 0 = ∑ i∈D∪j (mi −yi)ki,+ + δK+α ⇒α = (K+ + δI)−1y, which is the quantity which gives us the posterior mean. A similar condition condition for the covariance can be written as well. Clearly, when we use a limited memory, some of the data examples are removed and we get a sparse approximation similarly to approaches such as informative vector machine which uses a subset of data to build a sparse approximation [ 23]. Better sparse approximations can be built by carefully designing the functional divergence term. For example, we can choose the matrixB in the divergence, Df(f(w)∥f(w∗)) = 1 2 d⊤ mBdm ⇒ ∇Df(f(w)∥f(w∗)) = ∇f(w)⊤Bdm This type of divergence is used in Pan et al. [45], where the matrix B is set to correlate the examples in Mwith the examples in D. Design of such divergence function is a topic which requires more investigation in the future. E Further experimental results We provide more details on all our experiments, such as hyperparameters and more results. E.1 Adaptation tasks Logistic Regression on the ‘UCI Adult’ dataset.In Fig. 2(a) we show results for the 4 adaptation tasks on the UCI Adult dataset, and provide experimental details in Sec. 5. Note that for all but the ‘Change Model Class’ task, we used polynomial degree 1. For all but the ‘Change Regularizer’ task, we use δ= 5. We optimize using LBFGS (default PyTorch implementation) with a learning rate of 0.01 until convergence. Throughout our experiments in the paper, we used the same memorable points for Replay as for K-priors (the points with the highest h′(fi w∗)), and used τ = 1 (from Eq. 6). In Fig. 4 we provide an ablation study for Replay with different strategies: (i) we choose points byh′(fi w∗) and use τ = N/M, (ii) we choose points randomly and use τ = 1, (iii) we choose points randomly and use τ = N/M. Recall that N is the past data size (the size of D) and M is the number of datapoints stored in memory (the size of M). We see that choosing points by h′(fi w∗) and using τ = 1 performs very well, and we therefore choose this for all our experiments. 18Validation acc (%) Add new data Memory size (% of past data) Figure 4: This ﬁgure shows using τ = 1 works well for Replay, both for random selection of memory and choosing memory by sorting h′(fi w∗). We compare different methods for Replay on the Adult ‘Add Data’ task. ‘Random’ means the points in memory are chosen randomly as opposed to choosing the points with highest h′(fi w∗). We also consider using τ = N/M instead of τ = 1. Choosing randomly or by h′(fi w∗) are within standard deviations in this task, so we choose to report memory chosen by h′(fi w∗) in other experiments (this is then consistent with the memory in K-priors). Logistic Regression on the ‘USPS odd vs even’ dataset.For all but the ‘Change Model Class’ task, we used polynomial degree 1. For all but the ‘Change Regularizer’ task, we useδ= 50. We optimize using LBFGS with a learning rate of 0.1 until convergence. Neural Networks on the ‘USPS odd vs even’ dataset.For all but the ‘Change Regularizer’ task, we use δ= 5. We optimize using Adam with a learning rate of 0.005 for 1000 epochs (which is long enough to reach convergence). Neural Networks on the ‘MNIST’ dataset.We show results on 10-way classiﬁcation with MNIST in Fig. 5, which has 60,000 training images across 10 classes (handwritten digits), with each image of size 28 ×28. We use a two hidden-layer MLP with 100 units per layer, and report means and standard deviations across 3 runs. For the ‘Add Data’ task, we start with a random 90% of the dataset and add 10%. For the ‘Change Regularizer’ task, we changeδ= 1 to 5 (we use δ= 1 for all other tasks). For the ‘Change Architecture’ task, we compress to a single hidden layer with 100 hidden units. We optimize using Adam with a learning rate of 0.001 for 250 epochs, using a minibatch size of 512. Validation acc (%) Memory size (% of past data) Add new data Change regularizer Change architecture Memory size (% of past data) Memory size (% of past data) Figure 5: K-priors work well on MNIST (with an MLP), similar to other results on the USPS and UCI Adult datasets. For details on the experiments, see App. E.1. Neural Networks on the ‘CIFAR-10’ dataset.We provide results for CIFAR-10 using 10-way classiﬁcation. CIFAR-10 has 60,000 images (50,000 for training), and each image has 3 channels, each of size 32 ×32. We report mean and standard deviations over 3 runs. We use the CifarNet architecture from Zenke et al. [62].We optimize using Adam with a learning rate of 0.001 for 100 epochs, using a batch size of 128. In Fig. 6 we also provide results on the ‘Change Regularizer’ task, where we changeδ= 1 to 0.5 (we use δ= 1 for all the other tasks). We also provide results on the ‘Change Architecture’ task, where we change from the CifarNet architecture to a LeNet5-style architecture. This smaller architecture has two convolution layers followed by two fully-connected layers: the ﬁrst convolution layer has 6 output channels and kernel size 5, followed by the ReLU activation, followed by a Max Pool layer with kernel size 2 (and stride 2), followed by the second convolution layer with 16 output channels and kernel size 5, followed by the ReLU activation, followed by another Max Pool layer with kernel size 2 (and stride 2), followed by a fully-connected layer with 120 hidden units, followed 19by the last fully-connected layer with 84 hidden units. We also use ReLU activation functions in the fully-connected layers. Validation acc (%) Change regularizer Change architecture Memory size (% of past data) Memory size (% of past data) Figure 6: Results for two adaptation tasks on CIFAR-10 with CNNs. See also Fig. 3(c) for results on the ‘Add Data’ task. K-priors perform well, especially on the ‘Change Regularizer’ task. The ‘Change Architecture’ task is more difﬁcult, but we note that we do not use a temperature. Having a temperature greater than 1 is known to help in similar settings, such as knowledge distillation [24]. For the knowledge distillation task, we used K-priors with a temperature, similar to the temperature commonly used in knowledge distillation [ 24]. We note that there is some disagreement in the literature regarding how the temperature should be applied, with some works using a temperature only on the teacher’s logits (such as in Eq. 15) [37], and other works having a temperature on both the teacher and student’s logits [24]. In our experiments, we use a temperature T on both the student and teacher logits, as written in the ﬁnal term of Eq. 27. We also multiply the ﬁnal term by T2 so that the gradient has the same magnitude as the other data term (as is common in knowledge distillation). ℓKD,expt(w) = λ ∑ i∈D ℓ ( yi,h(fi w) ) + δ∥w∥2 + (1 −λ) T2 ∑ i∈D ℓ ( h(fi w∗/T), h(fi w/T) ) . (27) We used λ = 0.5 in the experiment. We performed a hyperparameter sweep for the temperature (across T = [1,5,10,20]), and used T = 5. For K-priors in this experiment, we optimize for 10 epochs instead of 100 epochs, and use τ = 1. In Fig. 3(c) we also showed initial results using a temperature on the ‘Add Data’ task on CIFAR-10. We used the same temperature from the knowledge distillation experiment (T = 5 and λ= 0.5), but did not perform an additional hyperparameter sweep. We ﬁnd that using a temperature improved results for CNNs, and we expect increased improvements if we perform further hyperparameter tuning. Note that many papers that use knowledge distillation perform more extensive hyperparameter sweeps than we have here. E.2 Weight-priors vs K-priors In Fig. 7 we provide results comparing with weight-priors for all the ‘Add Data’ tasks. We see that for homogeneous data splits (such as UCI Adult, MNIST and CIFAR), weight-priors perform relatively well. For inhomogeneous data splits (USPS with logistic regression and USPS with neural networks), weight-priors perform worse. Validation acc (%) Memory size (% of past data) (a) Adult, logistic regression (b) USPS, NN (c) MNIST, NN (d) CIFAR-10, NN Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Figure 7: Results on the ‘Add Data’ task, with a comparison to weight-priors. (a), (c), (d) For homogeneous data splits, weight-priors can perform relatively well. (b) For inhomogeneous data splits, weight-priors perform worse (see also Fig. 3(b)). 20E.3 K-priors ablation with weight-term In this section we perform an ablation study on the importance of the weight-term 1 2 δ∥w −w∗∥2 in Eq. 8. In Fig. 8 we show results on logistic regression on USPS where we do not havew∗in this term (the update equation is the same as Eq. 8 except the weight-term is1 2 δ∥w∥2 instead of 1 2 δ∥w−w∗∥2). We see that the weight-term is important: including the weight-term always improves performance. Validation acc (%) Add new data Remove old data Change regularizer Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Figure 8: Comparing K-priors with a version of K-priors without the weight-term on USPS logistic regression. We see that the weight-term is important, especially on the ‘Add Data’ task. E.4 K-priors with random initialization In all experiments so far, when we train on a new task, we initialize the parameters at the previous parameters w∗. Note that this is not possible in the “Change architecture” task, where weights were initialized randomly. Our results are independent of initialization strategy: we get the same results whether we use random initialization or initializing at previous values. The only difference is that random initialization can sometimes take longer until convergence (for all methods: Batch, Replay and K-priors). For GLMs, where we always train until convergence and there is a single optimum, it is clear that the exact same solution will always be reached. We now also provide the result for ‘USPS odd vs even’, with random initialization in Fig. 9, for the 3 tasks where we had earlier initialized at previous values (compare with Fig. 1 (right)). We use exactly the same hyperparameters and settings as in Fig. 1 (right), aside from initialization method. Validation acc (%) Memory size (% of past data) Add new data Remove old data Change regularizer Memory size (% of past data) Memory size (% of past data) Figure 9: K-priors obtain the same results when randomly initializing the weights for the ‘Add new data’, ‘Remove old data’ and ‘Change regularizer’ tasks on USPS odd vs even with neural networks. Previous results, including Fig. 1 (right), initialized parameters at previously learnt values. The ‘Change architecture’ task originally used random initialization and so is not repeated here. E.5 K-priors converge cheaply In this section, we show that K-priors with limited memory converge to the ﬁnal solution cheaply, converging in far fewer passes through data than the batch solution. This is because we use a limited memory, and only touch the more important datapoints. Table 1 shows the “number of backprops” until reaching speciﬁc accuracies (90% and 97%) on USPS with a neural network (using the same settings as in Fig. 1 (right)). This is one way of measuring the “time taken”, as backprops through the model are the time-limiting step. For K-priors and Replay, we use 10% of past memory. All methods use random initializations when starting training on a new task. 21We see that K-priors with 10% of past data stored are quicker to converge than Batch, even though both eventually converge to the same accuracy (as seen in Fig. 1 (right)). For example, to reach 97% accuracy for the Change Regularizer task, K-priors only need 54,000 backward passes, while Batch requires 2,700,000 backward passes. We also see that Replay is usually very slow to converge. This is because it does not use the same information as K-priors (as Replay uses hard labels), and therefore requires signiﬁcantly more passes through data to achieve the same accuracy. In addition, Replay with 10% of past data cannot achieve high accuracies (such as 97% accuracy), as seen in Fig. 1 (right). Table 1: Number of backpropagations required to achieve a speciﬁed accuracy on USPS with a neural network (1000s of backprops). K-priors with 10% past memory require much fewer backprops to achieve the same accuracy as Batch, while Replay with 10% memory cannot achieve high accuracies. Accuracy Method Add Remove Change Change achieved new data old data regularizer model class 90% Batch 87 94 94 86 90% Replay (10% memory) 348 108 236 75 90% K-prior (10% memory) 73 53 13 22 97% Batch 1,900 1,800 2,700 3,124 97% Replay (10% memory) – 340 – – 97% K-prior (10% memory) 330 120 54 68 E.6 Further details on Fig. 1 (middle), moons dataset. To create this dataset, we took 500 samples from the moons dataset, and split them into 5 splits of 100 datapoints each, with each split having 50 datapoints from each task. Additionally, the splits were ordered according to the x-axis, meaning the 1st split were the left-most points, and the 5th split had the right-most points. In the provided visualisations, we show transfer from ‘past data’ consisting of the ﬁrst 3 splits (so, 300 datapoints) and the ‘new data’ consisting of the 4th split (a new 100 datapoints). We store 3% of past data as past memory in K-priors, chosen as the points with the highest h′(fi w∗). F Changes in the camera-ready version compared to the submitted version This section lists the major changes we made for the camera-ready version of the paper, incorporating reviewer feedback. • Added a paragraph on the optimal K-prior after Eq. 12, as well as a detailed explanation in App. A. • Updated Fig. 3(d), following a more extensive sweep of hyperparameters. • Added App. E.4, showing K-priors with random initialization give the same results as K-priors that are initialized at the previous model parameters. • Added App. E.5, showing that K-priors with limited memory converge to the ﬁnal solution cheaply, requiring fewer passes through the data than the batch solution. 22",
      "meta_data": {
        "arxiv_id": "2106.08769v2",
        "authors": [
          "Mohammad Emtiyaz Khan",
          "Siddharth Swaroop"
        ],
        "published_date": "2021-06-16T13:27:22Z",
        "pdf_url": "https://arxiv.org/pdf/2106.08769v2.pdf",
        "github_url": "https://github.com/team-approx-bayes/kpriors"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Knowledge-adaptation priors (K-priors) to address the high financial and environmental costs of repeatedly retraining machine learning models. K-priors enable quick and accurate adaptation to incremental changes across a wide variety of tasks and models by combining weight and function-space divergences to reconstruct past gradients. This principle unifies and generalizes many seemingly unrelated existing adaptation strategies in ML. K-priors achieve performance similar to full retraining but require training on only a small subset (a handful) of past data examples, making ML pipelines more dynamic and efficient.",
        "methodology": "K-priors are a class of priors defined as a combination of weight-space and function-space Bregman divergences, `K(w; w∗,M) = Df (f(w)∥f(w∗)) + τDw(w∥w∗)`. The core principle is to construct these divergences to faithfully reconstruct the gradients of the past training objective. For Generalized Linear Models (GLMs) with an L2 regularizer, the K-prior is shown to exactly recover the gradients of the full past training objective. For practical applications with limited memory (`M`), examples are selected based on having the highest `h'(f_i w*)` (referred to as 'memorable past') to minimize gradient-reconstruction error. Adaptation is performed by using K-priors as a regularizer in a new objective, where model predictions (`h(f_i w*)`) act as soft labels. This framework is shown to unify various existing methods like weight-priors, Knowledge Distillation (which K-priors extend by adding a weight-space term and limited memory), SVMs, Bayesian learning, and continual learning.",
        "experimental_setup": "K-priors were empirically evaluated against full retraining ('Batch') and replay methods ('Replay') on four adaptation tasks: 'Add Data', 'Remove Data', 'Change Regularizer', and 'Change Architecture'. For fair comparison, both K-priors and Replay used the same memory selection strategy (points with highest `h'(f_i w*)`). Experiments covered logistic regression on the 'UCI Adult' and 'USPS odd vs even' datasets, and neural networks (MLPs, CifarNet, LeNet5) on 'USPS odd vs even', 'MNIST', and 'CIFAR-10'. Optimizers like L-BFGS and Adam were used. Performance was primarily measured by validation accuracy. Additional experiments included ablations on the weight-term, sensitivity to random initialization (vs. warm-starts), and convergence speed (number of backpropagations). For CIFAR-10, experiments also explored K-priors with a temperature parameter, similar to Knowledge Distillation.",
        "limitations": "The paper notes that extensive hyperparameter tuning, particularly for parameters like `τ`, temperature `T`, and `λ`, is often required to achieve consistent optimal performance across various memory sizes, especially in complex deep learning scenarios. While theoretically an optimal K-prior can achieve perfect gradient reconstruction, its practical realization is challenging. The current memory selection strategy (based on `h'(f_i w*)`) is not necessarily optimal for all cases, and more advanced sketching methods are left for future work. Simple weight-priors are shown to perform poorly on inhomogeneous data splits and lack mechanisms to correct past mistakes, highlighting a challenge that K-priors aim to overcome.",
        "future_research_directions": "Future research directions include further investigation into more optimal memory selection strategies, such as applying sketching methods to choose memorable past examples. More empirical effort is needed to extensively tune various hyperparameters to ensure consistent behavior and improved performance of K-priors across all memory sizes, particularly for larger deep learning problems. The design of specific divergence functions, like the matrix `B` in the functional divergence term, for better sparse approximations requires more investigation. The authors also express a long-term goal of developing new ML systems that learn quickly, flexibly, and incrementally in a continual fashion, with a dedicated Bayesian version of this paper also suggested as a future avenue.",
        "experimental_code": "import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\nfrom torch.nn.utils import parameters_to_vector, vector_to_parameters\n\ndef softmax_hessian(f):\n    s = torch.nn.functional.softmax(f, dim=-1)\n    return s - s*s\n\n# From adamreg.py or lbfgsreg.py\nclass AdamReg(Optimizer): # or LBFGSReg, relevant parts for K-priors are identical\n\n    def __init__(self, model, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, prior_prec=1e-3,\n                 prior_prec_old=0., amsgrad=False):\n        # ... (validation checks for hyperparameters) ...\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, prior_prec=prior_prec,\n                        prior_prec_old=prior_prec_old, amsgrad=amsgrad)\n        super(AdamReg, self).__init__(model.parameters(), defaults)\n\n        # State initialisation (optimizer specific, but K-priors related members are common)\n        # ...\n\n        # Additional for K-priors and Replay\n        self.memory_labels = None # Stores f(w*) or true labels for memory points\n        self.model = model\n        self.previous_weights = None # Stores w*\n        self.prior_prec_old = None # Stores tau, the precision for the weight-space prior\n        self.train_set_size = 0\n\n    # Iteration step for this optimiser\n    def step(self, closure_data, closure_memory=None, adaptation_method=None):\n\n        # ... (initial gradient calculation for current data) ...\n\n        # Loss term over memory points (only if K-priors or Replay)\n        if closure_memory is not None:\n            # Forward pass through memory points to get f(w)\n            preds = closure_memory()\n            self.total_datapoints_this_iter += len(preds)\n\n            # Softmax on output to get predictions f(w)\n            preds_soft = torch.softmax(preds, dim=-1)\n\n            # Calculate the gradient of Df(f(w) || f(w*)) with respect to f(w)\n            # This is (f(w) - f(w*)) using soft labels from memory\n            delta_logits = preds_soft.detach() - self.memory_labels\n\n            # Autograd to get the gradient of Df(f(w) || f(w*)) with respect to w\n            grad_message = torch.autograd.grad(preds, self.model.parameters(), grad_outputs=delta_logits)\n\n            # Convert grad_message into a vector\n            grad_vec = []\n            for i in range(len(grad_message)):\n                grad_vec.append(grad_message[i].data.view(-1))\n            grad_vec = torch.cat(grad_vec, dim=-1)\n\n            # Add function-space divergence gradient to total gradient\n            grad.add_(grad_vec.detach())\n\n            # Weight regularisation: adds the gradient of τDw(w || w*)\n            # The term is -τw*, as part of ∇w K(w; w*,M) = ∇w Df + ∇w τDw\n            if adaptation_method == \"K-priors\" and self.prior_prec_old is not None:\n                grad.add_(self.previous_weights, alpha=-self.prior_prec_old)\n\n        # ... (rest of gradient additions and optimizer update logic) ...\n\n# From utils.py\ndef select_memory_points(dataloader, model, num_points, additional_memory_data=None, use_cuda=False, descending=True):\n\n    memory_points_list = {}\n    points_indices = {}\n\n    # Data\n    data, target = dataloader\n\n    # Choose number of points per class correctly weighted\n    num_points_per_class = [int(num_points/2),int(num_points/2)]\n    if torch.sum(target==0) < num_points_per_class[0]:\n        num_points_per_class[0] = torch.sum(target==0).numpy()\n        num_points_per_class[1] = num_points - num_points_per_class[0]\n    elif torch.sum(target==1) < num_points_per_class[1]:\n        num_points_per_class[1] = torch.sum(target==1).numpy()\n        num_points_per_class[0] = num_points - num_points_per_class[1]\n\n    # Forward pass through all data\n    if use_cuda:\n        data_in = data.cuda()\n    else:\n        data_in = data\n\n    preds = model.forward(data_in)\n\n    # h'(f) (== lambda) on output for selection criteria\n    lamb = softmax_hessian(preds)\n    if use_cuda:\n        lamb = lamb.cpu()\n    lamb = torch.sum(lamb, dim=-1)\n    lamb = lamb.detach()\n\n    for cid in range(2):\n        p_c = data[target == cid]\n        indices_for_points = torch.argwhere(target == cid).cpu().numpy().flatten()\n        if len(p_c) > 0:\n            scores = lamb[target == cid]\n            _, indices = scores.sort(descending=descending)\n            memory_points_list[cid] = p_c[indices[:num_points_per_class[cid]]]\n            points_indices[cid] = indices_for_points[indices[:num_points_per_class[cid]]]\n\n    r_points = []\n    r_labels = []\n    r_indices = []\n    for cid in range(2):\n        r_points.append(memory_points_list[cid])\n        r_labels.append(cid*torch.ones(memory_points_list[cid].shape[0], dtype=torch.long,\n                                   device=memory_points_list[cid].device))\n        r_indices.append(points_indices[cid])\n\n    memory_points = {}\n    memory_points['inputs'] = torch.cat(r_points, dim=0)\n    memory_points['true_labels'] = torch.cat(r_labels, dim=0)\n    if torch.sum(torch.tensor(num_points_per_class)) > 2:\n        memory_points['indices'] = torch.cat([torch.tensor(arr) for arr in r_indices], dim=0).numpy()\n    else:\n        memory_points['indices'] = r_indices # This seems to be a minor inconsistency from original code for single element, for small num_points\n\n    # If there is additional_memory_data, add that to memory_points['inputs']\n    if additional_memory_data is not None:\n        memory_points['inputs'] = torch.cat((memory_points['inputs'], additional_memory_data[0]))\n        memory_points['true_labels'] = torch.cat((memory_points['true_labels'], additional_memory_data[1]))\n\n    # Soft labels f(w*) from the base model for K-priors\n    if use_cuda:\n        memory_points['inputs'] = memory_points['inputs'].cuda()\n    memory_points['soft_labels'] = torch.softmax(model.forward(memory_points['inputs']), dim=-1)\n\n    return memory_points\n\n# From main.py, how memory_points['labels'] is set for K-priors\n# Within the loop for adaptation methods, after selecting memory_points\n# if adaptation_method == \"K-priors\":\n#     memory_points['labels'] = memory_points['soft_labels']\n\n# From main.py, how optimizer's K-prior related attributes are initialized\n# After model/optimiser creation and deepcopying base_model\n# optimiser.previous_weights = base_model.return_parameters() # Sets w*\n# optimiser.prior_prec_old = prior_prec # or prior_prec_old/new based on task, sets tau\n# optimiser.memory_labels = memory_points['labels'] # Passes soft labels to optimizer",
        "experimental_info": "The K-priors method, defined as `K(w; w∗,M) = Df (f(w)∥f(w∗)) + τDw(w∥w∗)`, is implemented for adaptation tasks. The `Df` term represents a function-space Bregman divergence, and `Dw` is a weight-space Bregman divergence.\n\nKey experimental settings and procedures:\n- **Adaptation Methods:** K-priors are evaluated against a 'Replay' baseline.\n- **Regularization Objective:** Adaptation is performed by training a new model where K-priors are used as a regularizer. This involves combining gradients from the current task's data and the K-prior term.\n- **Function-space term (`Df`):** The gradient of this term is computed using `torch.autograd.grad` with `grad_outputs` set to `softmax(f(w)) - f(w*))`, where `f(w*)` (soft labels) are stored in `optimiser.memory_labels`.\n- **Weight-space term (`Dw`):** The gradient of this term is added as `-τw*` to the total gradient, where `w*` is `optimiser.previous_weights` (parameters of the `base_model`) and `τ` is `optimiser.prior_prec_old`.\n- **Memorable Past Selection:** Memory points are selected from the base training data based on having the highest `h'(f_i w*)`, where `h'(f)` is calculated as `softmax_hessian(preds)` (with `preds` from the `base_model`) summed across output dimensions. `softmax_hessian(f)` is defined as `s - s*s` where `s = softmax(f)`.\n- **Soft Labels:** For K-priors, `f(w*)` (soft labels) for the memory points are generated by performing a forward pass of the memory inputs through the `base_model` and applying `softmax` (`memory_points['soft_labels']`). These are then assigned to `optimiser.memory_labels`.\n- **Limited Memory (`M`):** The number of stored memory points is a fraction of the base task training data. The fractions used are `[1., 0.5, 0.2, 0.1, 0.07, 0.05, 0.02]`.\n- **Adaptation Tasks:** Experiments are conducted on four types of adaptation tasks: `add_data` (new data introduced), `remove_data` (some data points removed), `change_regulariser` (regularization strength `τ` changed), and `change_model` (model architecture or input feature transformation changed).\n- **Datasets:** \n    - `adult`: Binary classification using the UCI Adult dataset.\n    - `usps_binary`: Binary classification of USPS digits (odd vs. even).\n- **Models:** \n    - `LinearModel`: Used for `adult` dataset and `usps_binary` with `polynomial_degree=1` or `2` (for `change_model` task).\n    - `MLP`: Used for `usps_binary` with `hidden_sizes=[100]` (base) or `[100, 100]` (`change_model` task).\n- **Optimizers:** `LBFGSReg` is used for `LinearModel`, and `AdamReg` is used for `MLP` architectures.\n- **Hyperparameters (examples from `main.py`):\n    - `learning_rate`: `0.005` (Adult MLP, USPS MLP), `0.1` (USPS Linear).\n    - `num_epochs`: `1000` (Adult, USPS MLP), `300` (USPS Linear).\n    - `prior_prec` (initial `τ`): `5` (Adult), `50` (USPS Linear), `50` (USPS MLP).\n    - `prior_prec_old` (specific `τ` for `change_regulariser` task): `50` (Adult, USPS Linear) to `5`, `5` (USPS MLP) to `10`.\n    - `num_points_to_remove`: `100` for `adult` `remove_data` task.\n- **Number of Runs:** Experiments are repeated `3` times with different random seeds (`seed_init = 43`, `seed_init + random_run`)."
      }
    },
    {
      "title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
      "abstract": "This paper proposes a novel online evaluation protocol for Test Time\nAdaptation (TTA) methods, which penalizes slower methods by providing them with\nfewer samples for adaptation. TTA methods leverage unlabeled data at test time\nto adapt to distribution shifts. Although many effective methods have been\nproposed, their impressive performance usually comes at the cost of\nsignificantly increased computation budgets. Current evaluation protocols\noverlook the effect of this extra computation cost, affecting their real-world\napplicability. To address this issue, we propose a more realistic evaluation\nprotocol for TTA methods, where data is received in an online fashion from a\nconstant-speed data stream, thereby accounting for the method's adaptation\nspeed. We apply our proposed protocol to benchmark several TTA methods on\nmultiple datasets and scenarios. Extensive experiments show that, when\naccounting for inference speed, simple and fast approaches can outperform more\nsophisticated but slower methods. For example, SHOT from 2020, outperforms the\nstate-of-the-art method SAR from 2023 in this setting. Our results reveal the\nimportance of developing practical TTA methods that are both accurate and\nefficient.",
      "full_text": "Evaluation of Test-Time Adaptation Under Computational Time Constraints Motasem Alfarra 1 2 Hani Itani 1 Alejandro Pardo 1 Shyma Alhuwaider 1 Merey Ramazanova 1 Juan C. P´erez 1 Zhipeng Cai 2 Matthias M¨uller 2 Bernard Ghanem 1 Abstract This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) meth- ods, which penalizes slower methods by provid- ing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Although many effec- tive methods have been proposed, their impressive performance usually comes at the cost of signif- icantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA meth- ods, where data is received in an online fashion from a constant-speed data stream, thereby ac- counting for the method’s adaptation speed. We apply our proposed protocol to benchmark sev- eral TTA methods on multiple datasets and sce- narios. Extensive experiments show that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower methods. For example, SHOT from 2020, outperforms the state-of-the-art method SAR from 2023 in this setting. Our results re- veal the importance of developing practical TTA methods that are both accurate and efficient1. 1. Introduction In recent years, Deep Neural Networks (DNNs) have demon- strated remarkable success in various tasks (He et al., 2016) thanks to their ability to learn from large datasets (Deng et al., 2009). However, a significant limitation of DNNs is their poor performance when tested on out-of-distribution 1King Abdullah University of Science and Technol- ogy (KAUST), Thuwal, Saudi Arabia 2Intel Labs, Munich, Germany. Correspondence to: Motasem Alfarra <mo- tasem.alfarra@kaust.edu.sa>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1Code: github/MotasemAlfarra/Online-Test-Time-Adaptation Current Evaluation Realistic Evaluation40 45 50 55 60 65 70 75Error Rate (%)  AdaBN 17  AdaBN 17  SHOT 20  SHOT 20  TENT 21  TENT 21  SAR 23  SAR 23 Figure 1: The trend of average error rate using offline evaluation vs our proposed online evaluation. In the offline setup, TTA methods demonstrate progress across time with a decreasing average error rate, e.g. from 68.5% using AdaBN to 56.2% using SAR. We propose a realistic evaluation protocol that accounts for the adaptation speed of TTA methods. Under this protocol, fast methods ( e.g. AdaBN) are unaffected, while slower (but more recent and sophisticated) methods (e.g. SAR) are penalized. data, which violates the i.i.d. assumption that the training and testing data are from the same distribution (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Kar et al., 2022). Such failure cases are concerning, since distribu- tion shifts are common in real-world applications, e.g., im- age corruptions (Hendrycks & Dietterich, 2019), chang- ing weather conditions (Sakaridis et al., 2021), or security breaches (Goodfellow et al., 2014). Test Time Adaptation (TTA) (Saenko et al., 2010; Sun et al., 2020; Liu et al., 2021) has demonstrated promising results for solving the above problem. TTA leverages the unlabeled data that arrives at test time by adapting the forward pass of pre-trained DNNs according to some proxy task (Liang et al., 2020; Lee et al., 2013). Though recent methods have made significant progress at improving accuracy under dis- tribution shifts (Wang et al., 2020; Niu et al., 2022; Gao et al., 2022), many of them incur high computational over- head. For instance, some methods require self-supervised fine-tuning on the data (Chen et al., 2022), while others perform a diffusion process per input (Gao et al., 2022). The computational overhead of TTA methods decreases 1 arXiv:2304.04795v2  [cs.LG]  23 May 2024Evaluation of Test-Time Adaptation Under Computational Time Constraints their inference speed, which is a critical property in many real-world applications that require the TTA method to pro- duce predictions at the speed of the stream itself. This property, however, is overlooked in the current evaluation protocols for TTA methods. In particular, these protocols assume a setting, which neglects how events constantly un- fold regardless of the model’s speed, causing the model to miss incoming samples when it is busy processing previous ones. For TTA methods that adapt using test data, missing samples has a direct effect on the method’s accuracy, as it will have fewer samples for adaptation. That is, the slower the TTA method, the fewer samples it can leverage for adapt- ing to the distribution shift. Thus, the current protocol for evaluating TTA methods is not suitable for assessing their efficacy in real-world deployment. In this work, we propose a novel realistic evaluation proto- col that factors in inference speed to assess the real-world applicability of TTA methods. Our evaluation protocol is in- spired by Online Learning (Cai et al., 2021; Shalev-Shwartz et al., 2012) and mimics real-world scenarios by exposing all TTA methods to a constant-speed stream of data. In this setting, the performance of slow TTA methods is in- trinsically penalized, as the time spent adapting to a sample may lead to dropped samples that could have been useful for adaptation. Specifically, our protocol dictates that if a method gslow is k times slower than the stream, then it may only use every kth sample for adaptation. In contrast, a method gfast that is as fast as the stream is allowed to adapt to every sample. Figure 1 shows the effect of evaluating several methods under our proposed protocol, where slower methods (e.g., SAR (Niu14 et al., 2023)) are penalized and faster but simpler methods become better alternatives (e.g., SHOT (Liang et al., 2020) and AdaBN (Li et al., 2016)). We apply our proposed evaluation protocol to benchmark several TTA methods on multiple datasets, and provide a fair assessment of their performance subject to the realistic consequences of slower inference speeds. Our experimental results highlight the importance of developing TTA methods that adapt to distribution shifts with minimal impact on inference speed. Our contributions are two-fold: 1. We propose a realistic evaluation protocol for TTA methods that penalizes slower methods by providing them with fewer samples for adaptation. Our approach is effective at assessing TTA methods’ efficacy in sce- narios where data arrives as a constant-speed stream. 2. Following our proposed protocol, we provide a com- prehensive experimental analysis of 15 TTA methods evaluated on 3 large-scale datasets under 3 different evaluation scenarios. These scenarios consider adap- tation to a single domain and continual adaptation to several domains. Our analysis shows that, when in- ference speed is accounted for, simple (but faster) ap- proaches can benefit from adapting to more data, and thus outperform more sophisticated (but slower) meth- ods. Figure 1 demonstrates this for four TTA methods. We hope our evaluation scheme inspires future TTA methods to consider inference speed as a critical di- mension that affects their real-world performance. 2. Related Work Test Time Adaptation. The Test Time Adaptation (TTA) setup relaxes the “i.i.d” assumption between the training and testing distributions (Sun et al., 2020; Boudiaf et al., 2022). This relaxation is usually attained through a lifelong learning scheme on all received unlabeled data (Chen et al., 2022; Gong et al.). Earlier approaches such as TTT (Sun et al., 2020) and TTT++ (Liu et al., 2021), among others (Torralba & Efros, 2011; Tzeng et al., 2017), include a self-supervised loss (Gidaris et al., 2018) during training, which can then provide an error signal during adaptation. Despite their effectiveness, such approaches assume having control over how the model is trained. Fully Test Time Adaptation. Fully TTA methods are a subtype of TTA method that adapts at test time by modify- ing the model’s parameters (Liang et al., 2020; Lee et al., 2013; Mirza et al., 2022b; Mancini et al., 2018; Kojima et al., 2022) or its input (Gao et al., 2022) by using the incoming unlabeled data. Fully TTA methods are practi- cal, as they avoid assumptions on the training phase of a given model (Wang et al., 2020; Gao et al., 2022; Iwasawa & Matsuo, 2021). The first of these approaches adjusts the statistics of the Batch Normalization (BN) layers (Mirza et al., 2022a; Schneider et al., 2020; Li et al., 2016). For example, BN-adaptation (Schneider et al., 2020) leverages the statistics of the source data as a prior and infers the statis- tics for every received sample. On the other hand, AdaBN (Li et al., 2016) discards the statistics of the source domain and uses the statistics computed on the target domain. In line with light TTA methods, LAME (Boudiaf et al., 2022) proposes to only adapt the model’s output by finding the latent assignments that optimize a manifold-regularized like- lihood of the data. In this work, we found that such efficient methods preserve their accuracy under our proposed eval- uation. While fully TTA methods have been studied in the context of adversarial domain shifts (Alfarra et al., 2022; Croce et al., 2022; P´erez et al., 2021), in this work we focus on the context of natural shifts such as realistic image cor- ruptions (Hendrycks & Dietterich, 2019; Kar et al., 2022). Another line of work aims at adapting to distribution shifts by minimizing entropy. For instance, SHOT (Liang et al., 2020) adapts the feature extractor to minimize the entropy of individual predictions; while maximizing the entropy of the predicted classes. TENT (Wang et al., 2020) updates the learnable parameters of the BN layers to minimize the 2Evaluation of Test-Time Adaptation Under Computational Time Constraints Adapted SampleNon-AdaptedSampleTTA method Current evaluation . . . . . . Realistic evaluation . . . . . . Model Figure 2: Inference under the current and realistic evaluation protocols. The current evaluation setting (left) assumes that the incoming batches of stream S can wait until the adaptation process of a TTA method g finishes. This assumption is untenable in a real-time deployment scenario. Our proposed realistic evaluation (right) simulates a more realistic scenario where S reveals data at a constant speed. In this setup, slower TTA methods will adapt to a smaller portion of the stream. The remaining part of the stream will be predicted without adaptation by employing the most recent adapted model. We refer to the most recent adapted model as fθt+1 , with t denoting the time when the last sample was adapted to by g. When g is still adapting to a sample, the incoming sample is fed to fθt+1 to produce predictions. entropy of predictions. EATA (Niu et al., 2022) combines TENT with an active selection of reliable and non-redundant samples from the target domain and an anti-forgetting loss (Kirkpatrick et al., 2017). Further, SAR (Niu14 et al., 2023) equips TENT with an active sampling scheme that filters samples with noisy gradients. Other works use data-augmentation at test time (Ashukha et al., 2020). For example, MEMO (Zhang et al., 2021) adapts model parameters to minimize the entropy over a sample and multiple augmentations of it. CoTTA (Wang et al., 2022) uses augmentations to generate reliable pseudo- labels and then peform distillation. Finally, DDA (Gao et al., 2022) proposes to leverage a diffusion model (Ho et al., 2020) to restore corrupted inputs back to the source data distribution. These methods require multiple forward passes through the network or a diffusion model, leading to slower inference speeds. 3. Methodology In this section, we present our proposed Realistic TTA evalu- ation protocol. We first describe the current TTA evaluation protocol and its limitations Then, we introduce our Realistic TTA evaluation protocol, which addresses the shortcomings of the offline protocol. 3.1. Current Protocol TTA considers the practical setup, in which trained models are deployed in a target domain that exhibits distribution shifts to which they must adapt. Let fθ : X → Ybe a clas- sifier, parameterized by θ, that predicts the label y ∈ Yfor a given input x ∈ X. Before test time, fθ is assumed to have been trained on the dataset Dtrain ⊂ X × Y. At test time, i.e. when executing TTA,fθ is presented with a stream of data S, sampled from X, with potentially multiple distribution shifts w.r.t. Dtrain. Under this setup, a TTA method is a function g(θ, x) that sequentially adapts the model’s param- eters θ and/or the input x to enhance the performance under distributions shifts. Currently, TTA methods are evaluated in an offline setting. Formally, the Current TTA evaluation protocol simulates the interaction between the stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: Curr.1 S reveals a sample xt. Curr.2 g adapts xt to ˆxt, θt to ˆθt, generates prediction ˆyt, and updates parameters θt+1 = αθt + (1 − α)ˆθt.2 Note that all existing TTA methods can be modeled using this framework. For example, TENT (Wang et al., 2020) adapts network parameters to minimize entropy with α = 0, while leaving inputs unchanged, i.e. ˆxt = xt and θt+1 = ˆθt. DDA (Gao et al., 2022) adapts inputs via a diffusion process while preserving network parameters with α = 1, i.e. ˆxt = ˆxt and θt+1 = θt. CoTTA (Wang et al., 2022) applies knowledge distillation, and updates network parameters with an exponential moving average, i.e. setting 0 < α <1. Shortcomings of the Current TTA protocol.In the current protocol, the performance of a TTA method g is measured by comparing the ground truth labels yt with the predic- tions after adaptation ˆyt. An evaluation based only on this measure implicitly assumes that the stream is not constant 2Note that some methods abstain from adapting either xt or θt. 3Evaluation of Test-Time Adaptation Under Computational Time Constraints speed, but rather waits for g to adapt to xt (Curr.2) before revealing the next batch xt+1 (Curr.1). Figure 2 provides an illustration of this situation. This assumption results in the offline protocol favoring slower TTA methods, as the method’s performance is agnostic to its inference speed. However, in practical applications where the test data ar- rives at a constant speed, the offline protocol is not suitable for assessing a method’s performance. Next, we propose a remedy for this shortcoming. 3.2. Realistic Online Evaluation Protocol We propose a realistic evaluation of TTA methods that explicitly considers the relation between the speed of the method and the speed at which the stream reveals new data. This setup is more realistic, as it intrinsically penalizes the performance of slower TTA methods: long times spent in adaptation result in fewer samples to adapt to. A crucial aspect of our realistic TTA protocol is accounting for the implications of simulating a constant speed data stream S. For instance, consider a stream S that reveals data at a constant rate r samples per second. If a method gfast adapts to samples at speed r, then gfast will be able to adapt to every sample. On the other hand, if gslow adapts to samples at a speed r/2, then gslow will skip every other sample. We formalize the notion of the relation between the speed of the stream and the speed of a method g as the “relative adaptation speed of g”. This quantity, denoted by C(g) ∈ N, is simply the integer ratio of the speed of S to the speed of g. For instance, in the previous example, C(gfast) = 1, meaning gfast adjusts as fast as S reveals data, while C(gslow) = 2 , indicating S reveals its second batch while gslow is still adapting to the first one. Without loss of generality, we assume that fθ runs in real- time, i.e. that its speed is equal to r, and thus C(fθ) = 1 . This assumption allows us to suppose that the samples that are not processed by g can be processed by fθ. Under this setup, we define our realistic protocol by introducing the relative adaptation speed C(g) into the offline protocol. In particular, we simulate g’s availability by conditionally performing the adaptation step (Curr.2), depending on C(g). In this manner,g is only permitted to adapt when its previous adaptation step has finished. Formally, the realistic TTA evaluation protocol simulates the interaction between the constant speed stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: RTTA 1 S reveals a sample xt. RTTA 2 If (t mod C(g)) = 0, then g adapts xt to ˆxt, θt to ˆθt, generates a prediction ˆyt, and updates pa- rameters via θt+1 ← αθt + (1 − α)ˆθt. Otherwise, fθt generates a prediction ˆyt. Table 1: Average C(g(xt)). We report the average relative adaptation speed C(g) for 5 TTA methods. The higher C(g) is, the smaller the portion of data to which g adapts is. Method AdaBN TENT TTAC-NQ MEMO DDA C(g) 1 3 12 54 810 Here, “mod” represents the modulo operation. The above protocol assesses the performance of TTA methods by fac- toring in their speed. As such, faster methods are granted more adaptation steps and, conversely, slower methods are granted fewer (see Figure 2). Note that explicitly modeling the relative adaptation speeds allows us to evaluate TTA methods under different adaptation speeds by setting C(g) to arbitrary values. For instance, note that our realistic proto- col recovers the original offline protocol by settingC(g) = 1 for all methods. Next, we explain the calculation of C(g) for our realistic protocol. Online computation of C(g). In practice, estimating the relative adaptation speed C(g) can be a noisy process. The noise in this estimation essentially comes from two factors: hardware and input dependence. Hardware-induced noise applies to all methods, while input dependence applies to methods like ETA (Niu et al., 2022) which, upon receiving an input, may optionally abstain from adapting to it. This noise means that C(g) potentially varies across iterations. Our protocol accounts for this variability by conducting an online computation of C(g) on each revealed input. That is, instead of using a fixed value of C(g) at each itera- tion t, our protocol rather uses C (g(xt)). Formally, if we let R (g(x)) denote the speed at which g processes x, then the relative adaptation speed of g at x is defined as C (g(xt)) = ⌈r/R(g(x))⌉, where the ceiling function ac- counts for the stream’s discrete-time nature. Note that since we assumed C(fθ) = 1, then R (fθ(x)) = r. We report the empirical behavior of this online computation of C (g(xt)) for various TTA methods in Table 1, and leave the rest of the methods and the computation details to the Appendix. Next, we leverage our Realistic TTA protocol to conduct a comprehensive empirical study of several TTA methods. 4. Experiments We follow prior art (Wang et al., 2020; Niu14 et al., 2023; Gao et al., 2022) and focus on the task of image classifica- tion. In all our experiments, we assume that fθ is a ResNet- 50-BN3 (He et al., 2016) trained on ImageNet (Deng et al., 2009) (pretrained weights obtained from torchvision). We further assume that the stream S reveals batches of size 3SAR demonstrated the superiority of using batch independent normalization layers under batch size of 1. We leave this ablation to the Appendix along with experiments on other architectures. 4Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 2: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on ImageNet-C benchmark under both the realistic and the current setup. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The current setup is merely the reproduction of every method. The first sub-table corresponds to methods that do not incur any or few extra computations, i.e. C(g) = 1. We show that methods generally perform worse in the realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Noise Blur Weather DigitalMethod Realisticgauss. shot impul.defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 97.8 97.1 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.4 82.0 - AdaBN ✓ 84.9 84.3 84.3 85.0 84.7 73.6 61.1 65.8 66.9 52.1 34.8 83.3 56.1 51.1 60.3 68.5 - LAME ✓ 98.3 97.6 98.6 82.4 90.9 86.1 78.1 84.5 77.5 77.3 41.4 94.8 84.8 80.0 68.9 82.7 - BN ✓ 84.6 83.9 83.8 80.1 80.2 71.7 60.4 65.4 65.2 51.6 34.6 76.3 54.4 49.7 59.2 66.7 - ✗ 73.4 70.2 73.0 76.6 75.5 59.8 53.8 54.2 63.4 44.7 35.5 79.3 46.9 43.2 49.7 59.9SHOT ✓ 73.6 69.0 71.1 74.6 74.8 60.0 52.9 54.1 61.3 44.1 34.1 77.8 46.8 43.1 49.2 59.1 (-0.8) ✗ 71.3 69.4 70.2 72.0 72.9 58.7 50.7 52.8 58.8 42.7 32.7 73.3 45.5 41.5 47.7 57.3TENT ✓ 75.7 78.3 75.2 76.3 77.3 64.6 55.6 57.3 61.4 45.9 33.5 77.1 50.1 44.2 51.4 61.6 (+4.3) ✗ 69.5 69.7 69.0 71.2 71.7 58.1 50.5 52.9 57.9 42.7 32.7 62.9 45.5 41.6 47.8 56.2SAR ✓ 79.4 78.5 78.1 79.9 79.3 67.5 56.1 60.5 63.1 47.4 34.0 75.3 51.7 46.6 53.8 63.4 (+7.2) ✗ 78.4 77.8 77.2 80.5 79.1 64.0 53.3 57.8 60.7 44.1 32.9 73.1 48.6 42.3 52.6 61.5CoTTA ✓ 82.9 81.6 81.9 87.4 85.6 75.6 61.1 63.1 64.9 49.9 34.8 91.2 54.0 48.8 56.6 68.0 (+6.5) ✗ 71.3 70.3 70.8 82.1 77.4 63.9 53.9 49.9 55.5 43.9 32.8 81.4 43.7 41.1 46.7 59.0TTAC-NQ ✓ 79.4 75.7 78.9 86.6 86.2 77.1 61.8 58.8 62.4 51.5 34.4 88.5 52.1 49.1 55.5 66.5 (+7.5) ✗ 65.5 62.4 63.5 66.6 67.2 52.0 47.3 48.2 54.1 39.9 32.1 55.0 42.3 39.2 44.8 52.0EATA ✓ 69.3 67.1 69.2 71.1 71.7 57.5 49.9 51.9 57.4 42.4 32.6 60.7 45.1 41.4 47.4 55.6 (+3.6) ✗ 92.5 91.3 91.0 84.0 87.0 79.3 72.4 74.6 71.3 67.9 39.0 89.0 76.2 67.0 62.4 76.3MEMO ✓ 97.7 97.0 98.0 82.1 90.1 85.1 77.4 83.0 76.6 75.4 41.0 94.5 82.9 79.2 68.2 81.9 (+5.6) ✗ 58.6 57.8 59.0 87.0 81.6 76.6 65.9 67.9 66.7 64.0 40.0 92.2 52.2 46.6 49.9 64.4DDA ✓ 97.8 97.0 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.3 82.0 (+17.6) 644, except for MEMO (Zhang et al., 2021), which pre- dicts on single images to incentivize prediction consistency over an input and its augmentations. Regarding datasets, we follow earlier works (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Gao et al., 2022; Zhang et al., 2021), and thus evaluate on the ImageNet-C dataset (Hendrycks & Dietterich, 2019) with a corruption level of 5 for all 15 corruptions. We further extend our evaluation and consider CIFAR10-C, ImageNet-R (Hendrycks et al., 2021), and the more recent ImageNet-3DCC (Kar et al., 2022), which lever- ages depth estimates to construct more spatially-consistent corruptions. Our experiments compare the performance of the base- line model fθ (without test time adaptation) against 15 state-of-the-art TTA methods published in top-tier venues (e.g., CVPR, NeurIPS, and ICLR) between 2017 and 2023. In particular, we consider: BN (Schneider et al., 2020) and AdaBN (Li et al., 2016), which adjust the statistics of the batch normalization layers; SHOT (Liang et al., 2020) and SHOT-IM (Liang et al., 2020), which fine-tune the feature extractor to maximize mutual information; entropy mini- mization approaches such as TENT (Wang et al., 2020), 4This batch size is recommended by most baselines (Wang et al., 2020; Niu et al., 2022) ETA (Niu et al., 2022) (a more efficient version of TENT), and SAR (Niu14 et al., 2023), which trains the learnable parameters of the batch normalization layers; distillation approaches, such as CoTTA (Wang et al., 2022), Pseudo Labeling (PL) (Lee et al., 2013), and the very recent and efficient LAME (Boudiaf et al., 2022); EATA (Niu et al., 2022) and TTAC (Su et al., 2022) that assume access to the source training data; data-dependent approaches such as MEMO (Zhang et al., 2021) and the diffusion-based method DDA (Gao et al., 2022). For all methods, we use their official implementation with their recommended hyper- parameters. We report our experimental results on a subset of 12 baselines, while leaving ETA, SHOT-IM, and PL to the appendix due to space constraints and their similarity to SHOT and EATA. As mentioned in Section 3.2 , our protocol performs an online computation of the relative adaptation speed of g. In particular, for each batch revealed by the stream, we compute C (g(x)). Then, if C(g(xi)) = k, all the samples {xi+1, xi+2, . . . , xi+k} are processed by fθi without adap- tation. Otherwise, if C(g(xi)) = 1, then these samples are processed by g. For methods that accumulate parameter updates such as TENT (Wang et al., 2020), fθi is the most recent updated model g(fθi−1 ). We report all our main re- sults as the average across three seeds, and leave the detailed 5Evaluation of Test-Time Adaptation Under Computational Time Constraints SHOT TENT TTAC-NQ SAR EATA COTTA brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%) (a) Current Continual TTA. brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%)  (b) Realistic Continual TTA. Figure 3: Continual Error Rate on ImageNet-C. We report the continual error rate of several TTA methods on ImageNet-C benchmark under both realistic and current setups. A lower error rate indicates a better TTA method. Continual evaluation means the corruptions are presented in a sequence without resetting the model in between. We choose the same order as presented along the x-axis; starting with brightness and ending with clean validation set. In the current setup, we observe an increasing trend for SHOT, TENT, and TTAC-NQ. This is hypothesized to be due to overfitting on the early distribution shifts. This behavior is mitigated in the realistic setup due to adapting to fewer batches. EATA and SAR perform equally well in both realistic and current continual setups due to sample rejection. We report the standard deviation across 3 seeds. analysis to the Appendix. Throughout the experiments, we refer to our realistic evaluation protocol as “realistic/on- line”, and refer to the current protocol as “current/offline”. Next, we evaluate all methods on four different scenarios: (i) when domain shifts happen in an episodic manner, (ii) when domain shifts happen continually, i.e. one after the other, (iii) when the stream speed varies, (iii) when domain shifts happen continually with label correlation; practical evaluation (Yuan et al., 2023) ,and (v) when the baseline fθ is unavailable for evaluating the samples skipped by the TTA method g (left for the appendix). 4.1. Episodic Evaluation of TTA First, we consider an episodic evaluation of domain shifts, whereby S contains a single domain (e.g. one corruption) from ImageNet-C. We analyze this simple and most com- mon setup to assess the performance of TTA methods under real-time evaluation. We report the error rates on all corrup- tions in Table 2 and the average error rate across corruptions. We summarize the insights as follows: (i) The performance of TTA methods often degrades significantly under the realistic setup. Most methods induce a significant computational overhead, which prevents them from adapting to every sample from the test stream. For example, the error rate increases by 7.5% for TTAC- NQ and 4.3% for TENT, where C(gTTAC-NQ) = 12 and C(gTENT) = 3 (see Table 1). That is, TENT adapts to one- third of the batches revealed by the stream, while TTAC-NQ adapts to one every twelve batches. (ii) Very efficient methods, withC(g) = 1, such as LAME and BN, do not lose in performance. Evaluating such methods in offline or realistic setups is inconsequential, as their adaptation incurs negligible additional computation (since they adapt during the forward pass (Li et al., 2016; Schneider et al., 2020) or by adjusting the logits (Boudiaf et al., 2022) at a speed that pales in comparison to that of the stream). Interestingly, in our realistic evaluation, the simple BN (published in 2020) with an average error rate of 66.7% outperforms more recent and advanced methods such as SAR (published in 2023) by 1.7%. Furthermore, AdaBN (published in 2017) significantly outperforms the very recent diffusion-based DDA by a notable 13%. (iii) Data-dependent approaches, such as MEMO and DDA, are extremely inefficient. Despite the independence of MEMO and DDA on batch size, they incur a massive computational burden. For instance, C(gMEMO) = 54 and C(gDDA) = 810. Thus, both methods will be busy adapting for considerable portions of the stream, leaving most predic- tions to the non-adapted classifier. This phenomenon is the reason behind the reported performance of these methods being so close to that of fθ (i.e. around 82%). This result calls for future research to focus on increasing the efficiency of data-dependent adaptation methods. (iv) Sample rejection-oriented methods can perform well under the realistic protocol. EATA adapts efficiently due to its fast sample rejection algorithm, which relies solely on 6Evaluation of Test-Time Adaptation Under Computational Time Constraints the forward pass to admit samples for adaptation. EATA’s low error rate of 55.6%, combined with a small performance drop of less than 4%, positions it as the top performer under the realistic evaluation protocol on ImageNet-C. On the other hand, SAR does not benefit from sample rejection. SAR’s performance drop of 7.5% is due to its dependence on gradients for sample rejection, which reduces its speed. (v) SHOT benefits from the realistic protocol. Interest- ingly, we found that SHOT (and SHOT-IM in the Appendix), a fine-tuning-based approach, benefits from our realistic evaluation. In particular, we found that SHOT’s error rate decreases by 2% on fog corruption and by 0.8% on average. This observation could suggest that SHOT could potentially improve performance by disposing of fine-tuning on every batch. It is also worth mentioning that, under our realis- tic evaluation, SHOT (introduced in 2020) outperforms all methods except EATA. (vi) Performance changes are consistent across corrup- tions. Note that all methods that are somewhat efficient can improve the source model across all corruptions, in both the offline and realistic setups. Furthermore, the performance changes when comparing the offline and realistic setups are consistent across all corruptions. This finding suggests that the performance of these methods is independent of the do- main shift being considered. We further test this hypothesis by benchmarking these methods on two other datasets with other types of domain shifts in Section 4.4. 4.2. Continual Evaluation of TTA Next, we analyze the more challenging continual setup, fol- lowing (Wang et al., 2022; Niu et al., 2022). In particular, we construct the stream S by concatenating all corruptions from ImageNet-C. That is, we adapt TTA methods continu- ally on all corruptions followed by the clean validation set, without ever resetting the network weights. We introduce the notion of realistic adaptation to the continual setup to study the effects of a constant stream speed on the bench- mark. We report results in Figure 3 for both the offline and realistic protocols, where the horizontal-axis shows how cor- ruptions are ordered in the stream. We limit the experiments in this section to six TTA methods (SHOT, TENT, TTAC- NQ, COTTA, EATA, and SAR), and leave the remaining details for the Appendix. We observe: (i) Methods that do not perform sample rejection (SHOT, TENT, TTAC) scale poorly in the offline-continual setup. This phenomenon can be attributed to these methods over- fitting to early distributions. However, methods that do perform sample rejection (SAR and EATA) do not overfit as easily to corruptions, and can thus adapt to the rest of the stream. Even worse, such methods tend to even significantly degrade the performance on clean data. 1/16 1/8 1/4 1/2 1 η 52 55 58 61 64 67Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 4: Average Error Rate on ImageNet-C Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-C under slower stream speeds. In our proposed realistic model evaluation, the stream speed r is normalized by the time needed for a for- ward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 different random seeds. Different TTA methods degrade differently when varying η. (ii) In the realistic-continual setup, methods that do not perform sample rejection benefit from skipping adapta- tion on some batches, and become competitive with the methods that perform sample rejection. That is, while skipping parts of the stream deteriorated the performance of such methods in the episodic evaluation , this skipping actu- ally helped in preventing these methods from over-fitting in the continual setup. 4.3. Stream Speed Analysis In the previous experiments, we normalized the stream speed to be the same as that of fθ’s forward pass. That is, we assumed that the rate r at which S reveals new batches is equal to R (fθ(x)). However, some applications may enjoy a slower stream, giving TTA methods more time to adapt to samples. To explore this scenario, we vary the speed at which the stream reveals new data. In particular, let the new stream rate be η rwith η ∈ (0, 1]. Hence, as η → 0, the stream slows down and allows methods to adapt to all samples. Conversely, as η → 1, the stream speeds up, and at η = 1 we recover our realistic evaluation protocol. We experiment with the stream speed by setting η ∈ {1/16, 1/8, 1/4, 1/2, 1}, and evaluate five representative TTA methods (SHOT, TENT, TTAC-NQ, SAR, and EATA) in the episodic setup . Figure 4 summarizes our results by reporting the average error rate across all corruptions. We next list our observations: (i) The performance of TTA methods varies widely.For 7Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 3: Episodic Error Rate on ImageNet-C with ViT. We report the error rate of three baselines (Source, Tent, SAR) on the 15 different corruptions on ImageNet-C when the backbone is ViT architecture pretrained on ImageNet. We observe that while generally better backbones yield smaller error rate, expensive methods perform worse under our realistic evaluation. The more expensive the method is (e.g. SAR compared to Tent), the more performance reduction it suffers. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 90.5 93.3 91.8 71.0 76.6 66.1 72.9 84.1 73.5 52.8 45.3 55.9 69.5 55.5 52.2 70.1 - ✗ 69.9 95.9 68.9 55.8 62.0 52.3 57.9 57.2 53.6 41.8 28.9 40.7 59.1 39.7 42.0 55.0Tent ✓ 80.7 88.9 81.0 63.0 69.5 58.3 64.9 65.8 59.7 47.7 33.2 47.3 64.6 45.1 46.4 61.1 (-6.1) ✗ 55.5 56.9 55.1 47.5 50.4 44.3 48.7 42.4 47.3 33.6 25.4 35.6 44.8 33.5 36.4 43.8SAR ✓ 70.0 72.5 69.4 56.6 63.4 54.0 60.0 56.4 53.5 43.0 30.5 43.3 58.7 41.5 43.8 54.5 (-10.7) example, TTAC-NQ starts degrading faster (at η = 1/16) due to its slow adaptation speed. For other methods, the η at which they degrade varies. For instance, while TENT has a higher error rate than SAR in slow streams (η ≤ 1/8), TENT outperforms SAR in the regime of faster streams η ≤ 1/4. Interestingly, SHOT (Liang et al., 2020) ranks the worst at η ≤ 1/8, then ranks second when η ≥ 1/2, becoming a viable alternative. At last, the order of different methods significantly changes depending on the speed of the stream. For example, SAR changes from being second best at η ≤ 1/8 to third at η = 1/4 and then to fifth ( i.e. second worst) at η ≥ 1/2. (ii) EATA provides a good trade-off between speed and performance. In fact, EATA gives the best overall perfor- mance (lowest error rate) independent of the stream’s speed. This virtue is attributable to EATA’s combination of good performance and adaptation speed based on efficient sample rejection. Results on other datasets are in the Appendix. 4.4. Results on Other Benchmarks and Architectures We extend our evaluation protocol to cover ImageNet- 3DCC (Kar et al., 2022) and ImageNet-R (Hendrycks et al., 2021) datasets and ResNet-18 (results in the ap- pendix) and ViT (Kolesnikov et al., 2021) architectures. ImageNet-R contains rendition versions of ImageNet span- ning 200 classes. ImageNet-3DCC constructs more spatially-consistent corruptions than ImageNet-C by lever- aging depth estimates. For ViT, we conduct episodic evalu- ation on ImageNet-C in a similar setup to Section 4.1 and report the results in Table 3 for the non-adapted model, Tent, and SAR. For ImageNet-R and ImageNet-3DCC, we fix the architecture to ResNet-50 and experiment on the entire datasets and set the severity level to 5 in ImageNet-3DCC. Due to the space constraint, we limit our experiments to the episodic evaluation, and leave other results and analyses to the Appendix. We evaluate the effectiveness of 10 TTA methods in Table 4, where we report the average error rate across all corruptions. We observe that our results are consistent across all con- Table 4: Average Error Rate on ImageNet-R and ImageNet-3DCC. We report the average error rate of dif- ferent TTA methods on ImageNet-R and ImageNet-3DCC under both the realistic and current setups. A lower error rate indicates a better TTA method. The highlighted num- bers indicate a better performance per method across setups. We observe that methods generally perform worse in the more realistic realistic setup. The conclusions are consistent with what we observed on ImageNet-C (Table 2). Method ImageNet-R ImageNet-3DCC Current Realistic ∆ Current Realistic ∆ Source 63.8 63.8 - 73.9 73.9 - AdaBN 60.6 60.6 0 72.1 72.1 0 BN 60.0 60.0 0 70.5 70.5 0 LAME 60.5 60.5 0 72.1 72.1 0 SHOT 70.3 62.6 (+7.7) 69.2 67.0 (+2.2) TENT 58.1 59.1 (-1.0) 64.5 66.8 (-2.3) SAR 57.5 59.6 (-2.1) 63.5 71.4 (-7.9) CoTTA 57.3 61.5 (-4.5) 66.4 75.6 (-9.2) EATA 55.7 57.1 (-1.4) 60.9 63.1 (-2.2) TTAC-NQ 59.2 60.8 (-1.6) 65.7 73.6 (-7.9) sidered datasets and architectures. Similar to our results in Table 2, the more computationally involved SAR de- grades more than Tent when leveraging ViT architecture. Regarding other datasets, we find that simple methods that adapt during the forward pass are unaffected by the realis- tic setup. All the other methods, except SHOT, experience degradation in their results on both datasets. We observe again that, on these two datasets, while SHOT actually ben- efits from the realistic evaluation, EATA remains the best alternative on both ImageNet-R and ImageNet-3DCC. 4.5. Evaluation under Practical TTA Recently, (Yuan et al., 2023) extended the continual test- time adaptation evaluation to include label-imbalances; known as Practical Test-Time Adaptation (PTTA) setup. In this setting, the stream not only reveals a continual se- quence of distribution shifts, but also the revealed batches 8Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 5: Episodic Error Rate on CIFAR10-C under Practical Evaluation (Yuan et al., 2023).We report the error rate of two baselines (Source, RoTTA (Yuan et al., 2023)) on the 15 different corruptions on CIFAR10-C when the backbone is ResNet-18. We observe that under our computational constrained evaluation, the only method tailored to this setting; RoTTA, performs worse than the non-adapted baseline. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 72.3 65.7 72.9 46.9 54.3 34.8 42.0 25.1 41.3 26.0 9.3 46.7 26.6 58.5 30.3 43.5 - ✗ 36.9 34.9 45.8 16.6 44.2 19.9 16.53 21.6 22.4 18.8 9.8 20.6 28.4 27.1 34.5 26.5RoTTA ✓ 55.0 54.4 63.2 43.3 62.3 43.7 43.5 44.8 47.7 43.4 35.3 41.8 54.0 47.7 54.6 49.0 (-22.5) have significant label imbalances. To combat this combined challenge, the work of (Yuan et al., 2023) proposed to lever- age a balanced memory bank for adaptation. In this section, we extend our computational constrained evaluation to the PTTA setup and compare RoTTA (Yuan et al., 2023) with a non-adapted model on CIFAR10-C benchmark. Table 5 summarizes the results. We observe that while RoTTA indeed reduces the error rate under the PTTA setup on CIFAR10-C (17% below the non-adapted model), our realistic evaluation uncovers its computational limitation. We found that RoTTA’s error rate increases by over 22% surpassing the error rate of the non-adapted model. Note that RoTTA stores samples from the stream in a memory bank then adapts the model on sampled samples from the memory bank. Thus, the slower the adaptation of RoTTA, the less diverse the samples in the memory bank, hindering its adaptation. 4.6. Effect of Hyper-parameter Tuning The performance of different TTA methods heavily depends on their hyper-parameter settings (Zhao et al., 2023). Here, we assess the impact of our proposed evaluation on TTA methods when tuning their hyperparameters. For that regard, we conduct hyper parameter search for Tent (as a fundamen- tal baseline) and experiment with different learning rates (the only hyper-parameter for Tent). Table 6 summarizes the results under episodic evaluation for 4 different corruptions on ImageNet-C. We observe that while conducting hyper-parameter search indeed improves the performance of TENT, its error rate increases under our realistic evaluation across all hyperparameters. That is, while conducting hyper-parameter search might indeed result in a better performance for TTA methods, the insights obtained through our proposed evaluation scheme remains consistent: more efficient TTA methods will have a smaller performance drop under the realistic evaluation. 5. Conclusions In this work, we find that the performance of Test Time Adaptation (TTA) methods can vary depending on the con- Table 6: Effect of our evaluation under hyperparameter tuning. We report the error rate for Tent under different learning rates under both the current and our proposed real- istic evaluation. While carefully tuning the learning rate for Tent results in a better performance, our realistic evaluation causes a performance drop under all learning rates. lr Realisticgauss. motion fog pixel. Avg. ∆ ✗ 74.1 63.3 44.7 43.5 56.41×10−4 ✓ 79.7 69.0 47.8 46.8 60.8 (-4.4) ✗ 71.1 59.7 43.1 41.9 53.92×10−4 ✓ 77.6 66.1 46.0 45.0 58.7 (-4.7) ✗ 69.6 58.1 42.4 41.1 52.83×10−4 ✓ 74.9 64.0 45.0 44.0 57.0 (-4.2) ✗ 68.8 57.1 42.0 40.8 52.24×10−4 ✓ 73.7 62.3 44.5 43.2 55.9 (-3.7) text in which they are used. In the episodic evaluation, the efficiency of the method is the most important factor, with more efficient methods like AdaBN and BN showing consistent performance, while data-dependent approaches suffer. Sample rejection methods generally perform well, and fine-tuning approaches such as SHOT can even improve when adapting to fewer samples. In the continual evalua- tion, methods that do not perform sample rejection scale poorly in the offline-continual setup but benefit from skip- ping adaptation on some batches in the realistic-continual setup. Furthermore, our stream speed analysis shows that the performance of TTA methods can vary widely at differ- ent speeds. Our findings are consistent across corruptions and multiple datasets. They can help researchers and practi- tioners to better understand the strengths and weaknesses of different TTA methods, and to choose the most appropriate method for their specific use case. Acknowledgements This work was partially done during a research internship of the first author at Intel Labs. This work was supported by the King Abdullah University of Science and Technol- ogy (KAUST) Office of Sponsored Research (OSR) under Award No. OSR-CRG2021-4648. We would like to thank Yasir Ghunaim and Mattia Soldan for the helpful discussion. 9Evaluation of Test-Time Adaptation Under Computational Time Constraints Impact Statement Our work advances Machine Learning by proposing a re- alistic evaluation protocol for Test Time Adaptation meth- ods, prioritizing computational efficiency. This approach promotes the development of AI systems that are both ac- cessible in resource-limited settings and environmentally sustainable, by favoring simpler, faster methods. Such ad- vancements contribute to more inclusive and responsible AI deployment, aligning with ethical goals of broadening access and reducing environmental impacts References Alfarra, M., P´erez, J. C., Thabet, A., Bibi, A., Torr, P. H., and Ghanem, B. Combating adversaries with anti-adversaries. In Proceedings of the AAAI Conference on Artificial In- telligence, volume 36, pp. 5992–6000, 2022. Ashukha, A., Lyzhov, A., Molchanov, D., and Vetrov, D. Pitfalls of in-domain uncertainty estimation and ensem- bling in deep learning. arXiv preprint arXiv:2002.06470, 2020. Boudiaf, M., Mueller, R., Ben Ayed, I., and Bertinetto, L. Parameter-free online test-time adaptation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8344–8353, 2022. Cai, Z., Sener, O., and Koltun, V . Online continual learning with natural distribution shifts: An empirical study with visual data. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pp. 8281–8290, 2021. Chen, D., Wang, D., Darrell, T., and Ebrahimi, S. Con- trastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 295–305, 2022. Croce, F., Gowal, S., Brunner, T., Shelhamer, E., Hein, M., and Cemgil, T. Evaluating the adversarial robustness of adaptive test-time defenses. In International Conference on Machine Learning, pp. 4421–4435. PMLR, 2022. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Gao, J., Zhang, J., Liu, X., Darrell, T., Shelhamer, E., and Wang, D. Back to the source: Diffusion-driven test-time adaptation. arXiv preprint arXiv:2207.03442, 2022. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, T., Jeong, J., Kim, T., Kim, Y ., Shin, J., and Lee, S.-J. Note: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and pertur- bations. Proceedings of the International Conference on Learning Representations, 2019. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba- bilistic models. Advances in Neural Information Process- ing Systems, 33:6840–6851, 2020. Iwasawa, Y . and Matsuo, Y . Test-time classifier adjustment module for model-agnostic domain generalization. Ad- vances in Neural Information Processing Systems , 34: 2427–2440, 2021. Kar, O. F., Yeo, T., Atanov, A., and Zamir, A. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18963–18974, 2022. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Kojima, T., Matsuo, Y ., and Iwasawa, Y . Robustifying vision transformer without retraining from scratch by test- time class-conditional feature alignment. arXiv preprint arXiv:2206.13951, 2022. Kolesnikov, A., Dosovitskiy, A., Weissenborn, D., Heigold, G., Uszkoreit, J., Beyer, L., Minderer, M., Dehghani, M., Houlsby, N., Gelly, S., Unterthiner, T., and Zhai, X. An image is worth 16x16 words: Transformers for image recognition at scale. 2021. 10Evaluation of Test-Time Adaptation Under Computational Time Constraints Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896, 2013. Li, Y ., Wang, N., Shi, J., Liu, J., and Hou, X. Revisit- ing batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016. Liang, J., Hu, D., and Feng, J. Do we really need to access the source data? source hypothesis transfer for unsuper- vised domain adaptation. In International Conference on Machine Learning, pp. 6028–6039. PMLR, 2020. Liu, Y ., Kothari, P., Van Delft, B., Bellot-Gurlet, B., Mordan, T., and Alahi, A. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021. Mancini, M., Karaoguz, H., Ricci, E., Jensfelt, P., and Ca- puto, B. Kitting in the wild through online domain adap- tation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1103–1109. IEEE, 2018. Mirza, M. J., Micorek, J., Possegger, H., and Bischof, H. The norm must go on: dynamic unsupervised do- main adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14765–14775, 2022a. Mirza, M. J., Soneira, P. J., Lin, W., Kozinski, M., Possegger, H., and Bischof, H. Actmad: Activation matching to align distributions for test-time-training, 2022b. URL https://arxiv.org/abs/2211.12870. Niu, S., Wu, J., Zhang, Y ., Chen, Y ., Zheng, S., Zhao, P., and Tan, M. Efficient test-time model adaptation with- out forgetting. In International conference on machine learning, pp. 16888–16905. PMLR, 2022. Niu14, S., Wu, J., Zhang, Y ., Wen, Z., Chen, Y ., Zhao, P., and Tan15, M. Towards stable test-time adaptation in dynamic wild world. International Conference on Learning Representations, 2023. P´erez, J. C., Alfarra, M., Jeanneret, G., Rueda, L., Thabet, A., Ghanem, B., and Arbel´aez, P. Enhancing adversarial robustness via test-time transformation ensembling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 81–91, 2021. Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains. In Computer Vision–ECCV 2010: 11th European Conference on Com- puter Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11 , pp. 213–226. Springer, 2010. Sakaridis, C., Dai, D., and Van Gool, L. Acdc: The ad- verse conditions dataset with correspondences for seman- tic driving scene understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10765–10775, 2021. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., and Bethge, M. Improving robustness against com- mon corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 2020. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Su, Y ., Xu, X., and Jia, K. Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering. arXiv preprint arXiv:2206.02721, 2022. Sun, Y ., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generaliza- tion under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020. Torralba, A. and Efros, A. A. Unbiased look at dataset bias. In CVPR 2011, pp. 1521–1528. IEEE, 2011. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7167–7176, 2017. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimiza- tion. arXiv preprint arXiv:2006.10726, 2020. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual test- time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022. Yuan, L., Xie, B., and Li, S. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15922–15932, 2023. Zhang, M., Levine, S., and Finn, C. Memo: Test time ro- bustness via adaptation and augmentation. arXiv preprint arXiv:2110.09506, 2021. Zhao, H., Liu, Y ., Alahi, A., and Lin, T. On pitfalls of test- time adaptation. International Conference on MAchine Learning, 2023. 11Evaluation of Test-Time Adaptation Under Computational Time Constraints A. Methodology A.1. Online Computation of C(g) Section 3.2 discussed the online evaluation protocol of TTA methods. Here, we give more details on the calcu- lation of C(g), the relative adaptation speed of g, during our online evaluation. First, we set R (g(x)) as the time recording function for g to perform a forward pass for a single batch. To ensure a reliable time calculation, we exe- cute torch.cuda.synchronize() before starting the timer and before ending it. This ensures all GPU operations are finished for the moment time is computed. To alleviate hardware dependence, we also calculate R(fθ(x)) for each evaluation step computing the relative adaptation complex- ity. It is worth mentioning that C(g) for SHOT, EATA, SAR, and COTTA are[3, 3, 8, 103] on average, respectively. B. Experiments B.1. Episodic Evaluation of TTA SHOT, PL, and ETA For completeness, we report the results on 3 baselines: Pseudo Label (Lee et al., 2013), SHOT-IM (Liang et al., 2020), and ETA (Niu et al., 2022) in Table 7. We follow the same setup as in the main paper. Our results are consistent with the findings of Section 4.1 and Table 2. In particular, SHOT-IM improves its perfor- mance under the online evaluation, similar to SHOT. Further, the performance of ETA and PL degrades under the online evaluation due to the additional computational burden. Nev- ertheless, ETA is similar to EATA in providing the best tradeoff between additional computational requirements and performance improvements. SAR with GN We equip our results to include ResNet50 with Group Normalization (GN) layers, following (Niu14 Figure 5: C(g) computation across iterations. We report our online calculations for the relative adaptation speed ofg, C(g), for SAR, SHOT, EATA, and TENT throughout a full evaluation episode. We observe that, overall, C(g) has a stable behavior throughout evaluation iterations. et al., 2023). We report the results in Table 7, where we observe that: (i) Under a relatively large batch size (64), ResNet50 with GN underperforms ResNet50 with Batch Normalization. In fact, the average error rate for SAR in- creases from 56.2% to 65.8%. (ii) The online evaluation penalizes SAR in both architecture choices with a perfor- mance degradation of 3.6% under the GN-based ResNet. Finally, it is worth mentioning that SAR with GN layers attains a similar performance under a batch size of 1. Ablating Batch Sizes In the experiments section, we fixed the batch size to 64 following the recommendations of ear- lier works (Wang et al., 2020; Niu et al., 2022). Here, we investigate the effect of our proposed online evaluation un- der different choices of batch sizes. To that end, we vary the batch size in {1, 16, 32, 128}, and report the results in Figure 6. We draw the following observations: Table 7: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on the ImageNet-C benchmark under both the online and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup is merely the reproduction of every method. We show that methods generally perform worse in the more realistic online setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse its performance. SAR-GN represents SAR when deployed on ResNet50 with Group Normalization (GN) layers, following (Niu14 et al., 2023). Noise Blur Weather DigitalMethod Online gauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ ✗ 73.1 69.8 72.0 76.9 75.9 58.5 52.7 53.3 62.2 43.8 34.6 82.6 46.0 42.3 48.9 59.5SHOT-IM ✓ 71.1 68.6 70.7 73.2 73.6 59.1 51.9 52.8 60.5 43.7 33.6 77.3 45.7 42.1 48.6 58.2 (-0.3) ✗ 92.2 92.2 92.8 97.0 89.8 57.7 49.6 50.7 57.1 41.5 32.6 91.1 44.3 40.3 46.6 65.0PL ✓ 90.6 86.3 83.6 93.2 89.7 63.0 51.7 55.0 59.3 43.8 32.9 92.3 47.3 42.4 49.3 65.3 (+0.3) ✗ 64.9 62.7 63.6 66.4 66.3 52.4 47.3 48.2 54.1 40.2 32.2 54.8 42.3 39.2 44.7 52.0ETA ✓ 70.2 67.0 69.6 71.5 71.5 56.9 50.2 51.9 57.0 42.0 32.5 60.5 44.6 40.8 47.1 55.6 (+3.6) ✗ 71.8 69.0 70.3 81.5 81.0 69.6 69.5 57.1 56.6 94.3 29.2 56.0 84.8 51.4 44.7 65.8SAR-GN ✓ 82.0 80.2 82.1 80.2 88.6 78.5 75.1 59.6 53.9 66.9 30.7 63.3 81.3 71.3 47.5 69.4 (+3.6) 12Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) ADABN OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  BN-ADAPTATION OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 COTTA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) EATA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ETA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  LAME OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) PL OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SAR OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SHOT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) SHOTIM OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TENT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TTAC-NQ OFFLINE ONLINE Figure 6: Batch Size Analysis current vs. realistic setups for every method. We assess the performance variation of 12 different TTA methods under varying batch sizes. We experiment with batch sizes in{1, 16, 32, 128}. We do not include the baseline, MEMO, and DDA, since they are data-dependent approaches and are unaffected by batch size. All TTA methods, except LAME, are severely affected by smaller batch sizes. Nonetheless, the realistic evaluation degrades the performance of all methods, except SHOT and SHOT-IM. (i) Online evaluation improves the performance of SHOT and SHOT-IM. This result is consistent with the earlier observations in Table 2. Note that PL shares a similar trend as well. (ii) The performance of TTA methods degrades when switching from offline to online evaluation, regardless of the batch size. This result is highlighted in COTTA, ETA, EATA, SAR, TENT, and TTAC-NQ. (iii) Performance of TTA methods vastly varies when varying the batch size. This result is consistent with earlier findings in the literature (Gao et al., 2022; Niu14 et al., 2023), where most TTA methods fail with small batch sizes. At last, and to ease comparison across methods, we summa- rize all the plots for all methods in Figure 7. Consistency with 3 random seeds. For all of our exper- iments, we run each experiment with 3 random seeds. In most of our results, we found out that the standard deviation of performance across runs is very small. Our results in Figures 3 and 4 demonstrate this variation in the shaded area for 5 different TTA methods. B.2. Continual Evaluation of TTA We further explore another setup for the continual evalua- tion of TTA. In particular, we follow (Wang et al., 2022) in concatenating all corruptions in ImageNet-C with 11 differ- ent orders. We then report the average performance of each method across all runs and corruptions in Table 8. We run each experiment with 3 random seeds, and report our results with standard deviations. For the remaining implementation 13Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) OFFLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ONLINE ADABN BN-ADAPTATION COTTA EATA ETA LAME PL SAR SHOT SHOTIM TENT TTAC-NQ Figure 7: Summary of batch size analysis: current vs. realistic setups. Left: Current evaluation, i.e.,Section 3.1. Right: Realistic evaluation,i.e.,Section 3.2. While EATA achieves the lowest error rate under batch sizes≥ 32, SHOT becomes a very competitive baseline, outperforming EATA, at a batch size of 128. Table 8: Continual Error Rate on ImageNet-C. We report the average continual error rate for 11 different corruption orders, with 3 different seeds, under both the offline and online setups with a corruption severity level of 5. Continual refers to continually adapting after each corruption without resetting. This metric indicates the model’s capability to learn from previous corruptions. The offline setup refers to the performance of the model in a continual learning scheme, whereas the online setup refers to the performance of the model in a continual learning scheme, under our more realistic online setup. We show that the more complex a method is, the fewer samples it adapts to, achieving better performance in a continual learning scheme. Avg. Error (%) COTTA ETA TENT SAR EATA SHOT TTAC-NQ Offline 65.3 ± 5.9 56 .4 ± 2.3 84 .6 ± 16.0 59 .8 ± 3.0 56 .4 ± 2.3 88 .4 ± 11.4 81 .8 ± 11.4 Online 69.3 ± 2.8 57 .7 ± 2.0 65 .6 ± 5.0 60 .4 ± 1.8 57 .7 ± 1.9 78 .2 ± 7.7 65 .1 ± 3.8 details, we follow our setup in main paper. We observe that, similar to our conclusions in Section 4.2, online eval- uation helps methods that do not perform sample rejection (e.g.,TENT). Nonetheless, both ETA and EATA provide the best trade-off between performance and additional compu- tational burden. B.3. Stream Speed Analysis For completeness, we extend our stream speed analysis in Section 4.3 to cover the ImageNet-3DCC dataset. We preserve our experimental setup by varying the stream speed according to ηr, with η ∈ {1/16, 1/8, 1/4, 1/2, 1. Figure 8 summarizes our results for SHOT, TENT, TTAC-NQ, EATA, and SAR. We observe similar trends to the ones in Figure 4, where the performance of different TTA methods varies widely under different stream speeds. The large relative adaptation speed of TTAC-NQ degrades its performance under even slow streams (e.g.,η = 1/8), while SHOT reduces its error rate under faster streams. Furthermore, EATA is consistently outperforming all other considered approaches under different stream speeds. B.4. Evaluation on Other Benchmarks We report the error rates on all corruptions of ImageNet- 3DCC (Kar et al., 2022), along with the overall average error rate, in Table 9. The conclusions we draw for ImageNet- 3DCC (Kar et al., 2022) are very similar to the ones ob- served on ImageNet-C (Hendrycks & Dietterich, 2019) (in Section 4.1). We observe that efficient methods, with C(g) = 1, such as LAME and BN, maintain performance. Furthermore, the performance of some TTA methods (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Wang et al., 2022) degrades in the online setup, while others that use pseudo labeling (Lee et al., 2013; Liang et al., 2020) actually improve. This degradation seems to be directly proportional to the amount of data a method misses according to its C(g). 14Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 9: Episodic Error Rate on ImageNet-3DCommonCorruptions. We report the error rate of different TTA methods on ImageNet-3DCC (Kar et al., 2022) benchmark under both the realistic and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup corresponds to reproducing the reported performance of every method. The first sub-table corresponds to methods that incur none or few additional computations, i.e.,C(g) = 1. We show that methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the fewer data it will adapt to, and the worse its performance. Depth of field Noise LightingWeather Video Camera motionMethod RealisticNear focus Far focusColor quant. ISO noise Low lightFlash Fog 3DBit error H.265 ABR H.265 CRFXY-mot. blur Z-mot. blurAvg. ∆ Source ✓ 46.9 55.6 82.5 94.0 71.7 78.7 75.3 88.6 70.6 65.4 82.0 75.3 73.9 -AdaBN ✓ 45.2 55.0 71.8 76.8 64.1 80.8 75.0 91.8 80.9 76.7 79.1 67.5 72.1 -LAME ✓ 45.3 55.0 71.9 76.9 64.1 80.8 75.1 91.8 80.9 76.8 79.2 67.6 72.1 -BN ✓ 43.9 54.3 72.3 76.6 60.9 80.1 72.4 90.9 78.7 73.8 76.9 65.6 70.5 - PL ✗ 39.8 49.8 65.5 72.6 48.9 79.0 66.1 97.5 92.1 86.2 88.7 57.6 70.3(-1.6)✓ 41.0 51.3 66.5 71.5 52.8 77.4 68.1 95.6 86.0 78.7 77.0 59.2 68.7 SHOT ✗ 43.0 53.6 67.1 64.2 51.9 81.1 73.2 97.2 83.5 77.8 77.3 60.1 69.2(-2.2)✓ 41.7 51.4 64.4 63.8 51.6 77.5 71.6 95.1 79.9 74.6 73.7 58.5 67.0 SHOT-IM✗ 42.2 52.7 66.6 63.7 51.0 81.0 72.1 97.0 83.3 77.6 75.6 59.2 68.5(-1.9)✓ 41.2 51.2 64.4 63.3 51.3 77.5 70.9 94.9 79.4 74.1 72.3 58.3 66.6 TENT ✗ 39.9 49.6 62.4 62.2 50.7 75.6 68.5 91.6 75.7 70.2 70.4 57.0 64.5(+2.3)✓ 41.7 51.4 65.5 67.2 54.7 77.4 70.1 90.7 76.8 71.9 74.0 60.8 66.8 SAR ✗ 40.3 50.0 62.0 61.2 50.6 73.8 65.8 90.1 73.9 68.8 69.1 56.8 63.5(+6.9)✓ 44.9 54.7 71.1 75.4 62.6 80.3 73.8 91.7 80.5 76.1 78.6 66.9 71.4 ETA ✗ 38.7 47.9 59.1 56.7 46.8 71.0 62.1 90.6 72.8 67.3 64.7 52.9 60.9(+2.3)✓ 39.7 49.3 61.6 60.7 50.0 73.5 65.2 90.3 74.4 69.1 68.8 55.9 63.2 CoTTA ✗ 40.8 50.9 66.3 68.3 54.6 77.2 68.0 90.2 76.4 71.1 73.1 60.4 66.4(+9.2)✓ 55.4 63.1 74.1 77.0 64.7 83.4 78.1 93.7 84.0 80.3 81.7 71.9 75.6 TTAC-NQ✗ 40.7 50.5 61.0 61.1 51.5 72.8 66.6 93.8 81.1 74.7 75.7 59.1 65.7(+7.9)✓ 49.9 57.0 69.3 72.3 58.9 79.8 76.3 95.8 86.5 83.0 84.6 69.8 73.6 EATA ✗ 38.6 47.8 59.2 56.6 46.9 71.2 62.2 90.9 72.5 67.4 64.6 52.9 60.9(+2.2)✓ 39.8 49.3 61.6 60.5 49.9 73.5 64.8 90.6 73.7 69.1 68.6 55.7 63.1 C. Single Model Evaluation Scheme In Section 3.2, we assume fθt can generate predictions whenever g is occupied with adapting to a batch. This setup assumes the capacity to concurrently deploy two models. However, this assumption might be unfair to methods with C(g) = 1, since it allows expensive methods to skip batches without large penalties. We thus also study the case where only one model can be deployed. Studying this setup requires establishing a policy on how samples missed by the TTA method g are treated. That is, when g is busy adapting, all skipped samples still must be predicted without access to fθt . Depending on the applica- tion, this prediction could leverage prior knowledge about the problem e.g. temporal correlation across samples, or the bias of the distribution. In our setup, we consider the most strict scenario in which, whenever g is busy, a ran- dom classifier generates predictions for the incoming sam- ples. This naive design choice results from our evaluation on ImageNet-based datasets, which contain images whose classes display no bias nor temporal correlation. We conduct episodic evaluation, similar to Section 4.1, on ImageNet-C dataset. We average the error rates per corruption category (e.g. averaging error rates for gaussian, shot, and impulse noises) and present the results of this study in Table 10. We draw the following observation. Single model evaluation strongly favors methods with C(g) = 1. We observe that all models that are slower than the stream are heavily penalized to the point that using the original pre-trained model becomes a better alternative. However, methods that can be as fast as the stream, like AdaBN or BN, become the best alternative due to their speed. This result encourages more research toward devel- oping efficient TTA methods that have negligible additional computational overhead. D. Results on ResNet18 In our experiments in the main paper, we focused on the stan- dard ResNet18-architecture, following the common practice in the literature. Here, and for completeness, we extend our results to cover the smaller and more efficient ResNet18 architecture. Teble 11 summarizes the episodic evaluation of 6 TTA methods on ImageNet-C dataset. Similar to our conclusions in the episodic evaluation section in the main paper, more expensive adaptation methods degrade more under our realistic evaluation scheme. 15Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 10: Per Corruption Category Average Error Rate Using Single Model Evaluation on ImageNet-C. We re- port the average error rate per corruption category of dif- ferent TTA methods under single model realistic evaluation mode on ImageNet-C. Single model mode assumes the de- ployment of a single modelg instead of two under a constant speed stream S. We assume the most extreme scenario, that is if a model g is occupied adapting to a batch, the incoming batch is fed to a random classifier. We observe that the best TTA methods to use in this scenario are AdaBN (Li et al., 2016) and BN (Schneider et al., 2020), which simply adapt the BN statistics. Method Realistic Noise Blur Weather Digital Avg. Source ✓ 97.7 83.8 69.1 81.4 82.0 AdaBN ✓ 84.5 76.1 54.9 62.7 68.5 BN ✓ 84.1 73.1 54.2 59.9 66.7 SHOT ✓ 92.6 91.3 87.0 88.5 89.7 TENT ✓ 91.9 89.4 83.0 85.0 87.0 SAR ✓ 95.6 94.0 90.1 91.3 92.6 EATA ✓ 89.4 87.6 82.0 83.2 85.3 TTAC-NQ ✓ 96.6 96.9 96.3 96.4 96.5 Table 11: Evaluating different TTA methods with ResNet- 18 architecture on ImageNet-C. We report the average error rate across all different types of corruptions (lower is bet- ter). TTA methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Method Basic BN SHOT Tent EATA SAR Current 85.4 70.1 64.4 64.9 59.7 63.8 Realistic 85.4 70.1 64.5 68.3 63.2 69.5 Diff - - 0.1 3.4 3.5 5.7 1/16 1/8 1/4 1/2 1 η 62 64 66 68 70 72 74Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 8: Average Error Rate on ImageNet-3DCC Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-3DCC under slower stream speeds. In our proposed online model evaluation, the stream speed r is normalized by the time needed for a forward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 random seeds. Different TTA methods degrade differently when varying η. 16",
      "meta_data": {
        "arxiv_id": "2304.04795v2",
        "authors": [
          "Motasem Alfarra",
          "Hani Itani",
          "Alejandro Pardo",
          "Shyma Alhuwaider",
          "Merey Ramazanova",
          "Juan C. Pérez",
          "Zhipeng Cai",
          "Matthias Müller",
          "Bernard Ghanem"
        ],
        "published_date": "2023-04-10T18:01:47Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04795v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods that accounts for computational time constraints by penalizing slower methods with fewer samples for adaptation. The core problem addressed is that current TTA evaluation protocols overlook the significant computational overhead incurred by many effective methods, affecting their real-world applicability. The key finding is that when inference speed is considered, simpler and faster TTA approaches can outperform more sophisticated but slower methods (e.g., SHOT outperforms SAR), emphasizing the importance of developing TTA methods that are both accurate and efficient. The paper provides a comprehensive experimental analysis of 15 TTA methods on multiple datasets and scenarios.",
        "methodology": "The paper introduces a Realistic TTA evaluation protocol that simulates data arrival from a constant-speed stream. This protocol defines a 'relative adaptation speed' C(g) for each TTA method g, which is the integer ratio of the stream's speed (r) to the method's processing speed (R(g(x))) (C(g) = ⌈r/R(g(x))⌉). If a method g is C(g) times slower than the stream, it is allowed to adapt to only every C(g)-th sample. For skipped samples, the most recent adapted model generates predictions. The base model fθ is assumed to run in real-time (C(fθ)=1), processing any samples missed by g. C(g) is computed online for each input to account for hardware and input-dependent variability. The protocol intrinsically penalizes slower methods as long adaptation times lead to fewer samples being used for adaptation.",
        "experimental_setup": "The evaluation focused on image classification tasks. The base model fθ used was a ResNet-50-BN (pretrained on ImageNet), with additional experiments on ViT and ResNet-18 architectures. The data stream provided batches of size 64, except for MEMO which used single images. Experiments were conducted on ImageNet-C (corruption level 5 for all 15 corruptions), CIFAR10-C, ImageNet-R, and ImageNet-3DCC datasets. The study benchmarked 15 state-of-the-art TTA methods (e.g., BN, AdaBN, SHOT, TENT, SAR, EATA, DDA, MEMO, CoTTA, LAME, TTAC-NQ, RoTTA). Evaluation scenarios included episodic adaptation (single domain shift, model reset), continual adaptation (sequential domain shifts, no model reset), varying stream speeds (ηr with η ∈ {1/16, 1/8, 1/4, 1/2, 1}), and practical TTA (continual adaptation with label imbalances). All main results were averaged across three random seeds, and hyper-parameter tuning for TENT was also analyzed.",
        "limitations": "The proposed realistic evaluation protocol, in its primary form, assumes that the baseline model fθ can process samples skipped by the TTA method g in real-time. This assumption might unfairly benefit computationally intensive methods by allowing them to skip batches without severe performance penalties, an issue explored in an alternative 'single model evaluation' scheme where skipped samples are handled by a random classifier. Additionally, the estimation of the relative adaptation speed C(g) can be noisy due to hardware and input dependencies, although the online computation method attempts to mitigate this variability. From the findings, data-dependent TTA methods (like MEMO and DDA) were shown to be extremely inefficient, incurring massive computational burdens that limited their practical utility.",
        "future_research_directions": "Future research should prioritize increasing the efficiency of data-dependent adaptation methods. There is a strong call for developing practical TTA methods that are both accurate and computationally efficient, with inference speed being a critical dimension for real-world performance. The authors encourage more research towards creating efficient TTA methods that have negligible additional computational overhead, moving beyond just accuracy improvements to consider deployment feasibility."
      }
    },
    {
      "title": "Efficient Non-stationary Online Learning by Wavelets with Applications to Online Distribution Shift Adaptation"
    },
    {
      "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
      "abstract": "In this paper, we propose Test-Time Training, a general approach for\nimproving the performance of predictive models when training and test data come\nfrom different distributions. We turn a single unlabeled test sample into a\nself-supervised learning problem, on which we update the model parameters\nbefore making a prediction. This also extends naturally to data in an online\nstream. Our simple approach leads to improvements on diverse image\nclassification benchmarks aimed at evaluating robustness to distribution\nshifts.",
      "full_text": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Yu Sun1 Xiaolong Wang1 2 Zhuang Liu1 John Miller1 Alexei A. Efros1 Moritz Hardt1 Abstract In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a sin- gle unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on di- verse image classiﬁcation benchmarks aimed at evaluating robustness to distribution shifts. 1. Introduction Supervised learning remains notoriously weak at generaliza- tion under distribution shifts. Unless training and test data are drawn from the same distribution, even seemingly minor differences turn out to defeat state-of-the-art models (Recht et al., 2018). Adversarial robustness and domain adapta- tion are but a few existing paradigms that try to anticipate differences between the training and test distribution with either topological structure or data from the test distribution available during training. We explore a new take on gener- alization that does not anticipate the distribution shifts, but instead learns from them at test time. We start from a simple observation. The unlabeled test sample xpresented at test time gives us a hint about the distribution from which it was drawn. We propose to take advantage of this hint on the test distribution by allowing the model parameters θto depend on the test sample x, but not its unknown label y. The concept of a variable decision boundary θ(x) is powerful in theory since it breaks away from the limitation of ﬁxed model capacity (see additional discussion in Section A1), but the design of a feedback mechanism from xto θ(x) raises new challenges in practice that we only begin to address here. 1University of California, Berkeley 2University of California, San Diego. Correspondence to: Yu Sun <yusun@berkeley.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Our proposed test-time training method creates a self- supervised learning problem based on this single test sample x, updating θat test time before making a prediction. Self- supervised learning uses an auxiliary task that automatically creates labels from unlabeled inputs. In our experiments, we use the task of rotating each input image by a multiple of 90 degrees and predicting its angle (Gidaris et al., 2018). This approach can also be easily modiﬁed to work outside the standard supervised learning setting. If several test samples arrive in a batch, we can use the entire batch for test-time training. If samples arrive in an online stream, we obtain further improvements by keeping the state of the parameters. After all, prediction is rarely a single event. The online version can be the natural mode of deployment under the additional assumption that test samples are produced by the same or smoothly changing distribution shifts. We experimentally validate our method in the context of object recognition on several standard benchmarks. These include images with diverse types of corruption at various levels (Hendrycks & Dietterich, 2019), video frames of moving objects (Shankar et al., 2019), and a new test set of unknown shifts collected by (Recht et al., 2018). Our algorithm makes substantial improvements under distribu- tion shifts, while maintaining the same performance on the original distribution. In our experiments, we compare with a strong baseline (labeled joint training) that uses both supervised and self- supervised learning at training-time, but keeps the model ﬁxed at test time. Recent work shows that training-time self- supervision improves robustness (Hendrycks et al., 2019a); our joint training baseline corresponds to an improved imple- mentation of this work. A comprehensive review of related work follows in Section 5. We complement the empirical results with theoretical inves- tigations in Section 4, and establish an intuitive sufﬁcient condition on a convex model of when Test-Time Training helps; this condition, roughly speaking, is to have correlated gradients between the loss functions of the two tasks. Project website: https://test-time-training.github.io/. arXiv:1909.13231v3  [cs.LG]  1 Jul 2020Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 2. Method This section describes the algorithmic details of our method. To set up notation, consider a standard K-layer neural net- work with parameters θk for layer k. The stacked parameter vector θ = ( θ1,...,θ K) speciﬁes the entire model for a classiﬁcation task with loss function lm(x,y; θ) on the test sample (x,y). We call this the main task, as indicated by the subscript of the loss function. We assume to have training data (x1,y1),..., (xn,yn) drawn i.i.d. from a distribution P. Standard empirical risk minimization solves the optimization problem: min θ 1 n n∑ i=1 lm(xi,yi; θ). (1) Our method requires a self-supervised auxiliary task with loss function ls(x). In this paper, we choose the rotation prediction task (Gidaris et al., 2018), which has been demon- strated to be simple and effective at feature learning for convolutional neural networks. The task simply rotates x in the image plane by one of 0, 90, 180 and 270 degrees and have the model predict the angle of rotation as a four- way classiﬁcation problem. Other self-supervised tasks in Section 5 might also be used for our method. The auxiliary task shares some of the model parameters θe = ( θ1,...,θ κ) up to a certain κ ∈ {1,...,K }. We designate those κlayers as a shared feature extractor. The auxiliary task uses its own task-speciﬁc parameters θs = (θ′ κ+1,...,θ ′ K). We call the unshared parameters θs the self-supervised task branch, and θm = (θκ+1,...,θ K) the main task branch . Pictorially, the joint architecture is a Y-structure with a shared bottom and two branches. For our experiments, the self-supervised task branch has the same architecture as the main branch, except for the output dimensionality of the last layer due to the different number of classes in the two tasks. Training is done in the fashion of multi-task learning (Caru- ana, 1997); the model is trained on both tasks on the same data drawn fromP. Losses for both tasks are added together, and gradients are taken for the collection of all parameters. The joint training problem is therefore min θe,θm,θs 1 n n∑ i=1 lm(xi,yi; θm,θe) + ls(xi; θs,θe). (2) Now we describe the standard version of Test-Time Training on a single test sample x. Simply put, Test-Time Training ﬁne-tunes the shared feature extractor θe by minimizing the auxiliary task loss on x. This can be formulated as min θe ls(x; θs,θe). (3) Denote θ∗ e the (approximate) minimizer of Equation 3. The model then makes a prediction using the updated parameters θ(x) = (θ∗ e,θm). Empirically, the difference is negligible between minimizing Equation 3 over θe versus over both θe and θs. Theoretically, the difference exists only when optimization is done with more than one gradient step. Test-Time Training naturally beneﬁts from standard data augmentation techniques. On each test sample x, we per- form the exact same set of random transformations as for data augmentation during training, to form a batch only con- taining these augmented copies of xfor Test-Time Training. Online Test-Time Training. In the standard version of our method, the optimization problem in Equation 3 is al- ways initialized with parameters θ= (θe,θs) obtained by minimizing Equation 2. After making a prediction on x, θ∗ e is discarded. Outside of the standard supervised learning setting, when the test samples arrive online sequentially, the online version solves the same optimization problem as in Equation 3 to update the shared feature extractor θe. How- ever, on test sample xt, θis instead initialized with θ(xt−1) updated on the previous sample xt−1. This allows θ(xt) to take advantage of the distributional information available in x1,...,x t−1 as well as xt. 3. Empirical Results We experiment with both versions of our method (standard and online) on three kinds of benchmarks for distribution shifts, presented here in the order of visually low to high- level. Our code is available at the project website. Network details. Our architecture and hyper-parameters are consistent across all experiments. We use ResNets (He et al., 2016b), which are constructed differently for CIFAR-10 (Krizhevsky & Hinton, 2009) (26-layer) and Ima- geNet (Russakovsky et al., 2015) (18-layer). The CIFAR-10 dataset contains 50K images for training, and 10K images for testing. The ImageNet contains 1.2M images for train- ing and the 50K validation images are used as the test set. ResNets on CIFAR-10 have three groups, each containing convolutional layers with the same number of channels and size of feature maps; our splitting point is the end of the second group. ResNets on ImageNet have four groups; our splitting point is the end of the third group. We use Group Normalization (GN) instead of Batch Nor- malization (BN) in our architecture, since BN has been shown to be ineffective when training with small batches, for which the estimated batch statistics are not accurate (Ioffe & Szegedy, 2015). This technicality hurts Test-Time Training since each batch only contains (augmented) copies of a single image. Different from BN, GN is not dependent on batch size and achieves similar results on our baselines.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure 1.Test error (%) on CIFAR-10-C with level 5 corruptions.We compare our approaches, Test-Time Training (TTT) and its online version (TTT-Online), with two baselines: object recognition without self-supervision, and joint training with self-supervision but keeping the model ﬁxed at test time. TTT improves over the baselines and TTT-Online improves even further. We report results with BN in Section A4 of the appendix for completeness. We directly compare our architecture to that of Hendrycks et al. (2018) in subsection A4.5. Optimization details. For joint training (Equation 2), we use stochastic gradient descent with standard hyper- parameters as (Huang et al., 2016; He et al., 2016a). For Test-Time Training (Equation 3), we use stochastic gradient descent with the learning rate set to that of the last epoch during training, which is 0.001 in all our experiments. We set weight decay and momentum to zero during Test-Time Training, inspired by practice in (He et al., 2018; Liu et al., 2018). For the standard version of Test-Time Training, we take ten gradient steps, using batches independently gener- ated by the same image. For online version of Test-Time Training, we take only one gradient step given each new im- age. We use random crop and random horizontal ﬂip for data augmentation. See Section A2 of the appendix for computa- tional aspects of our method. In all the tables and ﬁgures, object recognition task onlyrefers to the plain ResNet model (using GN, unless otherwise speciﬁed); joint training refers to the model jointly trained on both the main task and the self-supervised task, ﬁxed at test time; this has been pro- posed as the method in Hendrycks et al. (2019a); Test-Time Training (TTT) refers to the standard version described sec- tion 2; and online Test-Time Training (TTT-Online)refers to the online version that does not discardθ(xt) for xt arriving sequentially from the same distribution. Performance for TTT-Online is calculated as the average over the entire test set; we always shufﬂe the test set before TTT-Online to avoid ordering artifacts. 3.1. Object Recognition on Corrupted Images Hendrycks & Dietterich (2019) propose to benchmark ro- bustness of object recognition with 15 types of corruptions from four broad categories: noise, blur, weather and digital. Each corruption type comes in ﬁve levels of severity, with level 5 the most severe (details and sample images in the ap- pendix). The corruptions are simulated to mimic real-world corruptions as much as possible on copies of the test set for both CIFAR-10 and ImageNet. The new test sets are named as CIFAR-10-C and ImageNet-C, respectively. In the pro- posed benchmark, training should be done on the original training set, and the diversity of corruption types should make it difﬁcult for any methods to work well across the board if it relies too much on corruption speciﬁc knowledge. For online Test-Time Training, we take the entire test set as a stream of incoming images, and update and test on each image in an online manner as it arrives. CIFAR-10-C. Our results on the level 5 corruptions (most severe) are shown in Figure 1. The results on levels 1-4 are shown in Section A4 in appendix. Across all ﬁve levels and 15 corruption types, both standard and online versions of Test-Time Training improve over the object recognition task only baseline by a large margin. The standard version always improves over joint training, and the online version often improves signiﬁcantly (>10%) over joint training and never hurts by more than 0.2%. Speciﬁcally, TTT-Online contributes >24% on the three noise types and 38% on pix- elation. For a learning problem with the seemingly unstable setup that abuses a single image, this kind of consistency is rather surprising. The baseline ResNet-26 with object recognition task only has error 8.9% on the original test set of CIFAR-10. The joint training baseline actually improves performance on the original to 8.1%. More surprisingly, unlike many other methods that trade off original performance for robustness, Test-Time Training further improves on the original test set by 0.2% consistently over multiple independent trials. This suggests that our method does not choose between speciﬁcity and generality.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Accuracy (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online 0 20000 40000 Number of samples 60 62 64 66 68 70 72 74 76Accuracy (%) Original Sliding window average 0 20000 40000 Number of samples 12 15 18 21 24 27 30 33Accuracy (%) Gaussian Noise Sliding window average 0 20000 40000 Number of samples 16 18 20 22 24 26 28 30 32Accuracy (%) Defocus Blur Sliding window average 0 20000 40000 Number of samples 28 30 32 34 36 38Accuracy (%) Zoom Blur Sliding window average 0 20000 40000 Number of samples 33 36 39 42 45 48 51 54Accuracy (%) Fog Sliding window average 0 20000 40000 Number of samples 30 33 36 39 42 45 48 51Accuracy (%) Elastic Transform Sliding window average Figure 2.Test accuracy (%) on ImageNet-C with level 5 corruptions.Upper panel: Our approaches, TTT and TTT-Online, show signiﬁcant improvements in all corruption types over the two baselines. Lower panel: We show the accuracy of TTT-Online as the average over a sliding window of 100 samples; TTT-Online generalizes better as more samples are evaluated (x-axis), without hurting on the original distribution. We use accuracy instead of error here because the baseline performance is very low for most corruptions. Separate from our method, it is interesting to note that joint training consistently improves over the single-task baseline, as discovered by Hendrycks et al. (2019a). Hendrycks & Dietterich (2019) have also experimented with various other training methods on this benchmark, and point to Adversar- ial Logit Pairing (ALP) (Kannan et al., 2018) as the most effective approach. Results of this additional baseline on all levels of CIFAR-10-C are shown in the appendix, along with its implementation details. While surprisingly robust under some of the most severe corruptions (especially the three noise types), ALP incurs a much larger error (by a factor of two) on the original distribution and some corruptions (e.g. all levels of contrast and fog), and hurts performance signiﬁcantly when the corruptions are not as severe (espe- cially on levels 1-3); this kind of tradeoff is to be expected for methods based on adversarial training. ImageNet-C. Our results on the level 5 corruptions (most severe) are shown in Figure 2. We use accuracy instead of error for this dataset because the baseline performance is very low for most corruptions. The general trend is roughly the same as on CIFAR-10-C. The standard version of TTT always improves over the baseline and joint training, while the online version only hurts on the original by 0.1% over the baseline, but signiﬁcantly improves (by a factor of more than three) on many of the corruption types. In the lower panel of Figure 2, we visualize how the accu- racy (averaged over a sliding window) of the online version changes as more images are tested. Due to space constraints, we show this plot on the original test set, as well as every third corruption type, following the same order as in the original paper. On the original test set, there is no visible trend in performance change after updating on the 50,000 samples. With corruptions, accuracy has already risen sig- niﬁcantly after 10,000 samples, but is still rising towards the end of the 50,000 samples, indicating room for additional improvements if more samples were available. Without seeing a single label, TTT-Online behaves as if we were training on the test set from the appearance of the plots.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 Table 1.Test error (%) on CIFAR-10-C with level 5 corruption.Comparison between online Test-Time Training (TTT-Online) and unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019) with access to the entire (unlabeled) test set during training. We highlight the lower error in bold. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. The reported numbers for TTT-Online are the same as in Figure 1. See complete table in Table A2. 0 2000 4000 6000 8000 Number of samples 12 16 20 24 28 32 36 40 44 48Error (%) Gaussian Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 9 12 15 18 21 24 27 30 33 36Error (%) Shot Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 15 20 25 30 35 40 45 50Error (%) Impulse Noise Joint training TTT TTT-Online UDA-SS Figure 3.Test error (%) on CIFAR-10-C, for the three noise types, with gradually changing distribution.The distribution shifts are created by increasing the standard deviation of each noise type from small to large, the further we go on the x-axis. As the samples get noisier, all methods suffer greater errors the more we evaluate into the test set, but online Test-Time Training (TTT-Online) achieves gentler slopes than joint training. For the ﬁrst two noise types, TTT-Online also achieves better results over unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019). Comparison with unsupervised domain adaptation. Table 1 empirically compares online Test-Time Training (TTT-Online) with unsupervised domain adaptation through self-supervision (UDA-SS) (Sun et al., 2019), which is sim- ilar to our method in spirit but is designed for the setting of unsupervised domain adaptation (Section 5 provides a sur- vey of other related work in this setting). Given labeled data from the training distribution and unlabeled data from the test distribution, UDA-SS hopes to ﬁnd an invariant repre- sentation that extracts useful features for both distributions by learning to perform a self-supervised task, speciﬁcally rotation prediction, simultaneously on data from both. It then learns a labeling function on top of the invariant rep- resentation using the labeled data. In our experiments, the unlabeled data given to UDA-SS is the entire test set itself without the labels. Because TTT-Online can only learn from the unlabeled test samples that have already been evaluated on, it is given less information than UDA-SS at all times. In this sense, UDA- SS should be regarded as an oracle rather than a baseline. Surprisingly, TTT-Online outperforms UDA-SS on 13 out of the 15 corruptions as well as the original distribution. Our explanation is that UDA-SS has to ﬁnd an invariant representation for both distributions, while TTT-Online only adapts the representation to be good for the current test distribution. That is, TTT-Online has the ﬂexibility to forget the training distribution representation, which is no longer relevant. This suggests that in our setting, forgetting is not harmful and perhaps should even be taken advantage of. Gradually changing distribution shifts.In our previous experiments, we have been evaluating the online version under the assumption that the test inputs xt for t= 1...nare all sampled from the same test distribution Q, which can be different from the training distribution P. This assumption is indeed satisﬁed for i.i.d. samples from a shufﬂed test set. But here we show that this assumption can in fact be relaxed to allow xt ∼Qt, where Qt is close to Qt+1 (in the sense of distributional distance). We call this the assumption of gradually changing distribution shifts. We perform experiments by simulating such distribution shifts on the three noise types of CIFAR-10-C. For each noise type, xt is corrupted with standard deviation σt, and σ1,...,σ n interpolate between the standard deviation of level 1 and level 5. So xt is more severely corrupted as we evaluate further into the test set and t grows larger. As shown in Figure 3, TTT-Online still improves upon joint training (and our standard version) with this relaxed assumption, and even upon UDA-SS for the ﬁrst two noise types.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Accuracy (%) Airplane Bird Car Dog Cat Horse Ship Average Object recognition task only 67.9 35.8 42.6 14.7 52.0 42.0 66.7 41.4 Joint training (Hendrycks et al., 2019a) 70.2 36.7 42.6 15.5 52.0 44.0 66.7 42.4 TTT (standard version) 70.2 39.2 42.6 21.6 54.7 46.0 77.8 45.2 TTT-Online 70.2 39.2 42.6 22.4 54.7 46.0 77.8 45.4 Table 2.Class-wise and average classiﬁcation accuracy (%) on CIFAR classes in VID-Robust, adapted from (Shankar et al., 2019). Test-Time Training (TTT) and online Test-Time Training (TTT-Online) improve over the two baselines on average, and by a large margin on “ship” and “dog” classes where the rotation task is more meaningful than in classes like “airplane” (sample images in Figure A7). 3.2. Object Recognition on Video Frames The Robust ImageNet Video Classiﬁcation (VID-Robust) dataset was developed by Shankar et al. (2019) from the Ima- geNet Video detection dataset (Russakovsky et al., 2015), to demonstrate how deep models for object recognition trained on ImageNet (still images) fail to adapt well to video frames. The VID-Robust dataset contains 1109 sets of video frames in 30 classes; each set is a short video clip of frames that are similar to an anchor frame. Our results are reported on the anchor frames. To map the 1000 ImageNet classes to the 30 VID-Robust classes, we use the max-conversion function in Shankar et al. (2019). Without any modiﬁcations for videos, we apply our method to VID-Robust on top of the same ImageNet model as in the previous subsection. Our classiﬁcation accuracy is reported in Table 3. In addition, we take the seven classes in VID-Robust that overlap with CIFAR-10, and re-scale those video frames to the size of CIFAR-10 images, as a new test set for the model trained on CIFAR-10 in the previous subsection. Again, we apply our method to this dataset without any modiﬁcations. Our results are shown in Table 2, with a breakdown for each class. Noticing that Test-Time Training does not improve on the airplane class, we inspect some airplane samples (Figure A7), and observe black margins on two sides of most images, which provide a trivial hint for rotation prediction. In addition, given an image of airplanes in the sky, it is often impossible even for humans to tell if it is rotated. This shows that our method requires the self-supervised task to be both well deﬁned and non-trivial. 3.3. CIFAR-10.1: Unknown Distribution Shifts CIFAR-10.1 (Recht et al., 2018) is a new test set of size 2000 modeled after CIFAR-10, with the exact same classes and image dimensionality, following the dataset creation process documented by the original CIFAR-10 paper as closely as possible. The purpose is to investigate the distribution shifts present between the two test sets, and the effect on object recognition. All models tested by the authors suffer a large performance drop on CIFAR-10.1 comparing to CIFAR-10, even though there is no human noticeable difference, and Method Accuracy (%) Object recognition task only 62.7 Joint training (Hendrycks et al., 2019a) 63.5 TTT (standard version) 63.8 TTT-Online 64.3 Table 3.Test accuracy (%) on VID-Robust dataset (Shankar et al., 2019). TTT and TTT-Online improve over the baselines. Method Error (%) Object recognition task only 17.4 Joint training (Hendrycks et al., 2019a) 16.7 TTT (standard version) 15.9 Table 4.Test error (%) on CIFAR-10.1 (Recht et al., 2018). TTT is the ﬁrst method to improve the performance of an existing model on this new test set. both have the same human accuracy. This demonstrates how insidious and ubiquitous distribution shifts are, even when researchers strive to minimize them. The distribution shifts from CIFAR-10 to CIFAR-10.1 pose an extremely difﬁcult problem, and no prior work has been able to improve the performance of an existing model on this new test set, probably because: 1) researchers cannot even identify the distribution shifts, let alone describe them mathematically; 2) the samples in CIFAR-10.1 are only revealed at test time; and even if they were revealed during training, the distribution shifts are too subtle, and the sample size is too small, for domain adaptation (Recht et al., 2018). On the original CIFAR-10 test set, the baseline with only object recognition has error 8.9%, and with joint training has 8.1%; comparing to the ﬁrst two rows of Table 4, both suffer the typical performance drop (by a factor of two). TTT yields an improvement of 0.8% (relative improvement of 4.8%) over joint training. We recognize that this improve- ment is small relative to the performance drop, but see it as an encouraging ﬁrst step for this very difﬁcult problem.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 0 10 20 30 40 50 60 Gradient inner product 0 1 2 3 4 5Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 0 10 20 30 40 50 60 Gradient inner product 0 5 10 15 20 25 30 35Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 Figure 4.Scatter plot of the inner product between the gradients (on the shared feature extractor θe) of the main task lm and the self- supervised task le, and the improvement in test error (%) from Test-Time Training, for the standard (left) and online (right) version. Each point is the average over a test set, and each scatter plot has 75 test sets, from all 15 types of corruptions over ﬁve levels as described in subsection 3.1. The blue lines and bands are the best linear ﬁts and the 99% conﬁdence intervals. The linear correlation coefﬁcients are 0.93 and 0.89 respectively, indicating strong positive correlation between the two quantities, as suggested by Theorem 1. 4. Theoretical Results This section contains our preliminary study of when and why Test-Time Training is expected to work. For convex models, we prove that positive gradient correlation between the loss functions leads to better performance on the main task after Test-Time Training. Equipped with this insight, we then empirically demonstrate that gradient correlation governs the success of Test-Time Training on the deep learning model discussed in Section 3. Before stating our main theoretical result, we ﬁrst illustrate the general intuition with a toy model. Consider a regression problem where x∈Rd denotes the input, y1 ∈R denotes the label, and the objective is the square loss (ˆy−y1)2/2 for a prediction ˆy. Consider a two layer linear network parametrized by A∈Rh×d and v ∈Rh (where hstands for the hidden dimension). The prediction according to this model is ˆy= v⊤Ax, and the main task loss is lm(x,y1; A,v) = 1 2 ( y1 −v⊤Ax )2 . (4) In addition, consider a self-supervised regression task that also uses the square loss and automatically generates a label ys for x. Let the self-supervised head be parametrized by w∈Rh. Then the self-supervised task loss is ls(x,y2; A,w) = 1 2 ( y2 −w⊤Ax )2 . (5) Now we apply Test-Time Training to update the shared feature extractor Aby one step of gradient descent on ls, which we can compute with y2 known. This gives us A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (6) where A′is the updated matrix and ηis the learning rate. If we set η= η∗where η∗= y1 −v⊤Ax (y2 −w⊤Ax) v⊤wx⊤x, (7) then with some simple algebra, it is easy to see that the main task loss lm(x,y1; A′,v) = 0. Concretely, Test-Time Training drives the main task loss down to zero with a single gradient step for a carefully chosen learning rate. In prac- tice, this learning rate is unknown since it depends on the unknown y1. However, since our model is convex, as long as η∗is positive, it sufﬁces to set η to be a small positive constant (see details in the appendix). If x̸= 0, one sufﬁ- cient condition for η∗to be positive (when neither loss is zero) is to have sign ( y1 −v⊤Ax ) = sign ( y2 −w⊤Ax ) (8) and v⊤w>0 . (9) For our toy model, both parts of the condition above have an intuition interpretation. The ﬁrst part says that the mistakes should be correlated, in the sense that predictions from both tasks are mistaken in the same direction. The second part, v⊤w>0, says that the decision boundaries on the feature space should be correlated. In fact, these two parts hold iff. ⟨∇lm(A),∇ls(A)⟩>0 (see a simple proof of this fact in the appendix). To summarize, if the gradients have positive correlation, Test-Time Training is guaranteed to reduce the main task loss. Our main theoretical result extends this to general smooth and convex loss functions.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Theorem 1. Let lm(x,y; θ) denote the main task loss on test instance x,y with parameters θ, and ls(x; θ) the self- supervised task loss that only depends onx. Assume that for all x,y, lm(x,y; θ) is differentiable, convex andβ-smooth in θ, and both ∥∇lm(x,y; θ)∥,∥∇ls(x,θ)∥≤ Gfor all θ. With a ﬁxed learning rate η= ϵ βG2 , for every x,y such that ⟨∇lm(x,y; θ),∇ls(x; θ)⟩>ϵ, (10) we have lm(x,y; θ) >lm(x,y; θ(x)), (11) where θ(x) = θ−η∇ls(x; θ) i.e. Test-Time Training with one step of gradient descent. The proof uses standard techniques in optimization, and is left for the appendix. Theorem 1 reveals gradient correlation as a determining factor of the success of Test-Time Training in the smooth and convex case. In Figure 4, we empirically show that our insight also holds for non-convex loss func- tions, on the deep learning model and across the diverse set of corruptions considered in Section 3; stronger gradient cor- relation clearly indicates more performance improvement over the baseline. 5. Related Work Learning on test instances. Shocher et al. (2018) pro- vide a key inspiration for our work by showing that image super-resolution could be learned at test time simply by try- ing to upsample a downsampled version of the input image. More recently, Bau et al. (2019) improve photo manipula- tion by adapting a pre-trained GAN to the statistics of the input image. One of the earlier examples of this idea comes from Jain & Learned-Miller (2011), who improve Viola- Jones face detection (Viola et al., 2001) by bootstrapping the more difﬁcult faces in an image from the more easily detected faces in that same image. The online version of our algorithm is inspired by the work of Mullapudi et al. (2018), which makes video segmentation more efﬁcient by using a student model that learns online from a teacher model. The idea of online updates has also been used in Kalal et al. (2011) for tracking and detection. A recent work in echocardiography (Zhu et al., 2019) improves the deep learning model that tracks myocardial motion and cardiac blood ﬂow with sequential updates. Lastly, we share the philosophy of transductive learning (Vapnik, 2013; Gam- merman et al., 1998), but have little in common with their classical algorithms; recent work by Tripuraneni & Mackey (2019) theoretically explores this for linear prediction, in the context of debiasing the LASSO estimator. Self-supervised learning studies how to create labels from the data, by designing various pretext tasks that can learn semantic information without human annotations, such as context prediction (Doersch et al., 2015), solving jig- saw puzzles (Noroozi & Favaro, 2016), colorization (Lars- son et al., 2017; Zhang et al., 2016), noise prediction (Bo- janowski & Joulin, 2017), feature clustering (Caron et al., 2018). Our paper uses rotation prediction (Gidaris et al., 2018). Asano et al. (2019) show that self-supervised learn- ing on only a single image, surprisingly, can produce low- level features that generalize well. Closely related to our work, Hendrycks et al. (2019a) propose that jointly training a main task and a self-supervised task (our joint training baseline in Section 3) can improve robustness on the main task. The same idea is used in few-shot learning (Su et al., 2019), domain generalization (Carlucci et al., 2019), and unsupervised domain adaptation (Sun et al., 2019). Adversarial robustness studies the robust risk RP,∆(θ) = Ex,y∼P maxδ∈∆ l(x + δ,y; θ), where l is some loss function, and ∆ is the set of perturbations; ∆ is often chosen as the Lp ball, for p ∈{1,2,∞}. Many popular algorithms formulate and solve this as a robust optimization problem (Goodfellow et al., 2014; Madry et al., 2017; Sinha et al., 2017; Raghunathan et al., 2018; Wong & Kolter, 2017; Croce et al., 2018), and the most well known technique is adversarial training. Another line of work is based on randomized smoothing (Cohen et al., 2019; Salman et al., 2019), while some other approaches, such as input transformations (Guo et al., 2017; Song et al., 2017), are shown to be less effective (Athalye et al., 2018). There are two main problems with the approaches above. First, all of them can be seen as smoothing the decision boundary. This establishes a theoretical tradeoff between accuracy and robustness (Tsipras et al., 2018; Zhang et al., 2019), which we also observe empirically with our adversarial training baseline in Section 3. Intuitively, the more diverse ∆ is, the less effective this one-boundary-ﬁts-all approach can be for a particular element of ∆. Second, adversarial methods rely heavily on the mathematical structure of ∆, which might not accurately model perturbations in the real world. Therefore, generalization remains hard outside of the ∆ we know in advance or can mathematically model, especially for non-adversarial distribution shifts. Empirically, Kang et al. (2019) shows that robustness for one ∆ might not transfer to another, and training on the L∞ball actually hurts robustness on the L1 ball. Non-adversarial robustness studies the effect of corrup- tions, perturbations, out-of-distribution examples, and real- world distribution shifts (Hendrycks et al., 2019b;a; 2018; Hendrycks & Gimpel, 2016). Geirhos et al. (2018) show that training on images corrupted by Gaussian noise makes deep learning models robust to this particular noise type, but does not improve performance on images corrupted by another noise type e.g. salt-and-pepper noise.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Unsupervised domain adaptation (a.k.a. transfer learn- ing) studies the problem of distribution shifts, when an unlabeled dataset from the test distribution (target domain) is available at training time, in addition to a labeled dataset from the training distribution (source domain) (Chen et al., 2011; Gong et al., 2012; Long et al., 2015; Ganin et al., 2016; Long et al., 2016; Tzeng et al., 2017; Hoffman et al., 2017; Csurka, 2017; Chen et al., 2018). The limitation of the problem setting, however, is that generalization might only be improved for this speciﬁc test distribution, which can be difﬁcult to anticipate in advance. Prior work try to anticipate broader distributions by using multiple and evolv- ing domains (Hoffman et al., 2018; 2012; 2014). Test-Time Training does not anticipate any test distribution, by chang- ing the setting of unsupervised domain adaptation, while taking inspiration from its algorithms. Our paper is a follow- up to Sun et al. (2019), which we explain and empirically compare with in Section 3. Our update rule can be viewed as performing one-sample unsupervised domain adaptation on the ﬂy, with the caveat that standard domain adaptation techniques might become ill-deﬁned when there is only one sample from the target domain. Domain generalization studies the setting where a meta distribution generates multiple environment distributions, some of which are available during training (source), while others are used for testing (target) (Li et al., 2018; Shankar et al., 2018; Muandet et al., 2013; Balaji et al., 2018; Ghifary et al., 2015; Motiian et al., 2017; Li et al., 2017a; Gan et al., 2016). With only a few environments, information on the meta distribution is often too scarce to be helpful, and with many environments, we are back to the i.i.d. setting where each environment can be seen as a sample, and a strong baseline is to simply train on all the environments (Li et al., 2019). The setting of domain generalization is limited by the inherent tradeoff between speciﬁcity and generality of a ﬁxed decision boundary, and the fact that generalization is again elusive outside of the meta distribution i.e. the actual P learned by the algorithm. One (few)-shot learning studies how to learn a new task or a new classiﬁcation category using only one (or a few) sample(s), on top of a general representation that has been learned on diverse samples (Snell et al., 2017; Vinyals et al., 2016; Fei-Fei et al., 2006; Ravi & Larochelle, 2016; Li et al., 2017b; Finn et al., 2017; Gidaris & Komodakis, 2018). Our update rule can be viewed as performing one-shot self- supervised learning and can potentially be improved by progress in one-shot learning. Continual learning (a.k.a. learning without forgetting) studies the setting where a model is made to learn a sequence of tasks, and not forget about the earlier ones while training for the later (Li & Hoiem, 2017; Lopez-Paz & Ranzato, 2017; Kirkpatrick et al., 2017; Santoro et al., 2016). In contrast, with Test-Time Training, we are not concerned about forgetting the past test samples since they have already been evaluated on; and if a past sample comes up by any chance, it would go through Test-Time Training again. In addition, the impact of forgetting the training set is minimal, because both tasks have already been jointly trained. Online learning (a.k.a. online optimization) is a well- studied area of learning theory (Shalev-Shwartz et al., 2012; Hazan et al., 2016). The basic setting repeats the following: receive xt, predict ˆyt, receive yt from a worst-case oracle, and learn. Final performance is evaluated using the regret, which colloquially translates to how much worse the online learning algorithm performs in comparison to the best ﬁxed model in hindsight. In contrast, our setting never reveals any yt during testing even for the online version, so we do not need to invoke the concept of the worst-case oracle or the regret. Also, due to the lack of feedback from the envi- ronment after predicting, our algorithm is motivated to learn (with self-supervision) before predicting ˆyt instead of after. Note that some of the previously covered papers (Hoffman et al., 2014; Jain & Learned-Miller, 2011; Mullapudi et al., 2018) use the term “online learning” outside of the learning theory setting, so the term can be overloaded. 6. Discussion The idea of test-time training also makes sense for other tasks, such as segmentation and detection, and in other ﬁelds, such as speech recognition and natural language process- ing. For machine learning practitioners with prior domain knowledge in their respective ﬁelds, their expertise can be leveraged to design better special-purpose self-supervised tasks for test-time training. Researchers for general-purpose self-supervised tasks can also use test-time training as an evaluation benchmark, in addition to the currently prevalent benchmark of pre-training and ﬁne-tuning. More generally, we hope this paper can encourage re- searchers to abandon the self-imposed constraint of a ﬁxed decision boundary for testing, or even the artiﬁcial division between training and testing altogether. Our work is but a small step toward a new paradigm where much of the learning happens after a model is deployed. Acknowledgements. This work is supported by NSF grant 1764033, DARPA and Berkeley DeepDrive. This paper took a long time to develop, and beneﬁted from con- versations with many of our colleagues, including Ben Recht and his students Ludwig Schmidt, Vaishaal Shanker and Becca Roelofs; Ravi Teja Mullapudi, Achal Dave and Deva Ramanan; and Armin Askari, Allan Jabri, Ashish Kumar, Angjoo Kanazawa and Jitendra Malik.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts References Asano, Y . M., Rupprecht, C., and Vedaldi, A. Surprising effectiveness of few-image unsupervised feature learning. arXiv preprint arXiv:1904.13132, 2019. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security: Circumvent- ing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. Balaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems, pp. 998–1008, 2018. Bau, D., Strobelt, H., Peebles, W., Wulff, J., Zhou, B., Zhu, J.-Y ., and Torralba, A. Semantic photo manipulation with a generative image prior. ACM Transactions on Graphics (TOG), 38(4):59, 2019. Bojanowski, P. and Joulin, A. Unsupervised learning by predicting noise. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 517– 526. JMLR. org, 2017. Carlucci, F. M., D’Innocente, A., Bucci, S., Caputo, B., and Tommasi, T. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pp. 2229–2238, 2019. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 132–149, 2018. Caruana, R. Multitask learning. Machine learning, 28(1): 41–75, 1997. Chen, M., Weinberger, K. Q., and Blitzer, J. Co-training for domain adaptation. In Advances in neural information processing systems, pp. 2456–2464, 2011. Chen, X., Sun, Y ., Athiwaratkun, B., Cardie, C., and Wein- berger, K. Adversarial deep averaging networks for cross- lingual sentiment classiﬁcation. Transactions of the Asso- ciation for Computational Linguistics, 6:557–570, 2018. Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed adversarial robustness via randomized smoothing. arXiv preprint arXiv:1902.02918, 2019. Croce, F., Andriushchenko, M., and Hein, M. Provable robustness of relu networks via maximization of linear regions. arXiv preprint arXiv:1810.07481, 2018. Csurka, G. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374, 2017. Ding, G. W., Wang, L., and Jin, X. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422–1430, 2015. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594–611, 2006. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–1135. JMLR. org, 2017. Gammerman, A., V ovk, V ., and Vapnik, V . Learning by transduction. In Proceedings of the Fourteenth conference on Uncertainty in artiﬁcial intelligence , pp. 148–155. Morgan Kaufmann Publishers Inc., 1998. Gan, C., Yang, T., and Gong, B. Learning attributes equals multi-source domain generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 87–97, 2016. Ganin, Y ., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V . Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. In Advances in Neural Information Processing Systems, pp. 7538–7550, 2018. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., and Balduzzi, D. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pp. 2551– 2559, 2015. Gidaris, S. and Komodakis, N. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, B., Shi, Y ., Sha, F., and Grauman, K. Geodesic ﬂow kernel for unsupervised domain adaptation. In2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2066–2073. IEEE, 2012.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun- tering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017. Hazan, E. et al. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2(3-4):157– 325, 2016. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a. He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016b. He, K., Girshick, R., and Doll ´ar, P. Rethinking imagenet pre-training. arXiv preprint arXiv:1811.08883, 2018. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. Using trusted data to train deep networks on labels cor- rupted by severe noise. InAdvances in neural information processing systems, pp. 10456–10465, 2018. Hendrycks, D., Lee, K., and Mazeika, M. Using pre-training can improve model robustness and uncertainty. arXiv preprint arXiv:1901.09960, 2019a. Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Improving model robustness and uncertainty estimates with self-supervised learning. arXiv preprint, 2019b. Hoffman, J., Kulis, B., Darrell, T., and Saenko, K. Discover- ing latent domains for multisource domain adaptation. In European Conference on Computer Vision, pp. 702–715. Springer, 2012. Hoffman, J., Darrell, T., and Saenko, K. Continuous man- ifold based adaptation for evolving visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 867–874, 2014. Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y ., Isola, P., Saenko, K., Efros, A. A., and Darrell, T. Cycada: Cycle- consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017. Hoffman, J., Mohri, M., and Zhang, N. Algorithms and theory for multiple-source adaptation. In Advances in Neural Information Processing Systems, pp. 8246–8256, 2018. Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep networks with stochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Jain, V . and Learned-Miller, E. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR 2011, pp. 577–584. IEEE, 2011. Kalal, Z., Mikolajczyk, K., and Matas, J. Tracking-learning- detection. IEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422, 2011. Kang, D., Sun, Y ., Brown, T., Hendrycks, D., and Steinhardt, J. Transfer of adversarial robustness between perturbation types. arXiv preprint arXiv:1905.01034, 2019. Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Larsson, G., Maire, M., and Shakhnarovich, G. Colorization as a proxy task for visual understanding. In CVPR, 2017. Li, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 5542–5550, 2017a. Li, D., Zhang, J., Yang, Y ., Liu, C., Song, Y .-Z., and Hospedales, T. M. Episodic training for domain gen- eralization. arXiv preprint arXiv:1902.00113, 2019. Li, Y ., Tian, X., Gong, M., Liu, Y ., Liu, T., Zhang, K., and Tao, D. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624–639, 2018.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Li, Z. and Hoiem, D. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Li, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017b. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. Long, M., Cao, Y ., Wang, J., and Jordan, M. I. Learn- ing transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015. Long, M., Zhu, H., Wang, J., and Jordan, M. I. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136–144, 2016. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017. Motiian, S., Piccirilli, M., Adjeroh, D. A., and Doretto, G. Uniﬁed deep supervised domain adaptation and gen- eralization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5715–5725, 2017. Muandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pp. 10– 18, 2013. Mullapudi, R. T., Chen, S., Zhang, K., Ramanan, D., and Fatahalian, K. Online model distillation for efﬁcient video inference. arXiv preprint arXiv:1812.02699, 2018. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision , pp. 69–84. Springer, 2016. Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018. Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. IEEE transactions on pattern analysis and machine intelligence, 2016. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen- shteyn, I., and Bubeck, S. Provably robust deep learn- ing via adversarially trained smoothed classiﬁers. arXiv preprint arXiv:1906.04584, 2019. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. Meta-learning with memory-augmented neu- ral networks. In International conference on machine learning, pp. 1842–1850, 2016. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Shankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S., Jyothi, P., and Sarawagi, S. Generalizing across domains via cross-gradient training. arXiv preprint arXiv:1804.10745, 2018. Shankar, V ., Dave, A., Roelofs, R., Ramanan, D., Recht, B., and Schmidt, L. Do image classiﬁers generalize across time? arXiv, 2019. Shocher, A., Cohen, N., and Irani, M. zero-shot super- resolution using deep internal learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3118–3126, 2018. Sinha, A., Namkoong, H., and Duchi, J. Certifying some dis- tributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077–4087, 2017. Song, Y ., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017. Su, J.-C., Maji, S., and Hariharan, B. Boosting supervi- sion with self-supervision for few-shot learning. arXiv preprint arXiv:1906.07079, 2019. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint, 2019.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Tripuraneni, N. and Mackey, L. Debiasing linear prediction. arXiv preprint arXiv:1908.02341, 2019. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7167–7176, 2017. Vapnik, V .The nature of statistical learning theory. Springer science & business media, 2013. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. Viola, P., Jones, M., et al. Rapid object detection using a boosted cascade of simple features. CVPR (1), 1(511- 518):3, 2001. Wong, E. and Kolter, J. Z. Provable defenses against adver- sarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017. Zhang, H., Yu, Y ., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jor- dan, M. I. Theoretically principled trade-off between ro- bustness and accuracy. arXiv preprint arXiv:1901.08573, 2019. Zhang, R., Isola, P., and Efros, A. A. Colorful image col- orization. In European conference on computer vision, pp. 649–666. Springer, 2016. Zhu, W., Huang, Y ., Vannan, M. A., Liu, S., Xu, D., Fan, W., Qian, Z., and Xie, X. Neural multi-scale self-supervised registration for echocardiogram dense tracking. arXiv preprint arXiv:1906.07357, 2019.Appendix: Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A1. Informal Discussion on Our Variable Decision Boundary In the introduction, we claim that in traditional supervised learning θgives a ﬁxed decision boundary, while ourθgives a variable decision boundary. Here we informally discuss this claim. Denote the input space Xand output space Y. A decision boundary is simply a mapping f : X →Y. Let Θ be a model class e.g Rd. Now consider a family of parametrized functions gθ : X→Y , where θ∈Θ. In the context of deep learning, gis the neural network architecture and θcontains the parameters. We say that f is a ﬁxed decision boundary w.r.t. g and Θ if there exists θ ∈Θ s.t. f(x) = gθ(x) for every x ∈X , and a variable decision boundary if for every x∈X, there exists θ∈Θ s.t. f(x) = gθ(x). Note how selection of θcan depend on xfor a variable decision boundary, and cannot for a ﬁxed one. It is then trivial to verify that our claim is true under those deﬁnitions. A critical reader might say that with an arbitrarily large model class, can’t every decision boundary be ﬁxed? Yes, but this is not the end of the story. Let d = dim( X) × dim(Y), and consider the enormous model class Θ′= Rd which is capable of representing all possible mappings be- tween Xand Y. Let g′ θ′ simply be the mapping represented by θ′ ∈Θ′. A variable decision boundary w.r.t. g and Θ then indeed must be a ﬁxed decision boundary w.r.t. g′and Θ′, but we would like to note two things. First, without any prior knowledge, generalization in Θ′is impossible with any ﬁnite amount of training data; reasoning about g′and Θ′is most likely not productive from an algorithmic point of view, and the concept of a variable decision boundary is to avoid such reasoning. Second, selecting θbased on xfor a variable decision boundary can be thought of as “training” on all points x ∈Rd; however, “training” only happens when necessary, for the xthat it actually encounters. Altogether, the concept of a variable decision boundary is different from what can be described by traditional learning theory. A formal discussion is beyond the scope of this paper and might be of interest to future work. A2. Computational Aspects of Our Method At test time, our method is 2 × batch size × number of iterations times slower than regular test- ing, which only performs a single forward pass for each sample. As the ﬁrst work on Test-Time Training, this paper is not as concerned about computational efﬁciency as improving robustness, but here we provide two poten- tial solutions that might be useful, but have not been thor- oughly veriﬁed. The ﬁrst is to use the thresholding trick on ls, introduced as a solution for the small batches prob- lem in the method section. For the models considered in our experiments, roughly 80% of the test instances fall below the threshold, so Test-Time Training can only be performed on the other 20% without much effect on per- formance, because those 20% contain most of the sam- ples with wrong predictions. The second is to reduce the number of iterations of test-time updates. For the online version, the number of iterations is al- ready 1, so there is nothing to do. For the standard ver- sion, we have done some preliminary experiments setting number of iterations to 1 (instead of 10) and learn- ing rate to 0.01 (instead of 0.001), and observing results almost as good as the standard hyper-parameter setting. A more in depth discussion on efﬁciency is left for future works, which might, during training, explicitly make the model amenable to fast updates. A3. Proofs Here we prove the theoretical results in the main paper. A3.1. The Toy Problem The following setting applies to the two lemmas; this is simply the setting of our toy problem, reproduced here for ease of reference.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Consider a two layer linear network parametrized by A∈ Rh×d (shared) and v,w ∈Rh (ﬁxed) for the two heads, respectively. Denote x∈Rd the input and y1,y2 ∈R the labels for the two tasks, respectively. For the main task loss lm(A; v) = 1 2 ( y1 −v⊤Ax )2 , (12) and the self-supervised task loss ls(A; w) = 1 2 ( y2 −w⊤Ax )2 , (13) Test-Time Training yields an updated matrix A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (14) where ηis the learning rate. Lemma 1. Following the exposition of the main paper, let η∗= (y1 −v⊤Ax) (y2 −w⊤Ax)v⊤wx⊤x. (15) Assume η∗∈[ϵ,∞) for some ϵ> 0. Then for any η∈(0,ϵ], we are guaranteed an improvement on the main loss i.e. lm(A′) <lm(A). Proof. From the exposition of the main paper, we know that lm(A−η∗∇lsA)) = 0, which can also be derived from simple algebra. Then by convexity, we have lm(A−η∇ls(A)) (16) = lm (( 1 − η η∗ ) A+ η η∗(A−η∗∇ls(A)) ) (17) ≤ ( 1 − η η∗ ) lm(A) + 0 (18) ≤ ( 1 −η ϵ ) lm(A) (19) <lm(A), (20) where the last inequality uses the assumption that lm(A) > 0, which holds because η∗>0. Lemma 2. Deﬁne ⟨U,V⟩= vec (U)⊤vec (V) i.e. the Frobenious inner product, then sign (η∗) = sign (⟨∇lm(A),∇ls(A)⟩) . (21) Proof. By simple algebra, ⟨∇lm(A),∇ls(A)⟩ = ⟨ ( y1 −v⊤Ax )( −vx⊤) , ( y2 −w⊤Ax )( −wx⊤) ⟩ = ( y1 −v⊤Ax )( y2 −w⊤Ax ) Tr ( xv⊤wx⊤) = ( y1 −v⊤Ax )( y2 −w⊤Ax ) v⊤wx⊤x, which has the same sign as η∗. A3.2. Proof of Theorem 1 For any η, by smoothness and convexity, lm(x,y; θ(x)) = lm(x,y; θ−η∇ls(x; θ)) ≤lm(x,y; θ) + η⟨∇lm(x,y; θ),∇ls(x,θ)⟩ + η2β 2 ∥∇ls(x; θ)∥2 . Denote η∗= ⟨∇lm(x,y; θ),∇ls(x,θ)⟩ β∥∇ls(x; θ)∥2 . Then Equation 22 becomes lm(x,y; θ−η∗∇ls(x; θ)) (22) ≤lm(x,y; θ) −⟨∇lm(x,y; θ),∇ls(x,θ)⟩2 2β∥∇ls(x; θ)∥2 . (23) And by our assumptions on the gradient norm and gradient inner product, lm(x,y; θ) −lm(x,y; θ−η∗∇ls(x; θ)) ≥ ϵ2 2βG2 . (24) Because we cannot observe η∗in practice, we instead use a ﬁxed learning rate η = ϵ βG2 , as stated in Theorem 1. Now we argue that this ﬁxed learning rate still improves performance on the main task. By our assumptions, η∗ ≥ ϵ βG2 , so η ∈(0,η∗]. Denote g= ∇ls(x; θ), then by convexity of lm, lm(x,y; θ(x)) = lm(x,y; θ−ηg) (25) = lm ( x,y; ( 1 − η η∗ ) θ+ η η∗(θ−η∗g) ) (26) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗lm(x,y; θ−η∗g) (27) Combining with Equation 24, we have lm(x,y; θ(x)) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗ ( lm(x,y; θ) − ϵ2 2βG2 ) = lm(x,y; θ) − η η∗ ϵ2 2βG2 Since η/η∗>0, we have shown that lm(x,y; θ) −lm(x,y; θ(x)) >0. (28)Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A4. Additional Results on the Common Corruptions Dataset For table aethetics, we use the following abbreviations: B for baseline, JT for joint training, TTT for Test-Time Train- ing standard version, and TTT-Online for online Test-Time Training i.e. the online version. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. A4.1. Results Using Batch Normalization As discussed in the results section, Batch Normalization (BN) is ineffective for small batches, which are the inputs for Test-Time Training (both standard and online version) since there is only one sample available when forming each batch; therefore, our main results are based on a ResNet using Group Normalization (GN). Figure A2 and Table A1 show results of our method on CIFAR-10-C level 5, with a ResNet using Batch Normalization (BN). These results are only meant to be a point of reference for the curious readers. In the early stage of this project, we have experimented with two potential solutions to the small batches problem with BN. The naive solution is to ﬁx the BN layers during Test-Time Training. but this diminishes the performance gains since there are fewer shared parameters. The better solution, adopted for the results below, is hard example mining: instead of updating on all inputs, we only update on inputs that incur large self-supervised task loss ls, where the large improvements might counter the negative effects of inaccurate statistics. Test-Time Training (standard version) is still very effective with BN. In fact, some of the improvements are quite dra- matic, such as on contrast (34%), defocus blue (18%) and Gaussian noise (22% comparing to joint-training, and 16% comparing to the baseline). Performance on the original distribution is still almost the same, and the original error with BN is in fact slightly lower than with GN, and takes half as many epochs to converge. We did not further experiment with BN because of two rea- sons: 1) The online version does not work with BN, because the problem with inaccurate batch statistics is exacerbated when training online for many (e.g. 10000) steps. 2) The baseline error for almost every corruption type is signiﬁ- cantly higher with BN than with GN. Although unrelated to the main idea of our paper, we make the interesting note that GN signiﬁcantly improves model robustness. A4.2. Additional Baseline: Adversarial Logit Pairing As discussed in the results section, Hendrycks & Dietterich (2019) point to Adversarial Logit Pairing (ALP) (Kannan et al., 2018) as an effective method for improving model robustness to corruptions and perturbations, even though it was designed to defend against adversarial attacks. We take ALP as an additional baseline on all benchmarks based on CIFAR-10 (using GN), following the training proce- dure in Kannan et al. (2018) and their recommended hyper- parameters. The implementation of the adversarial attack comes from the codebase of Ding et al. (2019). We did not run ALP on ImageNet because the two papers we reference for this method, Kannan et al. (2018) and Hendrycks & Di- etterich (2019), did not run on ImageNet or make any claim or recommendation. A4.3. Results on CIFAR-10-C and ImageNet-C, Level 5 Table A2 and Table A3 correspond to the bar plots in the results section. Two rows of Table A2 have been presented as Table 1 in the main text. A4.4. Results on CIFAR-10-C, Levels 1-4 The following bar plots and tables are on levels 1-4 of CIFAR-10-C. The original distribution is the same for all levels, so are our results on the original distribution. A4.5. Direct Comparison with Hendrycks et al. (2019a) The following comparison has been requested by an anony- mous reviewer for our ﬁnal version. Our joint training baseline is based on Hendrycks et al. (2019a), but also incor- porates some architectural changes (see below). We found these changes improved the robustness of our method, and felt that it was important to give the baseline the same ben- eﬁt. Note that our joint training baseline overall performs better than Hendrycks: Compare Table S2 to Figure 3 of Hendrycks et al. (2019a) (provided by the authors), our baseline has average error of 22.8% across all corruptions and levels, while their average error is 28.6%. Summary of architectural changes: 1) Group Normalization (GN) instead of Batch Normalization (BN). For complete- ness, the results with BN are provided in Table S1; c.f. GN results in Table S2 which signiﬁcantly improves robustness, with or without self-supervision. 2) We split after the sec- ond residual group, while they split after the third residual group right before the linear layer. This consistently gives about 0.5% - 1% improvement. 3) We use a ResNet-26, while they use a 40-2 Wide ResNet. But our baseline still performs better than their method even though our network is 4x smaller, due to the two tricks above.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure A1.Sample images from the Common Corruptions Benchmark, taken from the original paper by Hendrycks & Dietterich (2019). originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT Figure A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 7.9 63.9 58.8 64.3 46.3 54.6 41.6 45.9 31.9 44.0 37.5 13.0 69.2 33.8 61.4 31.7 JT 7.5 70.7 65.6 67.2 43.1 55.4 40.9 42.7 30.3 44.5 42.5 12.7 58.6 30.7 62.6 31.9 TTT 7.9 47.9 45.2 54.8 27.6 50.4 31.5 30.9 28.7 34.3 26.9 12.6 35.2 30.6 51.2 31.3 Table A1.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 50.5 47.2 56.1 23.7 51.7 24.3 26.3 25.6 34.4 28.1 13.5 25.0 27.4 55.8 29.8 JT 8.1 49.4 45.3 53.4 24.2 48.5 24.8 26.4 25.0 32.5 27.5 12.6 25.3 24.0 51.6 28.7 TTT 7.9 45.6 41.8 50.0 21.8 46.1 23.0 23.9 23.9 30.0 25.1 12.2 23.9 22.6 47.2 27.2 TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 ALP 16.5 22.7 22.9 28.3 25.0 25.6 27.4 23.1 25.2 27.2 64.8 21.7 73.6 23.0 20.2 18.9 Table A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 68.9 1.3 2.0 1.3 7.5 6.6 11.8 16.2 15.7 14.9 15.3 43.9 9.7 16.5 15.3 23.4 JT 69.1 2.1 3.1 2.1 8.7 6.7 12.3 16.0 15.3 15.8 17.0 45.3 11.0 18.4 19.7 22.9 TTT 69.0 3.1 4.5 3.5 10.1 6.8 13.5 18.5 17.1 17.9 20.0 47.0 14.4 20.9 22.8 25.3 TTT-Online 68.8 26.3 28.6 26.9 23.7 6.6 28.7 33.4 35.6 18.7 47.6 58.3 35.3 44.3 47.8 44.3 Table A3.Test accuracy (%) on ImageNet-C, level 5, ResNet-18.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A3.Test error (%) on CIFAR-10-C, level 4. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 46.4 39.2 44.8 15.3 52.5 19.1 20.5 21.3 26.9 13.3 10.5 13.7 20.8 35.3 26.9 JT 8.1 45.0 38.3 42.2 16.4 50.2 20.7 20.5 21.1 25.4 14.1 10.0 14.7 19.0 33.2 25.1 TTT 7.9 41.5 35.4 39.8 15.0 47.8 19.1 18.4 20.1 24.0 13.5 10.0 14.1 17.7 29.4 24.5 TTT-Online 8.2 22.9 20.0 23.9 11.2 35.1 15.6 13.8 18.6 15.9 12.3 9.7 11.9 16.7 13.6 19.8 ALP 16.5 21.3 20.5 24.5 20.7 25.9 23.7 21.4 24.2 23.9 42.2 17.5 53.7 22.1 19.1 18.5 Table A4.Test error (%) on CIFAR-10-C, level 4, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A4.Test error (%) on CIFAR-10-C, level 3. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 42.2 35.1 30.7 12.2 41.7 18.6 17.5 19.0 25.3 10.8 9.7 11.6 15.3 21.7 24.6 JT 8.1 40.2 34.4 29.9 12.2 37.9 20.8 17.3 18.4 25.0 11.4 9.2 12.0 15.2 20.8 22.8 TTT 7.9 37.2 31.6 28.6 11.5 35.8 19.1 15.8 17.8 23.3 11.0 9.1 11.6 14.3 18.9 22.3 TTT-Online 8.2 21.3 17.7 17.9 9.0 23.4 15.3 12.5 16.4 15.8 10.9 9.0 10.7 12.8 12.2 18.7 ALP 16.5 20.0 19.3 20.5 19.2 21.2 24.0 20.5 20.9 24.2 30.1 16.6 39.6 20.9 17.8 18.0 Table A5.Test error (%) on CIFAR-10-C, level 3, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A5.Test error (%) on CIFAR-10-C, level 2. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 31.7 22.6 24.3 9.9 42.6 14.9 14.7 21.7 18.4 9.8 9.1 10.0 13.1 17.1 22.4 JT 8.1 31.0 22.6 23.4 9.1 39.2 16.4 14.2 21.2 17.5 9.4 8.3 10.6 12.8 15.9 20.5 TTT 7.9 28.8 20.7 23.0 9.0 36.6 15.4 13.1 20.2 16.9 9.2 8.3 10.2 12.5 14.8 19.7 TTT-Online 8.2 16.8 13.8 15.5 8.5 23.4 13.3 11.5 16.8 12.7 9.4 8.4 9.7 12.4 11.5 17.0 ALP 16.5 18.0 17.2 19.0 17.8 20.7 21.2 19.3 19.0 20.1 22.4 16.3 29.2 20.3 17.4 17.8 Table A6.Test error (%) on CIFAR-10-C, level 2, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A6.Test error (%) on CIFAR-10-C, level 1. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 21.7 17.1 17.0 9.0 44.0 12.1 13.9 14.3 13.4 9.2 8.9 9.0 13.2 12.0 17.3 JT 8.1 20.4 16.6 16.9 8.2 40.5 12.2 13.0 13.1 12.3 8.4 8.1 8.5 12.9 11.3 15.9 TTT 7.9 19.1 15.8 16.5 8.0 37.9 11.7 12.2 12.8 11.9 8.2 8.0 8.3 12.6 11.1 15.5 TTT-Online 8.2 13.8 11.9 12.2 8.5 24.4 10.5 11.5 12.4 10.7 8.5 8.3 8.6 12.4 10.7 14.4 ALP 17.0 16.8 17.6 16.8 20.9 18.7 19.0 17.3 17.5 17.4 16.1 18.4 20.4 17.0 17.2 17.5 Table A7.Test error (%) on CIFAR-10-C, level 1, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Figure A7.Sample Images from the VID-Robust dataset (Shankar et al., 2019) in the results section adapted to CIFAR-10. Each row shows eight sample images from one class. The seven classes shown are, in order: airplane, bird, car, dog, cat, horse, ship.",
      "meta_data": {
        "arxiv_id": "1909.13231v3",
        "authors": [
          "Yu Sun",
          "Xiaolong Wang",
          "Zhuang Liu",
          "John Miller",
          "Alexei A. Efros",
          "Moritz Hardt"
        ],
        "published_date": "2019-09-29T08:09:15Z",
        "pdf_url": "https://arxiv.org/pdf/1909.13231v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Test-Time Training (TTT), a novel approach to improve the generalization of predictive models under distribution shifts. TTT converts a single unlabeled test sample into a self-supervised learning problem, updating model parameters at test time before making a prediction. This method extends naturally to online data streams. Key findings include substantial performance improvements on diverse image classification benchmarks (e.g., CIFAR-10-C, ImageNet-C, VID-Robust, CIFAR-10.1) under various distribution shifts, while maintaining or even slightly improving performance on the original data distribution. It also provides theoretical insights, demonstrating that positive gradient correlation between the main task and the self-supervised task is a key factor for TTT's success in convex models, a finding empirically validated for deep learning models.",
        "methodology": "The core methodology involves adapting model parameters to a test sample via a self-supervised auxiliary task. The model architecture uses a Y-structure: a shared feature extractor (bottom layers, θe) and two task-specific branches for the main classification task (θm) and the self-supervised task (θs). Initially, the model is trained jointly on both tasks using multi-task learning on the original data (e.g., ImageNet, CIFAR-10). At test time, for a given unlabeled test sample 'x': (1) **Standard TTT**: The shared feature extractor θe is fine-tuned by minimizing the self-supervised auxiliary task loss (ls) on augmented copies of 'x' for ten gradient steps, starting from the pre-trained parameters. Then, a prediction is made using the updated θe and fixed θm. (2) **Online TTT**: For sequential test samples 'xt', θe is initialized with the parameters updated from the previous sample 'xt-1' and takes only one gradient step per new image. The self-supervised task used is rotation prediction, where the model predicts the angle (0, 90, 180, or 270 degrees) an input image has been rotated. Group Normalization (GN) is used instead of Batch Normalization (BN) due to the small batch size (single image) during test-time training.",
        "experimental_setup": "Experiments were conducted on various object recognition benchmarks designed to evaluate robustness to distribution shifts. The models used were ResNet-26 for CIFAR-10/CIFAR-10-C and ResNet-18 for ImageNet/ImageNet-C, using Group Normalization. Datasets included: (1) **CIFAR-10-C and ImageNet-C**: Corrupted versions of CIFAR-10 and ImageNet, featuring 15 types of corruptions across five severity levels (e.g., noise, blur, weather, digital). (2) **VID-Robust**: Video frames dataset from Shankar et al. (2019), derived from ImageNet Video detection, evaluating generalization to video frames. (3) **CIFAR-10.1**: A new test set for CIFAR-10 designed to expose subtle, unknown distribution shifts. Baselines for comparison included 'object recognition task only' (plain ResNet), 'joint training' (supervised + self-supervised training, fixed at test time), Adversarial Logit Pairing (ALP) for CIFAR-10-C, and Unsupervised Domain Adaptation by Self-Supervision (UDA-SS) as an oracle baseline. Performance was measured using test error (%) for CIFAR-10-C and CIFAR-10.1, and test accuracy (%) for ImageNet-C and VID-Robust.",
        "limitations": "The current Test-Time Training (TTT) method incurs a significant computational overhead, being `2 * batch_size * number_of_iterations` times slower than standard testing, prioritizing robustness over efficiency. The effectiveness of the self-supervised task is crucial; it must be well-defined and non-trivial for the specific dataset (e.g., rotation prediction is less effective on images like airplanes with ambiguous rotations or black margins that provide trivial hints). While theoretical results on gradient correlation are proven for convex models and empirically observed in deep learning, a formal proof for non-convex deep learning models is not provided. The online version of TTT does not work effectively with Batch Normalization due to issues with inaccurate batch statistics when training online for many steps. Furthermore, the observed improvement on CIFAR-10.1, while novel, is relatively small compared to the overall performance drop, indicating the extreme difficulty of unseen, subtle distribution shifts.",
        "future_research_directions": "Future work could explore designing better special-purpose self-supervised tasks tailored to specific domains like segmentation, detection, speech recognition, and natural language processing, leveraging domain expertise. Test-Time Training could also serve as a new evaluation benchmark for general-purpose self-supervised tasks, complementing existing pre-training and fine-tuning benchmarks. The authors encourage researchers to abandon the traditional self-imposed constraint of a fixed decision boundary at test time and the artificial division between training and testing, envisioning a paradigm where learning largely occurs post-deployment. Further research is needed to improve the computational efficiency of TTT, potentially through techniques like thresholding based on self-supervised loss, reducing the number of test-time iterations, or designing models explicitly amenable to fast updates during training. A formal theoretical discussion on the concept of a variable decision boundary is also suggested as a direction for future work, as is improving TTT by incorporating advancements in one-shot learning."
      }
    },
    {
      "title": "Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection"
    },
    {
      "title": "TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation",
      "abstract": "This paper proposes a novel batch normalization strategy for test-time\nadaptation. Recent test-time adaptation methods heavily rely on the modified\nbatch normalization, i.e., transductive batch normalization (TBN), which\ncalculates the mean and the variance from the current test batch rather than\nusing the running mean and variance obtained from the source data, i.e.,\nconventional batch normalization (CBN). Adopting TBN that employs test batch\nstatistics mitigates the performance degradation caused by the domain shift.\nHowever, re-estimating normalization statistics using test data depends on\nimpractical assumptions that a test batch should be large enough and be drawn\nfrom i.i.d. stream, and we observed that the previous methods with TBN show\ncritical performance drop without the assumptions. In this paper, we identify\nthat CBN and TBN are in a trade-off relationship and present a new test-time\nnormalization (TTN) method that interpolates the statistics by adjusting the\nimportance between CBN and TBN according to the domain-shift sensitivity of\neach BN layer. Our proposed TTN improves model robustness to shifted domains\nacross a wide range of batch sizes and in various realistic evaluation\nscenarios. TTN is widely applicable to other test-time adaptation methods that\nrely on updating model parameters via backpropagation. We demonstrate that\nadopting TTN further improves their performance and achieves state-of-the-art\nperformance in various standard benchmarks.",
      "full_text": "Published as a conference paper at ICLR 2023 TTN: A D OMAIN -SHIFT AWARE BATCH NORMALIZA - TION IN TEST-TIME ADAPTATION Hyesu Lim1,2∗, Byeonggeun Kim ∗, Jaegul Choo 2, Sungha Choi 1‡ 1Qualcomm AI Research†, 2KAIST ABSTRACT This paper proposes a novel batch normalization strategy for test-time adaptation. Recent test-time adaptation methods heavily rely on the modiﬁed batch normal- ization, i.e., transductive batch normalization (TBN), which calculates the mean and the variance from the current test batch rather than using the running mean and variance obtained from source data,i.e., conventional batch normalization (CBN). Adopting TBN that employs test batch statistics mitigates the performance degra- dation caused by the domain shift. However, re-estimating normalization statistics using test data depends on impractical assumptions that a test batch should be large enough and be drawn from i.i.d. stream, and we observed that the previous meth- ods with TBN show critical performance drop without the assumptions. In this paper, we identify that CBN and TBN are in a trade-off relationship and present a new test-time normalization(TTN) method that interpolates the standardization statistics by adjusting the importance between CBN and TBN according to the domain-shift sensitivity of each BN layer. Our proposed TTN improves model robustness to shifted domains across a wide range of batch sizes and in various realistic evaluation scenarios. TTN is widely applicable to other test-time adap- tation methods that rely on updating model parameters via backpropagation. We demonstrate that adopting TTN further improves their performance and achieves state-of-the-art performance in various standard benchmarks. 1 I NTRODUCTION When we deploy deep neural networks (DNNs) trained on the source domain into test environments (i.e., target domains), the model performance on the target domain deteriorates due to the domain shift from the source domain. For instance, in autonomous driving, a well-trained DNNs model may exhibit signiﬁcant performance degradation at test time due to environmental changes, such as camera sensors, weather, and region (Choi et al., 2021; Lee et al., 2022; Kim et al., 2022b). Test-time adaptation (TTA) has emerged to tackle the distribution shift between source and target domains during test time (Sun et al., 2020; Wang et al., 2020). Recent TTA approaches (Wang et al., 2020; Choi et al., 2022; Liu et al., 2021) address this issue by 1) (re-)estimating normaliza- tion statistics from current test input and 2) optimizing model parameters in unsupervised manner, such as entropy minimization (Grandvalet & Bengio, 2004; Long et al., 2016; Vu et al., 2019) and self-supervised losses (Sun et al., 2020; Liu et al., 2021). In particular, the former focused on the weakness of conventional batch normalization (CBN) (Ioffe & Szegedy, 2015) for domain shift in a test time. As described in Fig. 1(b), when standardizing target feature activations using source statistics, which are collected from the training data, the activations can be transformed into an un- intended feature space, resulting in misclassiﬁcation. To this end, the TTA approaches (Wang et al., 2020; Choi et al., 2022; Wang et al., 2022) have heavily depended on the direct use of test batch statistics to ﬁx such an invalid transformation in BN layers, called transductive BN (TBN) (Nado et al., 2020; Schneider et al., 2020; Bronskill et al., 2020) (see Fig. 1(c)). The approaches utilizing TBN showed promising results but have mainly been assessed in limited evaluation settings (Wang et al., 2020; Choi et al., 2022; Liu et al., 2021). For instance, such evalua- tion settings assume large test batch sizes (e.g., 200 or more) and a single stationary distribution shift ∗Work completed while at Qualcomm Technologies, Inc. ‡Corresponding author. †Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. 1 arXiv:2302.05155v2  [cs.CV]  18 Feb 2023Published as a conference paper at ICLR 2023 0 20 40 60 80 100 CBN TBN Ours TENT TENT+Ours SWR SWR+Ours Error rate (%) 200 64 16 4 2 1 TTN(Ours) TENT+TTN(Ours) SWR+TTN(Ours) Test batch size (a) Valid output using CBN (b) Invalid output using CBN (c) Valid output using TBN (d) Performance drops in small test batches using TBN ● Class A ● Class B Source mean● Source features Test features⨯ Test meanStandardize Figure 1: Trade-off between CBN & TBN.In conceptual illustrations (a), (b), and (c), the depicted standardization only considers making the feature distribution have a zero mean, disregarding mak- ing it have unit variance. When the source and test distributions are different, and the test batch size is large, (b) test features can be wrongly standardized when using CBN (Ioffe & Szegedy, 2015), but (c) TBN (Nado et al., 2020) can provide a valid output. (d) Error rates (↓) on shifted domains (CIFAR-10-C). TBN and TBN applied (TENT (Wang et al., 2020), SWR (Choi et al., 2022)) meth- ods suffer from severe performance drop when the batch size becomes small, while TTN (Ours) improves overall performance. (i.e., single corruption). Recent studies suggest more practical evaluation scenarios based on small batch sizes (Mirza et al., 2022; Hu et al., 2021; Khurana et al., 2021) or continuously changing data distribution during test time (Wang et al., 2022). We show that the performance of existing methods signiﬁcantly drops once their impractical assumptions of the evaluation settings are violated. For example, as shown in Fig. 1(d), TBN (Nado et al., 2020) and TBN applied methods suffer from severe performance drop when the test batch size becomes small, while CBN is irrelevant to the test batch sizes. We identify that CBN and TBN are in a trade-off relationship (Fig. 1), in the sense that one of each shows its strength when the other falls apart. To tackle this problem, we present a novel test-time normalization (TTN)strategy that controls the trade-off between CBN and TBN by adjusting the importance of source and test batch statistics according to the domain-shift sensitivity of each BN layer. Intuitively, we linearly interpolate be- tween CBN and TBN so that TBN has a larger weight than CBN if the standardization needs to be adapted toward the test data. We optimize the interpolating weight after the pre-training but before the test time, which we refer to as the post-training phase. Speciﬁcally, given a pre-trained model, we ﬁrst estimate channel-wise sensitivity of the afﬁne parameters in BN layers to domain shift by analyzing the gradients from the back-propagation of two input images, clean input and its aug- mented one (simulating unseen distribution). Afterward, we optimize the interpolating weight using the channel-wise sensitivity replacing BN with the TTN layers. It is noteworthy that none of the pre-trained model weights are modiﬁed, but we only train newly added interpolating weight. We empirically show that TTN outperforms existing TTA methods in realistic evaluation settings, i.e., with a wide range of test batch sizes for single, mixed, and continuously changing domain adaptation through extensive experiments on image classiﬁcation and semantic segmentation tasks. TTN as a stand-alone method shows compatible results with the state-of-the-art methods and com- bining our TTN with the baselines even boosts their performance in overall scenarios. Moreover, TTN applied methods ﬂexibly adapt to new target domains while sufﬁciently preserving the source knowledge. No action other than computing per batch statistics (which can be done simultaneously to the inference) is needed in test-time; TTN is compatible with other TTA methods without requir- ing additional computation cost. Our contributions are summarized as follows: • We propose a novel domain-shift aware test-time normalization (TTN) layer that combines source and test batch statistics using channel-wise interpolating weights considering the sensitivity to domain shift in order to ﬂexibly adapt to new target domains while preserving the well-trained source knowledge. 2Published as a conference paper at ICLR 2023 Per-batch        Frozen        Optimize S Standardize Ƹ𝑧 = 𝑧𝑖𝑛 −𝜇 𝜎 T Affine Transform 𝑧𝑜𝑢𝑡 = 𝛾⋅ Ƹ𝑧+𝛽 … … CBN CBN CBN … Pre-train (CBN) (a-1) Post-train (TTN)          Test time (TTN) (a) Overall procedure of train and test phases (b) Comparison of BN layers 𝑧𝑖𝑛 S Ƹ𝑧 T 𝑧𝑜𝑢𝑡 𝛾,𝛽+ 1−𝛼 𝜇𝑠,𝜎𝑠𝜇𝑖𝑛,𝜎𝑖𝑛 𝑧𝑖𝑛 S Ƹ𝑧 T 𝑧𝑜𝑢𝑡 𝜇𝑠,𝜎𝑠 𝛾,𝛽 CBN in test time TBN in test time (b-1) TTN(Ours) in post-train 𝑧𝑖𝑛 S Ƹ𝑧 T 𝑧𝑜𝑢𝑡 𝜇𝑖𝑛,𝜎𝑖𝑛 𝛾,𝛽 𝛼 𝑧𝑖𝑛 S Ƹ𝑧 T 𝑧𝑜𝑢𝑡 𝛾,𝛽+ 1−𝛼 𝜇𝑠,𝜎𝑠𝜇𝑖𝑛,𝜎𝑖𝑛 (b-2) TTN(Ours) in test time 𝛼 Figure 2: Method overview. (a) We introduce an additional training phase between pre-train and test time called (a-1) post-training phase. (b) Our proposed TTN layer combines per-batch statistics and frozen source statistics with interpolating weight α, which is (b-1) optimized in post-training phase and (b-2) ﬁxed in test time. • To show the broad applicability of our proposed TTN, which does not alter training or test- time schemes, we show that adding TTN to existing TTA methods signiﬁcantly improves the performance across a wide range of test batch sizes (from 200 to 1) and in three realistic evaluation scenarios; stationary, continuously changing, and mixed domain adaptation. • We evaluate our method through extensive experiments on image classiﬁcation using CIFAR-10/100-C, and ImageNet-C (Hendrycks & Dietterich, 2018) and semantic segmen- tation task using CityScapes (Cordts et al., 2016), BDD-100K (Yu et al., 2020), Mapil- lary (Neuhold et al., 2017), GTA V (Richter et al., 2016), and SYNTHIA (Ros et al., 2016). 2 M ETHODOLOGY In this section, we describe our method, the Test-Time Normalization(TTN) layer, whose design is suitable for test-time adaptation (TTA) in practical usages out of the large batch size and i.i.d assumptions during a test time. We ﬁrst deﬁne the problem setup in Section 2.1 and present our pro- posed TTN layers in Section 2.2. Finally, we discuss how we optimize TTN layers in Sections 2.3. 2.1 P ROBLEM SETUP Let the train and test data be DS and DT and the corresponding probability distributions be PS and PT, respectively, where DS and DT share the output space, i.e., {yi}∼D S = {yi}∼D T. The covariate shift in TTA is deﬁned asPS(x) ̸= PT(x) where PS(y|x) =PT(y|x) (Quinonero-Candela et al., 2008). A model, fθ, with parameters θ, is trained with a mini-batch, BS = {(xi,yi)}|BS| i=1 , from source data DS, where xi is an example and yi is the corresponding label. During the test, fθ encounters a test batch BT ∼DT, and the objective of TTA is correctly managing the test batch from the different distribution. To simulate more practical TTA, we mainly consider two modiﬁcations: (1) various test batch sizes, |BT|, where small batch size indicates small latency while handling the test data online, and (2) multi, N-target domains, DT = {DT,i}N i=1. Under this setting, each test batch BT is drawn by one of the test domains inDT, where DT may consist of a single target domain, multiple target domains, or mixture of target domains. 2.2 T EST-TIME NORMALIZATION LAYER We denote an input of a BN layer as z ∈RBCHW , forming a mini-batch size of B. The mean and variance of z are µand σ2, respectively, which are computed as follows: µc = 1 BHW B∑ b H∑ h W∑ w zbchw, σ 2 c = 1 BHW B∑ b H∑ h W∑ w (zbchw −µc)2, (1) where µand σ2 are in RC, and C, H, and W stand for the number of channels, dimension of height, and that of width, respectively. Based on µand σ2, the source statistics µs,σ2 s ∈RC are usually estimated with exponential moving average over the training data. 3Published as a conference paper at ICLR 2023 Conv CBN ReLU Conv ReLU Conv CBN ReLU FC… CBN 𝑥′ ℒ𝐶𝐸 (a-1) Gradient of affine parameters 𝛾,𝛽 ∇𝛾 (1),∇𝛽 (1) ∇𝛾 (2),∇𝛽 (2) ∇𝛾 (𝐿),∇𝛽 (𝐿) ∇𝛾′ (1),∇𝛽′ (1) ∇𝛾′ (2),∇𝛽′ (2) ∇𝛾′ (𝐿),∇𝛽′ (𝐿) Conv CBN ReLU Conv ReLU Conv CBN ReLU FC… CBN 𝑥 ℒ𝐶𝐸 (a-2) Prior 𝒜 (b-1) Initialize 𝛼 with prior 𝒜 … 1       0 (a) Obtain prior 𝒜 (b) Optimize 𝛼 Per-batch statistics        Frozen source statistics        Gradient flow Conv ReLU Conv ReLU Conv ReLU FC…𝑥′ ℒ𝐶𝐸 +ℒ𝑀𝑆𝐸 TTNTTNTTN 𝛼 1−𝛼 TTN 𝜇𝑠 (𝑙) 𝜇𝑖𝑛 (𝑙) 𝐶𝑙 𝛼(𝑙,𝑐) + 1−𝛼(𝑙,𝑐) … … ++ 𝛼(𝑙,𝐶𝑙) 1−𝛼(𝑙,𝐶𝑙) (b-2) Optimize 𝛼 C1 C2 C𝐿 Figure 3: Two stages in post-training phase. (a) Given a pre-trained model, which uses CBN, and its training data, we obtain prior knowledge of each BN layer. (a-1) We ﬁrst compute gradients of afﬁne parameters in each BN layer from clean x and augmented input x′and obtain the gradient distance score (Eq. 4). (a-2) For BN layers with larger distance score, we put more importance on current batch statistics than source statistics ( i.e., large α), and we deﬁne prior Aaccordingly (Eq. 5). (b) After obtaining prior A, we substitute BN layers from CBN to TTN.(b-1) Initializing α with prior A, (b-2) we optimize αusing CE and MSE loss (Eq. 6) with augmented training data x′. In BN layers, input z is ﬁrst standardized with statistics µand σ2 and then is scaled and shifted with learnable parameters γ and β in RC. The standardization uses current input batch statistics during training and uses estimated source statistics µs and σ2 s at test time (Fig. 2(b)). To address domain shifts in test time, we adjust the source statistics by combining the source and the test mini-batch statistics (Singh & Shrivastava, 2019; Summers & Dinneen, 2019) with a learnable interpolating weight α∈RC ranges [0,1]. Precisely, TTN standardizes a feature with ˜µ= αµ+ (1−α)µs, ˜σ2 = ασ2 + (1−α)σ2 s + α(1 −α)(µ−µs)2, (2) while using the same afﬁne parameters, γ and β. Note that we have different mixing ratios αc for every layer and channel. 2.3 P OST TRAINING Like Choi et al. (2022), we introduce an additional training phase, the post-training (after pre- training but before testing), to optimize the mixing parameters α in Eq. 2 (Fig. 2(a)). Note that all parameters except αare frozen and we have access to the labeled source data during the post- training. We ﬁrst obtain prior knowledge Aof αby identifying which layers and their channels are sensitive to domain shifts. Then, we optimizeαwith the prior knowledge and an additional objective term. The overall procedure is depicted in Fig. 3 and the pseudocode is provided in appendix A.3. Obtain Prior A. To identify which BN layers and corresponding channels are sensitive to domain shifts, we simulate the domain shifts by augmenting1 the clean image, i.e., original training data, and make a pair of (clean x, domain-shifted x′) images, where the semantic information is shared. To analyze in which layer and channel the standardization statistics should be corrected, we consider the standardized features ˆz(l,c) of z(l,c), for a channel index cat a layer l, whose input is clean x. We compare ˆz(l,c) to that of domain-shifted one, ˆz′(l,c) from x′. Since the pre-trained CBN uses the same µ(l,c) s and σ(l,c) s for both inputs, the difference between ˆz(l,c) and ˆz′(l,c) is caused by the domain discrepancy between xand x′. We argue that if the difference is signiﬁcant, the parameter at (l,c) is sensitive to the domain shift, i.e., intensely affected by the domain shift, and hence the standardization statistics at (l,c) should be adapted towards the shifted input. Drawing inspiration from Choi et al. (2022), we measure the domain-shift sensitivity by comparing gradients. Since the standardized feature ˆz is scaled and shifted by γ and β in each BN layer, we compare the gradients of afﬁne parameters γ and β, ∇γ and ∇β, respectively, to measure the dissimilarity of ˆzand ˆz′. As described in Fig. 3(a-1), we collect the ∇γ and ∇β using cross-entropy 1It is noteworthy that the post-training phase is robust to the choice of data augmentation types. Ablation study results and discussions are provided in the appendix B.4. 4Published as a conference paper at ICLR 2023 loss, LCE. To this end, we introduce a gradient distance score, d(l,c) ∈R for each channel cat layer las follows: s= 1 N N∑ i=1 gi ·g′ i ∥gi∥∥g′ i∥, (3) d(l,c) = 1−1 2 ( s(l,c) γ + s(l,c) β ) , (4) where (g,g′) is (∇(l,c) γ ,∇(l,c) γ′ ) and (∇(l,c) β ,∇(l,c) β′ ) for s(l,c) γ and s(l,c) β , respectively,N is the number of training data, and the resulting d(l,c) ∈[0,1]. Once we obtain sγ and sβ from Eq. 3, we conduct min-max normalization over all s(l,c) γ and s(l,c) β , before computing Eq. 4. To magnify the relative difference, we take the square as a ﬁnal step and denote the result as a prior A(Fig. 3(a-2)): A= [d(1,.),d(2,.),...,d (L,.)]2, (5) where d(l,.) = [d(l,c)]Cl c=1. Optimize α. The goal of optimizing α is to make the combined statistics correctly standardize the features when the input is sampled from an arbitrary target domain. After obtaining the prior A, we replace CBN with TTN layers while keeping the afﬁne parameters. Then, we initialize the interpolating weights α with A, which represents in which layer and channel the standardization statistics need to be adapted using test batch statistics (see Fig. 3(b-1)). To simulate distribution shifts, we use augmented training data. Expecting the model to make consistent predictions either given clean or augmented inputs, we use cross-entropy loss LCE. Furthermore, to prevent αfrom moving too far from the initial value A, we use mean-squared error loss LMSE between αand the prior A, i.e., LMSE = ∥α−A∥2 as a constraint. Total lossLcan be written asL= LCE +λLMSE (6), where λis a weighting hyperparameter (Details are provided in the appendix A.1 & B.1). 3 E XPERIMENTS In image classiﬁcation, we evaluate TTN for corruption robustness in realistic evaluation settings, i.e., where the test batch size can be variant and where the target domain can be either stationary, continuously changing, or mixed with multiple domains. Additionally, we further validate TTN on domain generalization benchmarks incorporating natural domain shifts ( e.g., changes in camera sensors, weather, time, and region) in semantic segmentation. 3.1 E XPERIMENTAL SETUP Given models pre-trained on clean source data, we optimize TTN parameter αwith the augmented source data in the post-training phase. Afterward, we evaluate our post-trained model on the cor- rupted target data. Implementation details are provided in the appendix A.1. Datasets and models. We use corruption benchmark datasets CIFAR-10/100-C and ImageNet-C, which consist of 15 types of common corruptions at ﬁve severity levels (Hendrycks & Dietterich, 2018). Each corruption is applied to test images of the clean datasets (CIFAR-10/100 and Ima- geNet). We use a training set of the clean dataset for post-training and the corrupted dataset for evaluation. As backbone models, we used WideResNet-40-2 (Hendrycks et al., 2019) trained on CIFAR-10/100, and ResNet-50 (He et al., 2016) trained on ImageNet. To validate our method in se- mantic segmentation, we conduct experiments on Cityscapes (Cordts et al., 2016), BDD-100K (Yu et al., 2020), Mapillary (Neuhold et al., 2017), GTA V (Richter et al., 2016), and SYNTHIA (Ros et al., 2016) datasets, in accordance with the experimental setup for domain generalization proposed in RobustNet (Choi et al., 2021). Baselines. To demonstrate that TTN successfully controls the trade-off between CBN and TBN, we compare TTN with (1) AdaptiveBN (Schneider et al., 2020), (2) α-BN (You et al., 2021) and (3) MixNorm (Hu et al., 2021), which combines or takes the moving average of the source and the test batch statistics with a pre-deﬁned hyperparameter ( i.e., a constant α). The following baselines are suggested on top of TBN (Nado et al., 2020); (4) TENT (Wang et al., 2020) optimizes BN afﬁne parameters via entropy minimization. (5) SWR (Choi et al., 2022) updates the entire model parame- ters considering the domain-shift sensitivity. (6) CoTTA (Wang et al., 2022) ensembles the output of 5Published as a conference paper at ICLR 2023 Table 1: Single domain adaptation on corruption benchmark. Error rate ( ↓) averaged over 15 corruptions with severity level 5 using WideResNet-40-2 as a backbone for each test batch size. We used reported results of MixNorm with ﬁxed parameters from the original paper and denoted as ∗. In appendix B.3, we provide variants of TTN, which show stronger performance for small test batch. CIFAR-10-C CIFAR-100-C Method 200 64 16 4 2 1 Avg. 200 64 16 4 2 1 Avg. Source (CBN)18.27 18.27 18.27 18.27 18.27 18.27 18.27 46.75 46.75 46.75 46.75 46.75 46.7546.75 Norm TBN 14.49 15.02 17.10 26.28 35.65 90.00 33.09 39.25 40.21 44.03 59.10 80.65 99.0460.38 AdaptiveBN12.21 12.31 12.89 14.51 15.79 16.14 13.98 36.56 36.85 38.19 41.18 43.2644.0140.01 α-BN 13.78 13.77 13.89 14.54 15.1615.4714.44 39.72 39.85 39.99 41.3442.6645.6441.53 MixNorm∗ 13.85 14.41 14.23 14.60 (B=5) - 15.0914.44 - - - - - - - Ours (TTN)11.6711.8012.13 13.93 15.8317.99 13.8935.5835.8436.7341.0846.6757.7142.27 Optim. TENT 12.08 14.78 16.90 25.61 35.69 90.00 32.51 35.52 39.90 43.78 59.02 80.68 99.0259.65 +Ours (TTN)11.2811.5212.04 13.95 15.8417.9413.77 35.1635.5736.5541.1846.6358.3342.24 SWR 10.26 13.51 16.61 27.33 40.48 90.04 33.04 32.6837.41 43.15 59.90 87.07 99.0559.88 +Ours (TTN)9.92 11.7713.41 18.02 24.0961.5623.13 32.8635.1338.6649.8060.7280.9049.68 Table 2: Continuously changing domain adaptation on corruption benchmark. Error rate (↓) averaged over 15 corruptions with severity level 5 using WideResNet-40-2 as backbone for each test batch size. We omitted ‘Norm’ methods results in this table since they are eqaul to that of Table 1. CIFAR-10-C CIFAR-100-C Method 200 64 16 4 2 1 Avg. 200 64 16 4 2 1 Avg. Source (CBN)18.27 18.27 18.27 18.27 18.27 18.2718.27 46.75 46.75 46.75 46.75 46.75 46.7546.75 Ours (TTN)11.6711.80 12.13 13.9315.8317.9913.89 35.58 35.84 36.73 41.08 46.67 57.71 42.27 Optim. CoTTA 12.46 14.60 21.26 45.69 58.87 90.0040.48 39.75 42.20 52.94 73.69 87.66 98.9965.87 TENT 12.54 13.52 15.69 26.23 35.77 90.0032.29 36.11 37.90 43.78 58.71 81.76 99.0459.55 +Ours (TTN)11.4411.60 12.08 16.1418.3622.4015.33 43.50 37.60 38.28 44.60 54.2980.63 49.82 SWR 11.04 11.53 13.90 23.99 34.02 90.0030.75 34.16 35.79 40.71 58.15 80.55 99.0362.56 +Ours (TTN)10.09 10.5111.28 14.2916.6784.1224.49 33.0934.07 36.15 42.41 53.63 93.08 48.74 augmented test inputs, updates the entire model parameters using a consistency loss between student and teacher models, and stochastically restores the pre-trained model. We refer to TBN, (1), (2), and (3) as normalization-based methods (Norm), the other as optimization-based methods (Optim.), and denote the pre-trained model using CBN as ‘source’. Evaluation scenarios. To show that TTN performs robust on various test batch sizes, we conduct experiments with test batch sizes of 200, 64, 16, 4, 2, and 1. We evaluate our method in three evalu- ation scenarios; single, continuously changing, and mixed domain adaptation. In the single domain adaptation, the model is optimized for one corruption type and then reset before adapting to the subsequent corruption, following the evaluation setting from TENT and SWR. In the continuously changing adaptation (Wang et al., 2022), the model is continuously adapted to 15 corruption types (w/o the reset), which is more realistic because it is impractical to precisely indicate when the data distribution has shifted in the real world. Finally, to simulate the non-stationary target domain where various domains coexist, we evaluate methods in the mixed domain adaptation setting, where a sin- gle batch contains multiple domains. We use a severity level of 5 (Hendrycks & Dietterich, 2018) for all experiments. It is noteworthy that we use a single checkpoint of TTN parameter αfor each dataset across all experimental settings. 3.2 E XPERIMENTS ON IMAGE CLASSIFICATION Tables 1, 2, and 3 show error rates on corruption benchmark datasets in three different evaluation scenarios; single domain, continuously changing, and mixed domain adaptation, respectively. Note that the performance of normalization-based methods in the single (Table 1) and in the continu- ously changing (Table 2) settings are identical. Tables 4 and 5 show the adaptation performance on the source and class imbalanced target domains, respectively. More results and discussions are provided in the appendix B, importantly, including results on ImageNet-C (B.5). Robustness to practical settings. In Table 1, 2, and 3, TTN and TTN applied methods show robust performance over the test batch size ranges from 200 to 1. Comparing with normalization-based baselines, we demonstrate that TTN, which uses channel-wisely optimized combining rateα, shows better results than deﬁning α as a constant hyperparameter, which can be considered as a special 6Published as a conference paper at ICLR 2023 Table 3: Mixed domain adaptation on corruption benchmark. Error rate (↓) of mixed domain with severity level 5 using WideResNet-40-2 as backbone for each test batch size. We used the reported results of MixNorm with ﬁxed parameters from the original paper and denoted them as ∗. CIFAR-10-C CIFAR-100-C Method 200 64 16 4 2 1 Avg. 200 64 16 4 2 1 Avg. Source (CBN)18.27 18.27 18.27 18.27 18.27 18.2718.27 46.75 46.75 46.75 46.75 46.75 46.7546.75 Norm TBN 14.99 15.29 17.38 26.65 35.59 90.0033.31 39.88 40.48 43.73 59.11 80.30 98.9160.40 AdaptiveBN12.62 12.48 12.97 14.59 15.74 16.0214.07 36.88 36.86 38.49 41.43 43.38 44.3140.23 α-BN 13.78 13.78 13.99 14.6115.07 15.2014.41 40.25 40.11 40.47 41.6442.39 43.8141.45 MixNorm∗ 18.80 18.80 18.80 18.80 18.80 18.8018.80 - - - - - - - Ours (TTN)12.1612.1912.3413.9615.5517.8314.00 36.2436.2336.8541.0145.8555.5241.95 Optim. TENT 14.33 14.97 17.30 26.07 35.37 90.0033.01 39.36 40.01 43.33 58.98 80.55 98.9260.19 +Ours (TTN)12.0212.0412.2013.7715.4216.4013.64 36.2936.2336.8941.3846.6557.9542.57 SWR 13.24 13.06 16.57 26.08 38.65 91.0359.54 37.84 37.93 44.37 59.50 78.66 98.9533.10 +Ours (TTN)11.8911.6513.3717.0523.5064.1050.29 36.4936.5139.6046.2058.2084.7623.59 case of TTN; TBN and α-BN corresponds to α = 1and 0.1, respectively. More comparisons with different constant αare provided in the appendix B.2. It is noteworthy that TTN as a stand-alone method favorably compares with optimization-based baselines in all three scenarios. Table 4: Source domain adaptation. Error rate (↓) on CIFAR-10 using WideResNet-40-2. Method Test batch size Avg.200 64 16 4 2 1 Source (CBN)4.92 4.92 4.92 4.92 4.92 4.924.92 Norm TBN 6.41 6.60 8.64 17.65 26.08 90.0025.90Ours (TTN)4.885.115.357.27 9.45 9.96 7.00 Optim. TENT 6.15 6.45 8.61 17.61 26.20 90.0032.2+Ours (TTN)4.935.115.327.22 9.3810.217.02 SWR 5.63 6.01 8.25 17.49 26.32 90.0025.62+Ours (TTN)4.795.025.516.68 7.91 9.34 6.54 Table 5: Class imbalanced target domain. Error rate (↓) averaged over 15 corruptions of CIFAR- 10-C with severity level 5 using WideResNet-40- 2. Details are provided in the appendix A.2. Method Test batch size Avg.200 64 16 4 2 1 Source (CBN)18.27 18.27 18.27 18.27 18.27 18.2718.27 TBN 77.60 76.66 77.72 78.59 77.84 90.0079.74Ours (TTN)35.7535.1334.9232.5128.6017.9930.82 Adopting TTN improves other TTA methods. We compare optimization-based methods with and without TTN layers. Since TENT, SWR, and CoTTA optimize model parameters on top of using TBN layers, they also suffer from performance drops when the test batch size becomes small. Adopting TTN reduces the dependency on large test batch size, i.e., makes robust to small batch size, and even improves their performance when using large test batch. Furthermore, in continual (Table 2) and mixed domain (Table 3) adaptation scenario, TENT and SWR shows higher error rate than in single domain (Table 1) adaptation. We interpret that because they update the model parameters based on the current output and predict the next input batch using the updated model, the model will not perform well if the consecutive batches have different corruption types ( i.e., mixed domain adaptation). Moreover, the error from the previous input batch propagates to the future input stream, and thus they may fall apart rapidly once they have a strongly wrong signal, which can happen in continual adaptation ( i.e., long-term adaptation without resetting). Applying TTN signiﬁcantly accelerates their model performance regardless of the evaluation scenarios. TTN preserves knowledge on source domain. In practice, data driven from the source domain (or a merely different domain) can be re-encountered in test time. We used clean domain test data in the single domain adaptation scenario to show how TTN and other TTA methods adapt to the seen source domain data (but unseen instance). As shown in Table 4, all baseline methods using TBN layers, show performance drops even with large batch sizes. We can conclude that it is still better to rely on source statistics collected from the large training data than using only current input statistics, even if its batch size is large enough to obtain reliable statistics ( i.e., 200). However, since TTN utilizes source statistics while leveraging the current input, TTN itself and TTN adopted methods well preserve the source knowledge compared to the TBN-based methods. With a batch size of 200, we observe that combining the source and a current test batch statistics outperforms the source model (see 3rd row of Table 4). TTN is robust to class imbalanced scenario. Heavily depending on current test batch statistics are especially vulnerable when the class labels are imbalanced (Boudiaf et al., 2022; Gong et al., 2022). To simulate this situation, we sorted test images in class label order and then sampled test batches following the sorted data order. In Table 5, we observe that TTN is more robust to the class imbalanced scenario than utilizing only test batch statistics (i.e., TBN). As explained in Section 3.5, 7Published as a conference paper at ICLR 2023 Table 6: Adaptation on DG benchmarks in semantic segmentation. mIoU(↑) on four unseen domains with test batch size of 2 using ResNet-50 based DeepLabV3+ as a backbone. Method(Cityscapes→) BDD-100K Mapillary GTA V SYNTHIA Cityscapes Source (Chen et al., 2018) 43.50 54.37 43.71 22.78 76.15 Norm TBN 43.12 47.61 42.51 25.71 72.94 Ours (TTN) 47.40 56.92 44.71 26.68 75.09 Optim. TENT 43.30 47.80 43.57 25.92 72.93 + Ours (TTN) 47.89 57.84 46.18 27.29 75.04 SWR 43.40 47.95 42.88 25.97 72.93 + Ours (TTN) 48.85 59.09 46.71 29.16 74.89 we are putting more importance on CBN than TBN, where semantic information is mainly handled, i.e., in deeper layers, so we can understand that TTN is less impacted by skewed label distribution. 3.3 E XPERIMENTS ON SEMANTIC SEGMENTATION We additionally conduct experiments on domain generalization (DG) benchmarks (Choi et al., 2021; Pan et al., 2018) for semantic segmentation, including natural domain shifts ( e.g., Cityscapes→BDD-100K), to demonstrate the broad applicability of TTN. Table 6 shows the results of evaluating the ResNet-50-based DeepLabV3+ (Chen et al., 2018) model trained on the Cityscapes training set using the validation set of real-world datasets such as Cityscapes, BDD-100K, and Map- illary, and synthetic datasets including GTA V and SYNTHIA. We employ a test batch size of 2 for test-time adaptation in semantic segmentation. We observe that even when exploiting test batch statistics for standardization in BN layers (TBN) or updating the model parameters on top of TBN (TENT, SWR) does not improve the model performance (i.e., perform worse than the source model), adopting TTN helps the model make good use of the strength of the test batch statistics. Implemen- tation details and additional results are provided in the appendix A.1 and B.7, respectively. 3.4 A BLATION STUDIES Prior Aregularizes αto be robust to overall test batch sizes. We conduct an ablation study on the importance of each proposed component,i.e., initializing αwith prior A, optimizing αusing CE and MSE losses, and the results are shown in Table 7. Using Afor initialization and MSE loss aims to optimize αfollowing our intuition that we discussed in Section 2.3. Optimizing αusing CE loss improves the overall performance, but without regularizing with MSE loss, αmay overﬁt to large batch size (rows 2 & 3). Initialization with Aor not does not show a signiﬁcant difference, but A provides a better starting point than random initialization when comparing the left and right of the 2nd row. We observe that when using MSE loss, regardless of initialization using A, the optimized αsufﬁciently reﬂects our intuition resulting in a low error rate to overall batch sizes (row 3). Table 7: Ablation study on importance of each component Method Test batch size Avg. Method Test batch size Avg.Init. CE MSE200 64 16 4 2 1 Init. CE MSE200 64 16 4 2 1 - - \u0013 13.36 13.43 13.85 15.50 17.43 20.0715.61 \u0013 - - 13.37 13.43 13.85 15.50 17.44 20.0715.61 - \u0013 - 11.64 11.7312.26 14.46 16.94 19.8814.49 \u0013 \u0013 - 11.73 11.82 12.23 14.18 16.41 19.2714.27 - \u0013 \u0013 11.6411.7812.21 13.97 15.86 18.0013.91 \u0013 \u0013 \u0013 11.67 11.80 12.13 13.93 15.83 17.9913.89 3.5 V ISUALIZATION OF α Fig. 4 shows the visualization of optimized αfor CIFAR-10 using WideResNet-40-2. We observe that α decreases from shallow to deep layers (left to right), which means CBN is more active in deeper layers, and TBN is vice versa. As shown in Table 4 and 6, CBN employing source statistics is superior to TBN when the distribution shift between source and target domains is small. Assum- ing that the αwe obtained is optimal, we can conjecture that CBN is more active ( i.e., αcloser to 0) in deeper layers because domain information causing the distribution shift has been diminished. In contrast, TBN has a larger weight ( i.e., α closer to 1) in shallower layers since the domain in- formation induces a large distribution shift. This interpretation is consistent with the observations of previous studies (Pan et al., 2018; Wang et al., 2021; Kim et al., 2022a) that style information mainly exists in shallower layers, whereas only content information remains in deeper layers. 8Published as a conference paper at ICLR 2023 𝐶𝑙 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Channel-wise 𝛼 Channel mean 𝛼 per layer Channels in all layers1 2661 Figure 4: Optimized α. x- and y-axes indicate all channels in order from shallow to deep layers and the interpolating weight α, respectively. Cl denotes the channel size of layer l. 4 R ELATED WORK Test-time adaptation/training (TTA) aims to adapt models towards test data to overcome the per- formance degradation caused by distribution shifts (Sun et al., 2020; Wang et al., 2020). There are other related problems, unsupervised domain adaptation (UDA) (Sun & Saenko, 2016; Ganin et al., 2016) and source-free domain adaptation (SFDA) (Liang et al., 2020; Huang et al., 2021; Liu et al., 2021). Both UDA and SFDA have access to sufﬁciently large enough unlabeled target datasets, and their objective is to achieve high performance on that particular target domain. Unlike UDA and SFDA, TTA utilizes test data in an online manner. There are two key factors of recent approaches: adapting standardization statistics in normalization layers and adapting model parameters. Normalization in Test Time. Nado et al. (2020) suggested prediction-time BN, which uses test batch statistics for standardization and Schneider et al. (2020) introduced to adapt BN statistics by combining source and test batch statistics considering the the test batch size to mitigate the inter- mediate covariate shift. In this paper, we refer to the former method as TBN. Similarly, You et al. (2021) and Khurana et al. (2021) mixed the statistics using predeﬁned hyperparameter. Also, Mirza et al. (2022) and Hu et al. (2021) adapted the statistics using moving average while augmenting the input to form a pseudo test batch when only a single instance is given. The primary difference with the existing approaches is that we consider the channel-wise domain-shift sensitivity of BN layers to optimize the interpolating weights between CBN and TBN. Concurrently, Zou et al. (2022) pro- posed to adjust the standardization statistics using a learnable calibration strength and showed its effectiveness focusing on the semantic segmentation task. Optimization in Test Time.TENT (Wang et al., 2020), SWR (Choi et al., 2022), and CoTTA (Wang et al., 2022) updated model parameters while using TBN. TENT optimized afﬁne parameters in BN layers using entropy minimization while freezing the others. To maximize the adaptability, SWR updated the entire model parameters minimizing the entropy loss based on the domain-shift sensitivity. To stabilize the adaptation in continuously changing domains, CoTTA used consistency loss between student and teacher models and stochastically restored random parts of the pre-trained model. Liu et al. (2021) and Chen et al. (2022) suggested to update the model through contrastive learning. We focus on correcting the standardization statistics using domain-shift aware interpolating weight α. Similar to Choi et al. (2022), we measure the domain-shift sensitivity by comparing gradients. The principle difference is that we use channel-wise sensitivity when optimizing αin post-training, while SWR used layer-wise sensitivity regularizing the entire model update in test time. 5 C ONCLUSIONS This paper proposes TTN, a novel domain-shift aware batch normalization layer, which combines the beneﬁts of CBN and TBN that have a trade-off relationship. We present a strategy for mixing CBN and TBN based on the interpolating weight derived from the optimization procedure utilizing the sensitivity to domain shift and show that our method signiﬁcantly outperforms other normaliza- tion techniques in various realistic evaluation settings. Additionally, our method is highly practical because it can complement other optimization-based TTA methods. The oracle mixing ratio between CBN and TBN can vary depending on the domain gap difference. However, our proposed method employs a ﬁxed mixing ratio during test time, where the mixing ratio is optimized before model deployment. If we could ﬁnd the optimal mixing ratio according to the distribution shift during test time, we can expect further performance improvement. We consider it as future work. In this regard, our efforts encourage this ﬁeld to become more practical and inspire new lines of research. 9Published as a conference paper at ICLR 2023 REPRODUCIBILITY STATEMENT To ensure the reproducibility of our method, we provide the experimental setup in Section 3.1. Moreover, the details on implementation and evaluation settings can be found in the appendix A.1 and A.2, respectively. The pseudocode for overall training and testing scheme is provided in the appendix A.3. Together with related references and publicly available codes, we believe our paper contains sufﬁcient information for reimplementation. ACKNOWLEDGEMENT We would like to thank Kyuwoong Hwang, Simyung Chang, and Seunghan Yang of the Qualcomm AI Research team for their valuable discussions. REFERENCES Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, and Richard Turner. Tas- knorm: Rethinking batch normalization for meta-learning. In International Conference on Ma- chine Learning (ICML). PMLR, 2020. Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder- decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), 2018. Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim, Seungryong Kim, and Jaegul Choo. Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In European Conference on Computer Vision (ECCV), 2022. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net- works. The journal of machine learning research, 2016. Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Note:robust continual test-time adaptation against temporal correlation. Advances in Neural In- formation Processing Systems (NeurIPS), 2022. Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An- drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Ad- vances in Neural Information Processing Systems (NeurIPS), 2004. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 10Published as a conference paper at ICLR 2023 Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor- ruptions and perturbations. In International Conference on Learning Representations (ICLR), 2018. Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi- narayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In International Conference on Learning Representations (ICLR), 2019. Xuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Nevatia, and Ser-Nam Lim. Mixnorm: Test-time adaptation through online normalization estimation. arXiv preprint arXiv:2110.11478, 2021. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. 2021. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. InInternational Conference on Machine Learning (ICML), 2015. Ansh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, and Gaurav Aggarwal. Sita: Single image test-time adaptation. arXiv preprint arXiv:2112.02355, 2021. Byeonggeun Kim, Seunghan Yang, Jangho Kim, Hyunsin Park, Juntae Lee, and Simyung Chang. Domain Generalization with Relaxed Instance Frequency-wise Normalization for Multi-device Acoustic Scene Classiﬁcation. In Conference of the International Speech Communication Asso- ciation (INTERSPEECH), 2022a. Jin Kim, Jiyoung Lee, Jungin Park, Dongbo Min, and Kwanghoon Sohn. Pin the memory: Learn- ing to generalize semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022b. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015. Suhyeon Lee, Hongje Seong, Seongwon Lee, and Euntai Kim. Wildnet: Learning domain general- ized semantic segmentation from the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML). PMLR, 2020. Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexan- dre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems (NeurIPS), 2021. Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adap- tation with residual transfer networks. In Advances in Neural Information Processing Systems (NeurIPS), 2016. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. 2017. M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsupervised domain adaptation by normalization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In International Conference on Computer Vision (ICCV), 2017. Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. InEuropean Conference on Computer Vision (ECCV), 2018. 11Published as a conference paper at ICLR 2023 Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. Mit Press, 2008. Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In European Conference on Computer Vision (ECCV), 2016. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Ad- vances in Neural Information Processing Systems (NeurIPS), 2020. Saurabh Singh and Abhinav Shrivastava. Evalnorm: Estimating batch normalization statistics for evaluation. In International Conference on Computer Vision (ICCV), 2019. Cecilia Summers and Michael J Dinneen. Four things everyone should know to improve batch normalization. In International Conference on Learning Representations (ICLR), 2019. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European Conference on Computer Vision (ECCV), 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time train- ing with self-supervision for generalization under distribution shifts. In International Conference on Machine Learning (ICML), 2020. Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick P´erez. Advent: Adver- sarial entropy minimization for domain adaptation in semantic segmentation. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Repre- sentations (ICLR), 2020. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, and Gao Huang. Revisiting locally supervised learning: an alternative to end-to-end training. In International Conference on Learning Representations (ICLR), 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madha- van, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Yuliang Zou, Zizhao Zhang, Chun-Liang Li, Han Zhang, Tomas Pﬁster, and Jia-Bin Huang. Learn- ing instance-speciﬁc adaptation for cross-domain segmentation. European Conference on Com- puter Vision (ECCV), 2022. 12Published as a conference paper at ICLR 2023 A A PPENDIX : F OR REPRODUCIBILITY This section provides supplemental material for Section 2 and 3.1. A.1 I MPLEMENTATION DETAILS Datasets and Models. For CIFAR-10/100-C, we optimized α using augmented CIFAR-10/100 training set on the pre-trained WideResNet-40-2 (WRN-40) (Hendrycks et al., 2019). For ImageNet- C, we used augmented ImageNet training set (randomly sampled 64000 instances per epoch) on the pre-trained ResNet-50. Augmentation. Following the setting of SWR (Choi et al., 2022), we used color jittering, random invert and random grayscale when obtaining the prior A. When optimizing α, we followed the augmentation choice of CoTTA (Wang et al., 2022), which are color jittering, padding, random afﬁne, gaussian blur, center crop and random horizontal ﬂip. We excluded for gaussian noise to avoid any overlap with corruption type of common corruptions (Hendrycks & Dietterich, 2018). For ImageNet, we only used the same augmentation both for obtaining Aand optimizing αfollowing the SWR augmentation choices. Post-training. When obtaining prior, we used randomly selected 1024 samples from the training set, following the setting of SWR. We used Adam (Kingma & Ba, 2015) optimizer using a learning rate (LR) of 1e-3, which is decayed with cosine schedule (Loshchilov & Hutter, 2017) for 30 epochs and used 200 training batch for CIFAR-10/100. For ImageNet, we lowered LR to 2.5e-4 and used 64 batch size. For semantic segmentation task, we trained TTN using Cityscapes training set, and training batch size of 2. We resized the image height to 800 while preserving the original aspect ratio Cordts et al. (2016). We can terminate the training when MSE loss saturates (We observed that αdoes not show signiﬁcant difference after the MSE loss is saturated). We used the weighting hyperparmeter to MSE loss λas 1. Test time. For AdaptiveBN, which adjusts the interpolating weight using two factors: hyperpa- rameter N and test batch size n, we followed the suggested N, which is empirically obtained best hyperparameter from the original paper, for each n(Figure 11, ResNet architecture from Schneider et al. (2020)). In detail, we set N as 256, 128, 64, 32, and 16 for test batch size 200, 64, 16, 4, 2, and 1, which yields αas 0.44, 0.33, 0.2, 0.11, 0.06, and 0.06, respectively. For optimization-based TTA methods, we followed default setting in TENT, SWR, and CoTTA for test-time adaptation. We used LR of 1e-3 to test batch size of 200 for CIFAR-10/100-C in single domain (TENT, SWR) and continuously changing domain (CoTTA) scenarios. To avoid rapid error accumulation, we lowered LR to 1e-4 for TENT and SWR in continual and mixed do- main scenarios. Moreover, we updated model parameters after accumulating the gradients for 200 samples for CIFAR-10/100-C. In other words, we compute gradients per batch, but update, i.e., optimizer.step(), after seeing 200 data samples. Exceptionally, we used LR of 5e-5 for SWR and SWR+TTN in mixed domain setting. Additionally, in continuously changing and mixed domain scenarios, we used the stable version of SWR, which updates the model parameter based on the frozen source parameters instead of previously updated parameters (original SWR). For semantic segmentation, we set the test batch size as 2 and learning rate for optimization-based methods as 1e-6 for all datasets. For SWR, we set the importance of the regularization term λr as 500. The other hyperparameters are kept the same as Choi et al. (2022) choices. For all test-time adaptation, we used constant learning rate without scheduling. A.2 E VALUATION SCENARIO DETAILS Class imbalanced setting. In the main paper Table 5, we show the results under class imbalanced settings. In the setting, we sorted the test dataset of each corruption in the order of class labels, i.e., from class 0 to 9 for CIFAR-10-C. Then, we comprised test batches following the sorted order. Therefore, most batches consist of single class input data, which leads to biased test batch statistics. For larger batch size, the statistics are more likely to be extremely skewed, and that explains why error rates are higher with larger batch sizes than with the small ones. 13Published as a conference paper at ICLR 2023 A.3 P SEUDOCODE Pseudocode for post-training, i.e., obtaining Aand optimizing α, is provided in Algorithms 1 and 2, respectively, and that for test time is in Algorithm 3. Moreover, we provide PyTorch-friendly pseudocode for obtaining Ain Listing 1. Please see Section 2 for equations and terminologies used in the algorithms. Algorithm 1 Obtain prior A 1: Require: Pre-trained model fθ; source training data DS = (X,Y ) 2: Output: Prior A 3: for all (x,y) in DS do 4: Augment x: x′ 5: Collect gradients (∇γ,∇γ′ ) and (∇β,∇β′ ) from fθ using clean xand augmented x′ 6: end for 7: Compute gradient distance score dusing Eq. 4 8: Deﬁne prior Ausing Eq. 5 Algorithm 2 Post-train 1: Require: Pre-trained model fθ; source training data DS = (X,Y ); step size hyperparameter η; regularization weight hyperparameter λ 2: Output: Optimized interpolating weight α 3: Obtain prior Ausing Algorithm 1 4: Initialize αwith prior A 5: Replace all BN layers of fθ to TTN layers using αand Eq. 2 6: while not done do 7: Sample minibatches BS from DS 8: for all minibatches do 9: Augment all xin BS: x′ 10: Evaluate ∇αLusing fθ given {(x′ i,yi)}|BS| i=1 using Eq. 6 while adapting standardization statistics using Eq. 2 11: Update α←α−η∇αL 12: end for 13: end while Algorithm 3 Inference (Test time) 1: Require: Pre-trained model fθ; optimized α, target test data DT = (X); 2: Replace all BN layers of fθ to TTN layers using αand Eq. 2 3: Sample minibatches BT from DT 4: for all minibatches do 5: Make prediction using fθ given BT while adapting standardization statistics using Eq. 2 6: end for 1 def obtain_prior(model, train_data): 2 # make weight and bias of BN layers requires_grad=True, otherwise False 3 params = {n: p for n, p in model.named_parameters() if p.requires_grad} 4 5 grad_sim = {} 6 for x, y in train_data: 7 # collect gradients for clean and augmented input 8 grad_org = collect_grad(model, x, y, params) 9 grad_aug = collect_grad(model, augment(x), y, params) 10 11 # compute grad similarity 12 for n, p in params.items(): 13 grad_sim[n].data = cosine_sim(grad_org, grad_aug) 14 14Published as a conference paper at ICLR 2023 15 # average over data samples 16 for n, p in params.items(): 17 grad_sim[n].data /= len(train_data) 18 19 # min max normalization 20 max_grad = get_max_value(grad_sim) # scalar 21 min_grad = get_min_value(grad_sim) # scalar 22 grad_dist = {} 23 for n, p in grad_sim.items(): 24 grad_dist[n] = (p - min_grad) / (max_grad - min_grad) 25 26 prior = [] 27 j = 0 28 # integrate gradients of weight(gamma) and bias(beta) for each BN layer 29 for n, p in grad_dist.items(): 30 if \"weight\" in n: 31 prior.append(p) 32 elif \"bias\" in n: 33 prior[j] += p 34 prior[j] /= 2 35 prior[j] = (1-prior[j])**2 36 j += 1 37 38 return prior 39 40 def collect_grad(model, x, y, params): 41 model.zero_grad() 42 out = model(x) 43 loss = ce_loss(out, y) 44 loss.backward() 45 46 grad = {} 47 for n, p in params.items(): 48 grad[n].data = p.data 49 50 return grad Listing 1: PyTorch-friendly pseudo code for obtaining prior B A PPENDIX : E XPERIMENTAL RESULTS B.1 A BLATION STUDIES Channel-wise α. In Table 8, we compared different granularity levels of interpolating weightα, i.e., channel-wise, layer-wise, and a constant value with CIFAR-10-C and backbone WRN-40. We ob- served that channel-wise optimized αshows the best performance. We take average of the channel- wisely optimized α (the 1st row) over channels to make a layer-wise α (the 2nd row), and take average over all channels and layers to make a constant value (the 3rd row). αof the 1st and 2nd rows are visualized in the main paper Figure 4 colored in blue and red, respectively. The constant value α(the 3rd row) is 0.3988. We also optimized αlayer-wisely (the 4th row). Table 8: Ablation study on granularity level of α # Method Test batch size Avg. 200 64 16 4 2 1 1 Channel-wise (Optimized) 11.67 11.80 12.13 13.93 15.83 17.99 13.89 2 Layer-wise (Channel mean) 12.75 12.84 13.16 14.66 16.40 18.82 14.77 3 Constant (Mean) 12.07 12.21 13.05 16.72 20.04 21.26 15.89 4 Layer-wise (Optimized) 13.11 13.21 13.51 14.84 16.46 18.62 14.96 15Published as a conference paper at ICLR 2023 MSE loss strength λ. We empirically set the MSE loss strengthλin Eq. 6 as 1 through the ablation study using CIFAR-10-C and WRN-40 (Table 9). Using the MSE regularizer prevents the learnable αfrom moving too far from the prior, thus letting αfollow our intuition, i.e., putting smaller im- portance on the test batch statistics if the layer or channel is invariant to domain shifts. However, with too strong regularization (λ=10.0), the overall error rates are high, which means the αneeds to be sufﬁciently optimized. On the other hand, with too small regularization, the αmay overﬁt to the training batch size (B=200) and lose the generalizability to the smaller batch size. Table 9: MSE loss strength λ λ Test batch size Avg. 200 64 16 4 2 1 0.0 11.73 11.82 12.23 14.18 16.41 19.27 14.27 0.1 11.65 11.91 12.09 13.84 15.71 18.24 13.91 1.0 11.67 11.80 12.13 13.93 15.83 17.99 13.89 10.0 12.45 12.63 12.82 14.55 16.32 18.56 14.56 B.2 M ORE COMPARISONS ON CIFAR10-C Constant α. Table 10 shows results of simple baseline for normalization-based methods where the αis a constant value ranging from 0 to 1. α= 0 is identical to CBN (Ioffe & Szegedy, 2015), and α = 1 is identical to TBN (Nado et al., 2020). We observe that the lower α, i.e., using less test batch statistics, shows better performance for small test batch sizes. This observation is analogous to the ﬁnding of the previous work (Schneider et al., 2020). Table 10: Constant α. Error rate (↓) averaged over 15 corruptions of CIFAR-10-C (WRN-40). α Test batch size Avg. 200 64 16 4 2 1 0 18.26 18.39 18.26 18.26 18.25 18.25 18.28 0.1 13.95 14.1 14.05 14.65 15.14 15.45 14.56 0.2 12.46 12.66 12.89 14.30 15.53 15.64 13.91 0.3 12.05 12.29 12.72 15.18 17.35 17.42 14.50 0.4 12.13 12.41 13.12 16.69 19.81 20.51 15.78 0.5 12.42 12.78 13.73 18.32 22.52 24.88 17.44 0.6 12.88 13.32 14.48 20.02 25.17 31.97 19.64 0.7 13.37 13.9 15.23 21.75 27.91 46.65 23.14 0.8 13.82 14.37 15.94 23.44 30.59 77.15 29.22 0.9 14.18 14.8 16.58 24.94 33.12 89.81 32.24 1 14.50 15.15 17.10 26.29 35.67 90.00 33.12 B.3 V ARIANTS OF TTN FOR SMALL TEST BATCH SIZE Online TTN. TTN interpolating weight αcan also be adapted during test time. Table 11 shows the results of the TTN online version, which further optimizes the post-trained alpha using the entropy minimization and mean-squared error (MSE) loss between the updated alpha and the initial post- trained alpha. We followed entropy minimization loss used in TENT (Wang et al., 2020), and the MSE loss can be written as LMSE = ∥α−α0∥2, where α0 is the post-trained α. We set the learning rate as 1e-2, 2.5e-3, 5e-4, 1e-4, 5e-5, and 2.5e-5 by linearly decreasing according to the test batch size of 200, 64, 16, 4, 2, and 1 (Goyal et al., 2017). The online TTN shows improvements compared to the ofﬂine TTN in all three evaluation scenarios (single, continuously changing, and mixed). Scaled TTN. Following the observation from Table 10, we lowered the interpolating weight by mul- tiplying a constant scale value, ranging (0,1), to the optimized TTN α. In Table 11, we empirically set the scale value as 0.4. Dynamic training batch size. We observe that using the dynamic batch size in the post-training stage also improves the performance for small test batch sizes (2 or 1), while slightly deteriorating the performance for large test batch sizes (200 or 64). We randomly sampled training batch size from the range of [4,200] for each iteration. Other hyperparameters are kept as the same. 16Published as a conference paper at ICLR 2023 Table 11: TTN variants. Error rate (↓) averaged over 15 corruptions of CIFAR-10-C (WRN-40). Test batch size Avg. Method Eval. setting 200 64 16 4 2 1 TTN (ofﬂine, default) Single & Cont.11.67 11.80 12.13 13.93 15.83 17.99 13.89 TTN (ofﬂine, default) Mixed 12.16 12.19 12.34 13.96 15.55 17.83 14.00 TTN (online) Single 11.39 11.64 11.97 13.70 15.41 17.49 13.60 TTN (online) Cont. 11.67 11.96 12.15 13.90 15.67 17.72 13.85 TTN (online) Mixed 12.04 12.04 12.06 13.90 15.46 17.62 13.85 TTN (scaled) Single & Cont.13.20 13.38 13.35 13.88 14.54 15.17 13.92 TTN (scaled) Mixed 13.17 13.05 13.17 13.74 14.36 15.09 13.76 TTN (dynamic train batch size)Single & Cont.11.82 12.04 12.17 13.60 15.13 17.22 13.66 TTN (dynamic train batch size)Mixed 12.12 12.01 11.91 13.43 14.76 17.23 13.58 B.4 S TUDY ON AUGMENTATION TYPE TTN is robust to the data augmentation. We used data augmentation in the post-training phase to simulate domain shifts. The rationale for the simulation is to expose the model to different input domains. Especially when obtaining the prior, we compare the outputs from shifted domains with the clean (original) domain in order to analyze which part of the model is affected by the domain discrepancy. Therefore, changing the domain itself is what matters, not which domain the input is shifted to. We demonstrated this by conducting ablation studies by varying the augmentation type. We analyzed various augmentation types when obtaining prior and when optimizing alpha and the results are shown in Figure 5 (a) and (b), respectively. We use a true corruption, i.e., one of 15 corruption types in the corruption benchmark, as an augmentation type in the post-training phase to analyze how TTN works if the augmentation type and test corruption type are misaligned or perfectly aligned. Speciﬁcally, we used the true corruption when obtaining the prior while keeping the augmentation choices when optimizing alpha as described in the appendix A.1, and vice versa. Expectedly, the diagonal elements, i.e., where the same corruption type is used both for in post- training and in test time, tend to show the lowest error rate in Figure 5(b). The row-wise standard deviation is lower than 1 in most cases and even lower than 0.5 in Figure 5(a), which means the prior is invariant to the augmentation choice. Moreover, we observe that the average error rate over all elements, 11.74% in ablation for obtaining prior and 11.67% in ablation for optimizing alpha, is almost as same as TTN result 11.67% (See Table 14 and Figure 5). Moreover, we conducted an ablation study on the choice of the augmentation type using CIFAR-10-C and WRN-40 (Table12). We observe that obtaining prior and optimizing alpha steps are robust to the augmentation types. Table 12: Ablation study on augmentation choice. From left to right one augmentation type is added at a time. Default setting, which we used in all experiments, is colored in gray. Prioraugmentation type color jitter + grayscale + invert + gaussian blur + horizontal ﬂip 12.03 11.83 11.67 11.59 11.58 Optimizingαaugmentation type color jitter + padding + afﬁne + gaussian blur + horizontal ﬂip 11.78 11.78 11.7 11.70 11.67 B.5 R ESULTS ON IMAGE NET-C Table 13 shows the experimental results using ResNet-50 (He et al., 2016) on ImageNet- C (Hendrycks & Dietterich, 2018) dataset. We emphasized the effectiveness of our proposed method by showing the signiﬁcant improvement in large-scale dataset experiment. Similar to the results in CIFAR-10/100-C, TTN showed the best performance compared to normalization-based methods (TBN (Nado et al., 2020), AdaptiveBN Schneider et al. (2020), and α-BN (You et al., 2021)) and improved TTA performance when it is applied to optimization-based methods (TENT (Wang et al., 2020) and SWR (Choi et al., 2022)). During post-training, we used LR of 2.5e-4 and batch size of 64. In test time, we used the learning rate of 2.5e-4 for TENT following the setting from the original paper, and used 5e-6 for TENT+TTN. For SWR and SWR+TTN, we used learning rate of 2.5e-4 for B=64, and 2.5e-6 for B=16, 4, 2, and 1. We had to carefully tune the learning rate especially for SWR, since the method updates the 17Published as a conference paper at ICLR 2023 entire model parameters in an unsupervised manner and hence is very sensitive to the learning rate when the test batch size becomes small. Other hyperparameters and details are kept same (See the appendix A.1 for more details). Moreover, to avoid rapid accumulation, we stored gradients for sufﬁciently large test samples and then updated the model parameters (for example, we conducted adaptation in every 64 iterations in the case of batch size of 1) in both TENT and SWR. Table 13: Single domain adaptation on ImageNet-C (ResNet-50). Error rate (↓) averaged over 15 corruption types with severity level 5 is reported for each test batch size. Method Test batch size Avg. Error 64 16 4 2 1 Source (CBN)93.34 93.34 93.34 93.34 93.34 93.34 Norm TBN 74.24 76.81 85.74 95.35 99.86 86.40 AdaptiveBN 77.86 81.47 86.71 90.15 91.11 85.46 α-BN 86.06 86.32 87.16 88.33 90.45 87.66 Ours (TTN) 72.21 73.18 76.98 81.52 88.49 78.48 Optim. TENT 66.56 72.61 93.37 99.46 99.90 86.41 +Ours (TTN)71.42 72.45 76.66 81.89 91.00 78.68 SWR 64.41 74.19 84.30 93.05 99.86 83.16 +Ours (TTN)55.68 69.25 78.48 86.37 94.08 76.77 B.6 E RROR RATES OF EACH CORRUPTION In Table 14, we show the error rates of TTN for each corruption of CIFAR-10-C using the WRN-40 backbone. The averaged results over the corruptions are in Table 1. Table 14: Results of each corruption (CIFAR-10-C) batch sizegauss. shot impulse defocus glass motion zoom snow frost fog brightness contrast elastic pixel. jpeg.Avg. 200 14.81 12.78 17.32 7.37 17.87 8.51 7.23 10.29 9.88 11.29 6.06 8.36 13.42 14.89 14.94 11.6764 14.81 12.81 17.42 7.41 18.21 8.66 7.41 10.44 9.93 11.63 6.11 8.35 13.59 15.18 15.02 11.8016 15.23 13.00 17.98 7.71 18.46 8.95 7.68 10.85 10.17 12.21 6.25 8.54 13.95 15.67 15.3412.134 17.03 15.29 19.75 9.26 20.93 10.01 9.62 12.58 11.85 13.65 7.69 9.25 16.21 18.08 17.7013.932 19.21 16.80 21.86 10.70 23.74 11.39 11.92 14.00 13.39 15.38 8.95 10.13 18.92 20.68 20.4015.831 22.03 19.97 25.24 12.41 26.99 12.22 14.39 14.73 14.60 17.27 10.16 9.51 22.10 24.12 24.0917.99 B.7 S EGMENTATION RESULTS For more comparisons, we add segmentation results for TBN and TTN using test batch size (B) of 4 and 8 (Table 15). The results demonstrate that TTN is robust to the test batch sizes. In other words, the performance difference across the test batch size is small when using TTN (TTN with B=2, 4, and 8). The results are averaged over 3 runs (i.e., using 3 independently optimized α), and the standard deviation is denoted with a ±sign. We omitted the standard deviation for TBN, which is 0.0 for every result since no optimization is required. Since the backbone network is trained with a train batch size of 8, we assume that test batch statistics estimated from the test batch with B=8 are sufﬁciently reliable. Accordingly, TBN with B=8 shows compatible results. However, when B becomes small (i.e., in a more practical scenario), problematic test batch statistics are estimated, and thus TBN suffers from the performance drop while TTN keeps showing robust performance. It is worth noting that TTN outperforms TBN by 3.77% in average accuracy when B=2, i.e., in the most practical evaluation setting, and by 2.54% and 1.99% for B=4 and 8, respectively. Table 15: Adaptation on DG benchmarks in semantic segmentation. mIoU(↑) on four unseen domains with test batch size of 2, 4, and 8 using ResNet-50 based DeepLabV3+ as a backbone. Method(Cityscapes→) BDD-100K Mapillary GTA V SYNTHIA Cityscapes Norm TBN (B=2) 43.12 47.61 42.51 25.71 72.94 Ours (TTN)(B=2) 47.40(±0.02) 56.88(±0.04) 44.69(±0.03) 26.68(±0.01) 75.09(±0.01) TBN (B=4) 45.64 49.17 44.26 25.96 74.29 Ours (TTN)(B=4) 47.72(±0.01) 57.11(±0.01) 45.08(±0.02) 26.52(±0.01) 75.56(±0.01) TBN (B=8) 46.42 49.38 44.81 25.97 75.42 Ours (TTN)(B=8) 47.25(±0.02) 57.28(±0.02) 45.13(±0.03) 26.45(±0.01) 75.82(±0.01) 18Published as a conference paper at ICLR 2023 (b)Augmentation type used for optimizing alpha Test corruption type Error rate. Colored by normalized value (across test corruption type)avg.std.14.550.8612.640.6116.770.927.660.3117.750.738.760.297.490.2510.330.279.710.3212.401.466.110.098.260.4913.350.3714.581.2714.730.3911.67 (a)Augmentation type used for obtaining prior Test corruption type Error rate. Colored by normalized value (across test corruption type) avg.std.15.030.3412.790.2217.240.167.410.0617.910.148.480.087.260.0310.440.149.980.1411.550.106.050.048.570.4013.340.0715.211.5914.830.1111.74 Figure 5: True test corruption as augmentation. Each column represents the augmentation type used (a) when obtaining prior or (b) when optimizing α. Each row represents the test corruption type. The error rate (↓) of CIFAR-10-C with severity level 5 and test batch size 200 using WRN-40 is annotated in each element. Element (i,j) represents the error rate when j-th corruption type is used for augmenting the clean train data during post-training and tested on i-th corruption test set. The heatmap is colored by error rate normalized across the test corruption type (row-wise normalization). 19",
      "meta_data": {
        "arxiv_id": "2302.05155v2",
        "authors": [
          "Hyesu Lim",
          "Byeonggeun Kim",
          "Jaegul Choo",
          "Sungha Choi"
        ],
        "published_date": "2023-02-10T10:25:29Z",
        "pdf_url": "https://arxiv.org/pdf/2302.05155v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the performance degradation of Test-Time Adaptation (TTA) methods that rely on Transductive Batch Normalization (TBN) when faced with small test batch sizes or non-i.i.d. data streams, which contradict TBN's impractical assumptions. It proposes Test-Time Normalization (TTN), a novel domain-shift aware batch normalization strategy that interpolates standardization statistics between Conventional Batch Normalization (CBN) and TBN. TTN uses channel-wise interpolating weights, optimized during a post-training phase based on the domain-shift sensitivity of each BN layer. This approach significantly improves model robustness across a wide range of batch sizes and in various realistic evaluation scenarios (stationary, continuously changing, and mixed domain adaptation). TTN is shown to be widely applicable, boosting the performance of existing TTA methods and achieving state-of-the-art results in image classification and semantic segmentation, while also preserving source knowledge and being robust to class imbalance.",
        "methodology": "The core of TTN is a modified Batch Normalization layer that computes standardization statistics (mean and variance) as a weighted combination of current test batch statistics and frozen source statistics. The weights are determined by a learnable channel-wise interpolating parameter, 'alpha' (α). The methodology involves a 'post-training' phase to optimize α:\n1.  **Obtain Prior A**: To identify domain-shift sensitive BN layers and channels, the model is exposed to pairs of clean and augmented (domain-shifted) source images. The domain-shift sensitivity for each channel is quantified by comparing the gradients of the affine parameters (gamma and beta) of the BN layers using a cross-entropy loss. A gradient distance score is computed, normalized, and squared to create a prior 'A', which guides the initialization of α.\n2.  **Optimize α**: The interpolating weights α are initialized with the prior 'A'. Only α is trained during this phase, while all other pre-trained model parameters are frozen. The optimization uses a combined loss function: a cross-entropy loss (L_CE) on augmented training data to ensure consistent predictions, and a mean-squared error loss (L_MSE) between the current α and the prior 'A' to regularize α and prevent it from straying too far, ensuring generalizability across different batch sizes and domain shifts.",
        "experimental_setup": "The method was evaluated on two main tasks: image classification and semantic segmentation.\n*   **Image Classification**: Performed on corruption benchmark datasets including CIFAR-10/100-C and ImageNet-C, which feature 15 common corruption types across five severity levels. WideResNet-40-2 was used for CIFAR datasets, and ResNet-50 for ImageNet. Clean training sets were used for the post-training phase, and corrupted datasets for evaluation.\n*   **Semantic Segmentation**: Experiments were conducted on domain generalization benchmarks using datasets like Cityscapes (source), BDD-100K, Mapillary, GTA V, and SYNTHIA (target domains). A ResNet-50-based DeepLabV3+ model was used.\n*   **Baselines**: Comparisons were made against various normalization-based methods (AdaptiveBN, α-BN, MixNorm, TBN) and optimization-based TTA methods (TENT, SWR, CoTTA), as well as the source model (using CBN).\n*   **Evaluation Scenarios**: Robustness was tested across three realistic scenarios: single domain adaptation, continuously changing domain adaptation, and mixed domain adaptation. A wide range of test batch sizes (200, 64, 16, 4, 2, 1) was used, with all corruptions set to severity level 5. A single checkpoint of the optimized α parameter was used per dataset across all experimental settings.",
        "limitations": "The primary limitation identified is that the proposed method employs a fixed mixing ratio (interpolating weight α) during test time. This α is optimized before model deployment and is not dynamically adjusted according to the specific distribution shift encountered during inference. This static nature might prevent achieving further performance improvements if the optimal mixing ratio could be adapted on-the-fly based on real-time test-time distribution shifts.",
        "future_research_directions": "A key future research direction is to explore methods for dynamically finding the optimal mixing ratio (α) during test time, based on the actual distribution shift encountered. This would allow the model to adapt α on-the-fly to varying domain gaps, potentially leading to further performance improvements beyond the currently fixed mixing ratio."
      }
    },
    {
      "title": "Towards Stable Test-time Adaptation in Dynamic Wild World",
      "abstract": "Test-time adaptation (TTA) has shown to be effective at tackling distribution\nshifts between training and testing data by adapting a given model on test\nsamples. However, the online model updating of TTA may be unstable and this is\noften a key obstacle preventing existing TTA methods from being deployed in the\nreal world. Specifically, TTA may fail to improve or even harm the model\nperformance when test data have: 1) mixed distribution shifts, 2) small batch\nsizes, and 3) online imbalanced label distribution shifts, which are quite\ncommon in practice. In this paper, we investigate the unstable reasons and find\nthat the batch norm layer is a crucial factor hindering TTA stability.\nConversely, TTA can perform more stably with batch-agnostic norm layers, \\ie,\ngroup or layer norm. However, we observe that TTA with group and layer norms\ndoes not always succeed and still suffers many failure cases. By digging into\nthe failure cases, we find that certain noisy test samples with large gradients\nmay disturb the model adaption and result in collapsed trivial solutions, \\ie,\nassigning the same class label for all samples. To address the above collapse\nissue, we propose a sharpness-aware and reliable entropy minimization method,\ncalled SAR, for further stabilizing TTA from two aspects: 1) remove partial\nnoisy samples with large gradients, 2) encourage model weights to go to a flat\nminimum so that the model is robust to the remaining noisy samples. Promising\nresults demonstrate that SAR performs more stably over prior methods and is\ncomputationally efficient under the above wild test scenarios.",
      "meta_data": {
        "arxiv_id": "2302.12400v1",
        "authors": [
          "Shuaicheng Niu",
          "Jiaxiang Wu",
          "Yifan Zhang",
          "Zhiquan Wen",
          "Yaofo Chen",
          "Peilin Zhao",
          "Mingkui Tan"
        ],
        "published_date": "2023-02-24T02:03:41Z",
        "pdf_url": "https://arxiv.org/pdf/2302.12400v1.pdf"
      }
    },
    {
      "title": "Towards Stable Test-time Adaptation in Dynamic Wild World",
      "abstract": "Test-time adaptation (TTA) has shown to be effective at tackling distribution\nshifts between training and testing data by adapting a given model on test\nsamples. However, the online model updating of TTA may be unstable and this is\noften a key obstacle preventing existing TTA methods from being deployed in the\nreal world. Specifically, TTA may fail to improve or even harm the model\nperformance when test data have: 1) mixed distribution shifts, 2) small batch\nsizes, and 3) online imbalanced label distribution shifts, which are quite\ncommon in practice. In this paper, we investigate the unstable reasons and find\nthat the batch norm layer is a crucial factor hindering TTA stability.\nConversely, TTA can perform more stably with batch-agnostic norm layers, \\ie,\ngroup or layer norm. However, we observe that TTA with group and layer norms\ndoes not always succeed and still suffers many failure cases. By digging into\nthe failure cases, we find that certain noisy test samples with large gradients\nmay disturb the model adaption and result in collapsed trivial solutions, \\ie,\nassigning the same class label for all samples. To address the above collapse\nissue, we propose a sharpness-aware and reliable entropy minimization method,\ncalled SAR, for further stabilizing TTA from two aspects: 1) remove partial\nnoisy samples with large gradients, 2) encourage model weights to go to a flat\nminimum so that the model is robust to the remaining noisy samples. Promising\nresults demonstrate that SAR performs more stably over prior methods and is\ncomputationally efficient under the above wild test scenarios.",
      "meta_data": {
        "arxiv_id": "2302.12400v1",
        "authors": [
          "Shuaicheng Niu",
          "Jiaxiang Wu",
          "Yifan Zhang",
          "Zhiquan Wen",
          "Yaofo Chen",
          "Peilin Zhao",
          "Mingkui Tan"
        ],
        "published_date": "2023-02-24T02:03:41Z",
        "pdf_url": "https://arxiv.org/pdf/2302.12400v1.pdf"
      }
    }
  ]
}