{
  "research_topic": "Graph Attention Networkã®å­¦ç¿’ã®é«˜é€ŸåŒ–",
  "queries": [
    "GAT training acceleration",
    "Graph Attention Network optimization",
    "efficient GAT training",
    "scalable graph attention",
    "approximate attention GNN"
  ],
  "research_study_list": [
    {
      "title": "GACT: Activation Compressed Training for Generic Network Architectures",
      "abstract": "Training large neural network (NN) models requires extensive memory\nresources, and Activation Compressed Training (ACT) is a promising approach to\nreduce training memory footprint. This paper presents GACT, an ACT framework to\nsupport a broad range of machine learning tasks for generic NN architectures\nwith limited domain knowledge. By analyzing a linearized version of ACT's\napproximate gradient, we prove the convergence of GACT without prior knowledge\non operator type or model architecture. To make training stable, we propose an\nalgorithm that decides the compression ratio for each tensor by estimating its\nimpact on the gradient at run time. We implement GACT as a PyTorch library that\nreadily applies to any NN architecture. GACT reduces the activation memory for\nconvolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training\nwith a 4.2x to 24.7x larger batch size, with negligible accuracy loss. We\nimplement GACT as a PyTorch library at\nhttps://github.com/LiuXiaoxuanPKU/GACT-ICML.",
      "full_text": "GACT: Activation Compressed Training for Generic Network Architectures Xiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, Michael Mahoney, Alvin Cheung University of California, Berkeley xiaoxuan liu@berkeley.edu, jianfeic@tsinghua.edu.cn Abstract Training large neural network (NN) models requires extensive memory resources, and Activation Compressed Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACTâ€™s approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1Ã—, enabling training with a 4.2Ã— to 24.7Ã— larger batch size, with negligible accuracy loss. We implement GACT as a PyTorch library at https://github.com/LiuXiaoxuanPKU/GACT-ICML. 1 Introduction In recent years, we have witnessed the trend of using larger and larger neural network (NN) models to deliver improved accuracy and generalization in various machine learning tasks [1, 2]. However, training these models requires a considerable amount of on-device GPU memory. Unfortunately, the increase of GPU memory capacity has been relatively slow, leading to a fundamental barrier to the development of large NN models. Activation Compressed Training (ACT) is a promising approach to reduce the memory footprint of models during training. As all layersâ€™ activations need to be kept in the memory for computing the gradients during training, ACT reduces memory consumption by compressing these saved activations. Prior work [3, 4, 5, 6] has shown the eï¬€ectiveness of ACT by reducing activation footprint by up to 12 Ã—with 2-bit activations. Although ACT has already demonstrated impressive compression capabilities, previous work on ACT is restricted to speciï¬c NN architectures. For example, ActNN [ 5] is a quantization framework for convolutional NNs only; Mesa [ 7] proposes a per head/layer quantization method for vision transformers; and AC-GC [ 6] derives convergence error bound for diï¬€erent types of operators separately. Developing a generic ACT framework is challenging. Theoretically, convergence guarantees must be made without assumptions on the network architecture. Algorithmically, the framework should ï¬nd eï¬€ective compression strategies for all kinds of networks automatically. From the system perspective, the framework should support arbitrary NN operations, including user-deï¬ned ones. In this work, we propose GACT, a general framework for ACT that is agnostic to the NN architecture. Neither specialized mathematical derivations nor customized implementation is needed to support diï¬€erent operators. To enable this, we develop a general convergence theory by analyzing the stochastic gradient (SG) introduced by ACT. We show that the SG can be well approximated by a linearized version, which is unbiased to stochastic compressors. The variance of the linearized gradient has a particularly simple structure that allows a numerical algorithm to predict the variance given a compression strategy. Then, we generate the strategy by approximately solving an integer program. 1 arXiv:2206.11357v4  [cs.LG]  3 Sep 2022We implement our method as a library based on PyTorch that can be quickly integrated into real-world machine learning systems. The library also provides several optimization levels to explore the trade-oï¬€ between memory and speed. We demonstrate the ï¬‚exibility and eï¬ƒciency of GACT on various tasks, including image classiï¬cation, object detection, text, and graph node classiï¬cation. Our evaluation shows that GACT can reduce activation memory by up to 8.1 Ã—, enabling training with a 24.7 Ã—larger batch size on the same GPU. In sum, our main contributions are as follows: â€¢ We propose a general convergence theory for ACT. â€¢ We develop an algorithm that automatically estimates the sensitivity of each compressed tensor and selects the optimal compression strategy. â€¢ We build eï¬ƒcient implementation of GACT in PyTorch with an easy-to-use API that can also be combined with other memory-saving techniques seamlessly. 2 Related Work Activation Compressed Training.ACT has been applied to convolutional NNs using diï¬€erent compressors, such as quantizers [3, 4, 5], JPEG [8], or scientiï¬c data compression algorithms [ 9, 6]. ACT is also applied to transformers [7] and graph NNs [10]. However, the existing theory for ACT [ 3, 4, 5, 6] relies on the case-by-case analysis of speciï¬c network operators, such as convolution, ReLU, and batch normalization. It also requires dedicated implementations for each operator. On the contrary, GACT focuses on the generality of activation compressed training, not a speciï¬c quantizer design, which is the main topic of previous work. Instead of assuming that the network is a stack of layers, GACT formulates the problem as a computational graph of operators. This is general enough to cover transformers [11], graph NNs [12], second-order derivatives, and unknown future architectures. Reduced Precision Training. Apart from ACT, reduced precision training [13, 14, 15, 16, 17, 18] performs calculations directly on low precision data, reducing the computation cost and memory footprint simultaneously. To achieve this, specialized kernels are used to calculate on low precision data. In contrast, ACT only considers storage, and it can thus use more ï¬‚exible compression strategies and achieve a much better compression ratio with the same accuracy loss. Memory-Eï¬ƒcient Training. Gradient checkpointing [19, 20] trades computation for memory by dropping some of the activations in the forward pass from memory and recomputing them in the backward pass. Swapping [21, 22, 23, 24] oï¬„oads activation or model parameters to an external memory (e.g., CPU memory). Recent work [25] explores the possibility of combining the gradient checkpointing and swapping. All these methods save memory by storing fewer tensors on the GPU. In contrast, GACT compresses the saved tensors and is complementary to these approaches. Moreover, the generality of GACT enables easy combination with these methods, which we explore in this paper. 3 Formulation We ï¬rst present the mathematical formulation of our activation compressed training (ACT) framework. As we would like to develop a general ACT algorithm, applicable to a wide range of NN architectures, we make minimal assumptions on our formulation. Throughout the paper, we deï¬ne the variance of a vector x as Var [x] = E [ âˆ¥xâˆ¥2 ] âˆ’âˆ¥E[x]âˆ¥2. 3.1 Activation Compressed Training In this work, we abstract the forward propagation as two functions â„“(x; Î¸) and h(x; Î¸). Both take a datum x and the model parameter Î¸ as the input. The loss function â„“(x; Î¸) outputs the loss of the network Î¸ on datum x. The context function h(x; Î¸) outputs tensors to be stored in the memory for computing the gradients, which are referred as the context. Assume that the context consists of L tensors, where each tensor h(l)(x; Î¸) is represented by a ï¬‚attened Dl-dimensional vector. Denote h(x; Î¸) = ( h(l)(x; Î¸))L l=1. Our notations are somewhat unconventional in the sense that we do not explicitly deï¬ne each layerâ€™s activation. We do not even assume that there is a NN. It could be any computational graph that saves context tensors. Given a dataset X = {xn}N n=1, deï¬ne the batch loss L(Î¸) := 1 N âˆ‘N n=1 â„“(x; Î¸). The dataset can be equivalently represented as an empirical data distribution pX(x) := 1 N âˆ‘N n=1 Î´(xâˆ’xn), where Î´ is the Dirac 2Type\tequation\there. ConvBNConvâ€¦â€¦ReLU ReLUâ€¦â€¦BNConvConv Compressed context tensors (GPU) â„“(ğ‘¥;ğœƒ) Pack hook Unpackhook context â„(ğ‘¥;ğœƒ) Swap outSwap in ğ‘„(â„(ğ‘¥;ğœƒ)) Forward Computation Graph Backward Computation Graph CPU Memory bits ğ‘”(ğ‘„â„(ğ‘¥;ğœƒ);ğœƒ) Adaptive AlgorithmCompressorGACT Decompressor ğ‘¥,\t gradientğ‘”(ğ‘„â„(ğ‘¥;ğœƒ);ğœƒ) (ğœƒ)model Figure 1: The architecture of GACT. delta function. The batch loss can be written as L(Î¸) = EX[â„“(x; Î¸)], where EX denotes for taking expectation over pX. The network is trained with stochastic gradient descent (SGD) [ 26]. Starting from an initial model Î¸0, at the t-th iteration, SGD updates the model with: Î¸t+1 â†Î¸t âˆ’Î·âˆ‡Î¸â„“(x; Î¸t), (1) where Î· is a learning rate, and the SG âˆ‡Î¸â„“(x; Î¸) is computed on a random datum x âˆ¼pX. Notice that EX[âˆ‡Î¸â„“(x; Î¸)] = âˆ‡Î¸L(Î¸), i.e., the SG is an unbiased estimator of the batch gradient âˆ‡Î¸L(Î¸). Crucially, the SG can be written in the form âˆ‡Î¸â„“(x; Î¸t) = g(h(x; Î¸t); Î¸t). In other words, the back propagation only depends on the forward propagation through the context h(x; Î¸t). The entire context must be kept in memory for computing the gradients. The context dominates the memory consumption in many applications. ACT reduces the training memory footprint by compressing the context. Let Q(h) be a compressor, which converts h to compact formats while keeping Q(h) â‰ˆh. Then, ACT computes the gradient with compressed context: Î¸t+1 â†Î¸t âˆ’Î·g(Q(h(x; Î¸t)); Î¸t). (2) We refer to g(Q(h(x; Î¸t); Î¸t) as the activation compressed (AC) gradient. ACT is signiï¬cantly more memory eï¬ƒcient then the plain SGD, Eq. (1), since it only needs to store a compressed version of the context. Suppose the original context h(x; Î¸t) consists of 32-bit ï¬‚oating point tensors, and Q(Â·) is a compressor which quantizes tensors to 2-bit integers, ACT will reduce the context memory by 16 Ã—. Fig. 1 illustrates the computational graph of ACT with these notations. In the following presentation, we might denote h(x,Î¸) simply by h when there is no confusion. 3.2 Convergence of ACT ACT is a lossy approximation of SGD, as it uses an approximate gradient g(Q(h); Î¸). Therefore, some kind of theoretical guarantee is required for ACT to be useful. Fortunately, analyzing ACT is made signiï¬cantly simpler by introducing an unbiased stochastic compressor Q(Â·), such that EQ[Q(x)] = x for any x. EQ[Â·] means taking expectation over the compressor. In this way, g(Q(h); Î¸) can be viewed as a stochastic estimator of the batch gradient âˆ‡L(Î¸), but the randomness comes not only from the datum x but also the compressor Q(Â·). Therefore, ACT is still an SGD algorithm. Standard analytical tools for SGD [ 27] are applicable for studying ACT. SGD algorithms have particular good properties when the SG is unbiased. In our case, this means EQ[g(Q(h); Î¸)] = g(h; Î¸). However, the SG is biased general, even when the stochastic compressor itself is unbiased.1 The key technique in this work is to construct an unbiased approximation of the AC gradient by linearizing 1Consider the exampleg(h) =I(h â‰¥0.5), whereh âˆˆ[0, 1] and its AC gradientg(Q(h)) =I(Q(h) â‰¥0.5) with the compressor Q(h) âˆ¼Bernoulli(h). Then, E [g(Q(h))] =P(Q(h) = 1) =h Ì¸= g(h). 3the gradient function g(Â·; Î¸). Consider the ï¬rst-order Taylor expansion of g(Â·; Î¸) at h: Ë†g(Q(h); h,Î¸) := g(h; Î¸) + J(h,Î¸)âˆ†h, (3) where J(h,Î¸) := âˆ‚g(h;Î¸) âˆ‚h is a Jacobian matrix, âˆ† h := Q(h) âˆ’h is the compression error. We further denote Ë†gxÎ¸(Q(h); h) := Ë†g(Q(h); h,Î¸)|h=h(x;Î¸) and JxÎ¸(h) := J(h,Î¸)|h=h(x;Î¸) for short. Since E[âˆ†h(x; Î¸)] = 0, Ë†gxÎ¸(Q(h); h) is an unbiased SG, Furthermore, the approximation error is small: Proposition 1. Assuming that g(h; Î¸) is twice diï¬€erentiable w.r.t. h, and the second order derivative is bounded, then E[âˆ¥g(Q(h); Î¸) âˆ’Ë†gxÎ¸(Q(h); h)âˆ¥2] = O(VarQ[âˆ†h]). Since âˆ†h itself is unbiased, VarQ[âˆ†h] = EQ [ âˆ¥âˆ†hâˆ¥2 ] is simply the expected compression error. Prop. 1 implies that the linearization error is bounded by the compression error. The linearized gradient Ë†g is accurate if the compression is accurate. Using Ë†g as a bridge, we arrive in the following convergence theorem: Theorem 1. Assume that: A1. L(Î¸) is a continuous diï¬€erentiable, âˆ‡L(Î¸) is Î²-Lipschitz continuous. A2. L(Î¸) is bounded below by Lâˆ—. A3. g(h; Î¸) is diï¬€erentiable w.r.t. h and âˆƒb> 0, s.t. âˆ€Î¸,Eâˆ¥g(Q(h(x; Î¸)); Î¸) âˆ’Ë†gxÎ¸(Q(h); h)âˆ¥â‰¤ b. A4. âˆƒÏƒ2 >0, s.t., âˆ€Î¸, Var [Ë†gxÎ¸(Q(h); h)] â‰¤Ïƒ2. Then, for all Î· <1 2Î², if we run ACT deï¬ned as Eq. (2) for T iterations, then we have min t=0,...,Tâˆ’1 E [ âˆ¥âˆ‡L(Î¸t)âˆ¥2 ] â‰¤4(L(Î¸0) âˆ’Lâˆ—) Î·T + 3b2 + Î·Î²Ïƒ2 Remark: The analytical technique used in Thm. 1 is rather standard, see Thm. 4.8 in [ 27]. However, we consider the variance term Ïƒ2 of the linearized gradient, rather than the SG itself. This formulation brings better analytical properties and an adaptive algorithm for determining the compression scheme, as we shall see soon in Sec. 4. The convergence of ACT is aï¬€ected by both the linearization error (A3) and the variance of the unbiased gradient Ë†g(Â·; Î¸) (A4). The latter is characterized as: Proposition 2. Var [Ë†gxÎ¸(Q(h); h)] = VarX[g(h; Î¸)] + EX[VarQ[Ë†gxÎ¸(Q(h); h)]] , where the second term on the RHS equals to EX[VarQ[JxÎ¸(h)âˆ†h]] = O(VarQ[âˆ†h]) . Prop. 2 separates the variance from diï¬€erent noise sources. VarX[g(h(x,Î¸); Î¸)] is the variance raised by random sampling of data (â€œsampling varianceâ€). EX[VarQ[JxÎ¸(h)âˆ†h(x,Î¸)]] is the variance raised by compression. Now, the convergence in Thm. 1 is depicted by 3 b2 + Î·Î²Ïƒ2. By Prop. 1, b2 = O(VarQ[âˆ†h]2). By Prop. 2, Ïƒ2 = O(1) +O(VarQ[âˆ†h]), since the sampling variance is not aï¬€ected by compression. Therefore, when the compression is accurate (âˆ† h â†’0), the impact of the linearization error is negligible, and the variance of the unbiased gradient dominates. ACT behaves as if the AC gradient is unbiased. 4 Adapting the Compression Rate In a network, some context tensors (such as those stored for computing the cross entropy loss) are extremely sensitive, a small amount of compression would result in diverged training, while other tensors are quite robust to compression. Therefore, we must apply diï¬€erent amounts of compression for each context tensor. As a general framework, we have no prior knowledge of the usersâ€™ model architecture, so we designed an algorithm to infer the sensitivity for each context tensor and determine their compression rate automatically. There is a tradeoï¬€ between the compression error and the storage requirement. We represent the storage requirement of the compressed context in bits per dimension . We assume that bl bits/dim. are used for compression h(l), and Qbl(h(l)) be the compression result. Let b = (bl)L l=1 be a compression scheme, Qb(h) := {Qbl(h(l))}L l=1, and âˆ†bh= Qb(h) âˆ’h. 44.1 Structure of Variance As discussed in Sec. 3.2, when the compression is relatively accurate, the variance plays the main role in determining the convergence. Therefore, we would like to investigate how the compression scheme would impact the variance. Formally, we are interested in: V(b; h,Î¸) := VarQ[Ë†g(Qb(h); h,Î¸)] . Once V(b,h; Î¸) is known, we can ï¬nd the minimum variance compression scheme under a given total bits budget B, by solving the integer programming problem: min b V(b; h(x; Î¸),Î¸), s.t. Lâˆ‘ i=1 blDl â‰¤B, (4) where Dl is the dimensionality of h(l). To proceed, we need the following assumptions on the compressor Qb(Â·): Assumption B1: The compressed result is element-wise uncorrelated. That is, for anyiÌ¸= j, Cov [Qb(h)i,Qb(h)j] = 0. Assumption B2: For compressing h(l)(x; Î¸) to bl bits/dim., the compression error can be written in the form Var [ âˆ†blh(l)(x; Î¸)j ] â‰¤Rlj(x; Î¸)S(bl), where S(bl) is a known function. This isolates the eï¬€ect of bl through the unary factor S(bl). Both assumptions can be achieved by a stochastic rounding quantizer [28], where Rlj(x; Î¸) = 1 4 ( maxkh(l) k âˆ’minkh(l) k )2 and S(b) = (2bl âˆ’1)âˆ’2. See Appendix A.4 for the derivations. The following theorem reveals the structure of the variance: Theorem 2. Under assumptions B1, B2, there exists a family of functions {cl(h,Î¸)}L l=1, such that the compression variance can be written in the form V(b; h,Î¸) â‰¤ Lâˆ‘ l=1 cl(h,Î¸)S(bl). (5) 4.2 Computing Sensitivity Thm. 2 reveals two good properties of the variance: (1) the impact of compressing diï¬€erent context tensors simply sums up, without aï¬€ecting each other; and (2) the compression scheme only impacts the variance through S(bl). Both properties are brought about by linearization. Since S(Â·) is a known function, we only need to know cl(h,Î¸) to solve problem Eq. (4). cl(h,Î¸) can be understood as the sensitivity of the AC gradient to the compression of the l-th tensor. We can compute cl(h,Î¸) numerically by leveraging the idempotence of compressing a tensor: Assumption B3: If h= Q(hâ€²) for some hâ€²with non-zero probability, then Q(h) = h and VarQ[Q(h)] = 0. Let QÂ¬(l) b (h) = {Qb1 (h(1)),...,h (l),...,Q bL(h(L))}be some tensors, where every tensor except h(l) is compressed. Plug h= QÂ¬(l) b (h) into Eq. (5), and use B3, we have V(b; QÂ¬(l) b (h),Î¸) â‰¤cl(QÂ¬(l) b (h),Î¸)S(bl). The left hand side can be approximated by taking Ë†g(Qb(h); h,Î¸) â‰ˆg(Qb(h); Î¸). Assume that cl(Â·,Î¸) is reasonably continuous, we have cl(h,Î¸) â‰ˆVarQ[g(Qb(h); Î¸)] |h=QÂ¬(l) b (h)/S(bl). The variance can be replaced by empirical variance. Alg. 1 illustrates this idea. To compute VarQ[g(Qb(h); Î¸)] at h= QÂ¬(l) b (h), we keep the random seeds ï¬xed for all the compressors except the l-th one. We compute the empirical variance by two evaluations of g(Qb(h); Î¸), which are two NN iterations (forward + backward propagation). Finally, we assume thatc(h,Î¸) remains stable for diï¬€erent mini-batches h, and along the training trajectory (Î¸t). Therefore, we maintain a cl for each tensor l, which is updated by periodically running Alg. 1. Eq. (4) is approximately solved by the O(Llog2 L) greedy algorithm [5]. Another useful feature of this approach is predicting failure (in an a posteriori manner). If the compression variance V(b; h,Î¸) is dominating the overall gradient variance Var [g(Q(h); Î¸t)], compression is adding too much noise to the gradient, and the convergence might be aï¬€ected. The overall gradient variance can be 5Algorithm 1 Numerical algorithm for computing cl(h,Î¸). Require: A gradient evaluation function g(Â·; Î¸) Require: A series of L+ 1 random seeds (rl)L+1 l=1 . Require: Any compression scheme b= (bl)L l=1 âˆ€l, seed Q(l) with rl g0 â†g(Qb(h); Î¸) {First iteration} âˆ€l, seed Q(l) with rl seed Q(l) with rL+1 g1 â†g(Qb(h); Î¸) {Second iteration, with another seed } Return 1 2 âˆ¥g0 âˆ’g1âˆ¥2 /S(bl) 1importtorch2 importgact3 4model = ... # user defined model5 controller = gact.controller(model, opt_level='L2')6  controller.install_hook()78  # trainingloop9forepochin...10 foriterin...11......12 # instruct gact how to perform forward and backward13deffwdbwdprop():14output = model(data)15loss =loss_func(output,target)16optimizer.zero_grad()17loss.backward()1819controller.iterate(fwdbwdprop) Figure 2: Usage example of GACT computed by maintaining a running mean of the gradient. If V(b; Î¸)/Var [Ë†g(Q(h); Î¸t)] is too large, we can raise an alert to the user to increase the storage budget. 5 System Implementation We implemented GACT as a lightweight library in PyTorch. Users can use GACT for any NN architecture with several lines of code change. GACT uses low-level PyTorch hooks to capture context tensors, so it supports arbitrary operators, including custom operators deï¬ned by users. We implemented eï¬ƒcient CUDA kernels to infer tensor sensitivity and to perform compression during run time. GACT uses the same per-group quantizer in ActNN [5] as the compressor. However, GACT diï¬€ers from ActNN in several aspects. ActNN relies on manual analytical deduction to compute the sensitivity for diï¬€erent operators, while GACT infers tensor sensitivity automatically, as described in Sec. 4.2. Moreover, ActNN performs layer-level quantization. It has to implement an activation compressed version for each operator and substitute operators during the training (e.g., replace torch.nn.Conv2d with actnn.Conv2d). In contrast, GACT runs at tensor level and uses a single hook interface to compress saved tensors for all operators. 5.1 General API As shown in Fig. 2, the interface of GACT is straightforward and intuitive, requiring the user to (i) initialize the GACT controller and specify an optimization level (Line 5); (ii) install hooks (Line 6); and (iii) instruct GACT how to perform forward and backward propagation (Lines 13-17) and pass it as a function (fwdbwdprop) to the controller (Line 19). We require users to specify (iii) because GACT needs to numerically run the forward and backward pass to infer tensor sensitivity. Although fwdbwdprop is passed to the controller every iteration, it is only called internally every adapt interval iterations when tensor sensitivity changes. As shown in Sec. 6.1, tensor sensitivity stabilizes quickly after the ï¬rst several epochs, adapt interval can thus be set to a large number, introducing negligible impact on training speed. 65.2 System Architecture Fig. 1 shows an overview of GACT. The GACT controller has three modules: Adaptivate Algorithm; Compressor; and Decompressor. In the forward pass, the controller uses PyTorch pack hook to capture all context tensors. Then Adaptive Algorithm infers tensor sensitivity based on gradients and assigns higher bits to more sensitive tensors, as described in Sec. 4.2. The bits information is used to instruct Compressor to perform quantization. In the backward pass, Decompressor dequantizes context tensors and uses unpack hook to send the dequantized results back to the PyTorchâ€™s auto diï¬€erentiation engine. The controller is also responsible for swapping quantized tensors to the CPU and prefetching them back during the backward propagation if swapping is enabled. 5.3 Identifying Tensors to Quantize The pack hook and unpack hook process all types of context tensors, including activation, parameters trained by the optimizer, and training states such as running mean/variance used by batch normalization. To guarantee that only the activations are quantized, we ï¬lter out saved parameters by recording the data pointers of all the model parameters before training, and we skip quantization if the input tensor pointer exists in the parameter pointer set. Similarly, GACT does not quantize training states by checking if the input tensor requires gradients. However, using hooks blindly disables some memory-saving optimization. For example, in a transformerâ€™s self-attention layer, the keys, query, value tensors are all calculated from the same input tensor. The saved objects of the three operations thus all refer to the same tensor. In this case, PyTorch triggers the pack hook three times. If we perform quantization blindly, we waste computation resources and introduce extra memory consumption because the same underlying tensor is quantized and saved more than once. GACT avoids duplication by generating footprints for each input context tensor. We use the CUDA data pointer, sampled data points, and the tensor statistics (e.g., sum) as the footprint. GACT manages all quantized context tensors and uses the footprint to diï¬€erentiate them. If a tensor is already quantized, GACT will skip quantization and return previous results directly. 5.4 Parallel Swap and Prefetch To further reduce activation memory, we combine GACT with swapping. All compressed tensors are oï¬„oaded to the CPU during the forward pass and swapped back in the backward pass. Here, we replace the original tensor with quantized activation, as data movement is more expensive than computation. Swapping the original tensor saves the quantization overhead but adds more data movement cost between CPU and GPU. As shown in Sec. 6.4, quantization overhead is much smaller than copying full-precision data to CPU in modern GPU architecture. Furthermore, we create two new streams (swap in/out) to parallelize the computation and swapping operation to reduce the swap overhead. The forward computation and swap-out process happen in parallel during the forward pass. During the backward pass, in each layer the swap-in stream is responsible for prefetching the compressed activation of the previous layer to avoid synchronization overhead. We leverage the CUDA event to ensure tasks in diï¬€erent streams are executed in the correct order. 5.5 Other Memory Optimizations Gradient checkpointing. Gradient checkpointing [ 19] works by dividing the NN into segments. The algorithm only stores the inputs of each segment and recomputes the dropped activations segment by segment during backpropagation. The memory consumption is thus the cost of storing the inputs of all segments plus the maximum memory cost to backpropagate each segment. When combined with gradient checkpointing, GACT can reduce the memory consumption of both parts. GACT reduces the memory consumption of the ï¬rst part by quantizing the segment inputs. Moreover, the activations saved during the recompute phase are also quantized, reducing the memory cost of the second part. Combining GACT with gradient checkpointing might introduce more training noise because the recompute starts from quantized segment inputs, making the forward pass of recompute phase not exact. However, in Sec. 6.4, we show the noise introduced by forwarding from the quantized tensors is negligible. Memory eï¬ƒcient self-attention. When the batch size is very large, the single layer after dequantization occupies a large amount of memory and prevents the batch size from increasing further. We observe this 7Table 1: Optimization levels for GACT. Level Compression Strategy Bits L0 Do not compress 32 L1 per-group quantization with auto-precision 4 L2 L1 + swapping/prefetching 4 CB1 L1 + gradient checkpointing 4 CB2 CB1 + eï¬ƒcient self-attention 4 relu conv *pool conv *conv *pool conv *conv *pool conv *conv *pool linear *drop linear *drop linear loss 1 2 4 8 32bits 10âˆ’1 100 101 cl (a) Inferred per-tensor cl (line) and bits/dim. (bar) for VGG-11. Layers with * have a preceding ReLU layer with shared context. drop=dropout, loss=cross entropy loss. 1 2 4 8 bits/dim. 10âˆ’2 10âˆ’1 100 101 grad. var. uniform adapt (b) Gradient variance. 0 50 100 epoch 10âˆ’2 100 102 104 cl (c) Evolution of the per- tensor sensitivity. Each line iscl for a tensor. Figure 3: Eï¬€ectiveness of the adaptive algorithm. problem in transformer-based models where self-attention has quadratic space complexity in terms of sequence length. To reduce the memory footprint of the self-attention layer, we implement the algorithm introduced in [29] that achieves linear space complexity, and combines it with GACT. 5.6 Optimization level To exploit the trade-oï¬€ between memory saving and training speed, GACT provides several optimization levels. Higher levels can save more memory but with more overhead. Tab. 1 lists these optimization levels. L1 uses per-group quantization with the adaptive algorithm. L2 combines per-group quantization with swapping and prefetching. For transformer-based models, CB1 combines GACT with gradient checkpointing. CB2 further reduces the peak memory by adding eï¬ƒcient self-attention to CB1. 6 Experiments We ï¬rst demonstrate the eï¬€ectiveness of the GACT adaptive algorithm. We further apply GACT to a wide range of machine learning tasks, including image classiï¬cation, object detection, text, and graph node classiï¬cation. We compare the training accuracy and activation compression rate for full precision, adaptive 4/3/2 (using GACT to adaptively decide quantization bits with an average of 4/3/2 bit) and ï¬x-4 bit (quantizating all tensors uniformly with 4 bits). Next, we study the trade-oï¬€ between compression rate and training throughput and compare GACT with other state-of-the-art memory-saving methods. Lastly, we demonstrate the ï¬‚exibility of GACT by exploring the possibility of combining it with other memory optimization methods (CB1, CB2 as listed in Table 1). We use open-source model implementations for all tasks. 6.1 Compression Strategy We ï¬rst test the eï¬€ectiveness of our adaptive compression rate algorithm for training VGG-11 [ 30] on ImageNet. Fig. 3(a) plots the inferred per-tensor sensitivity cl and the corresponding optimal bits/dim. GACT assigns more bits to more sensitive layers. The context tensor saved by the cross-entropy loss operator is most sensitive. A small amount of compression leads to a huge gradient variance. This makes sense since the loss is the ï¬rst operator to back-propagate through, where the error accumulates. Therefore, GACT assigns 32 bits/dim. for the tensors in the classiï¬cation head. With the adaptive algorithm, GACT with an average of 4 bits/dim. achieves smaller gradient variance than uniformly assigning 8 bits/dim. for all the tensors, as shown in Fig. 3(b). Finally, Fig. 3(c) shows that the sensitivity cl(h; Î¸t) remains stable during training.Therefore, periodically updating cl at a large interval is reasonable, and this introduces negligible impact on training speed. 8Table 2: For classiï¬cation, we train VGG11 [ 30], ResNet-50 [31], and Swin-Tiny [ 32] on ImageNet [ 33]. For object detection, we train RetinaNet [ 34], Faster R-CNN [35] on Coco [ 36]. We report accuracy on validation sets (Div. indicates diverge) and the compression rate of context tensors (numbers in brackets) for both tasks. Task Model FP32 GACT Adapt 4bit (L1) GACTAdapt 2bit Cls. VGG11 68.75 68.77 (2.84Ã—) 68.49 (3.34Ã—) ResNet-50 77.29 76.96 (6.69Ã—) 76.13 (11.39Ã—) Swin-tiny 81.18 80.92 (7.44Ã—) 77.91 (13.73Ã—) Det. Faster RCNN37.4 37.0 (4.86Ã—) 36.1 (6.81Ã—) RetinaNet 36.5 36.3 (3.11Ã—) Div. 6.2 Optimization level We apply GACT on various computer vision tasks, including image classiï¬cation and object detection, as shown in Fig. 2. We also vary the average bits used by the adaptive algorithm to explore the memory accuracy trade-oï¬€. On both tasks, GACT L1 achieves comparable ( <0.5% accuracy drop) or even better results than the full precision training, while reducing activation memory by up to 7.44 Ã—. Here, we list the accuracy of FP32 as the strongest accuracy baseline. For other lossy methods we consider in Sec. 6.3, the accuracy is no better than FP32, and we list their training accuracy in Appendix C. Notice that here GACT Adapt 2bit diverges on the detection task. This is because, as shown in Sec.3.2, although ACT has unbiased gradients, the compression error and learning rate aï¬€ect the convergence. When using 2 bit, the compression error is large and the learning rate has to be reduced accordingly to guarantee convergence. However, we do not want to slow training by decreasing the learning rate. All experiments are run with the same learning rate as the full precision. Therefore when compression error is large, the training diverges. Furthermore, we observe that the memory reduction varies among networks because GACT does not quantize intermediate states, and the size of intermediate states diï¬€ers between networks. For example, in VGG11, when the batch size is 128, GACT reduces the saved tensor size from 5889MB to 2080MB, among which 78% (1494MB) is used to store the intermediate index for the max-pooling layer that is not quantized by GACT. Next, we demonstrate the ï¬‚exibility of GACT by applying it to a wider variety of natural language processing (NLP) and graph machine learning (Graph) tasks. We run multiple seeds for each task, and we report the mean Â±std of accuracy/F1 across runs as shown in Tab. 3. We include the detailed experimental setup in Appendix B. For both NLP and Graph tasks, GACT L1 achieves comparable training results with FP32, introducing less than 0.3% accuracy/F1-score drop, while reducing activation memory by 4.18 Ã—to 7.93Ã—. Moreover, the results are stable across runs, introducing similar accuracy variance as FP32. We also show the training results of ï¬x-4bit quantization, where all tensors are uniformly quantized with 4 bits. As shown in Tab. 3, ï¬x-4 bit quantization causes signiï¬cant accuracy/F1-score loss on various graph models. For Bert-large, ï¬xed-4 bit quantization works ï¬ne because all the context tensors have similar sensitivity. On the other hand, GACT L1, using a similar amount of memory as always quantizing each layer to 4 bits, still performs on par with full precision training on all the models. This shows the necessity of using adaptive algorithms to assign bits based on tensor sensitivity for stabilized training. Moreover, for Bert-large and three graph models (GCN/GAT/GCNII), GACT converges and gives lossless results with 3 bits. Remarkably, across all the graph models, training with 2-bit GACT causes little accuracy loss ( <1%). This shows the robustness of our adaptive algorithm. 6.3 Memory Saving and Computational Overhead Settings and baselines. We implement the benchmark with PyTorch 1.10 and measure the memory saving and overhead of GACT on an AWS g4dn.4xlarge instance, which has a 16GB NVIDIA T4 GPU and 64GB CPU memory. On ResNet-50, we compare with ActNN [ 5], a dedicated quantization framework for convolutional NNs, and DTR [ 21], a state-of-the-art rematerialization method for dynamic graphs. â€œswapâ€ is a simple swapping strategy that swaps all activations to the CPU. For Bert-large, we also show the results on Mesa [ 7], a memory-saving resource-eï¬ƒcient training framework for transformers, and ZeRO-Oï¬„oad [ 37], a highly optimized system for training large-scale language models. Gradient checkpointing uses the default checkpointing policy provided by the transformer library [ 38], where only the input to each transformer block is saved before the backward pass. On Swin-tiny, we only include Mesa and swap because other baselines 9Table 3: Accuracy and activation compression rate for NLP and Graph tasks. Accuracy that drops > 1% is in italic font. Model Dataset FP32 Fix 4bit GACT Adapt 4bit (L1)GACT Adapt 3bitGACT Adapt 2bit GCN Flickr 51.17Â±0.19 50.93Â±0.16 (7.56Ã—) 51.08Â±0.18 (7.93Ã—) 51.14Â±0.18 (11.34Ã—) 51.20Â±0.18 (17.56Ã—) Reddit 95.33Â±0.07 94.42Â±0.11 (7.55Ã—) 95.32Â±0.07 (7.90Ã—) 95.31Â±0.07 (9.70Ã—) 95.34Â±0.06 (13.68Ã—) Yelp 39.86Â±0.94 39.85Â±1.22 (5.94Ã—) 40.06Â±0.74 (6.42Ã—) 40.21Â±0.82 (7.46Ã—) 39.89Â±1.45 (9.00Ã—) ogbn-arxiv71.51Â±0.65 68.61Â±0.77 (7.54Ã—) 71.35Â±0.36 (8.09Ã—) 70.82Â±0.95 (10.45Ã—) 70.87Â±0.66 (13.75Ã—) GAT Flickr 52.40Â±0.28 35.24Â±11.90 (4.23Ã—) 52.26Â±0.31 (4.34Ã—) 51.68Â±1.13 (5.04Ã—) 51.62Â±1.19 (5.46Ã—) Reddit 95.95Â±0.06 59.37Â±11.48 (4.12Ã—) 96.02Â±0.09 (4.29Ã—) 95.96Â±0.06 (4.64Ã—) 95.82Â±0.06 (5.24Ã—) Yelp 52.41Â±0.69 36.09Â±13.70 (4.04Ã—) 52.18Â±0.38 (4.18Ã—) 51.63Â±0.83 (4.53Ã—) 51.15Â±0.53 (5.24Ã—) ogbn-arxiv71.68Â±0.54 54.64Â±5.62 (5.04Ã—) 71.80Â±0.47 (5.09Ã—) 71.47Â±0.50 (6.14Ã—) 71.21Â±0.68 (6.98Ã—) GCNII Flickr 52.37Â±0.16 52.28Â±0.16 (4.84Ã—) 52.31Â±0.16 (4.91Ã—) 52.36Â±0.16 (5.54Ã—) 52.23Â±0.15 (6.44Ã—) Reddit 96.32Â±0.24 86.50Â±1.08 (4.51Ã—) 96.11Â±0.22 (4.52Ã—) 96.01Â±0.33 (5.16Ã—) 95.54Â±0.29 (5.92Ã—) Yelp 62.33Â±0.20 62.21Â±0.22 (5.26Ã—) 62.28Â±0.26 (5.34Ã—) 62.53Â±0.36 (6.29Ã—) 62.33Â±0.37 (7.28Ã—) ogbn-arxiv72.52Â±0.12 44.57Â±5.01 (6.54Ã—) 72.28Â±0.35 (6.74Ã—) 72.22Â±0.28 (7.98Ã—) 71.74Â±0.26 (10.24Ã—) Bert- large MNLI 86.74Â±0.24 85.98Â±0.16 (7.55Ã—) 86.61Â±0.11 (7.38Ã—) 86.68Â±0.08 (9.13Ã—) 84.24Â±0.74 (12.87Ã—) SST-2 93.69Â±0.30 93.46Â±0.23 (7.55Ã—) 93.54Â±0.52 (7.30Ã—) 93.20Â±0.37 (9.05Ã—) 91.90Â±1.04 (12.91Ã—) MRPC 88.20Â±0.02 87.36Â±0.19 (7.55Ã—) 87.90Â±0.10 (7.40Ã—) 87.69Â±0.07 (9.19Ã—) 82.54Â±0.38 (12.91Ã—) QNLI 92.29Â±0.14 92.34Â±0.07 (7.55Ã—) 92.44Â±0.07 (7.42Ã—) 92.43Â±0.31 (9.19Ã—) 90.74Â±0.13 (12.95Ã—) Table 4: Largest models GACT can train with 16G GPU memory. In ResNet (batch size=64), D (depth): number of layers, W (width): base width of the bottleneck block, R (resolution): width and height of input images. In Bert-large (batch size=16) and GCN, D (depth): number of transformer/gcn blocks, W (width): hidden size. Dim Maximum Value Throughput (TFLOPS) FP L1 L2 FP L1 L2 ResNet- 152 D 160 460 1124 0.43 0.47 0.41 W 88 304 320 0.44 0.89 0.6 R 232 548 716 0.41 0.39 0.44 Bert- large D 32 56 64 0.67 0.56 0.53 W 1280 1488 2032 0.68 0.61 0.60 GCN D 24 152 240 0.20 0.14 0.15 W 2464 3948 4244 0.36 0.38 0.40 lack the support for this network. Results. We compare the training throughput of GACT against other memory saving systems in Fig. 4. On ResNet-50, GACT achieves similar throughput as ActNN (ActNN optimization L5 is not listed because it optimizes PyTorch memory allocation, which is unrelated to quantization and can also be applied to GACT), but ActNN enables training with a larger batch size. This is expected because ActNN implements eï¬ƒcient, customized layers for diï¬€erent operators in convolutional NNs. For Bert-large, Zero-oï¬„oad fails quickly because it only oï¬„oads optimizer states that occupy a small portion of total memory to CPU. GACT L1 outperforms Mesa because Mesa only compresses tensors to 8 bit. When the batch is bigger, the activation size of each segment becomes the memory bottleneck and prevents gradient checkpointing from increasing the batch size. Moreover, combining GACT with gradient checkpointing and eï¬ƒcient self-attention further reduces the peak memory, increasing the batch size by up to 24.7 Ã—. Meanwhile, it introduces a small throughput overhead compared with the original gradient checkpointing. Across all the network architectures, GACT enables training with a 4.2 Ã—to 24.9Ã—larger batch size under the same memory budget. Network scaling. With GACT, we can construct larger models or train with a higher image resolution. Tab. 4 compares the largest model we can train against full precision. With the same batch size and memory budget, GACT can scale a ResNet-152 to 7.0 Ã—deeper, 3.6 Ã—wider or 3.0 Ã—higher resolution. Similarly, Bert-large can be scaled to 2.0 Ã—deeper or 1.6Ã—wider. In GCN, GACT enables training 10.0 Ã—deeper and 1.7Ã—wider network. Overall, GACT maintains 75% - 136% original training throughput. 6.4 Other Optimizations We evaluate the idea of combining GACT with swapping on Bert-large-cased. As shown in Tab. 5, swapping compressed tensors is faster than swapping the original ones because communication between CPU and GPU is more time-consuming than computation. Combining GACT with swapping increases training speed by 10(a) 0 200 400 600 800 Batch Size 0 50 100Training Throughput 4.3Ã— L0 L1 L2 ResNet-50 DTR Swap ActNN GACT (b) 0 100 200 300 400 500 600 Batch Size 0 10 20Training Throughput 24.7Ã— L0 L1 L2 CB1 CB2 Bert-large ZeroOff Swap Mesa CKPT GACT (c) 0 100 200 300 400 500 600 Batch Size 0 25 50 75Training Throughput 5.6Ã— L0 L1 L2 Swin-tiny Mesa Swap GACT Figure 4: Training throughput vs batch size. Red cross mark means out-of-memory. The shaded yellow region denotes the batch sizes with full precision training given the memory budget. CKPT: Gradient checkpointing, ZeroOï¬€: ZeRO-Oï¬„oad. Table 5: Swap and prefetch speed/memory on Bert-large. Algorithm Speed (sequence/s) Peak Mem. (MB) Total Mem. (MB) FP32 16.41 9573 9527 FP32 + swap 6.02 5215 5093 GACT swap 12.95 5426 5325 GACT swap + prefetch14.02 5426 5324 up to 2.3Ã—. Notice here that the peak memory use of â€œGACT swapâ€ is slightly higher than â€œFP32 + swapâ€ because GACT does not quantize and swap intermediate states such as running mean/var of BatchNorm layer. Moreover, prefetch increases the speed by about 7% with negligible memory overhead. We next demonstrate combining GACT with gradient checkpointing (CB1). Gradient checkpointing is performed at the beginning of each transformer block, thus avoiding saving tensors generated within the block. We then apply GACT with gradient checkpointing, where the saved tensors are quantized with 4 bits. As shown in Tab. 6, the accuracy is unaï¬€ected. We also compare the activation memory and peak memory of CB1 and CB2 in Tab. 7. AM2 denotes the peak activation memory, which is the size of saved tensors after reforwarding the ï¬rst transformer block. When batch size = 288, compared with gradient checkpointing on full precision (FP32), CB1 and CB2 reduce the peak activation size by 4.7 Ã—and 5.4Ã—respectively. 7 Conclusion This paper presents GACT, an ACT framework for generic NN architectures. We prove the convergence of GACT without prior knowledge about operator type or network architecture by analyzing a linearized Table 6: Accuracy of Bert-large-cased on SST-2 and QNLI datasets Algorithm SST-2 QNLI Algorithm SST-2 QNLI FP32 93.58 92.42 CB1 93.81 92.26 11Table 7: Memory use of diï¬€erent algorithms on Bert-large. AM1: Activation size before backward, AM2: Activation size after reforwading the ï¬rst transformer block. When batch size = 288, L0 runs out of memory, and therefore it is not listed below. Batch Size Algorithm AM1(MB) AM2(MB) Peak Mem.(MB) 16 L0 4434 - 9573 FP32 + CKPT210 394 5541 CB1 37 99 5286 CB2 31 79 5269 288 FP32 + CKPT3783 7092 12885 CB1 515 1497 8251 CB2 486 1307 8102 approximation of ATCâ€™s gradients. With the adaptive algorithm, GACT achieves negligible accuracy loss on various tasks, reducing activation memory by up to 8.1 Ã—and enabling training with up to 24.7 Ã—batch size compared with full precision training. Acknowledgements This work was supported by the National Key Research and Development Project of China (No. 2021ZD0110502); NSF of China Project (No. 62106120), by the National Science Foundation through grants IIS-1955488, IIS-2027575, CCF-1723352, ARO W911NF2110339, ONR N00014-21-1-2724, and DOE award DE-SC0016260. We would also like to acknowledge partial support from DARPA, IARPA, the Sloan Foundation, NSF, and ONR. Our conclusions do not necessarily reï¬‚ect the position or the policy of our sponsors, and no oï¬ƒcial endorsement should be inferred. References [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. [2] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and eï¬ƒcient sparsity. arXiv preprint arXiv:2101.03961 , 2021. [3] Ayan Chakrabarti and Benjamin Moseley. Backprop with approximate activations for memory-eï¬ƒcient network training. arXiv preprint arXiv:1901.07988 , 2019. [4] Fangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, and Bin Cui. Donâ€™t waste your bits! squeeze activations and gradients for deep neural networks via tinyscript. In International Conference on Machine Learning, pages 3304â€“3314. PMLR, 2020. [5] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael W Mahoney, and Joseph E Gonzalez. Actnn: Reducing training memory footprint via 2-bit activation compressed training. In International Conference on Machine Learning , 2021. [6] R David Evans and Tor Aamodt. AC-GC: Lossy activation compression with guaranteed convergence. Advances in Neural Information Processing Systems , 34, 2021. [7] Zizheng Pan, Peng Chen, Haoyu He, Jing Liu, Jianfei Cai, and Bohan Zhuang. Mesa: A memory-saving training framework for transformers. arXiv preprint arXiv:2111.11124 , 2021. [8] R David Evans, Lufei Liu, and Tor M Aamodt. Jpeg-act: accelerating deep learning via transform-based lossy compression. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), pages 860â€“873. IEEE, 2020. [9] Sian Jin, Guanpeng Li, Shuaiwen Leon Song, and Dingwen Tao. A novel memory-eï¬ƒcient deep learning training framework via error-bounded lossy compression. In 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming , pages 485â€“487, 2021. 12[10] Anonymous. EXACT: Scalable graph neural networks training via extreme activation compression. In Submitted to The Tenth International Conference on Learning Representations , 2022. under review. [11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, / Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998â€“6008, 2017. [12] Thomas N Kipf and Max Welling. Semi-supervised classiï¬cation with graph convolutional networks. arXiv preprint arXiv:1609.02907 , 2016. [13] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations, 2018. [14] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. In International Conference on Learning Representations, 2018. [15] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit ï¬‚oating point numbers. In Advances in Neural Information Processing Systems, pages 7675â€“7684, 2018. [16] Ron Banner, Itay Hubara, Elad Hoï¬€er, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems , pages 5145â€“5153, 2018. [17] Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and Joseph E Gonzalez. A statistical framework for low-bitwidth training of deep neural networks. In Advances in neural information processing systems , 2020. [18] Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkatara- mani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. In Advances in Neural Information Processing Systems , volume 33, 2020. [19] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174 , 2016. [20] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt Keutzer, Ion Stoica, and Joseph E Gonzalez. Checkmate: Breaking the memory wall with optimal tensor rematerialization. arXiv preprint arXiv:1910.02653 , 2019. [21] Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock. Dynamic tensor rematerialization. arXiv preprint arXiv:2006.09616 , 2020. [22] Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping. In Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 1341â€“1355, 2020. [23] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu, and Tim Kraska. Superneurons: Dynamic GPU memory management for training deep neural networks. In 23rd ACM SIGPLAN symposium on principles and practice of parallel programming , pages 41â€“53, 2018. [24] Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma, Qian Xiong, Fan Yang, and Xuehai Qian. Capuchin: Tensor-based gpu memory management for deep learning. In Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 891â€“905, 2020. 13[25] Olivier Beaumont, Lionel Eyraud-Dubois, and Alena Shilova. Eï¬ƒcient combination of rematerialization and oï¬„oading for training dnns. Advances in Neural Information Processing Systems , 34, 2021. [26] LÂ´ eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP- STATâ€™2010, pages 177â€“186. Springer, 2010. [27] LÂ´ eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223â€“311, 2018. [28] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pages 3123â€“3131, 2015. [29] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [30] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. [31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE conference on computer vision and pattern recognition , pages 770â€“778, 2016. [32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. International Conference on Computer Vision (ICCV) , 2021. [33] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE conference on computer vision and pattern recognition , pages 248â€“255. Ieee, 2009. [34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÂ´ ar. Focal loss for dense object detection. In International Conference on Computer Vision (ICCV) , pages 2980â€“2988, 2017. [35] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. Advances in neural information processing systems , 28:91â€“99, 2015. [36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÂ´ ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740â€“755. Springer, 2014. [37] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-oï¬„oad: Democratizing billion-scale model training. arXiv preprint arXiv:2101.06840, 2021. [38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÂ´ emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38â€“45, Online, October 2020. Association for Computational Linguistics. [39] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931 , 2019. [40] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. 14[41] Petar VeliË‡ ckoviÂ´ c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903 , 2017. [42] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International Conference on Machine Learning , pages 1725â€“1735. PMLR, 2020. [43] Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Xingcheng Yao, Aohan Zeng, Shiguang Guo, Peng Zhang, Guohao Dai, Yu Wang, Chang Zhou, Hongxia Yang, and Jie Tang. Cogdl: Toolkit for deep learning on graphs. arXiv preprint arXiv:2103.00959 , 2021. [44] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353â€“355, 2018. [45] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171â€“4186, 2019. 15A Proof of Theorems A.1 Theorem 1: Convergence of ACT Assume that: A1. L(Î¸) is a continuous diï¬€erentiable, âˆ‡L(Î¸) is Î²-Lipschitz continuous. . A2. L(Î¸) is bounded below by Lâˆ—. A3. g(h; Î¸) is diï¬€erentiable w.r.t. h and âˆƒb> 0, s.t. âˆ€Î¸,Eâˆ¥g(Q(h(x,Î¸)); Î¸) âˆ’Ë†g(h(x,Î¸); Î¸)âˆ¥â‰¤ b. A4. âˆƒÏƒ2 >0, s.t., âˆ€Î¸, Var [Ë†g(h(x,Î¸)] â‰¤Ïƒ2. Then, for all Î· <1 2Î², if we run ACT deï¬ned as Eq. (2) for T iterations, then we have min t=0,...,Tâˆ’1 E [ âˆ¥âˆ‡L(Î¸t)âˆ¥2 ] â‰¤4(L(Î¸0) âˆ’Lâˆ—) Î·T + 3b2 + Î·Î²Ïƒ2 Proof. Denote m:= âˆ‡Î¸L(Î¸t), Ïµ:= Ë†g(h(x,Î¸t); Î¸t) âˆ’m, d:= g(Q(h(x; Î¸t)); Î¸t) âˆ’Ë†g(h(x,Î¸t); Î¸t). Then, by A3 and A4, we have E[Ïµ] = E[g(h(x,Î¸t); Î¸t) âˆ’âˆ‡Î¸L(Î¸t)] + E[âŸ¨J(x,Î¸t),âˆ†Q(h(x,Î¸t))âŸ©] = âŸ¨J(x,Î¸t),E[âˆ†Q(h(x,Î¸t))]âŸ©= 0. (6) E [ âˆ¥Ïµâˆ¥2 ] = âˆ¥E[Ïµ]âˆ¥2 + Var [Ïµ] = Var [Ë†g(h(x,Î¸t); Î¸t)] â‰¤Ïƒ2. (7) E[âˆ¥dâˆ¥] â‰¤b. (8) By the deï¬nitions, the ACT dynamics can be written as Î¸t+1 â†Î¸t âˆ’Î·(m+ d+ Ïµ). By A1, we have L(Î¸t+1) â‰¤L(Î¸t) âˆ’Î·âŸ¨m,m + d+ ÏµâŸ©+ Î²Î·2 2 âˆ¥m+ d+ Ïµâˆ¥2 . (9) By Eq.s (6,8) E[âŸ¨m,m + d+ ÏµâŸ©] â‰¥âˆ¥mâˆ¥2 âˆ’âˆ¥mâˆ¥âˆ¥dâˆ¥+ âŸ¨m,E[Ïµ]âŸ©â‰¥âˆ¥ mâˆ¥2 âˆ’âˆ¥mâˆ¥b. (10) By Eq.s (6,7,8), and âˆ¥x+ yâˆ¥2 â‰¤2 âˆ¥xâˆ¥2 + 2âˆ¥yâˆ¥2, E [ âˆ¥m+ d+ Ïµâˆ¥2 ] = E [ âˆ¥m+ dâˆ¥2 ] + Var [Ïµ] â‰¤2E[âˆ¥mâˆ¥]2 + 2E[âˆ¥dâˆ¥]2 + Var [Ïµ] = 2E[âˆ¥mâˆ¥]2 + 2b2 + Ïƒ2. (11) Taking expectation on both sides of Eq. (9), plug in Eq.s (10, 11), and use Î· <1 2Î², we have E[L(Î¸t+1)] â‰¤L(Î¸t) âˆ’Î·(âˆ¥mâˆ¥2 âˆ’âˆ¥mâˆ¥b) + Î²Î·2 2 (2E[âˆ¥mâˆ¥]2 + 2b2 + Ïƒ2). =L(Î¸t) âˆ’(Î·âˆ’Î²Î·2) âˆ¥mâˆ¥2 + Î·âˆ¥mâˆ¥b+ Î²Î·2 2 (2b2 + Ïƒ2) =L(Î¸t) âˆ’Î· 2 âˆ¥mâˆ¥2 + Î·âˆ¥mâˆ¥b+ Î²Î·2 2 (2b2 + Ïƒ2). Completing the squares, E[L(Î¸t+1)] â‰¤L(Î¸t) âˆ’Î· 2(âˆ¥mâˆ¥âˆ’b)2 + Î²Î·2 2 (2b2 + Ïƒ2). Take expectation on both sides and sum up for t= 0,...,T âˆ’1, E[L(Î¸T)] âˆ’L(Î¸0) â‰¤âˆ’Î· 2 Tâˆ’1âˆ‘ t=0 E(âˆ¥âˆ‡L(Î¸t)âˆ¥âˆ’b)2 + Î²Î·2T 2 (2b2 + Ïƒ2). Reorganize the terms, Et [ E(âˆ¥âˆ‡L(Î¸t)âˆ¥âˆ’b)2 ] â‰¤2(L(Î¸0) âˆ’L(Î¸T)) Î·T + Î·Î²(2b2 + Ïƒ2). Let tâˆ—= argmintE[âˆ¥âˆ‡L(Î¸t)âˆ¥], and use A1, we have E(âˆ¥âˆ‡L(Î¸tâˆ—)âˆ¥âˆ’b)2 â‰¤2(L(Î¸0) âˆ’Lâˆ—) Î·T + Î·Î²(2b2 + Ïƒ2). 16Use (a+ b)2 â‰¤2a2 + 2b2, we have E [ âˆ¥âˆ‡L(Î¸tâˆ—)âˆ¥2 ] â‰¤4(L(Î¸0) âˆ’Lâˆ—) Î·T + (2Î²Î·+ 2)b2 + Î·Î²Ïƒ2 â‰¤4(L(Î¸0) âˆ’Lâˆ—) Î·T + 3b2 + Î·Î²Ïƒ2. A.2 Proposition 1: The Linearization Error Proof. Consider the gradient function g(Q(h(x; Î¸); Î¸)), whose output is a P-dimensional vector. Since it is twice diï¬€erentiable, we construct the Taylorâ€™s expansion at h(x; Î¸) with Lagrange remainder: âˆƒH1,...,H P,s.t., âˆ€i, gi(Q(h(x; Î¸)); Î¸) = gi(h(x,Î¸); Î¸) + Ji(x,Î¸)âˆ†h(x,Î¸) + âˆ†h(x,Î¸)âŠ¤Hiâˆ†h(x,Î¸), where Ji(h(x; Î¸),Î¸) := âˆ‚gi(h(x;Î¸);Î¸) âˆ‚h . By the assumption, there exists P >0, such that the linearization error is âˆ¥g(Q(h(x; Î¸)); Î¸) âˆ’Ë†g(h(x; Î¸); h(x; Î¸),Î¸)âˆ¥1 = Pâˆ‘ i=1 âˆ†h(x,Î¸)âŠ¤Hiâˆ†h(x,Î¸) â‰¤Î³P âˆ¥âˆ†h(x,Î¸)âˆ¥2 . Taking expectation, E[âˆ¥g(Q(h(x; Î¸)); h(x; Î¸),Î¸) âˆ’Ë†g(h(x; Î¸); Î¸)âˆ¥2] â‰¤E[âˆ¥g(Q(h(x; Î¸)); Î¸) âˆ’Ë†g(h(x; Î¸); h(x; Î¸),Î¸)âˆ¥1] â‰¤Î³PVar [âˆ†h(x,Î¸)] = O(Var [âˆ†h(x,Î¸)]). A.3 Proposition 2: The Order of the Variance The following proposition is convenient for isolating the diï¬€erent noise sources. Proposition A. (Law of Total Variance) Var [X] = E[Var [X |Y]] + Var [E[X |Y]] . Proof. By deï¬nition Var [Ë†g(h(x; Î¸t); h(x; Î¸),Î¸t)] = Var [g(h(x,Î¸); Î¸)] + Var [J(h(x; Î¸),Î¸)âˆ†h(x,Î¸)] , where Var [g(h(x,Î¸); Î¸)] is the noise introduced by subsampling the data x. By law of total variance, Var [J(h(x; Î¸),Î¸)âˆ†h(x,Î¸)] = EX[VarQ[J(h(x; Î¸); Î¸t)âˆ†h(x,Î¸)]] + VarX[EQ[J(h(x; Î¸); Î¸t)âˆ†h(x,Î¸)]]î´™ î´˜î´— î´š =0 , where VarQ[J(h(x; Î¸); Î¸t)âˆ†h(x,Î¸)] =EQ [ âˆ¥J(h(x; Î¸); Î¸t)âˆ†h(x,Î¸)âˆ¥2 ] â‰¤EQ [ âˆ¥J(h(x; Î¸); Î¸t)âˆ¥2 âˆ¥âˆ†h(x,Î¸)âˆ¥2 ] = âˆ¥J(h(x; Î¸); Î¸t)âˆ¥2 EQ [ âˆ¥âˆ†h(x,Î¸)âˆ¥2 ] = O(Var [âˆ†h(x,Î¸)]) . A.4 Proposition 3: The Structure of the Variance Before investigating the structure of VarQ[J(x; Î¸t)âˆ†h(x,Î¸)], letâ€™s do some recap: the parameter Î¸t is a P-dimensional vector; the context diï¬€erence âˆ† h(x,Î¸) is a D-dimensional vector, and J(x; Î¸t) is a P Ã—D matrix. Recall that âˆ† h(x,Î¸) is the concatenation of L-vectors, âˆ†h(l)(x,Î¸), and let J(l)(x,Î¸) := âˆ‚g âˆ‚h(l) g ( (h(l)(x; Î¸))L l=1,Î¸ ) , which is a P Ã—Dl matrix. Furthermore, let h(l) j (x,Î¸) be the j-th dimension, and J(l) j (x,Î¸) be its j-th column. To proceed, we need to make the following assumptions to the compressor Q(Â·) : RD â†’RD: B1: The compressed result is element-wise uncorrelated. That is, for any iÌ¸= j, Cov [Q(h)i,Q(h)j] = 0. B2: For compressing a vector h to b bits, the compression variance of each dimension can be written in the form Var [Q(h)j] â‰¤Rj(h)S(b), where S(Â·) is a known function. Both assumptions can be achieved by a stochastic rounding [28] quantizer, where Q(h)j = { Tâˆ’1 h,b (âŒˆTh,b(hj)âŒ‰) w.p. Th,b(hj) âˆ’âŒŠTh,b(hj)âŒ‹ Tâˆ’1 h,b (âŒŠTh,b(hj)âŒ‹) otherwise , 17where Th,b(hj) = (2bâˆ’1) hjâˆ’minj h maxj hâˆ’minj h. Since each dimension is quantized independently, B1 is met. Moreover, Var [Q(h)j] â‰¤1 4 (maxjhâˆ’minjh (hj âˆ’minjh) )2 (2b âˆ’1)âˆ’2 = Rj(h)S(b), where Rj(h) = 1 4 (maxjhâˆ’minjh (hj âˆ’minjh) )2 , S (b) = (2b âˆ’1)âˆ’2. Proof. By deï¬nition, J(h; Î¸)âˆ†h= Lâˆ‘ l=1 Dlâˆ‘ j=1 J(l) j (h; Î¸t)âˆ†h(l) j . Using Assumption B1, we have VarQ[J(h; Î¸)âˆ†h] = EQ ï£® ï£¯ï£° îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ Lâˆ‘ l=1 Dlâˆ‘ j=1 J(l) j (h; Î¸t)âˆ†h(l) j îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2ï£¹ ï£ºï£» = Lâˆ‘ l=1 Dlâˆ‘ j=1 EQ [îµ¹îµ¹îµ¹J(l) j (h; Î¸t)âˆ†h(l) j îµ¹îµ¹îµ¹ 2] . = Lâˆ‘ l=1 Dlâˆ‘ j=1 îµ¹îµ¹îµ¹J(l) j (h; Î¸t) îµ¹îµ¹îµ¹ 2 VarQ [ âˆ†h(l) j ] Using Assumption B2, we have VarQ[J(h; Î¸)âˆ†h] â‰¤ Lâˆ‘ l=1 Dlâˆ‘ j=1 îµ¹îµ¹îµ¹J(l) j (h; Î¸t) îµ¹îµ¹îµ¹ 2 Rl(h)S(bl) = Lâˆ‘ l=1 cl(h,Î¸)S(bl), where cl(Î¸,h) := Rl(h) îµ¹îµ¹J(l)(h; Î¸t) îµ¹îµ¹2 F. B Experiment Setup B.1 Node classiï¬cation task on graphs We conduct experiments on four node classiï¬cation datasets with standard splits, including Flickr, Reddit, Yelp from GraphSAINT [39], and ogbn-arxiv from Open Graph Benchmark (OGB) [ 40]. The four datasets cover extensive downstream applications with diï¬€erent scales. We use accuracy as the evaluation metric for multi-class classiï¬cation and micro-F1 for multi-label classiï¬cation. We run ten seeds (0 to 9) and report the average accuracy across runs. We evaluate GACT on three representative GNN models, including GCN [ 12], GAT [41], and GCNII [ 42] under the full-batch training setting. All three models are implemented by CogDL [ 43], a toolkit for graph neural networks. B.2 Text classiï¬cation task We select four largest datasets, MNLI, QQP, SST-2, and QNLI, from the GLUE benchmark [ 44]. The four datasets cover diï¬€erent aspects of natural language understanding, including sentiment classiï¬cation, natural language inference and paraphrase detection. We use the mainstream transformer implementation [ 38] to train Bert-large [45]. We run three seeds (42, 43, 44) and report F1 for QQP, accuracy for the others. C Training Accuracy of Baselines For all the baselines we compared in Sec. 6.3, only ActNN, Mesa, and ZeRO-Oï¬„oad are lossy methods. All other methods are lossless and have the same training accuracy as FP32. For ResNet-50 on ImageNet, the training accuracy for FP32, GACT, ActNN L2, and ActNN L3 are 77.3, 77.0, 77.4, and 76.9. For Bert-Large on SST-2, the accuracy for FP32, GACT, Mesa, and ZeRO-Oï¬„oad are 93.7, 93.5, 93.8, and 93.3. For Swin-tiny on ImageNet, the training accuracy for FP32, GACT, and Mesa are 81.2, 81.0, and 81.3 respectively. 18",
      "meta_data": {
        "arxiv_id": "2206.11357v4",
        "authors": [
          "Xiaoxuan Liu",
          "Lianmin Zheng",
          "Dequan Wang",
          "Yukuo Cen",
          "Weize Chen",
          "Xu Han",
          "Jianfei Chen",
          "Zhiyuan Liu",
          "Jie Tang",
          "Joey Gonzalez",
          "Michael Mahoney",
          "Alvin Cheung"
        ],
        "published_date": "2022-06-22T20:06:23Z",
        "pdf_url": "https://arxiv.org/pdf/2206.11357v4.pdf"
      }
    },
    {
      "title": "GACT: Activation Compressed Training for Generic Network Architectures",
      "abstract": "Training large neural network (NN) models requires extensive memory\nresources, and Activation Compressed Training (ACT) is a promising approach to\nreduce training memory footprint. This paper presents GACT, an ACT framework to\nsupport a broad range of machine learning tasks for generic NN architectures\nwith limited domain knowledge. By analyzing a linearized version of ACT's\napproximate gradient, we prove the convergence of GACT without prior knowledge\non operator type or model architecture. To make training stable, we propose an\nalgorithm that decides the compression ratio for each tensor by estimating its\nimpact on the gradient at run time. We implement GACT as a PyTorch library that\nreadily applies to any NN architecture. GACT reduces the activation memory for\nconvolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training\nwith a 4.2x to 24.7x larger batch size, with negligible accuracy loss. We\nimplement GACT as a PyTorch library at\nhttps://github.com/LiuXiaoxuanPKU/GACT-ICML.",
      "full_text": "GACT: Activation Compressed Training for Generic Network Architectures Xiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, Michael Mahoney, Alvin Cheung University of California, Berkeley xiaoxuan liu@berkeley.edu, jianfeic@tsinghua.edu.cn Abstract Training large neural network (NN) models requires extensive memory resources, and Activation Compressed Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACTâ€™s approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1Ã—, enabling training with a 4.2Ã— to 24.7Ã— larger batch size, with negligible accuracy loss. We implement GACT as a PyTorch library at https://github.com/LiuXiaoxuanPKU/GACT-ICML. 1 Introduction In recent years, we have witnessed the trend of using larger and larger neural network (NN) models to deliver improved accuracy and generalization in various machine learning tasks [1, 2]. However, training these models requires a considerable amount of on-device GPU memory. Unfortunately, the increase of GPU memory capacity has been relatively slow, leading to a fundamental barrier to the development of large NN models. Activation Compressed Training (ACT) is a promising approach to reduce the memory footprint of models during training. As all layersâ€™ activations need to be kept in the memory for computing the gradients during training, ACT reduces memory consumption by compressing these saved activations. Prior work [3, 4, 5, 6] has shown the eï¬€ectiveness of ACT by reducing activation footprint by up to 12 Ã—with 2-bit activations. Although ACT has already demonstrated impressive compression capabilities, previous work on ACT is restricted to speciï¬c NN architectures. For example, ActNN [ 5] is a quantization framework for convolutional NNs only; Mesa [ 7] proposes a per head/layer quantization method for vision transformers; and AC-GC [ 6] derives convergence error bound for diï¬€erent types of operators separately. Developing a generic ACT framework is challenging. Theoretically, convergence guarantees must be made without assumptions on the network architecture. Algorithmically, the framework should ï¬nd eï¬€ective compression strategies for all kinds of networks automatically. From the system perspective, the framework should support arbitrary NN operations, including user-deï¬ned ones. In this work, we propose GACT, a general framework for ACT that is agnostic to the NN architecture. Neither specialized mathematical derivations nor customized implementation is needed to support diï¬€erent operators. To enable this, we develop a general convergence theory by analyzing the stochastic gradient (SG) introduced by ACT. We show that the SG can be well approximated by a linearized version, which is unbiased to stochastic compressors. The variance of the linearized gradient has a particularly simple structure that allows a numerical algorithm to predict the variance given a compression strategy. Then, we generate the strategy by approximately solving an integer program. 1 arXiv:2206.11357v4  [cs.LG]  3 Sep 2022We implement our method as a library based on PyTorch that can be quickly integrated into real-world machine learning systems. The library also provides several optimization levels to explore the trade-oï¬€ between memory and speed. We demonstrate the ï¬‚exibility and eï¬ƒciency of GACT on various tasks, including image classiï¬cation, object detection, text, and graph node classiï¬cation. Our evaluation shows that GACT can reduce activation memory by up to 8.1 Ã—, enabling training with a 24.7 Ã—larger batch size on the same GPU. In sum, our main contributions are as follows: â€¢ We propose a general convergence theory for ACT. â€¢ We develop an algorithm that automatically estimates the sensitivity of each compressed tensor and selects the optimal compression strategy. â€¢ We build eï¬ƒcient implementation of GACT in PyTorch with an easy-to-use API that can also be combined with other memory-saving techniques seamlessly. 2 Related Work Activation Compressed Training.ACT has been applied to convolutional NNs using diï¬€erent compressors, such as quantizers [3, 4, 5], JPEG [8], or scientiï¬c data compression algorithms [ 9, 6]. ACT is also applied to transformers [7] and graph NNs [10]. However, the existing theory for ACT [ 3, 4, 5, 6] relies on the case-by-case analysis of speciï¬c network operators, such as convolution, ReLU, and batch normalization. It also requires dedicated implementations for each operator. On the contrary, GACT focuses on the generality of activation compressed training, not a speciï¬c quantizer design, which is the main topic of previous work. Instead of assuming that the network is a stack of layers, GACT formulates the problem as a computational graph of operators. This is general enough to cover transformers [11], graph NNs [12], second-order derivatives, and unknown future architectures. Reduced Precision Training. Apart from ACT, reduced precision training [13, 14, 15, 16, 17, 18] performs calculations directly on low precision data, reducing the computation cost and memory footprint simultaneously. To achieve this, specialized kernels are used to calculate on low precision data. In contrast, ACT only considers storage, and it can thus use more ï¬‚exible compression strategies and achieve a much better compression ratio with the same accuracy loss. Memory-Eï¬ƒcient Training. Gradient checkpointing [19, 20] trades computation for memory by dropping some of the activations in the forward pass from memory and recomputing them in the backward pass. Swapping [21, 22, 23, 24] oï¬„oads activation or model parameters to an external memory (e.g., CPU memory). Recent work [25] explores the possibility of combining the gradient checkpointing and swapping. All these methods save memory by storing fewer tensors on the GPU. In contrast, GACT compresses the saved tensors and is complementary to these approaches. Moreover, the generality of GACT enables easy combination with these methods, which we explore in this paper. 3 Formulation We ï¬rst present the mathematical formulation of our activation compressed training (ACT) framework. As we would like to develop a general ACT algorithm, applicable to a wide range of NN architectures, we make minimal assumptions on our formulation. Throughout the paper, we deï¬ne the variance of a vector x as Var [x] = E [ âˆ¥xâˆ¥2 ] âˆ’âˆ¥E[x]âˆ¥2. 3.1 Activation Compressed Training In this work, we abstract the forward propagation as two functions â„“(x; Î¸) and h(x; Î¸). Both take a datum x and the model parameter Î¸ as the input. The loss function â„“(x; Î¸) outputs the loss of the network Î¸ on datum x. The context function h(x; Î¸) outputs tensors to be stored in the memory for computing the gradients, which are referred as the context. Assume that the context consists of L tensors, where each tensor h(l)(x; Î¸) is represented by a ï¬‚attened Dl-dimensional vector. Denote h(x; Î¸) = ( h(l)(x; Î¸))L l=1. Our notations are somewhat unconventional in the sense that we do not explicitly deï¬ne each layerâ€™s activation. We do not even assume that there is a NN. It could be any computational graph that saves context tensors. Given a dataset X = {xn}N n=1, deï¬ne the batch loss L(Î¸) := 1 N âˆ‘N n=1 â„“(x; Î¸). The dataset can be equivalently represented as an empirical data distribution pX(x) := 1 N âˆ‘N n=1 Î´(xâˆ’xn), where Î´ is the Dirac 2Type\tequation\there. ConvBNConvâ€¦â€¦ReLU ReLUâ€¦â€¦BNConvConv Compressed context tensors (GPU) â„“(ğ‘¥;ğœƒ) Pack hook Unpackhook context â„(ğ‘¥;ğœƒ) Swap outSwap in ğ‘„(â„(ğ‘¥;ğœƒ)) Forward Computation Graph Backward Computation Graph CPU Memory bits ğ‘”(ğ‘„â„(ğ‘¥;ğœƒ);ğœƒ) Adaptive AlgorithmCompressorGACT Decompressor ğ‘¥,\t gradientğ‘”(ğ‘„â„(ğ‘¥;ğœƒ);ğœƒ) (ğœƒ)model Figure 1: The architecture of GACT. delta function. The batch loss can be written as L(Î¸) = EX[â„“(x; Î¸)], where EX denotes for taking expectation over pX. The network is trained with stochastic gradient descent (SGD) [ 26]. Starting from an initial model Î¸0, at the t-th iteration, SGD updates the model with: Î¸t+1 â†Î¸t âˆ’Î·âˆ‡Î¸â„“(x; Î¸t), (1) where Î· is a learning rate, and the SG âˆ‡Î¸â„“(x; Î¸) is computed on a random datum x âˆ¼pX. Notice that EX[âˆ‡Î¸â„“(x; Î¸)] = âˆ‡Î¸L(Î¸), i.e., the SG is an unbiased estimator of the batch gradient âˆ‡Î¸L(Î¸). Crucially, the SG can be written in the form âˆ‡Î¸â„“(x; Î¸t) = g(h(x; Î¸t); Î¸t). In other words, the back propagation only depends on the forward propagation through the context h(x; Î¸t). The entire context must be kept in memory for computing the gradients. The context dominates the memory consumption in many applications. ACT reduces the training memory footprint by compressing the context. Let Q(h) be a compressor, which converts h to compact formats while keeping Q(h) â‰ˆh. Then, ACT computes the gradient with compressed context: Î¸t+1 â†Î¸t âˆ’Î·g(Q(h(x; Î¸t)); Î¸t). (2) We refer to g(Q(h(x; Î¸t); Î¸t) as the activation compressed (AC) gradient. ACT is signiï¬cantly more memory eï¬ƒcient then the plain SGD, Eq. (1), since it only needs to store a compressed version of the context. Suppose the original context h(x; Î¸t) consists of 32-bit ï¬‚oating point tensors, and Q(Â·) is a compressor which quantizes tensors to 2-bit integers, ACT will reduce the context memory by 16 Ã—. Fig. 1 illustrates the computational graph of ACT with these notations. In the following presentation, we might denote h(x,Î¸) simply by h when there is no confusion. 3.2 Convergence of ACT ACT is a lossy approximation of SGD, as it uses an approximate gradient g(Q(h); Î¸). Therefore, some kind of theoretical guarantee is required for ACT to be useful. Fortunately, analyzing ACT is made signiï¬cantly simpler by introducing an unbiased stochastic compressor Q(Â·), such that EQ[Q(x)] = x for any x. EQ[Â·] means taking expectation over the compressor. In this way, g(Q(h); Î¸) can be viewed as a stochastic estimator of the batch gradient âˆ‡L(Î¸), but the randomness comes not only from the datum x but also the compressor Q(Â·). Therefore, ACT is still an SGD algorithm. Standard analytical tools for SGD [ 27] are applicable for studying ACT. SGD algorithms have particular good properties when the SG is unbiased. In our case, this means EQ[g(Q(h); Î¸)] = g(h; Î¸). However, the SG is biased general, even when the stochastic compressor itself is unbiased.1 The key technique in this work is to construct an unbiased approximation of the AC gradient by linearizing 1Consider the exampleg(h) =I(h â‰¥0.5), whereh âˆˆ[0, 1] and its AC gradientg(Q(h)) =I(Q(h) â‰¥0.5) with the compressor Q(h) âˆ¼Bernoulli(h). Then, E [g(Q(h))] =P(Q(h) = 1) =h Ì¸= g(h). 3the gradient function g(Â·; Î¸). Consider the ï¬rst-order Taylor expansion of g(Â·; Î¸) at h: Ë†g(Q(h); h,Î¸) := g(h; Î¸) + J(h,Î¸)âˆ†h, (3) where J(h,Î¸) := âˆ‚g(h;Î¸) âˆ‚h is a Jacobian matrix, âˆ† h := Q(h) âˆ’h is the compression error. We further denote Ë†gxÎ¸(Q(h); h) := Ë†g(Q(h); h,Î¸)|h=h(x;Î¸) and JxÎ¸(h) := J(h,Î¸)|h=h(x;Î¸) for short. Since E[âˆ†h(x; Î¸)] = 0, Ë†gxÎ¸(Q(h); h) is an unbiased SG, Furthermore, the approximation error is small: Proposition 1. Assuming that g(h; Î¸) is twice diï¬€erentiable w.r.t. h, and the second order derivative is bounded, then E[âˆ¥g(Q(h); Î¸) âˆ’Ë†gxÎ¸(Q(h); h)âˆ¥2] = O(VarQ[âˆ†h]). Since âˆ†h itself is unbiased, VarQ[âˆ†h] = EQ [ âˆ¥âˆ†hâˆ¥2 ] is simply the expected compression error. Prop. 1 implies that the linearization error is bounded by the compression error. The linearized gradient Ë†g is accurate if the compression is accurate. Using Ë†g as a bridge, we arrive in the following convergence theorem: Theorem 1. Assume that: A1. L(Î¸) is a continuous diï¬€erentiable, âˆ‡L(Î¸) is Î²-Lipschitz continuous. A2. L(Î¸) is bounded below by Lâˆ—. A3. g(h; Î¸) is diï¬€erentiable w.r.t. h and âˆƒb> 0, s.t. âˆ€Î¸,Eâˆ¥g(Q(h(x; Î¸)); Î¸) âˆ’Ë†gxÎ¸(Q(h); h)âˆ¥â‰¤ b. A4. âˆƒÏƒ2 >0, s.t., âˆ€Î¸, Var [Ë†gxÎ¸(Q(h); h)] â‰¤Ïƒ2. Then, for all Î· <1 2Î², if we run ACT deï¬ned as Eq. (2) for T iterations, then we have min t=0,...,Tâˆ’1 E [ âˆ¥âˆ‡L(Î¸t)âˆ¥2 ] â‰¤4(L(Î¸0) âˆ’Lâˆ—) Î·T + 3b2 + Î·Î²Ïƒ2 Remark: The analytical technique used in Thm. 1 is rather standard, see Thm. 4.8 in [ 27]. However, we consider the variance term Ïƒ2 of the linearized gradient, rather than the SG itself. This formulation brings better analytical properties and an adaptive algorithm for determining the compression scheme, as we shall see soon in Sec. 4. The convergence of ACT is aï¬€ected by both the linearization error (A3) and the variance of the unbiased gradient Ë†g(Â·; Î¸) (A4). The latter is characterized as: Proposition 2. Var [Ë†gxÎ¸(Q(h); h)] = VarX[g(h; Î¸)] + EX[VarQ[Ë†gxÎ¸(Q(h); h)]] , where the second term on the RHS equals to EX[VarQ[JxÎ¸(h)âˆ†h]] = O(VarQ[âˆ†h]) . Prop. 2 separates the variance from diï¬€erent noise sources. VarX[g(h(x,Î¸); Î¸)] is the variance raised by random sampling of data (â€œsampling varianceâ€). EX[VarQ[JxÎ¸(h)âˆ†h(x,Î¸)]] is the variance raised by compression. Now, the convergence in Thm. 1 is depicted by 3 b2 + Î·Î²Ïƒ2. By Prop. 1, b2 = O(VarQ[âˆ†h]2). By Prop. 2, Ïƒ2 = O(1) +O(VarQ[âˆ†h]), since the sampling variance is not aï¬€ected by compression. Therefore, when the compression is accurate (âˆ† h â†’0), the impact of the linearization error is negligible, and the variance of the unbiased gradient dominates. ACT behaves as if the AC gradient is unbiased. 4 Adapting the Compression Rate In a network, some context tensors (such as those stored for computing the cross entropy loss) are extremely sensitive, a small amount of compression would result in diverged training, while other tensors are quite robust to compression. Therefore, we must apply diï¬€erent amounts of compression for each context tensor. As a general framework, we have no prior knowledge of the usersâ€™ model architecture, so we designed an algorithm to infer the sensitivity for each context tensor and determine their compression rate automatically. There is a tradeoï¬€ between the compression error and the storage requirement. We represent the storage requirement of the compressed context in bits per dimension . We assume that bl bits/dim. are used for compression h(l), and Qbl(h(l)) be the compression result. Let b = (bl)L l=1 be a compression scheme, Qb(h) := {Qbl(h(l))}L l=1, and âˆ†bh= Qb(h) âˆ’h. 44.1 Structure of Variance As discussed in Sec. 3.2, when the compression is relatively accurate, the variance plays the main role in determining the convergence. Therefore, we would like to investigate how the compression scheme would impact the variance. Formally, we are interested in: V(b; h,Î¸) := VarQ[Ë†g(Qb(h); h,Î¸)] . Once V(b,h; Î¸) is known, we can ï¬nd the minimum variance compression scheme under a given total bits budget B, by solving the integer programming problem: min b V(b; h(x; Î¸),Î¸), s.t. Lâˆ‘ i=1 blDl â‰¤B, (4) where Dl is the dimensionality of h(l). To proceed, we need the following assumptions on the compressor Qb(Â·): Assumption B1: The compressed result is element-wise uncorrelated. That is, for anyiÌ¸= j, Cov [Qb(h)i,Qb(h)j] = 0. Assumption B2: For compressing h(l)(x; Î¸) to bl bits/dim., the compression error can be written in the form Var [ âˆ†blh(l)(x; Î¸)j ] â‰¤Rlj(x; Î¸)S(bl), where S(bl) is a known function. This isolates the eï¬€ect of bl through the unary factor S(bl). Both assumptions can be achieved by a stochastic rounding quantizer [28], where Rlj(x; Î¸) = 1 4 ( maxkh(l) k âˆ’minkh(l) k )2 and S(b) = (2bl âˆ’1)âˆ’2. See Appendix A.4 for the derivations. The following theorem reveals the structure of the variance: Theorem 2. Under assumptions B1, B2, there exists a family of functions {cl(h,Î¸)}L l=1, such that the compression variance can be written in the form V(b; h,Î¸) â‰¤ Lâˆ‘ l=1 cl(h,Î¸)S(bl). (5) 4.2 Computing Sensitivity Thm. 2 reveals two good properties of the variance: (1) the impact of compressing diï¬€erent context tensors simply sums up, without aï¬€ecting each other; and (2) the compression scheme only impacts the variance through S(bl). Both properties are brought about by linearization. Since S(Â·) is a known function, we only need to know cl(h,Î¸) to solve problem Eq. (4). cl(h,Î¸) can be understood as the sensitivity of the AC gradient to the compression of the l-th tensor. We can compute cl(h,Î¸) numerically by leveraging the idempotence of compressing a tensor: Assumption B3: If h= Q(hâ€²) for some hâ€²with non-zero probability, then Q(h) = h and VarQ[Q(h)] = 0. Let QÂ¬(l) b (h) = {Qb1 (h(1)),...,h (l),...,Q bL(h(L))}be some tensors, where every tensor except h(l) is compressed. Plug h= QÂ¬(l) b (h) into Eq. (5), and use B3, we have V(b; QÂ¬(l) b (h),Î¸) â‰¤cl(QÂ¬(l) b (h),Î¸)S(bl). The left hand side can be approximated by taking Ë†g(Qb(h); h,Î¸) â‰ˆg(Qb(h); Î¸). Assume that cl(Â·,Î¸) is reasonably continuous, we have cl(h,Î¸) â‰ˆVarQ[g(Qb(h); Î¸)] |h=QÂ¬(l) b (h)/S(bl). The variance can be replaced by empirical variance. Alg. 1 illustrates this idea. To compute VarQ[g(Qb(h); Î¸)] at h= QÂ¬(l) b (h), we keep the random seeds ï¬xed for all the compressors except the l-th one. We compute the empirical variance by two evaluations of g(Qb(h); Î¸), which are two NN iterations (forward + backward propagation). Finally, we assume thatc(h,Î¸) remains stable for diï¬€erent mini-batches h, and along the training trajectory (Î¸t). Therefore, we maintain a cl for each tensor l, which is updated by periodically running Alg. 1. Eq. (4) is approximately solved by the O(Llog2 L) greedy algorithm [5]. Another useful feature of this approach is predicting failure (in an a posteriori manner). If the compression variance V(b; h,Î¸) is dominating the overall gradient variance Var [g(Q(h); Î¸t)], compression is adding too much noise to the gradient, and the convergence might be aï¬€ected. The overall gradient variance can be 5Algorithm 1 Numerical algorithm for computing cl(h,Î¸). Require: A gradient evaluation function g(Â·; Î¸) Require: A series of L+ 1 random seeds (rl)L+1 l=1 . Require: Any compression scheme b= (bl)L l=1 âˆ€l, seed Q(l) with rl g0 â†g(Qb(h); Î¸) {First iteration} âˆ€l, seed Q(l) with rl seed Q(l) with rL+1 g1 â†g(Qb(h); Î¸) {Second iteration, with another seed } Return 1 2 âˆ¥g0 âˆ’g1âˆ¥2 /S(bl) 1importtorch2 importgact3 4model = ... # user defined model5 controller = gact.controller(model, opt_level='L2')6  controller.install_hook()78  # trainingloop9forepochin...10 foriterin...11......12 # instruct gact how to perform forward and backward13deffwdbwdprop():14output = model(data)15loss =loss_func(output,target)16optimizer.zero_grad()17loss.backward()1819controller.iterate(fwdbwdprop) Figure 2: Usage example of GACT computed by maintaining a running mean of the gradient. If V(b; Î¸)/Var [Ë†g(Q(h); Î¸t)] is too large, we can raise an alert to the user to increase the storage budget. 5 System Implementation We implemented GACT as a lightweight library in PyTorch. Users can use GACT for any NN architecture with several lines of code change. GACT uses low-level PyTorch hooks to capture context tensors, so it supports arbitrary operators, including custom operators deï¬ned by users. We implemented eï¬ƒcient CUDA kernels to infer tensor sensitivity and to perform compression during run time. GACT uses the same per-group quantizer in ActNN [5] as the compressor. However, GACT diï¬€ers from ActNN in several aspects. ActNN relies on manual analytical deduction to compute the sensitivity for diï¬€erent operators, while GACT infers tensor sensitivity automatically, as described in Sec. 4.2. Moreover, ActNN performs layer-level quantization. It has to implement an activation compressed version for each operator and substitute operators during the training (e.g., replace torch.nn.Conv2d with actnn.Conv2d). In contrast, GACT runs at tensor level and uses a single hook interface to compress saved tensors for all operators. 5.1 General API As shown in Fig. 2, the interface of GACT is straightforward and intuitive, requiring the user to (i) initialize the GACT controller and specify an optimization level (Line 5); (ii) install hooks (Line 6); and (iii) instruct GACT how to perform forward and backward propagation (Lines 13-17) and pass it as a function (fwdbwdprop) to the controller (Line 19). We require users to specify (iii) because GACT needs to numerically run the forward and backward pass to infer tensor sensitivity. Although fwdbwdprop is passed to the controller every iteration, it is only called internally every adapt interval iterations when tensor sensitivity changes. As shown in Sec. 6.1, tensor sensitivity stabilizes quickly after the ï¬rst several epochs, adapt interval can thus be set to a large number, introducing negligible impact on training speed. 65.2 System Architecture Fig. 1 shows an overview of GACT. The GACT controller has three modules: Adaptivate Algorithm; Compressor; and Decompressor. In the forward pass, the controller uses PyTorch pack hook to capture all context tensors. Then Adaptive Algorithm infers tensor sensitivity based on gradients and assigns higher bits to more sensitive tensors, as described in Sec. 4.2. The bits information is used to instruct Compressor to perform quantization. In the backward pass, Decompressor dequantizes context tensors and uses unpack hook to send the dequantized results back to the PyTorchâ€™s auto diï¬€erentiation engine. The controller is also responsible for swapping quantized tensors to the CPU and prefetching them back during the backward propagation if swapping is enabled. 5.3 Identifying Tensors to Quantize The pack hook and unpack hook process all types of context tensors, including activation, parameters trained by the optimizer, and training states such as running mean/variance used by batch normalization. To guarantee that only the activations are quantized, we ï¬lter out saved parameters by recording the data pointers of all the model parameters before training, and we skip quantization if the input tensor pointer exists in the parameter pointer set. Similarly, GACT does not quantize training states by checking if the input tensor requires gradients. However, using hooks blindly disables some memory-saving optimization. For example, in a transformerâ€™s self-attention layer, the keys, query, value tensors are all calculated from the same input tensor. The saved objects of the three operations thus all refer to the same tensor. In this case, PyTorch triggers the pack hook three times. If we perform quantization blindly, we waste computation resources and introduce extra memory consumption because the same underlying tensor is quantized and saved more than once. GACT avoids duplication by generating footprints for each input context tensor. We use the CUDA data pointer, sampled data points, and the tensor statistics (e.g., sum) as the footprint. GACT manages all quantized context tensors and uses the footprint to diï¬€erentiate them. If a tensor is already quantized, GACT will skip quantization and return previous results directly. 5.4 Parallel Swap and Prefetch To further reduce activation memory, we combine GACT with swapping. All compressed tensors are oï¬„oaded to the CPU during the forward pass and swapped back in the backward pass. Here, we replace the original tensor with quantized activation, as data movement is more expensive than computation. Swapping the original tensor saves the quantization overhead but adds more data movement cost between CPU and GPU. As shown in Sec. 6.4, quantization overhead is much smaller than copying full-precision data to CPU in modern GPU architecture. Furthermore, we create two new streams (swap in/out) to parallelize the computation and swapping operation to reduce the swap overhead. The forward computation and swap-out process happen in parallel during the forward pass. During the backward pass, in each layer the swap-in stream is responsible for prefetching the compressed activation of the previous layer to avoid synchronization overhead. We leverage the CUDA event to ensure tasks in diï¬€erent streams are executed in the correct order. 5.5 Other Memory Optimizations Gradient checkpointing. Gradient checkpointing [ 19] works by dividing the NN into segments. The algorithm only stores the inputs of each segment and recomputes the dropped activations segment by segment during backpropagation. The memory consumption is thus the cost of storing the inputs of all segments plus the maximum memory cost to backpropagate each segment. When combined with gradient checkpointing, GACT can reduce the memory consumption of both parts. GACT reduces the memory consumption of the ï¬rst part by quantizing the segment inputs. Moreover, the activations saved during the recompute phase are also quantized, reducing the memory cost of the second part. Combining GACT with gradient checkpointing might introduce more training noise because the recompute starts from quantized segment inputs, making the forward pass of recompute phase not exact. However, in Sec. 6.4, we show the noise introduced by forwarding from the quantized tensors is negligible. Memory eï¬ƒcient self-attention. When the batch size is very large, the single layer after dequantization occupies a large amount of memory and prevents the batch size from increasing further. We observe this 7Table 1: Optimization levels for GACT. Level Compression Strategy Bits L0 Do not compress 32 L1 per-group quantization with auto-precision 4 L2 L1 + swapping/prefetching 4 CB1 L1 + gradient checkpointing 4 CB2 CB1 + eï¬ƒcient self-attention 4 relu conv *pool conv *conv *pool conv *conv *pool conv *conv *pool linear *drop linear *drop linear loss 1 2 4 8 32bits 10âˆ’1 100 101 cl (a) Inferred per-tensor cl (line) and bits/dim. (bar) for VGG-11. Layers with * have a preceding ReLU layer with shared context. drop=dropout, loss=cross entropy loss. 1 2 4 8 bits/dim. 10âˆ’2 10âˆ’1 100 101 grad. var. uniform adapt (b) Gradient variance. 0 50 100 epoch 10âˆ’2 100 102 104 cl (c) Evolution of the per- tensor sensitivity. Each line iscl for a tensor. Figure 3: Eï¬€ectiveness of the adaptive algorithm. problem in transformer-based models where self-attention has quadratic space complexity in terms of sequence length. To reduce the memory footprint of the self-attention layer, we implement the algorithm introduced in [29] that achieves linear space complexity, and combines it with GACT. 5.6 Optimization level To exploit the trade-oï¬€ between memory saving and training speed, GACT provides several optimization levels. Higher levels can save more memory but with more overhead. Tab. 1 lists these optimization levels. L1 uses per-group quantization with the adaptive algorithm. L2 combines per-group quantization with swapping and prefetching. For transformer-based models, CB1 combines GACT with gradient checkpointing. CB2 further reduces the peak memory by adding eï¬ƒcient self-attention to CB1. 6 Experiments We ï¬rst demonstrate the eï¬€ectiveness of the GACT adaptive algorithm. We further apply GACT to a wide range of machine learning tasks, including image classiï¬cation, object detection, text, and graph node classiï¬cation. We compare the training accuracy and activation compression rate for full precision, adaptive 4/3/2 (using GACT to adaptively decide quantization bits with an average of 4/3/2 bit) and ï¬x-4 bit (quantizating all tensors uniformly with 4 bits). Next, we study the trade-oï¬€ between compression rate and training throughput and compare GACT with other state-of-the-art memory-saving methods. Lastly, we demonstrate the ï¬‚exibility of GACT by exploring the possibility of combining it with other memory optimization methods (CB1, CB2 as listed in Table 1). We use open-source model implementations for all tasks. 6.1 Compression Strategy We ï¬rst test the eï¬€ectiveness of our adaptive compression rate algorithm for training VGG-11 [ 30] on ImageNet. Fig. 3(a) plots the inferred per-tensor sensitivity cl and the corresponding optimal bits/dim. GACT assigns more bits to more sensitive layers. The context tensor saved by the cross-entropy loss operator is most sensitive. A small amount of compression leads to a huge gradient variance. This makes sense since the loss is the ï¬rst operator to back-propagate through, where the error accumulates. Therefore, GACT assigns 32 bits/dim. for the tensors in the classiï¬cation head. With the adaptive algorithm, GACT with an average of 4 bits/dim. achieves smaller gradient variance than uniformly assigning 8 bits/dim. for all the tensors, as shown in Fig. 3(b). Finally, Fig. 3(c) shows that the sensitivity cl(h; Î¸t) remains stable during training.Therefore, periodically updating cl at a large interval is reasonable, and this introduces negligible impact on training speed. 8Table 2: For classiï¬cation, we train VGG11 [ 30], ResNet-50 [31], and Swin-Tiny [ 32] on ImageNet [ 33]. For object detection, we train RetinaNet [ 34], Faster R-CNN [35] on Coco [ 36]. We report accuracy on validation sets (Div. indicates diverge) and the compression rate of context tensors (numbers in brackets) for both tasks. Task Model FP32 GACT Adapt 4bit (L1) GACTAdapt 2bit Cls. VGG11 68.75 68.77 (2.84Ã—) 68.49 (3.34Ã—) ResNet-50 77.29 76.96 (6.69Ã—) 76.13 (11.39Ã—) Swin-tiny 81.18 80.92 (7.44Ã—) 77.91 (13.73Ã—) Det. Faster RCNN37.4 37.0 (4.86Ã—) 36.1 (6.81Ã—) RetinaNet 36.5 36.3 (3.11Ã—) Div. 6.2 Optimization level We apply GACT on various computer vision tasks, including image classiï¬cation and object detection, as shown in Fig. 2. We also vary the average bits used by the adaptive algorithm to explore the memory accuracy trade-oï¬€. On both tasks, GACT L1 achieves comparable ( <0.5% accuracy drop) or even better results than the full precision training, while reducing activation memory by up to 7.44 Ã—. Here, we list the accuracy of FP32 as the strongest accuracy baseline. For other lossy methods we consider in Sec. 6.3, the accuracy is no better than FP32, and we list their training accuracy in Appendix C. Notice that here GACT Adapt 2bit diverges on the detection task. This is because, as shown in Sec.3.2, although ACT has unbiased gradients, the compression error and learning rate aï¬€ect the convergence. When using 2 bit, the compression error is large and the learning rate has to be reduced accordingly to guarantee convergence. However, we do not want to slow training by decreasing the learning rate. All experiments are run with the same learning rate as the full precision. Therefore when compression error is large, the training diverges. Furthermore, we observe that the memory reduction varies among networks because GACT does not quantize intermediate states, and the size of intermediate states diï¬€ers between networks. For example, in VGG11, when the batch size is 128, GACT reduces the saved tensor size from 5889MB to 2080MB, among which 78% (1494MB) is used to store the intermediate index for the max-pooling layer that is not quantized by GACT. Next, we demonstrate the ï¬‚exibility of GACT by applying it to a wider variety of natural language processing (NLP) and graph machine learning (Graph) tasks. We run multiple seeds for each task, and we report the mean Â±std of accuracy/F1 across runs as shown in Tab. 3. We include the detailed experimental setup in Appendix B. For both NLP and Graph tasks, GACT L1 achieves comparable training results with FP32, introducing less than 0.3% accuracy/F1-score drop, while reducing activation memory by 4.18 Ã—to 7.93Ã—. Moreover, the results are stable across runs, introducing similar accuracy variance as FP32. We also show the training results of ï¬x-4bit quantization, where all tensors are uniformly quantized with 4 bits. As shown in Tab. 3, ï¬x-4 bit quantization causes signiï¬cant accuracy/F1-score loss on various graph models. For Bert-large, ï¬xed-4 bit quantization works ï¬ne because all the context tensors have similar sensitivity. On the other hand, GACT L1, using a similar amount of memory as always quantizing each layer to 4 bits, still performs on par with full precision training on all the models. This shows the necessity of using adaptive algorithms to assign bits based on tensor sensitivity for stabilized training. Moreover, for Bert-large and three graph models (GCN/GAT/GCNII), GACT converges and gives lossless results with 3 bits. Remarkably, across all the graph models, training with 2-bit GACT causes little accuracy loss ( <1%). This shows the robustness of our adaptive algorithm. 6.3 Memory Saving and Computational Overhead Settings and baselines. We implement the benchmark with PyTorch 1.10 and measure the memory saving and overhead of GACT on an AWS g4dn.4xlarge instance, which has a 16GB NVIDIA T4 GPU and 64GB CPU memory. On ResNet-50, we compare with ActNN [ 5], a dedicated quantization framework for convolutional NNs, and DTR [ 21], a state-of-the-art rematerialization method for dynamic graphs. â€œswapâ€ is a simple swapping strategy that swaps all activations to the CPU. For Bert-large, we also show the results on Mesa [ 7], a memory-saving resource-eï¬ƒcient training framework for transformers, and ZeRO-Oï¬„oad [ 37], a highly optimized system for training large-scale language models. Gradient checkpointing uses the default checkpointing policy provided by the transformer library [ 38], where only the input to each transformer block is saved before the backward pass. On Swin-tiny, we only include Mesa and swap because other baselines 9Table 3: Accuracy and activation compression rate for NLP and Graph tasks. Accuracy that drops > 1% is in italic font. Model Dataset FP32 Fix 4bit GACT Adapt 4bit (L1)GACT Adapt 3bitGACT Adapt 2bit GCN Flickr 51.17Â±0.19 50.93Â±0.16 (7.56Ã—) 51.08Â±0.18 (7.93Ã—) 51.14Â±0.18 (11.34Ã—) 51.20Â±0.18 (17.56Ã—) Reddit 95.33Â±0.07 94.42Â±0.11 (7.55Ã—) 95.32Â±0.07 (7.90Ã—) 95.31Â±0.07 (9.70Ã—) 95.34Â±0.06 (13.68Ã—) Yelp 39.86Â±0.94 39.85Â±1.22 (5.94Ã—) 40.06Â±0.74 (6.42Ã—) 40.21Â±0.82 (7.46Ã—) 39.89Â±1.45 (9.00Ã—) ogbn-arxiv71.51Â±0.65 68.61Â±0.77 (7.54Ã—) 71.35Â±0.36 (8.09Ã—) 70.82Â±0.95 (10.45Ã—) 70.87Â±0.66 (13.75Ã—) GAT Flickr 52.40Â±0.28 35.24Â±11.90 (4.23Ã—) 52.26Â±0.31 (4.34Ã—) 51.68Â±1.13 (5.04Ã—) 51.62Â±1.19 (5.46Ã—) Reddit 95.95Â±0.06 59.37Â±11.48 (4.12Ã—) 96.02Â±0.09 (4.29Ã—) 95.96Â±0.06 (4.64Ã—) 95.82Â±0.06 (5.24Ã—) Yelp 52.41Â±0.69 36.09Â±13.70 (4.04Ã—) 52.18Â±0.38 (4.18Ã—) 51.63Â±0.83 (4.53Ã—) 51.15Â±0.53 (5.24Ã—) ogbn-arxiv71.68Â±0.54 54.64Â±5.62 (5.04Ã—) 71.80Â±0.47 (5.09Ã—) 71.47Â±0.50 (6.14Ã—) 71.21Â±0.68 (6.98Ã—) GCNII Flickr 52.37Â±0.16 52.28Â±0.16 (4.84Ã—) 52.31Â±0.16 (4.91Ã—) 52.36Â±0.16 (5.54Ã—) 52.23Â±0.15 (6.44Ã—) Reddit 96.32Â±0.24 86.50Â±1.08 (4.51Ã—) 96.11Â±0.22 (4.52Ã—) 96.01Â±0.33 (5.16Ã—) 95.54Â±0.29 (5.92Ã—) Yelp 62.33Â±0.20 62.21Â±0.22 (5.26Ã—) 62.28Â±0.26 (5.34Ã—) 62.53Â±0.36 (6.29Ã—) 62.33Â±0.37 (7.28Ã—) ogbn-arxiv72.52Â±0.12 44.57Â±5.01 (6.54Ã—) 72.28Â±0.35 (6.74Ã—) 72.22Â±0.28 (7.98Ã—) 71.74Â±0.26 (10.24Ã—) Bert- large MNLI 86.74Â±0.24 85.98Â±0.16 (7.55Ã—) 86.61Â±0.11 (7.38Ã—) 86.68Â±0.08 (9.13Ã—) 84.24Â±0.74 (12.87Ã—) SST-2 93.69Â±0.30 93.46Â±0.23 (7.55Ã—) 93.54Â±0.52 (7.30Ã—) 93.20Â±0.37 (9.05Ã—) 91.90Â±1.04 (12.91Ã—) MRPC 88.20Â±0.02 87.36Â±0.19 (7.55Ã—) 87.90Â±0.10 (7.40Ã—) 87.69Â±0.07 (9.19Ã—) 82.54Â±0.38 (12.91Ã—) QNLI 92.29Â±0.14 92.34Â±0.07 (7.55Ã—) 92.44Â±0.07 (7.42Ã—) 92.43Â±0.31 (9.19Ã—) 90.74Â±0.13 (12.95Ã—) Table 4: Largest models GACT can train with 16G GPU memory. In ResNet (batch size=64), D (depth): number of layers, W (width): base width of the bottleneck block, R (resolution): width and height of input images. In Bert-large (batch size=16) and GCN, D (depth): number of transformer/gcn blocks, W (width): hidden size. Dim Maximum Value Throughput (TFLOPS) FP L1 L2 FP L1 L2 ResNet- 152 D 160 460 1124 0.43 0.47 0.41 W 88 304 320 0.44 0.89 0.6 R 232 548 716 0.41 0.39 0.44 Bert- large D 32 56 64 0.67 0.56 0.53 W 1280 1488 2032 0.68 0.61 0.60 GCN D 24 152 240 0.20 0.14 0.15 W 2464 3948 4244 0.36 0.38 0.40 lack the support for this network. Results. We compare the training throughput of GACT against other memory saving systems in Fig. 4. On ResNet-50, GACT achieves similar throughput as ActNN (ActNN optimization L5 is not listed because it optimizes PyTorch memory allocation, which is unrelated to quantization and can also be applied to GACT), but ActNN enables training with a larger batch size. This is expected because ActNN implements eï¬ƒcient, customized layers for diï¬€erent operators in convolutional NNs. For Bert-large, Zero-oï¬„oad fails quickly because it only oï¬„oads optimizer states that occupy a small portion of total memory to CPU. GACT L1 outperforms Mesa because Mesa only compresses tensors to 8 bit. When the batch is bigger, the activation size of each segment becomes the memory bottleneck and prevents gradient checkpointing from increasing the batch size. Moreover, combining GACT with gradient checkpointing and eï¬ƒcient self-attention further reduces the peak memory, increasing the batch size by up to 24.7 Ã—. Meanwhile, it introduces a small throughput overhead compared with the original gradient checkpointing. Across all the network architectures, GACT enables training with a 4.2 Ã—to 24.9Ã—larger batch size under the same memory budget. Network scaling. With GACT, we can construct larger models or train with a higher image resolution. Tab. 4 compares the largest model we can train against full precision. With the same batch size and memory budget, GACT can scale a ResNet-152 to 7.0 Ã—deeper, 3.6 Ã—wider or 3.0 Ã—higher resolution. Similarly, Bert-large can be scaled to 2.0 Ã—deeper or 1.6Ã—wider. In GCN, GACT enables training 10.0 Ã—deeper and 1.7Ã—wider network. Overall, GACT maintains 75% - 136% original training throughput. 6.4 Other Optimizations We evaluate the idea of combining GACT with swapping on Bert-large-cased. As shown in Tab. 5, swapping compressed tensors is faster than swapping the original ones because communication between CPU and GPU is more time-consuming than computation. Combining GACT with swapping increases training speed by 10(a) 0 200 400 600 800 Batch Size 0 50 100Training Throughput 4.3Ã— L0 L1 L2 ResNet-50 DTR Swap ActNN GACT (b) 0 100 200 300 400 500 600 Batch Size 0 10 20Training Throughput 24.7Ã— L0 L1 L2 CB1 CB2 Bert-large ZeroOff Swap Mesa CKPT GACT (c) 0 100 200 300 400 500 600 Batch Size 0 25 50 75Training Throughput 5.6Ã— L0 L1 L2 Swin-tiny Mesa Swap GACT Figure 4: Training throughput vs batch size. Red cross mark means out-of-memory. The shaded yellow region denotes the batch sizes with full precision training given the memory budget. CKPT: Gradient checkpointing, ZeroOï¬€: ZeRO-Oï¬„oad. Table 5: Swap and prefetch speed/memory on Bert-large. Algorithm Speed (sequence/s) Peak Mem. (MB) Total Mem. (MB) FP32 16.41 9573 9527 FP32 + swap 6.02 5215 5093 GACT swap 12.95 5426 5325 GACT swap + prefetch14.02 5426 5324 up to 2.3Ã—. Notice here that the peak memory use of â€œGACT swapâ€ is slightly higher than â€œFP32 + swapâ€ because GACT does not quantize and swap intermediate states such as running mean/var of BatchNorm layer. Moreover, prefetch increases the speed by about 7% with negligible memory overhead. We next demonstrate combining GACT with gradient checkpointing (CB1). Gradient checkpointing is performed at the beginning of each transformer block, thus avoiding saving tensors generated within the block. We then apply GACT with gradient checkpointing, where the saved tensors are quantized with 4 bits. As shown in Tab. 6, the accuracy is unaï¬€ected. We also compare the activation memory and peak memory of CB1 and CB2 in Tab. 7. AM2 denotes the peak activation memory, which is the size of saved tensors after reforwarding the ï¬rst transformer block. When batch size = 288, compared with gradient checkpointing on full precision (FP32), CB1 and CB2 reduce the peak activation size by 4.7 Ã—and 5.4Ã—respectively. 7 Conclusion This paper presents GACT, an ACT framework for generic NN architectures. We prove the convergence of GACT without prior knowledge about operator type or network architecture by analyzing a linearized Table 6: Accuracy of Bert-large-cased on SST-2 and QNLI datasets Algorithm SST-2 QNLI Algorithm SST-2 QNLI FP32 93.58 92.42 CB1 93.81 92.26 11Table 7: Memory use of diï¬€erent algorithms on Bert-large. AM1: Activation size before backward, AM2: Activation size after reforwading the ï¬rst transformer block. When batch size = 288, L0 runs out of memory, and therefore it is not listed below. Batch Size Algorithm AM1(MB) AM2(MB) Peak Mem.(MB) 16 L0 4434 - 9573 FP32 + CKPT210 394 5541 CB1 37 99 5286 CB2 31 79 5269 288 FP32 + CKPT3783 7092 12885 CB1 515 1497 8251 CB2 486 1307 8102 approximation of ATCâ€™s gradients. With the adaptive algorithm, GACT achieves negligible accuracy loss on various tasks, reducing activation memory by up to 8.1 Ã—and enabling training with up to 24.7 Ã—batch size compared with full precision training. Acknowledgements This work was supported by the National Key Research and Development Project of China (No. 2021ZD0110502); NSF of China Project (No. 62106120), by the National Science Foundation through grants IIS-1955488, IIS-2027575, CCF-1723352, ARO W911NF2110339, ONR N00014-21-1-2724, and DOE award DE-SC0016260. We would also like to acknowledge partial support from DARPA, IARPA, the Sloan Foundation, NSF, and ONR. Our conclusions do not necessarily reï¬‚ect the position or the policy of our sponsors, and no oï¬ƒcial endorsement should be inferred. References [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. [2] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and eï¬ƒcient sparsity. arXiv preprint arXiv:2101.03961 , 2021. [3] Ayan Chakrabarti and Benjamin Moseley. Backprop with approximate activations for memory-eï¬ƒcient network training. arXiv preprint arXiv:1901.07988 , 2019. [4] Fangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, and Bin Cui. Donâ€™t waste your bits! squeeze activations and gradients for deep neural networks via tinyscript. In International Conference on Machine Learning, pages 3304â€“3314. PMLR, 2020. [5] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael W Mahoney, and Joseph E Gonzalez. Actnn: Reducing training memory footprint via 2-bit activation compressed training. In International Conference on Machine Learning , 2021. [6] R David Evans and Tor Aamodt. AC-GC: Lossy activation compression with guaranteed convergence. Advances in Neural Information Processing Systems , 34, 2021. [7] Zizheng Pan, Peng Chen, Haoyu He, Jing Liu, Jianfei Cai, and Bohan Zhuang. Mesa: A memory-saving training framework for transformers. arXiv preprint arXiv:2111.11124 , 2021. [8] R David Evans, Lufei Liu, and Tor M Aamodt. Jpeg-act: accelerating deep learning via transform-based lossy compression. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), pages 860â€“873. IEEE, 2020. [9] Sian Jin, Guanpeng Li, Shuaiwen Leon Song, and Dingwen Tao. A novel memory-eï¬ƒcient deep learning training framework via error-bounded lossy compression. In 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming , pages 485â€“487, 2021. 12[10] Anonymous. EXACT: Scalable graph neural networks training via extreme activation compression. In Submitted to The Tenth International Conference on Learning Representations , 2022. under review. [11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, / Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998â€“6008, 2017. [12] Thomas N Kipf and Max Welling. Semi-supervised classiï¬cation with graph convolutional networks. arXiv preprint arXiv:1609.02907 , 2016. [13] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations, 2018. [14] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. In International Conference on Learning Representations, 2018. [15] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit ï¬‚oating point numbers. In Advances in Neural Information Processing Systems, pages 7675â€“7684, 2018. [16] Ron Banner, Itay Hubara, Elad Hoï¬€er, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems , pages 5145â€“5153, 2018. [17] Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and Joseph E Gonzalez. A statistical framework for low-bitwidth training of deep neural networks. In Advances in neural information processing systems , 2020. [18] Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkatara- mani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. In Advances in Neural Information Processing Systems , volume 33, 2020. [19] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174 , 2016. [20] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt Keutzer, Ion Stoica, and Joseph E Gonzalez. Checkmate: Breaking the memory wall with optimal tensor rematerialization. arXiv preprint arXiv:1910.02653 , 2019. [21] Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock. Dynamic tensor rematerialization. arXiv preprint arXiv:2006.09616 , 2020. [22] Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping. In Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 1341â€“1355, 2020. [23] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu, and Tim Kraska. Superneurons: Dynamic GPU memory management for training deep neural networks. In 23rd ACM SIGPLAN symposium on principles and practice of parallel programming , pages 41â€“53, 2018. [24] Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma, Qian Xiong, Fan Yang, and Xuehai Qian. Capuchin: Tensor-based gpu memory management for deep learning. In Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 891â€“905, 2020. 13[25] Olivier Beaumont, Lionel Eyraud-Dubois, and Alena Shilova. Eï¬ƒcient combination of rematerialization and oï¬„oading for training dnns. Advances in Neural Information Processing Systems , 34, 2021. [26] LÂ´ eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP- STATâ€™2010, pages 177â€“186. Springer, 2010. [27] LÂ´ eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223â€“311, 2018. [28] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pages 3123â€“3131, 2015. [29] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [30] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. [31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE conference on computer vision and pattern recognition , pages 770â€“778, 2016. [32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. International Conference on Computer Vision (ICCV) , 2021. [33] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE conference on computer vision and pattern recognition , pages 248â€“255. Ieee, 2009. [34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÂ´ ar. Focal loss for dense object detection. In International Conference on Computer Vision (ICCV) , pages 2980â€“2988, 2017. [35] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. Advances in neural information processing systems , 28:91â€“99, 2015. [36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÂ´ ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740â€“755. Springer, 2014. [37] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-oï¬„oad: Democratizing billion-scale model training. arXiv preprint arXiv:2101.06840, 2021. [38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÂ´ emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38â€“45, Online, October 2020. Association for Computational Linguistics. [39] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931 , 2019. [40] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. 14[41] Petar VeliË‡ ckoviÂ´ c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903 , 2017. [42] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International Conference on Machine Learning , pages 1725â€“1735. PMLR, 2020. [43] Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Xingcheng Yao, Aohan Zeng, Shiguang Guo, Peng Zhang, Guohao Dai, Yu Wang, Chang Zhou, Hongxia Yang, and Jie Tang. Cogdl: Toolkit for deep learning on graphs. arXiv preprint arXiv:2103.00959 , 2021. [44] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353â€“355, 2018. [45] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171â€“4186, 2019. 15A Proof of Theorems A.1 Theorem 1: Convergence of ACT Assume that: A1. L(Î¸) is a continuous diï¬€erentiable, âˆ‡L(Î¸) is Î²-Lipschitz continuous. . A2. L(Î¸) is bounded below by Lâˆ—. A3. g(h; Î¸) is diï¬€erentiable w.r.t. h and âˆƒb> 0, s.t. âˆ€Î¸,Eâˆ¥g(Q(h(x,Î¸)); Î¸) âˆ’Ë†g(h(x,Î¸); Î¸)âˆ¥â‰¤ b. A4. âˆƒÏƒ2 >0, s.t., âˆ€Î¸, Var [Ë†g(h(x,Î¸)] â‰¤Ïƒ2. Then, for all Î· <1 2Î², if we run ACT deï¬ned as Eq. (2) for T iterations, then we have min t=0,...,Tâˆ’1 E [ âˆ¥âˆ‡L(Î¸t)âˆ¥2 ] â‰¤4(L(Î¸0) âˆ’Lâˆ—) Î·T + 3b2 + Î·Î²Ïƒ2 Proof. Denote m:= âˆ‡Î¸L(Î¸t), Ïµ:= Ë†g(h(x,Î¸t); Î¸t) âˆ’m, d:= g(Q(h(x; Î¸t)); Î¸t) âˆ’Ë†g(h(x,Î¸t); Î¸t). Then, by A3 and A4, we have E[Ïµ] = E[g(h(x,Î¸t); Î¸t) âˆ’âˆ‡Î¸L(Î¸t)] + E[âŸ¨J(x,Î¸t),âˆ†Q(h(x,Î¸t))âŸ©] = âŸ¨J(x,Î¸t),E[âˆ†Q(h(x,Î¸t))]âŸ©= 0. (6) E [ âˆ¥Ïµâˆ¥2 ] = âˆ¥E[Ïµ]âˆ¥2 + Var [Ïµ] = Var [Ë†g(h(x,Î¸t); Î¸t)] â‰¤Ïƒ2. (7) E[âˆ¥dâˆ¥] â‰¤b. (8) By the deï¬nitions, the ACT dynamics can be written as Î¸t+1 â†Î¸t âˆ’Î·(m+ d+ Ïµ). By A1, we have L(Î¸t+1) â‰¤L(Î¸t) âˆ’Î·âŸ¨m,m + d+ ÏµâŸ©+ Î²Î·2 2 âˆ¥m+ d+ Ïµâˆ¥2 . (9) By Eq.s (6,8) E[âŸ¨m,m + d+ ÏµâŸ©] â‰¥âˆ¥mâˆ¥2 âˆ’âˆ¥mâˆ¥âˆ¥dâˆ¥+ âŸ¨m,E[Ïµ]âŸ©â‰¥âˆ¥ mâˆ¥2 âˆ’âˆ¥mâˆ¥b. (10) By Eq.s (6,7,8), and âˆ¥x+ yâˆ¥2 â‰¤2 âˆ¥xâˆ¥2 + 2âˆ¥yâˆ¥2, E [ âˆ¥m+ d+ Ïµâˆ¥2 ] = E [ âˆ¥m+ dâˆ¥2 ] + Var [Ïµ] â‰¤2E[âˆ¥mâˆ¥]2 + 2E[âˆ¥dâˆ¥]2 + Var [Ïµ] = 2E[âˆ¥mâˆ¥]2 + 2b2 + Ïƒ2. (11) Taking expectation on both sides of Eq. (9), plug in Eq.s (10, 11), and use Î· <1 2Î², we have E[L(Î¸t+1)] â‰¤L(Î¸t) âˆ’Î·(âˆ¥mâˆ¥2 âˆ’âˆ¥mâˆ¥b) + Î²Î·2 2 (2E[âˆ¥mâˆ¥]2 + 2b2 + Ïƒ2). =L(Î¸t) âˆ’(Î·âˆ’Î²Î·2) âˆ¥mâˆ¥2 + Î·âˆ¥mâˆ¥b+ Î²Î·2 2 (2b2 + Ïƒ2) =L(Î¸t) âˆ’Î· 2 âˆ¥mâˆ¥2 + Î·âˆ¥mâˆ¥b+ Î²Î·2 2 (2b2 + Ïƒ2). Completing the squares, E[L(Î¸t+1)] â‰¤L(Î¸t) âˆ’Î· 2(âˆ¥mâˆ¥âˆ’b)2 + Î²Î·2 2 (2b2 + Ïƒ2). Take expectation on both sides and sum up for t= 0,...,T âˆ’1, E[L(Î¸T)] âˆ’L(Î¸0) â‰¤âˆ’Î· 2 Tâˆ’1âˆ‘ t=0 E(âˆ¥âˆ‡L(Î¸t)âˆ¥âˆ’b)2 + Î²Î·2T 2 (2b2 + Ïƒ2). Reorganize the terms, Et [ E(âˆ¥âˆ‡L(Î¸t)âˆ¥âˆ’b)2 ] â‰¤2(L(Î¸0) âˆ’L(Î¸T)) Î·T + Î·Î²(2b2 + Ïƒ2). Let tâˆ—= argmintE[âˆ¥âˆ‡L(Î¸t)âˆ¥], and use A1, we have E(âˆ¥âˆ‡L(Î¸tâˆ—)âˆ¥âˆ’b)2 â‰¤2(L(Î¸0) âˆ’Lâˆ—) Î·T + Î·Î²(2b2 + Ïƒ2). 16Use (a+ b)2 â‰¤2a2 + 2b2, we have E [ âˆ¥âˆ‡L(Î¸tâˆ—)âˆ¥2 ] â‰¤4(L(Î¸0) âˆ’Lâˆ—) Î·T + (2Î²Î·+ 2)b2 + Î·Î²Ïƒ2 â‰¤4(L(Î¸0) âˆ’Lâˆ—) Î·T + 3b2 + Î·Î²Ïƒ2. A.2 Proposition 1: The Linearization Error Proof. Consider the gradient function g(Q(h(x; Î¸); Î¸)), whose output is a P-dimensional vector. Since it is twice diï¬€erentiable, we construct the Taylorâ€™s expansion at h(x; Î¸) with Lagrange remainder: âˆƒH1,...,H P,s.t., âˆ€i, gi(Q(h(x; Î¸)); Î¸) = gi(h(x,Î¸); Î¸) + Ji(x,Î¸)âˆ†h(x,Î¸) + âˆ†h(x,Î¸)âŠ¤Hiâˆ†h(x,Î¸), where Ji(h(x; Î¸),Î¸) := âˆ‚gi(h(x;Î¸);Î¸) âˆ‚h . By the assumption, there exists P >0, such that the linearization error is âˆ¥g(Q(h(x; Î¸)); Î¸) âˆ’Ë†g(h(x; Î¸); h(x; Î¸),Î¸)âˆ¥1 = Pâˆ‘ i=1 âˆ†h(x,Î¸)âŠ¤Hiâˆ†h(x,Î¸) â‰¤Î³P âˆ¥âˆ†h(x,Î¸)âˆ¥2 . Taking expectation, E[âˆ¥g(Q(h(x; Î¸)); h(x; Î¸),Î¸) âˆ’Ë†g(h(x; Î¸); Î¸)âˆ¥2] â‰¤E[âˆ¥g(Q(h(x; Î¸)); Î¸) âˆ’Ë†g(h(x; Î¸); h(x; Î¸),Î¸)âˆ¥1] â‰¤Î³PVar [âˆ†h(x,Î¸)] = O(Var [âˆ†h(x,Î¸)]). A.3 Proposition 2: The Order of the Variance The following proposition is convenient for isolating the diï¬€erent noise sources. Proposition A. (Law of Total Variance) Var [X] = E[Var [X |Y]] + Var [E[X |Y]] . Proof. By deï¬nition Var [Ë†g(h(x; Î¸t); h(x; Î¸),Î¸t)] = Var [g(h(x,Î¸); Î¸)] + Var [J(h(x; Î¸),Î¸)âˆ†h(x,Î¸)] , where Var [g(h(x,Î¸); Î¸)] is the noise introduced by subsampling the data x. By law of total variance, Var [J(h(x; Î¸),Î¸)âˆ†h(x,Î¸)] = EX[VarQ[J(h(x; Î¸); Î¸t)âˆ†h(x,Î¸)]] + VarX[EQ[J(h(x; Î¸); Î¸t)âˆ†h(x,Î¸)]]î´™ î´˜î´— î´š =0 , where VarQ[J(h(x; Î¸); Î¸t)âˆ†h(x,Î¸)] =EQ [ âˆ¥J(h(x; Î¸); Î¸t)âˆ†h(x,Î¸)âˆ¥2 ] â‰¤EQ [ âˆ¥J(h(x; Î¸); Î¸t)âˆ¥2 âˆ¥âˆ†h(x,Î¸)âˆ¥2 ] = âˆ¥J(h(x; Î¸); Î¸t)âˆ¥2 EQ [ âˆ¥âˆ†h(x,Î¸)âˆ¥2 ] = O(Var [âˆ†h(x,Î¸)]) . A.4 Proposition 3: The Structure of the Variance Before investigating the structure of VarQ[J(x; Î¸t)âˆ†h(x,Î¸)], letâ€™s do some recap: the parameter Î¸t is a P-dimensional vector; the context diï¬€erence âˆ† h(x,Î¸) is a D-dimensional vector, and J(x; Î¸t) is a P Ã—D matrix. Recall that âˆ† h(x,Î¸) is the concatenation of L-vectors, âˆ†h(l)(x,Î¸), and let J(l)(x,Î¸) := âˆ‚g âˆ‚h(l) g ( (h(l)(x; Î¸))L l=1,Î¸ ) , which is a P Ã—Dl matrix. Furthermore, let h(l) j (x,Î¸) be the j-th dimension, and J(l) j (x,Î¸) be its j-th column. To proceed, we need to make the following assumptions to the compressor Q(Â·) : RD â†’RD: B1: The compressed result is element-wise uncorrelated. That is, for any iÌ¸= j, Cov [Q(h)i,Q(h)j] = 0. B2: For compressing a vector h to b bits, the compression variance of each dimension can be written in the form Var [Q(h)j] â‰¤Rj(h)S(b), where S(Â·) is a known function. Both assumptions can be achieved by a stochastic rounding [28] quantizer, where Q(h)j = { Tâˆ’1 h,b (âŒˆTh,b(hj)âŒ‰) w.p. Th,b(hj) âˆ’âŒŠTh,b(hj)âŒ‹ Tâˆ’1 h,b (âŒŠTh,b(hj)âŒ‹) otherwise , 17where Th,b(hj) = (2bâˆ’1) hjâˆ’minj h maxj hâˆ’minj h. Since each dimension is quantized independently, B1 is met. Moreover, Var [Q(h)j] â‰¤1 4 (maxjhâˆ’minjh (hj âˆ’minjh) )2 (2b âˆ’1)âˆ’2 = Rj(h)S(b), where Rj(h) = 1 4 (maxjhâˆ’minjh (hj âˆ’minjh) )2 , S (b) = (2b âˆ’1)âˆ’2. Proof. By deï¬nition, J(h; Î¸)âˆ†h= Lâˆ‘ l=1 Dlâˆ‘ j=1 J(l) j (h; Î¸t)âˆ†h(l) j . Using Assumption B1, we have VarQ[J(h; Î¸)âˆ†h] = EQ ï£® ï£¯ï£° îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ Lâˆ‘ l=1 Dlâˆ‘ j=1 J(l) j (h; Î¸t)âˆ†h(l) j îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2ï£¹ ï£ºï£» = Lâˆ‘ l=1 Dlâˆ‘ j=1 EQ [îµ¹îµ¹îµ¹J(l) j (h; Î¸t)âˆ†h(l) j îµ¹îµ¹îµ¹ 2] . = Lâˆ‘ l=1 Dlâˆ‘ j=1 îµ¹îµ¹îµ¹J(l) j (h; Î¸t) îµ¹îµ¹îµ¹ 2 VarQ [ âˆ†h(l) j ] Using Assumption B2, we have VarQ[J(h; Î¸)âˆ†h] â‰¤ Lâˆ‘ l=1 Dlâˆ‘ j=1 îµ¹îµ¹îµ¹J(l) j (h; Î¸t) îµ¹îµ¹îµ¹ 2 Rl(h)S(bl) = Lâˆ‘ l=1 cl(h,Î¸)S(bl), where cl(Î¸,h) := Rl(h) îµ¹îµ¹J(l)(h; Î¸t) îµ¹îµ¹2 F. B Experiment Setup B.1 Node classiï¬cation task on graphs We conduct experiments on four node classiï¬cation datasets with standard splits, including Flickr, Reddit, Yelp from GraphSAINT [39], and ogbn-arxiv from Open Graph Benchmark (OGB) [ 40]. The four datasets cover extensive downstream applications with diï¬€erent scales. We use accuracy as the evaluation metric for multi-class classiï¬cation and micro-F1 for multi-label classiï¬cation. We run ten seeds (0 to 9) and report the average accuracy across runs. We evaluate GACT on three representative GNN models, including GCN [ 12], GAT [41], and GCNII [ 42] under the full-batch training setting. All three models are implemented by CogDL [ 43], a toolkit for graph neural networks. B.2 Text classiï¬cation task We select four largest datasets, MNLI, QQP, SST-2, and QNLI, from the GLUE benchmark [ 44]. The four datasets cover diï¬€erent aspects of natural language understanding, including sentiment classiï¬cation, natural language inference and paraphrase detection. We use the mainstream transformer implementation [ 38] to train Bert-large [45]. We run three seeds (42, 43, 44) and report F1 for QQP, accuracy for the others. C Training Accuracy of Baselines For all the baselines we compared in Sec. 6.3, only ActNN, Mesa, and ZeRO-Oï¬„oad are lossy methods. All other methods are lossless and have the same training accuracy as FP32. For ResNet-50 on ImageNet, the training accuracy for FP32, GACT, ActNN L2, and ActNN L3 are 77.3, 77.0, 77.4, and 76.9. For Bert-Large on SST-2, the accuracy for FP32, GACT, Mesa, and ZeRO-Oï¬„oad are 93.7, 93.5, 93.8, and 93.3. For Swin-tiny on ImageNet, the training accuracy for FP32, GACT, and Mesa are 81.2, 81.0, and 81.3 respectively. 18",
      "meta_data": {
        "arxiv_id": "2206.11357v4",
        "authors": [
          "Xiaoxuan Liu",
          "Lianmin Zheng",
          "Dequan Wang",
          "Yukuo Cen",
          "Weize Chen",
          "Xu Han",
          "Jianfei Chen",
          "Zhiyuan Liu",
          "Jie Tang",
          "Joey Gonzalez",
          "Michael Mahoney",
          "Alvin Cheung"
        ],
        "published_date": "2022-06-22T20:06:23Z",
        "pdf_url": "https://arxiv.org/pdf/2206.11357v4.pdf"
      }
    },
    {
      "title": "Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data",
      "abstract": "This paper investigates the intriguing question of whether we can create\nlearning algorithms that automatically generate training data, learning\nenvironments, and curricula in order to help AI agents rapidly learn. We show\nthat such algorithms are possible via Generative Teaching Networks (GTNs), a\ngeneral approach that is, in theory, applicable to supervised, unsupervised,\nand reinforcement learning, although our experiments only focus on the\nsupervised case. GTNs are deep neural networks that generate data and/or\ntraining environments that a learner (e.g. a freshly initialized neural\nnetwork) trains on for a few SGD steps before being tested on a target task. We\nthen differentiate through the entire learning process via meta-gradients to\nupdate the GTN parameters to improve performance on the target task. GTNs have\nthe beneficial property that they can theoretically generate any type of data\nor training environment, making their potential impact large. This paper\nintroduces GTNs, discusses their potential, and showcases that they can\nsubstantially accelerate learning. We also demonstrate a practical and exciting\napplication of GTNs: accelerating the evaluation of candidate architectures for\nneural architecture search (NAS), which is rate-limited by such evaluations,\nenabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art,\nfinding higher performing architectures when controlling for the search\nproposal mechanism. GTN-NAS also is competitive with the overall state of the\nart approaches, which achieve top performance while using orders of magnitude\nless computation than typical NAS methods. Speculating forward, GTNs may\nrepresent a first step toward the ambitious goal of algorithms that generate\ntheir own training data and, in doing so, open a variety of interesting new\nresearch questions and directions.",
      "full_text": "GENERATIVE TEACHING NETWORKS : ACCELERATING NEURAL ARCHITECTURE SEARCH BY LEARNING TO GENERATE SYNTHETIC TRAINING DATA Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth O. Stanleyâˆ—& Jeff Cluneâˆ— Uber AI Labs ABSTRACT This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neu- ral networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneï¬cial property that they can theoretically gener- ate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and excit- ing application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, ï¬nding higher performing architectures when controlling for the search pro- posal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may repre- sent a ï¬rst step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions. 1 I NTRODUCTION AND RELATED WORK Access to vast training data is now common in machine learning. However, to effectively train neural networks (NNs) does not require using all available data. For example, recent work in cur- riculum learning (Graves et al., 2017), active learning (Konyushkova et al., 2017; Settles, 2010) and core-set selection (Sener & Savarese, 2018; Tsang et al., 2005) demonstrates that a surrogate dataset can be created by intelligently sampling a subset of training data, and that such surrogates enable competitive test performance with less training effort. Being able to more rapidly determine the per- formance of an architecture in this way could particularly beneï¬t architecture search, where training thousands or millions of candidate NN architectures on full datasets can become prohibitively ex- pensive. From this lens, related work in learning-to-teach has shown promise. For example, the learning to teach (L2T) (Fan et al., 2018) method accelerates learning for a NN learner (hereafter, just learner) through reinforcement learning, by learning how to subsample mini-batches of data. A key insight in this paper is that the surrogate data need not be drawn from the original data distribution (i.e. they may not need to resemble the original data). For example, humans can learn new skills from reading a book or can prepare for a team game like soccer by practicing skills, such as passing, dribbling, juggling, and shooting. This paper investigates the question of whether we can train a data-generating network that can produce synthetic data that effectively and efï¬ciently âˆ—co-senior authors. Corresponding authors: {felipe.such,kstanley,jeffclune}@uber.com 1 arXiv:1912.07768v1  [cs.LG]  17 Dec 2019teaches a target task to a learner. Related to the idea of generating data, Generative Adversarial Networks (GANs) can produce impressive high-resolution images (Goodfellow et al., 2014; Brock et al., 2018), but they are incentivized to mimic real data (Goodfellow et al., 2014), instead of being optimized to teach learners more efï¬ciently than real data. Another approach for creating surrogate training data is to treat the training data itself as a hyper- parameter of the training process and learn it directly. Such learning can be done through meta- gradients (also called hyper-gradients), i.e. differentiating through the training process to optimize a meta-objective. This approach was described in Maclaurin et al. (2015), where 10 synthetic training images were learned using meta-gradients such that when a network is trained on these images, the networkâ€™s performance on the MNIST validation dataset is maximized. In recent work concurrent with our own, Wang et al. (2019b) scaled this idea to learn 100 synthetic training examples. While the 100 synthetic examples were more effective for training than 100 original (real) MNIST training examples, we show that it is difï¬cult to scale this approach much further without the regularity across samples provided by a generative architecture (Figure 2b, green line). Being able to very quickly train learners is particularly valuable for neural architecture search (NAS), which is exciting for its potential to automatically discover high-performing architectures, which otherwise must be undertaken through time-consuming manual experimentation for new domains. Many advances in NAS involve accelerating the evaluation of candidate architectures by training a predictor of how well a trained learner would perform, by extrapolating from previously trained architectures (Luo et al., 2018; Liu et al., 2018a; Baker et al., 2017). This approach is still expensive because it requires many architectures to be trained and evaluated to train the predictor. Other approaches accelerate training by sharing training across architectures, either through shared weights (e.g. as in ENAS; Pham et al. (2018)), or Graph HyperNetworks (Zhang et al., 2018). We propose a scalable, novel, meta-learning approach for creating synthetic data called Generative Teaching Networks (GTNs). GTN training has two nested training loops: an inner loop to train a learner network, and an outer-loop to train a generator network that produces synthetic training data for the learner network. Experiments presented in Section 3 demonstrate that the GTN approach produces synthetic data that enables much faster learning, speeding up the training of a NN by a fac- tor of 9. Importantly, the synthetic data in GTNs is not only agnostic to the weight initialization of the learner network (as in Wang et al. (2019b)), but is also agnostic to the learnerâ€™sarchitecture. As a result, GTNs are a viable method for accelerating evaluation of candidate architectures in NAS. Indeed, controlling for the search algorithm (i.e. using GTN-produced synthetic data as a drop-in replacement for real data when evaluating a candidate architectureâ€™s performance), GTN-NAS im- proves the NAS state of the art by ï¬nding higher-performing architectures than comparable methods like weight sharing (Pham et al., 2018) and Graph HyperNetworks (Zhang et al., 2018); it also is competitive with methods using more sophisticated search algorithms and orders of magnitude more computation. It could also be combined with those methods to provide further gains. One promising aspect of GTNs is that they make very few assumptions about the learner. In contrast, NAS techniques based on shared training are viable only if the parameterizations of the learners are similar. For example, it is unclear how weight-sharing or HyperNetworks could be applied to architectural search spaces wherein layers could be either convolutional or fully-connected, as there is no obvious way for weights learned for one layer type to inform those of the other. In contrast, GTNs are able to create training data that can generalize between such diverse types of architectures. GTNs also open up interesting new research questions and applications to be explored by future work. Because they can rapidly train new architectures, GTNs could be used to create NNs on- demand that meet speciï¬c design constraints (e.g. a given balance of performance, speed, and en- ergy usage) and/or have a speciï¬c subset of skills (e.g. perhaps one needs to rapidly create a compact network capable of three particular skills). Because GTNs can generate virtually any learning en- vironment, they also one day could be a key to creating AI-generating algorithms, which seek to bootstrap themselves from simple initial conditions to powerful forms of AI by creating an open- ended stream of challenges (learning opportunities) while learning to solve them (Clune, 2019). 2 M ETHODS The main idea in GTNs is to train a data-generating network such that a learner network trained on data it rapidly produces high accuracy in a target task. Unlike a GAN, here the two networks 2cooperate (rather than compete) because their interests are aligned towards having the learner per- form well on the target task when trained on data produced by the GTN. The generator and the learner networks are trained with meta-learning via nested optimization that consists of inner and outer training loops (Figure 1a). In the inner-loop, the generator G(z,y) takes Gaussian noise ( z) and a label ( y) as input and outputs synthetic data (x). Optionally, the generator could take only noise as input and produce both data and labels as output (Appendix F). The learner is then trained on this synthetic data for a ï¬xed number of inner-loop training steps with any optimizer, such as SGD or Adam (Kingma & Ba, 2014): we use SGD with momentum in this paper. SI Equation 1 deï¬nes the inner-loop SGD with momentum update for the learner parameters Î¸t. We sample zt (noise vectors input to the generator) from a unit-variance Gaussian andyt labels for each generated sample) uniformly from all available class labels. Note that both zt and yt are batches of samples. We can also learn a curriculum directly by additionally optimizing zt directly (instead of sampling it randomly) and keeping yt ï¬xed throughout all of training. The inner-loop loss function â„“inner can be cross-entropy for classiï¬cation problems or mean squared error for regression problems. Note that the inner-loop objective does not depend on the outer- loop objective and could even be parameterized and learned through meta-gradients with the rest of the system (Houthooft et al., 2018). In the outer-loop, the learner Î¸T (i.e. the learner parameters trained on synthetic data after the T inner-loop steps) is evaluated on the realtraining data, which is used to compute the outer-loop loss (aka meta-training loss). The gradient of the meta-training loss with respect to the generator is computed by backpropagating through the entire inner-loop learning process. While computing the gradients for the generator we also compute the gradients of hyper- parameters of the inner-loop SGD update rule (its learning rate and momentum), which are updated after each outer-loop at no additional cost. To reduce memory requirements, we leverage gradient- checkpointing (Griewank & Walther, 2000) when computing meta-gradients. The computation and memory complexity of our approach can be found in Appendix D. (1) Noise Inner-loop Generator Learner (4) Meta-loss Real  Data (2) Data (3) SGD Step (5) Gradient of Meta-loss w.r.t. Generator Outer-loop (a) Overview of Generative Teaching Networks Without WN With WN 0.0 0.2 0.4 0.6 0.8 1.0 1.2Validation Loss (b) GTN stability with WN 0 500 1000 1500 2000 Outer-loop Iterations 0.850 0.875 0.900 0.925 0.950 0.975 1.000Test Accuracy No Curriculum All Shuffled Shuffled Batch Full Curriculum (c) GTN curricula comparison Figure 1: (a) Generative Teaching Network (GTN) Method. The numbers in the ï¬gure reï¬‚ect the order in which a GTN is executed. Noise is fed as an input to the Generator (1), which uses it to gen- erate new data (2). The learner is trained (e.g. using SGD or Adam) to perform well on the generated data (3). The trained learner is then evaluated on the real training data in the outer-loop to compute the outer-loop meta-loss (4). The gradients of the generator parameters are computed w.r.t. to the meta-loss to update the generator (5). Both a learned curriculum and weight normalization sub- stantially improve GTN performance. (b) Weight normalization improves meta-gradient training of GTNs, and makes the method much more robust to different hyperparameter settings. Each boxplot reports the ï¬nal loss of 20 runs obtained during hyperparameter optimization with Bayesian Opti- mization (lower is better). (c) shows a comparison between GTNs with different types of curricula. The GTN method with the most control over how samples are presented performs the best. A key motivation for this work is to generate synthetic data that is learner agnostic, i.e. that gener- alizes across different potential learner architectures and initializations. To achieve this objective, at the beginning of each new outer-loop training, we choose a new learner architecture according to a predeï¬ned set and randomly initialize it (details in Appendix A). Meta-learning with Weight Normalization. Optimization through meta-gradients is often unsta- ble (Maclaurin et al., 2015). We observed that this instability greatly complicates training because of its hyperparameter sensitivity, and training quickly diverges if they are not well-set. Combining the gradients from Evolution Strategies (Salimans et al., 2017) and backpropagation using inverse variance weighting (Fleiss, 1993; Metz et al., 2019) improved stability in our experiments, but op- timization still consistently diverged whenever we increased the number of inner-loop optimization 3steps. To mitigate this issue, we introduce applying weight normalization (Salimans & Kingma, 2016) to stabilize meta-gradient training by normalizing the generator and learner weights. Instead of updating the weights (W) directly, we parameterize them as W = gÂ·V/âˆ¥Vâˆ¥and instead update the scalar gand vector V. Weight normalization eliminates the need for (and cost of) calculating ES gradients and combining them with backprop gradients, simplifying and speeding up the algorithm. We hypothesize that weight normalization will help stabilize meta-gradient training more broadly, although future work is required to test this hypothesis in meta-learning contexts besides GTNs. The idea is that applying weight normalization to meta-learning techniques is analogous to batch normalization for deep networks (Ioffe & Szegedy, 2015). Batch normalization normalizes the forward propagation of activations in a long sequence of parameterized operations (a deep NN). In meta-gradient training both the activations and weights result from a long sequence of parameterized operations and thus both should be normalized. Results in section 3.1 support this hypothesis. Learning a Curriculum with Generative Teaching Networks. Previous work has shown that a learned curriculum can be more effective than training from uniformly sampled data (Graves et al., 2017). A curriculum is usually encoded with indexes to samples from a given dataset, rendering it non-differentiable and thereby complicating the curriculumâ€™s optimization. With GTNs however, a curriculum can be encoded as a series of input vectors to the generator (i.e. instead of sampling the zt inputs to the generator from a Gaussian distribution, a sequence of zt inputs can be learned). A curriculum can thus be learned by differentiating through the generator to optimize this sequence (in addition to the generatorâ€™s parameters). Experiments conï¬rm that GTNs more effectively teach learners when optimizing such a curriculum (Section 3.2). Accelerating NAS with Generative Teaching Networks.Since GTNs can accelerate learner train- ing, we propose harnessing GTNs to accelerate NAS. Rather than evaluating each architecture in a target task with a standard training procedure, we propose evaluating architectures with a meta- optimized training process (that generates synthetic data in addition to optimizing inner-loop hyper- parameters). We show that doing so signiï¬cantly reduces the cost of running NAS (Section 3.4). The goal of these experiments is to ï¬nd a high-performing CNN architecture for the CIFAR10 image-classiï¬cation task (Krizhevsky et al., 2009) with limited compute costs. We use the same architecture search-space, training procedure, hyperparameters, and code from Neural Architecture Optimization (Luo et al., 2018), a state-of-the-art NAS method. The search space consists of the topology of two cells: a reduction cell and a convolutional cell. Multiple copies of such cells are stacked according to a predeï¬ned blueprint to form a full CNN architecture (see Luo et al. (2018) for more details). The blueprint has two hyperparameters N and F that control how many times the convolutional cell is repeated (depth) and the width of each layer, respectively. Each cell contains B = 5nodes. For each node within a cell, the search algorithm has to choose two inputs as well as two operations to apply to those inputs. The inputs to a node can be previous nodes or the outputs of the last two layers. There are 11 operations to choose from (Appendix C). Following Luo et al. (2018), we report the performance of our best cell instantiated with N = 6,F = 36after the resulting architecture is trained for a signiï¬cant amount of time (600 epochs). Since evaluating each architecture in those settings (named ï¬nal evaluation from now on) is time consuming, Luo et al. (2018) uses a surrogate evaluation (named search evaluation) to estimate the performance of a given cell wherein a smaller version of the architecture ( N = 3,F = 32) is trained for less epochs (100) on real data. We further reduce the evaluation time of each cell by replacing the training data in the search evaluation with GTN synthetic data, thus reducing the training time per evaluation by 300x (which we call GTN evaluation). While we were able to train GTNs directly on the complex architectures from the NAS search space, training was prohibitively slow. Instead, for these experiments, we optimize our GTN ahead of time using proxy learners described in Appendix A.2, which are smaller fully-convolutional networks (this meta-training took 8h on one p6000 GPU). Interestingly, although we never train our GTN on any NAS architectures, because of generalization, synthetic data from GTNs were still effective for training them. 3 R ESULTS We ï¬rst demonstrate that weight normalization signiï¬cantly improves the stability of meta-learning, an independent contribution of this paper (Section 3.1). We then show that training with synthetic data is more effective when learning such data jointly with a curriculum that orders its presentation 4to the learner (Section 3.2). We next show that GTNs can generate a synthetic training set that enables more rapid learning in a few SGD steps than real training data in two supervised learning domains (MNIST and CIFAR10) and in a reinforcement learning domain (cart-pole, Appendix H). We then apply GTN-synthetic training data for neural architecture search to ï¬nd high performing architectures for CIFAR10 with limited compute, outperforming comparable methods like weight sharing (Pham et al., 2018) and Graph HyperNetworks (Zhang et al., 2018) (Section 3.4). We uniformly split the usual MNIST training set into training (50k) and validation sets (10k). The training set was used for inner-loop training (for the baseline) and to compute meta-gradients for all the treatments. We used the validation set for hyperparameter tuning and report accuracy on the usual MNIST test set (10k images). We followed the same procedure for CIFAR10, resulting in training, validation, and test sets with 45k, 5k, and 10k examples, respectively. Unless otherwise speciï¬ed, we ran each experiment 5 times and plot the mean and its 95% conï¬dence intervals from (n=1,000) bootstrapping. Appendix A describes additional experimental details. 3.1 I MPROVING STABILITY WITH WEIGHT NORMALIZATION To demonstrate the effectiveness of weight normalization for stabilizing and robustifying meta- optimization, we compare the results of running hyperparameter optimization for GTNs with and without weight normalization on MNIST. Figure 1b shows the distribution of the ï¬nal performance obtained for 20 runs during hyperparameter tuning, which reï¬‚ects how sensitive the algorithms are to hyperparameter settings. Overall, weight normalization substantially improved robustness to hyperparameters and ï¬nal learner performance, supporting the initial hypothesis. 3.2 I MPROVING GTN S WITH A CURRICULUM We experimentally evaluate four different variants of GTNs, each with increasing control over the ordering of the zcodes input to the generator, and thus the order of the inputs provided to the learner. The ï¬rst variant (called GTN - No Curriculum), trains a generator to output synthetic training data by sampling the noise vector zfor each sample independently from a Gaussian distribution. In the next three GTN variants, the generator is provided with a ï¬xed set of input samples (instead of a noise vector). These input samples are learned along with the generator parameters during GTN training. The second GTN variant (called GTN - All Shufï¬‚ed ) learns a ï¬xed set of 4,096 input samples that are presented in a random order without replacement (thus learning controls the data, but not the order in which they are presented). The third variant (calledGTN - Shufï¬‚ed Batch) learns 32 batches of 128 samples each (so learning controls which samples coexist within a batch), but the order in which the batches are presented is randomized (without replacement). Finally, the fourth variant (calledGTN - Full Curriculum) learns a deterministic sequence of 32 batches of 128 samples, giving learning full control. Learning such a curriculum incurs no additional computational expense, as learning the zt tensor is computationally negligible and avoids the cost of repeatedly sampling new Gaussian z codes. We plot the test accuracy of a learner (with random initial weights and architecture) as a function of outer-loop iterations for all four variants in Figure 1c. Although GTNs - No curriculum can seemingly generate endless data (see Appendix G), it performs worse than the other three variants with a ï¬xed set of generator inputs. Overall, training the GTN with exact ordering of input samples (GTN - Full Curriculum) outperforms all other variants. While curriculum learning usually refers to training on easy tasks ï¬rst and increasing their difï¬culty over time, our curriculum goes beyond presenting tasks in a certain order. Speciï¬cally, GTN - Full Curriculum learns both the order in which to present samples and the speciï¬c group of samples to present at the same time. The ability to learn a full curriculum improves GTN performance. For that reason, we adopt that approach for all GTN experiments. 3.3 GTN S FOR SUPERVISED LEARNING To explore whether GTNs can generate training data that helps networks learn rapidly, we compare to 3 treatments for MNIST classiï¬cation. 1)Real Data - Training learners with random mini-batches of real data, as is ubiquitous in SGD. 2) Dataset Distillation - Training learners with synthetic data, where training examples are directly encoded as tensors optimized by the meta-objective, as in Wang et al. (2019b). 3) GTN - Our method where the training data presented to the learner is generated by a neural network. Note that all three methods meta-optimize the inner-loop hyperparameters (i.e. the learning rate and momentum of SGD) as part of the meta-optimization. 5We emphasize that producing state-of-the-art (SOTA) performance (e.g. on MNIST or CIFAR) when training with GTN-generated data is not important for GTNs. Because the ultimate aim for GTNs is to accelerate NAS (Section 3.4), what matters ishow well and inexpensively we can identify architectures that achieve high asymptotic accuracy when later trained on the full (real) training set. A means to that end is being able to train architectures rapidly, i.e. with very few SGD steps, because doing so allows NAS to rapidly identify promising architectures. We are thus interested in â€œfew-step accuracy (i.e. accuracy after a fewâ€“e.g. 32 or 128â€“SGD steps). Besides, there are many reasons not to expect SOTA performance with GTNs (Appendix B). Figure 2a shows that the GTN treatment signiï¬cantly outperforms the other ones ( p <0.01) and trains a learner to be much more accurate whenin the few-step performance regime. Speciï¬cally, for each treatment the ï¬gure shows the test performance of a learner following 32 inner-loop training steps with a batch size of 128. We would not expect training on synthetic data to produce higher accuracy than unlimited SGD steps on real data, but here the performance gain comes because GTNs can compress the real training data by producing synthetic data that enables learners to learn more quickly than on real data. For example, the original dataset might contain many similar images, where only a few of them would be sufï¬cient for training (and GTN can produce just these few). GTN could also combine many different things that need to be learned about images into one image. Figure 2b shows the few-step performance of a learner from each treatment after 2000 total outer- loop iterations (âˆ¼1 hour on a p6000 GPU). For reference, Dataset Distillation (Wang et al., 2019b) reported 79.5% accuracy for a randomly initialized network (using 100 synthetic images vs. our 4,096) and L2T (Fan et al., 2018) reported needing 300x more training iterations to achieve >98% MNIST accuracy. Surprisingly, although recognizable as digits and effective for training, GTN- generated images (Figure 2c) were not visually realistic (see Discussion). 0 500 1000 1500 2000 Outer-loop Iterations 0.90 0.92 0.94 0.96 0.98 1.00Test Accuracy GTN Real Data Dataset Distillation (a) Meta-training curves 0 10 20 30 Inner-loop Iterations 0.90 0.92 0.94 0.96 0.98 1.00Test Accuracy GTN Real Data Dataset Distillation (b) Training curves  (c) GTN-generated samples Figure 2: Teaching MNIST with GTN-generated images. (a) MNIST test set few-step accuracy across outer-loop iterations for different sources of inner-loop training data. The inner-loop consists of 32 SGD steps and the outer-loop optimizes MNIST validation accuracy. Our method (GTN) outperforms the two controls (dataset distillation and samples from real data). (b) For the ï¬nal meta-training iteration, across inner-loop training, accuracy on the MNIST test set when inner-loop training on different data sources. (c) 100 random samples from the trained GTN. Samples are often recognizable as digits, but are not realistic (see Discussion). Each column contains samples from a different digit class, and each row is taken from different inner-loop iterations (evenly spaced from the 32 total iterations, with early iterations at the top). 3.4 A RCHITECTURE SEARCH WITH GTN S We next test the beneï¬ts of GTN for NAS (GTN-NAS) in CIFAR10, a domain where NAS has previously shown signiï¬cant improvements over the best architectures produced by armies of hu- man scientists. Figure 3a shows the few-step training accuracy of a learner trained with either GTN-synthetic data or real (CIFAR10) data over meta-training iterations. After 8h of meta-training, training with GTN-generated data was signiï¬cantly faster than with real data, as in MNIST. To explore the potential for GTN-NAS to accelerate CIFAR10 architecture search, we investigated the Spearman rank correlation (across architectures sampled from the NAS search space) between accelerated GTN-trained network performance (GTN evaluation) and the usual more expensive per- formance metric used during NAS (search evaluation). A correlation plot is shown in Figure 3c; note that a strong correlation implies we can train architectures using GTN evaluation as an inexpensive surrogate. We ï¬nd that GTN evaluation enables predicting the performance of an architecture efï¬- 6ciently. The rank-correlation between 128 steps of training with GTN-synthetic data vs. 100 epochs of real data is 0.3606. The correlation improves to 0.5582 when considering the top 50% of archi- tectures recommended by GTN evaluation scores, which is important because those are the ones that search would select. This improved correlation is slightly stronger than that from 3 epochs of training with real data (0.5235), a âˆ¼9Ã—cost-reduction per trained model. 20 40 60 80 100 120 Inner-loop Iterations 0.3 0.4 0.5 0.6 0.7Training Accuracy GTN Real Data (a) CIFAR10 inner-loop training  (b) CIFAR10 GTN samples 0.1 0.2 0.3 0.4 0.5 GTN Predicted Performance 0.90 0.91 0.92 0.93 0.94 0.95Real Data Predicted Perf.  (c) CIFAR10 correlation Figure 3: Teaching CIFAR10 with GTN-generated images. (a) CIFAR10 training set performance of the ï¬nal learner (after 1,700 meta-optimization steps) across inner-loop learning iterations. (b) Samples generated by GTN to teach CIFAR10 are unrecognizable, despite being effective for train- ing. Each column contains a different class, and each row is taken from the same inner-loop iteration (evenly spaced from all 128 iterations, early iterations at the top). (c) Correlation between perfor- mance prediction using GTN-data vs. Real Data. When considering the top half of architectures (as ranked by GTN evaluation), correlation between GTN evaluation and search evaluation is strong (0.5582 rank-correlation), suggesting that GTN-NAS has potential to uncover high performing ar- chitectures at a signiï¬cantly lower cost. Architectures shown are uniformly sampled from the NAS search space. The top 10% of architectures according to the GTN evaluation (blue squares)â€“ those likely to be selected by GTN-NASâ€“have high true asymptotic accuracy. Architecture search methods are composed of several semi-independent components, such as the choice of search space, search algorithm, and proxy evaluation of candidate architectures. GTNs are proposed as an improvement to this last component, i.e. as a new way to quickly evaluate a new architecture. Thus we test our method under the standard search space for CIFAR10, using a simple form of search (random search) for which there are previous benchmark results. In particular, we ran an architecture search experiment where we evaluated 800 randomly generated architectures trained with GTN-synthetic data. We present the performance after ï¬nal evaluation of the best architecture found in Table 1. This experimental setting is similar to that of Zhang et al. (2018). Highlighting the potential of GTNs as an improved proxy evaluation for architectures, we achieve state-of-the-art results when controlling for search algorithm (the choice of which is orthogonal to our contribution). While it is an apples-to-oranges comparison, GTN-NAS is competitive even with methods that use more advanced search techniques than random search to propose architectures (Appendix E). GTN is compatible with such techniques, and would likely improve their performance, an interesting area of future work. Furthermore, because of the NAS search space, the modules GTN found can be used to create even larger networks. A further test of whether GTNs predictions generalize is if such larger networks would continue performing better than architectures generated by the real- data control, similarly scaled. We tried F=128 and show it indeed does perform better (Table 1), suggesting additional gains can be had by searching post-hoc for the correct F and N settings. 4 D ISCUSSION , FUTURE WORK , AND CONCLUSION The results presented here suggest potential future applications and extensions of GTNs. Given the ability of GTNs to rapidly train new models, they are particularly useful when training many independent models is required (as we showed for NAS). Another such application would be to teach networks on demand to realize particular trade-offs between e.g. accuracy, inference time, and memory requirements. While to address a range of such trade-offs would ordinarily require training many models ahead of time and selecting amongst them (Elsken et al., 2019), GTNs could instead rapidly train a new network only when a particular trade-off is needed. Similarly, agents with unique combinations of skills could be created on demand when needed. 7Table 1: Performance of different architecture search methods. Our results report mean Â±SD of 5 evaluations of the same architecture with different initializations. It is common to report scores with and without Cutout (DeVries & Taylor, 2017), a data augmentation technique used during training. We found better architectures compared to other methods that reduce architecture evaluation speed and were tested with random search (Random Search+WS and Random Search+GHN). Increasing the width of the architecture found (F=128) further improves performance. Because each NAS method ï¬nds a different architecture, the number of parameters differs. Each method ran once. Model Error(%) #params GPU Days Random Search + GHN (Zhang et al., 2018) 4.3 Â±0.1 5.1M 0.42 Random Search + Weight Sharing (Luo et al., 2018) 3.92 3.9M 0.25 Random Search + Real Data (baseline) 3.88 Â±0.08 12.4M 10 Random Search + GTN (ours) 3.84 Â±0.06 8.2M 0.67 Random Search + Real Data + Cutout (baseline) 3.02 Â±0.03 12.4M 10 Random Search + GTN + Cutout (ours) 2.92 Â±0.06 8.2M 0.67 Random Search + Real Data + Cutout (F=128) (baseline) 2.51 Â±0.13 151.7M 10 Random Search + GTN + Cutout (F=128) (ours) 2.42 Â±0.03 97.9M 0.67 Interesting questions are raised by the lack of similarity between the synthetic GTN data and real MNIST and CIFAR10 data. That unrealistic and/or unrecognizable images can meaningfully affect NNs is reminiscent of the ï¬nding that deep neural networks are easily fooled by unrecognizable images (Nguyen et al., 2015). It is possible that if neural network architectures were functionally more similar to human brains, GTNsâ€™ synthetic data might more resemble real data. However, an alternate (speculative) hypothesis is that the human brain might also be able to rapidly learn an arbitrary skill by being shown unnatural, unrecognizable data (recalling the novel Snow Crash). The improved stability of training GTNs from weight normalization naturally suggests the hypoth- esis that weight normalization might similarly stabilize, and thus meaningfully improve, any tech- niques based on meta-gradients (e.g. MAML (Finn et al., 2017), learned optimizers (Metz et al., 2019), and learned update rules (Metz et al., 2018)). In future work, we will more deeply investigate how consistently, and to what degree, this hypothesis holds. Both weight sharing and GHNs can be combined with GTNs by using the shared weights or Hyper- Network for initialization of proposed learners and then ï¬ne-tuning on GTN-produced data. GTNs could also be combined with more intelligent ways to propose which architecture to sample next such as NAO (Luo et al., 2018). Many other extensions would also be interesting to consider. GTNs could be trained for unsupervised learning, for example by training a useful embedding function. Additionally, they could be used to stabilize GAN training and prevent mode collapse (Appendix I shows encouraging initial results). One particularly promising extension is to introduce a closed- loop curriculum (i.e. one that responds dynamically to the performance of the learner throughout training), which we believe could signiï¬cantly improve performance. For example, a recurrent GTN that is conditioned on previous learner outputs could adapt its samples to be appropriately easier or more difï¬cult depending on an agentâ€™s learning progress, similar in spirit to the approach of a human tutor. Such closed-loop teaching can improve learning (Fan et al., 2018). An additional interesting direction is having GTNs generate training environments for RL agents. Appendix H shows this works for the simple RL task of CartPole. That could be either for a pre- deï¬ned target task, or could be combined with more open-ended algorithms that attempt to con- tinuously generate new, different, interesting tasks that foster learning (Clune, 2019; Wang et al., 2019a). Because GTNs can encode any possible environment, they (or something similar) may be necessary to have truly unconstrained, open-ended algorithms (Stanley et al., 2017). If techniques could be invented to coax GTNs to produce recognizable, human-meaningful training environments, the technique could also produce interesting virtual worlds for us to learn in, play in, or explore. This paper introduces a new method called Generative Teaching Networks, wherein data genera- tors are trained to produce effective training data through meta-learning. We have shown that such an approach can produce supervised datasets that yield better few-step accuracy than an equivalent amount of real training data, and generalize across architectures and random initializations. We leverage such efï¬cient training data to create a fast NAS method that generates state-of-the-art ar- chitectures (controlling for the search algorithm). While GTNs may be of particular interest to the 8ï¬eld of architecture search (where the computational cost to evaluate candidate architectures often limits the scope of its application), we believe that GTNs open up an intriguing and challenging line of research into a variety of algorithms that learn to generate their own training data. 5 A CKNOWLEDGEMENTS For insightful discussions and suggestions, we thank the members of Uber AI Labs, especially Theofanis Karaletsos, Martin Jankowiak, Thomas Miconi, Joost Huizinga, and Lawrence Murray. REFERENCES Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pa- chocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018. Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. arXiv preprint arXiv:1705.10823, 2017. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ï¬delity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. Jeff Clune. Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artiï¬cial intelligence. arXiv preprint arXiv:1905.10985, 2019. Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. Gamaleldin F. Elsayed, Ian J. Goodfellow, and Jascha Sohl-Dickstein. Adversarial reprogramming of neural networks. CoRR, abs/1806.11146, 2018. URL http://arxiv.org/abs/1806. 11146. Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Efï¬cient multi-objective neural architecture search via lamarckian evolution. In International Conference on Learning Representations, 2019. Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. Learning to teach. arXiv preprint arXiv:1805.03643, 2018. Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm. arXiv preprint arXiv:1710.11622, 2017. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pp. 1126â€“1135. JMLR. org, 2017. JL Fleiss. Review papers: The statistical basis of meta-analysis. Statistical methods in medical research, 2(2):121â€“145, 1993. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor- mation processing systems, pp. 2672â€“2680, 2014. Alex Graves, Marc G. Bellemare, Jacob Menick, RÂ´emi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1311â€“1320, 2017. Andreas Griewank and Andrea Walther. Algorithm 799: revolve: an implementation of check- pointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on Mathematical Software (TOMS), 26(1):19â€“45, 2000. 9Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiï¬ers: Surpassing human-level performance on imagenet classiï¬cation. In Proceedings of the IEEE international conference on computer vision, pp. 1026â€“1034, 2015. Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho, and Pieter Abbeel. Evolved policy gradients. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 5405â€“5414. Curran Associates, Inc., 2018. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua. Learning active learning from data. In NIPS, 2017. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Proceed- ings of the European Conference on Computer Vision (ECCV), pp. 19â€“34, 2018a. Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018b. Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization. In Advances in neural information processing systems, pp. 7816â€“7827, 2018. Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiï¬er nonlinearities improve neural net- work acoustic models. In Proc. icml, volume 30, pp. 3, 2013. Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter opti- mization through reversible learning. In Proceedings of the 32Nd International Conference on In- ternational Conference on Machine Learning - Volume 37, ICMLâ€™15, pp. 2113â€“2122. JMLR.org, 2015. Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Meta-learning up- date rules for unsupervised representation learning. arXiv preprint arXiv:1804.00222, 2018. Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl-dickstein. Learned optimizers that outperform on wall-clock and validation loss, 2019. V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928â€“1937, 2016. Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High con- ï¬dence predictions for unrecognizable images. In In Computer Vision and Pattern Recognition (CVPR â€™15), 2015. Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efï¬cient neural architecture search via parameters sharing. In Jennifer Dy and Andreas Krause (eds.),Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4095â€“4104, Stockholmsmssan, Stockholm Sweden, 10â€“15 Jul 2018. PMLR. Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classiï¬er architecture search. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence , volume 33, pp. 4780â€“4789, 2019. 10Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems , pp. 901â€“909, 2016. Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NIPS, 2016. Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017. Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations, 2018. Burr Settles. Active learning literature survey. Technical report, 2010. Hava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. Journal of computer and system sciences, 50(1):132â€“150, 1995. Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, and Charles Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 3308â€“3318. Curran Associates, Inc., 2017. Kenneth O. Stanley, Joel Lehman, and Lisa Soros. Open-endedness: The last grand challenge youve never heard of. Oâ€™Reilly Online, 2017. URL https://www.oreilly.com/ideas/ open-endedness-the-last-grand-challenge-youve-never-heard-of . Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do- main randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pp. 23â€“30. IEEE, 2017. Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018. Ivor Tsang, James Kwok, and Pak-Ming Cheung. Core vector machines: Fast svm training on very large data sets. Journal of Machine Learning Research, 6:363â€“392, 04 2005. Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv preprint arXiv:1901.01753, 2019a. Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros. Dataset distillation, 2019b. Chris Zhang, Mengye Ren, and Raquel Urtasun. Graph hypernetworks for neural architecture search. arXiv preprint arXiv:1810.05749, 2018. Barret Zoph and Quoc V . Le. Neural architecture search with reinforcement learning. 2017. URL https://arxiv.org/abs/1611.01578. 11APPENDIX A A DDITIONAL EXPERIMENTAL DETAILS The outer loop loss function is domain speciï¬c. In the supervised experiments on MNIST and CIFAR, the outer loop loss was cross-entropy for logistic regression on real MNIST or CIFAR data. The inner-loop loss matches the outer-loop loss, but with synthetic data instead of real data. Appendix H describes the losses for the RL experiments. The following equation deï¬nes the inner-loop SGD with momentum update for the learner parame- ters Î¸t. Î¸t+1 = Î¸t âˆ’Î± âˆ‘ 0â‰¤tâ€²â‰¤t Î²tâˆ’tâ€² âˆ‡â„“inner(G(ztâ€² ,ytâ€² ),ytâ€² ,Î¸tâ€² ), (1) where Î±and Î² are the learning rate and momentum hyperparameters, respectively. zt is a batch of noise vectors that are input to the generator and are sampled from a unit-variance Gaussian.yt are a batch of labels for each generated sample/input and are sampled uniformly from all available class labels. Instead of randomly samplingzt, we can also learn a curriculum by additionally optimizingzt directly and keeping yt ï¬xed throughout all of training. Results for both approaches (and additional curriculum ablations) are reported in Section 3.2. A.1 MNIST E XPERIMENTS : For the GTN training for MNIST we sampled architectures from a distribution that produces ar- chitectures with convolutional (conv) and fully-connectd (FC) layers. All architectures had 2 conv layers, but the number of ï¬lters for each layer was sampled uniformly from the ranges U([32,128]) and U([64,256]), respectively. After each conv layer there is a max pooling layer for dimensionality reduction. After the last conv layer, there is a fully-connected layer with number of ï¬lters sampled uniformly from the range U([64,256]). We used Kaiming Normal initialization (He et al., 2015) and LeakyReLUs (Maas et al., 2013) (with Î±= 0.1). We use BatchNorm (Ioffe & Szegedy, 2015) for both the generator and the learners. The BatchNorm momentum for the learner was set to 0 (meta-training consistently converged to small values and we saw no signiï¬cant gain from learning the value). The generator consisted of 2 FC layers (1024 and 128 âˆ—H/4 âˆ—H/4 ï¬lters, respectively, where H is the ï¬nal width of the synthetic image). After the last FC layer there are 2 conv layers. The ï¬rst conv has 64 ï¬lters. The second conv has 1 ï¬lter followed by a Tanh. We found it particularly important to normalize (mean of zero and variance of one) all datasets. Hyperparameters are shown in Table 2. Hyperparameter Value Learning Rate 0.01 Initial LR 0.02 Initial Momentum 0.5 Adam Beta 1 0.9 Adam Beta 2 0.999 Size of latent variable 128 Inner-Loop Batch Size 128 Outer-Loop Batch Size 128 Table 2: Hyperparameters for MNIST experiments A.2 CIFAR10 E XPERIMENTS : For GTN training for CIFAR-10, the template architecture is a small learner with 5 convolutional layers followed by a global average pooling and an FC layer. The second and fourth convolution had stride=2 for dimensionality reduction. The number of ï¬lters of the ï¬rst conv layer was sam- pled uniformly from the range U([32,128]) while all others were sampled uniformly from the range U([64,256]). Other details including the generator architecture were the same as the MNIST exper- iments, except the CIFAR generatorâ€™s second conv layer had 3 ï¬lters instead of 1. Hyperparameters 12used can be found in Table 3. For CIFAR10 we augmented the real training set when training GTNs with random crops and horizontal ï¬‚ips. We do not add weight normalization to the ï¬nal architectures found during architecture search, but we do so when we train architectures with GTN-generated data during architecture search to provide an estimate of their asymptotic performance. Hyperparameter Value Learning Rate 0.002 Initial LR 0.02 Initial Momentum 0.5 Adam Beta 1 0.9 Adam Beta 2 0.9 Adam Ïµ 1e-5 Size of latent variable 128 Inner-loop Batch Size 128 Outer-loop Batch Size 256 Table 3: Hyperparameters for CIFAR10 experiments APPENDIX B R EASONS GTN S ARE NOT EXPECTED TO PRODUCE SOTA ACCURACY VS . ASYMPTOTIC PERFORMANCE WHEN TRAINING ON REAL DATA There are three reasons not to expect SOTA accuracy levels for the learners trained on synthetic data: (1) we train for very few SGD steps (32 or 128 vs. tens of thousands), (2) SOTA performance results from architectures explicitly designed (with much human effort) to achieve record accuracy, whereas GTN produces compressed training data optimized to generalize across diverse architectures with the aim of quickly evaluating a new architectureâ€™s potential, and (3) SOTA methods often use data outside of the benchmark dataset and complex data-augmentation schemes. APPENDIX C C ELL SEARCH SPACE When searching for the operations in a CNN cell, the 11 possible operations are listed below. â€¢ identity â€¢ 1 Ã—1 convolution â€¢ 3 Ã—3 convolution â€¢ 1 Ã—3 + 3Ã—1 convolution â€¢ 1 Ã—7 + 7Ã—1 convolution â€¢ 2 Ã—2 max pooling â€¢ 3 Ã—3 max pooling â€¢ 5 Ã—5 max pooling â€¢ 2 Ã—2 average pooling â€¢ 3 Ã—3 average pooling â€¢ 5 Ã—5 average pooling APPENDIX D C OMPUTATION AND MEMORY COMPLEXITY With the traditional training of DNNs with back-propagation, the memory requirements are pro- portional to the size of the network because activations during the forward propagation have to be stored for the backward propagation step. With meta-gradients, the memory requirement also grows with the number of inner-loop steps because all activations and weights have to be stored for the 132nd order gradient to be computed. This becomes impractical for large networks and/or many inner- loop steps. To reduce the memory requirements, we utilize gradient-checkpointing (Griewank & Walther, 2000) by only storing the computed weights of learner after each inner-loop step and re- computing the activations during the backward pass. This trick allows us to compute meta-gradients for networks with 10s of millions of parameters over hundreds of inner-loop steps in a single GPU. While in theory the computational cost of computing meta-gradients with gradient-checkpointing is 4x larger than computing gradients (and 12x larger than forward propagation), in our experiments it is about 2.5x slower than gradients through backpropagation due to parallelism. We could further reduce the memory requirements by utilizing reversable hypergradients (Maclaurin et al., 2015), but, in our case, we were not constrained by the number of inner-loop steps we can store in memory. APPENDIX E E XTENDED NAS RESULTS In the limited computation regime (less than 1 day of computation), the best methods were, in order, GHN, ENAS, GTN, and NAONet with a mean error of 2.84%, 2.89%, 2.92%, and 2.93%, respectively. A 0.08% difference on CIFAR10 represents 8 out of the 10k test samples. For that reason, we consider all of these methods as state of the art. Note that out of the four, GTN is the only one relying on Random Search for architecture proposal. Table 4: Performance of different architecture search methods. Search with our method required 16h total, of which 8h were spent training the GTN and 8h were spent evaluating 800 architectures with GTN-produced synthetic data. Our results report mean Â±SD of 5 evaluations of the same architec- ture with different initializations. It is common to report scores with and without Cutout (DeVries & Taylor, 2017), a data augmentation technique used during training.We found better architectures compared to other methods using random search (Random-WS and GHN-Top) and are competitive with algorithms that beneï¬t from more advanced search methods (e.g. NAONet and ENAS employ non-random architecture proposals for performance gains; GTNs could be combined with such non- random proposals, which would likely further improve performance). Increasing the width of the architecture found (F=128) further improves performance. Model Error(%) #params Random GPU Days NASNet-A (Zoph & Le, 2017) 3.41 3.3M \u0017 2000 AmoebaNet-B + Cutout (Real et al., 2019) 2.13 34.9M \u0017 3150 DARTS + Cutout (Liu et al., 2018b) 2.83 4.6M \u0017 4 NAONet + Cutout (Luo et al., 2018) 2.48 10.6M \u0017 200 NAONet-WS (Luo et al., 2018) 3.53 2.5M \u0017 0.3 NAONet-WS + Cutout (Luo et al., 2018) 2.93 2.5M \u0017 0.3 ENAS (Pham et al., 2018) 3.54 4.6M \u0017 0.45 ENAS + Cutout (Pham et al., 2018) 2.89 4.6M \u0017 0.45 GHN Top-Best + Cutout (Zhang et al., 2018) 2.84 Â±0.07 5.7M \u0017 0.84 GHN Top (Zhang et al., 2018) 4.3 Â±0.1 5.1M âœ“ 0.42 Random-WS (Luo et al., 2018) 3.92 3.9M âœ“ 0.25 Random Search + Real Data (baseline) 3.88 Â±0.08 12.4M âœ“ 10 RS + Real Data + Cutout (baseline) 3.02 Â±0.03 12.4M âœ“ 10 RS + Real Data + Cutout (F=128) (baseline) 2.51 Â±0.13 151.7M âœ“ 10 Random Search + GTN (ours) 3.84 Â±0.06 8.2M âœ“ 0.67 Random Search + GTN + Cutout (ours) 2.92 Â±0.06 8.2M âœ“ 0.67 RS + GTN + Cutout (F=128) (ours) 2.42 Â±0.03 97.9M âœ“ 0.67 APPENDIX F C ONDITIONED GENERATOR VS . XY-G ENERATOR Our experiments in the main paper conditioned the generator to create data with given labels, by concatenating a one-hot encoded label to the input vector. We also explored an alternative approach where the generator itself produced a target probability distribution to label the data it generates. Because more information is encoded into a soft label than a one-hot encoded one, we expected an improved training set to be generated by this variant. Indeed, such a â€œdark knowledgeâ€ dis- tillation setup has been shown to perform better than learning from labels (Hinton et al., 2015). 14However, the results in Figure 4 indicate that jointly generating both images and their soft labels under-performs generating only images, although the result could change with different hyperpa- rameter values and/or innovations that improve the stability of training. 0 500 1000 1500 2000 2500 3000 3500 Outer-loop Steps 0.800 0.825 0.850 0.875 0.900 0.925 0.950 0.975 1.000Validation Accuracy Real Data GTN DK Figure 4: Comparison between a conditional generator and a generator that outputs an image/label pair. We expected the latter â€œdark knowledgeâ€ approach to outperform the conditional generator, but that does not seem to be the case. Because initialization and training of the dark knowledge variant were more sensitive, we believe a more rigorous tuning of the process could lead to a different result. APPENDIX G GTN GENERATES (SEEMINGLY ) ENDLESS DATA While optimizing images directly (i.e. optimizing a ï¬xed tensor of images) would result in a ï¬xed number of samples, optimizing a generator can potentially result in an unlimited amount of new samples. We tested this generative capability by generating more data during evaluation (i.e. with no change to the meta-optimization procedure) in two ways. In the ï¬rst experiment, we increase the amount of data in each inner-loop optimization step by increasing the batch size (which results in lower variance gradients). In the second experiment, we keep the number of samples per batch ï¬xed, but increase the number of inner-loop optimization steps for which a new network is trained. Both cases result in an increased amount of training data. If the GTN generator has overï¬t to the number of inner-loop optimization steps during meta-training and/or the batch size, then we would not expect performance to improve when we have the generator produce more data. However, an alternate hypothesis is that the GTN is producing a healthy distribution of training data, irrespective of exactly how it is being used. Such a hypothesis would be supported by performance increase in these experiments. Figure 5a shows performance as a function of increasing batch size (beyond the batch size used during meta-optimization, i.e. 128). The increase in performance of GTN means that we can sample larger training sets from our generator (with diminishing returns) and that we are not limited by the choice of batch size during training (which is constrained due to both memory and computation requirements). Figure 5b shows the results of generating more data by increasing the number of inner-loop opti- mization steps. Generalization to more inner-loop optimization steps is important when the number of inner-loop optimization steps used during meta-optimization is not enough to achieve maximum performance. This experiment also tests the generalization of the optimizer hyperparameters be- cause they were optimized to maximize learner performance after a ï¬xed number of steps. There is an increase in performance of the learner trained on GTN-generated data as the number of inner- loop optimization steps is increased, demonstrating that the GTN is producing generally useful data instead of overï¬tting to the number of inner-loop optimization steps during training (Figure 5b). Extending the conclusion from Figure 2b, in the very low data regime, GTN is signiï¬cantly better than training on real data (p< 0.05). However, as more inner-loop optimization steps are taken and thus more unique data is available to the learner, training on the real data becomes more effective than learning from synthetic data (p< 0.05) (see Figure 5b). 15150 200 250 300 350 400 450 500 Inner loop batch size 0.915 0.920 0.925 0.930 0.935 0.940 0.945 0.950Validation Accuracy Real Data GTN (a) Increasing inner-loop batch size 20 40 60 80 100 120 Inner-loop Steps 0.92 0.93 0.94 0.95 0.96 0.97Validation Accuracy Real Data GTN (b) Increasing inner-loop optimization steps Figure 5: (a) The left ï¬gure shows that even though GTN was meta-trained to generate synthetic data of batch size 128, sampling increasingly larger batches results in improved learner performance (the inner-loop optimization steps are ï¬xed to 16). (b) The right ï¬gure shows that increasing the number of inner-loop optimization steps (beyond the 16 steps used during meta-training) improves learner performance. The performance gain with real data is larger in this setting. This improvement shows that GTNs do not overï¬t to a speciï¬c number of inner-loop optimization steps. Figure 6: GTN samples w/o curriculum. Another interesting test for our generative model is to test the distribution of learners after they have trained on the synthetic data. We want to know, for instance, if training on synthetic samples from one GTN results in a functionally similar set of learner weights regardless of learner initialization (this phenomena can be called learner mode collapse). Learner mode collapse would prevent the performance gains that can be achieved through ensembling diverse learners. We tested for learner mode collapse by evaluating the performance (on held-out data and held-out architecture) of an en- semble of 32 randomly initialized learners that are trained on independent batches from the same GTN. To construct the ensemble, we average the predicted probability distributions across the learn- ers to compute a combined prediction and accuracy. The results of this experiment can be seen in Figure 7, which shows that the combined performance of an ensemble is better (on average) than an individual learner, providing additional evidence that the distribution of synthetic data is healthy and allows ensembles to be harnessed to improve performance, as is standard with networks trained on real data. APPENDIX H GTN FOR RL To demonstrate the potential of GTNs for RL, we tested our approach with a small experiment on the classic CartPole test problem (see Brockman et al. (2016) for details on the domain. We conducted this experiment before the discovery that weight normalization improves GTN training, so these experiments do not feature it; it might further improve performance. For this experiment, the meta-objective the GTN is trained with is the advantage actor-critic formulation: log Ï€(a|Î¸Ï€)(Râˆ’ V(s; Î¸v)) (Mnih et al., 2016). The state-value V is provided by a separate neural network trained to estimate the average state-value for the learners produced so far during meta-training. The learners train on synthetic data via a single-step of SGD with a batch size of 512 and a mean squared error 160 500 1000 1500 2000 2500 3000 3500 Outer-loop Steps 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00Validation Accuracy Real Data - single Real Data - ensemble GTN - single GTN - ensemble Figure 7: Performance of an ensemble of GTN learners vs. individual GTN learners. Ensembling a set of neural networks that each had different weight initializations, but were trained on data from the same GTN substantially improves performance. This result provides more evidence that GTNs generate a healthy distribution of training data and are not somehow forcing the learners to all learn a functionally equivalent solution. regression loss, meaning the inner loop is supervised learning. The outer-loop is reinforced because the simulator is non-differentiable. We could have also used an RL algorithm in the inner loop. In that scenario the GTN would have to learn to produce an entire synthetic world an RL agent would learn in. Thus, it would create the initial state and then iteratively receive actions and generate the next state and optionally a reward. For example, a GTN could learn to produce an entire MDP that an agent trains on, with the meta-objective being that the trained agent then performs well on a target task. We consider such synthetic (PO)MDPs an exciting direction for future research. The score on CartPole is the number of frames out of 200 for which the pole is elevated. Both GTN and an A2C (Mnih et al., 2016) control effectively solve the problem (Figure 8). Interestingly, training GTNs takes the same number of simulator steps as training a single learner with policy- gradients (Figure 8). Incredibly, however, once trained, the synthetic data from a GTN can be used to train a learner to maximum performance in a single SGD step! While that is unlikely to be true for harder target RL tasks, these results suggest that the speed-up for architecture search from using GTNs in the RL domain can be even greater than in supervised domain. The CartPole experiments feature a single-layer neural network with 64 hidden units and a tanh activation function for both the policy and the value network. The inner-loop batch size was 512 and the number of inner-loop training iterations was 1. The observation space of this environment consists of a real-valued vector of size 4 (Cart position, Cart velocity, Pole position, Pole velocity). The action space consists of 2 discrete actions (move left or move right). The outer loop loss is the reward function for the target domain (here, pole-balancing). The inner loop loss is mean squared error (i.e. the network is doing supervised learning on the state-action mapping pairs provided by the GTN). APPENDIX I S OLVING MODE COLLAPSE IN GAN S WITH GTN S We created an implementation of generative adversarial networks (GANs) (Goodfellow et al., 2014) and found they tend to generate the same class of images (e.g. only 1s, Figure 9), which is a common training pathology in GANs known as mode collapse (Srivastava et al., 2017). While there are tech- niques to prevent mode collapse (e.g. minibatch discrimination and historical averaging (Salimans et al., 2016)), we hypothesized that combining the ideas behind GTNs and GANs might provide a different, additional technique to help combat mode collapse. The idea is to add a discriminator to the GTN forcing the data it generates to both be realistic and help a learner perform well on the meta-objective of classifying MNIST. The reason this approach should help prevent mode collapse is that if the generator only produces one class of images, a learner trained on that data will not be able to classify all classes of images. This algorithm (GTN-GAN) was able to produce realistic images with no identiï¬able mode collapse (Figure 10). GTNs offer a different type of solution to the issue of mode collapse than the many that have been proposed, adding a new tool to our toolbox 170 20000 40000 60000 80000 100000 Environment Steps 25 50 75 100 125 150 175 200Reward A2C Agent A2C + GTN Agent Figure 8: An A2C Agent control trains a single policy throughout all of training, while the GTN method starts with a new, randomly initialized network at each iteration and produces the plotted performance after a single step of SGD . This plot is difï¬cult to parse because of that difference: it compares the accumulated performance of A2C across all environment steps up to that point vs. the performance achieved with GTN data in a single step of SGD from a single batch of synthetic data. Thus, at the 100,000 th step of training, GTNs enable training a newly initialized network to the given performance (of around 190) 100,000 times faster with GTN synthetic data than with A2C from scratch. With GTNs, we can therefore train many new, high-performing agents quickly. That would be useful in many ways, such as greatly accelerating architecture search algorithms for RL. Of course, these results are on a simple problem, and (unlike our supervised learning experiments) have not yet shown that the GTN data works with different architectures, but these results demonstrate the intriguing potential of GTNs for RL. One reason we might expect even larger speedups for RL vs. supervised learning is because a major reason RL is sample inefï¬cient is because it requires exploration to ï¬gure out how to solve the problem. However, once that exploration has been done, the GTN can produce data to efï¬ciently teach that solution to a new architecture. RL thus represents an exciting area of future research for GTNs. Performing that research is beyond the scope of this paper, but we highlight the intriguing potential here to inspire such future work. for solving that problem. Note we do not claim this approach is better than other techniques to prevent mode collapse, only that it is an interesting new type of option, perhaps one that could be productively combined with other techniques. Figure 9: Images generated by a basic GAN on MNIST before and after mode collapse. The left image shows GAN-produced images early in GAN training and the right image shows GAN samples later in training after mode collapse has occurred due to training instabilities. APPENDIX J A DDITIONAL MOTIVATION There is an additional motivation for GTNs that involves long-term, ambitious research goals: GTN is a step towards algorithms that generate their own training environments, such that agents trained in them eventually solve tasks we otherwise do not know how to train agents to solve (Clune, 2019). It is important to pursue such algorithms because our capacity to conceive of effective training en- vironments on our own as humans is limited, yet for our learning algorithms to achieve their full potential they will ultimately need to consume vast and complex curricula of learning challenges 18Figure 10: Images generated by a GTN with an auxiliary GAN loss. Combining GTNs with GANs produces far more realistic images than GTNs alone (which produced alien, unrecognizable images, Figure 6). The combination also stabilizes GAN training, preventing mode collapse. and data. Algorithms for generating curricula, such as the the paired open-ended trailblazer (POET) algorithm (Wang et al., 2019a), have proven effective for achieving behaviors that would otherwise be out of reach, but no algorithm yet can generate completely unconstrained training conditions. For example, POET searches for training environments within a highly restricted preconceived space of problems. GTNs are exciting because they can encode a rich set of possible environments with min- imal assumptions, ranging from labeled data for supervised learning to (in theory) entire complex virtual RL domains (with their own learned internal physics). Because RNNs are Turing-complete (Siegelmann & Sontag, 1995), GTNs should be able to theoretically encode all possible learning environments. Of course, while what is theoretically possible is different from what is achievable in practice, GTNs give us an expressive environmental encoding to begin exploring what potential is unlocked when we can learn to generate sophisticated learning environments. The initial results presented here show that GTNs can be trained end-to-end with gradient descent through the entire learning process; such end-to-end learning has proven highly scalable before, and may similarly in the future enable learning expressive GTNs that encode complex learning environments. APPENDIX K O N THE REALISM OF IMAGES There are two phenomenon related to the recognizability of the GTN-generated images that are interesting. (1) Many of the images generated by GTNs are unrecognizable (e.g. as digits), yet a network trained on them still performs well on a real, target task (e.g. MNIST). (2) Some conditions increase the realism (recognizability) of the images. We will focus on the MNIST experiments because that is where we have conducted experiments to investigate this phenomenon. Figure 12 shows all of the images generated by a GTN with a curriculum. Most of the images do not resemble real MNIST digits, and many are alien and unrecognizable. Interestingly, there is a qualitative change in the recognizability of the images at the very end of the curriculum (the last 4-5 rows, which show the last two training batches). Both phenomena are interesting, and we do not have satisfactory explanations for either. Here we present many hypothesis we have generated that could explain these phenomenon. We also present a few experiments we have done to shed light on these issues. A more detailed investigation is an interesting area for future research. Importantly, the recognizable images at the end of the curriculum are not required to obtain high performance on MNIST. The evidence for that fact is in Figure 2, which shows that the performance of a learner trained on GTN-data is already high after around 23 inner-loop iterations, before the network has seen the recognizable images in the last 4-5 rows (which are shown in the last two training batches, i.e. training iterations 31 and 32). Thus, a network can learn to get over 98% accuracy on MNIST training only on unrecognizable images. At a high level, there are three possible camps of explanation for these phenomenon. Camp 1. Performance would be higher with higher realism, but optimization difï¬culties (e.g. vanishing/exploding gradients) prevent learning a generator that produces such higher- performing, more realistic images.Evidence in favor of this camp of hypotheses is that the realistic images come at the end of the curriculum, where the gradient ï¬‚ow is easiest (as gradients do not have to ï¬‚ow back through multiple inner-loop steps of learning). A prediction of this hypothesis is that as we improve our ability to train GTNs, the images will become more realistic. 19Camp 2. Performance is higher with lower realism (at least when not late in the curriculum), which is why unrealistic images are produced. There are at least two reasons why unrealistic images could generate higher performance. (A) Compression enables faster learning (i.e. learning with fewer samples). Being able to produce unrealistic images allows much more information to be packed into a single training example. For example, imagine a single image that could teach a network about many different styles of the digit 7 all at the same time (and/or different translations, rotations, and scales of a 7). It is well known that data augmentation improves performance because it teaches a network, for example, that the same image at different locations in the image is of the same class. It is conceivable that a single image could do something similar by showing multiple 7s at different locations. (B) Unrealistic images allow better generalization. When trying to produce high performance with very few samples, the risk of performance loss due to overï¬tting is high. A small set of realistic images may not have enough variation in non-essential aspects of the image (e.g. the background color) that allow a network to reliably learn the class of interest in a way that will generalize to instances of that class not in the training set (e.g. images of that class with a background color not in the training set). With the ability to produce unrealistic images (e.g. 7s against many different artiï¬cial backdrops, such as by adding seemingly random noise to the background color), GTNs could prevent the network from overï¬tting to spurious correlations in the training set (e.g. background color). In other words, GTNs couldlearn to produce something similar to domain randomization (Tobin et al., 2017; Andrychowicz et al., 2018) to improve generalization, an exciting prospect. Camp 3. It makes no difference on performance whether the images are realistic, but there are more unrealistic images that are effective than realistic ones, explaining why they tend to be produced. This hypothesis is in line with the fact that deep neural networks are easily fooled (Nguyen et al., 2015) and susceptible to adversarial examples (Szegedy et al., 2013). The idea is that images that are unrecognizeable to us are surprisingly meaningful to (i.e. impactful on) DNNs. This hypothesis is also in line with the fact that images can be generated that hack a trained DNN to cause it to perform other functions it was not trained to do (e.g. to perform a different function entirely, such as hacking an ImageNet classiï¬cation network to perfom a counting task like counting the number of occurences of Zebras in an image) (Elsayed et al., 2018). This hypothesis is also in line with recent research into meta-learning, showing that an initial weight vector can be carefully chosen such that it will produce a desired outcome (including implementing any learning algorithm) once subjected to data and SGD (Finn et al., 2017; Finn & Levine, 2017). One thing not explained by this hypothesis is why images at the end of the curriculum are more recognizable. Within this third camp of hypotheses is the possibility that the key features required to recognize a type of image (e.g. a 7) could be broken up across images. For example, one image could teach a network about the bottom half of a 7 and another about the top half. Recognizing either on its own is evidence for a seven, and if across a batch or training dataset the network learned to associate both features with the class 7, there is no reason that both the top half and bottom half ever have to co-occur. That could lead to unrealistic images with partial features. One prediction of this hypothesis (although one not exclusive to this hypothesis), is that averaging all of the images for each class across the entire GTN-produced training set should reveal recognizable digits. The idea is that no individual image contains a full seven, but on average the images combine to produce sevens (and the other digits). Figure 11 shows the results of this experiment. On average the digits are recognizable. This result is also consistent with Camp 1 of hypotheses: perhaps performance would increase further if the images were individually more recognizable. It is also consistent with Camp 2: perhaps the network is forced to combine many sevens into each image, making them individually unrecognizeable, but recognizable as 7s on average. Additionally, in line with Camp 2, if the network has learned to produce something like domain randomization, it could add variation across the dataset in the background (making each individual image less recognizable), but Hypothesis 2 would predict that, on average, the aspects of the image that do not matter (e.g. the background) average out to a neutral value or the true dataset mean (for MNIST, black), whereas the true class information (e.g. the digit itself) would be recognizable on average, exactly as we see in Figure 11. Thus, the average images shed light on the overall subject, but do not provide conclusive results regarding which camp of hypotheses is correct. An additional experiment we performed was to see if the alien images somehow represent the edges of the decision boundaries between images. The hypothesis is that images in the center of a cluster (e.g. a Platonic, archetypal 7) are not that helpful to establish neural network decision boundaries 20between classes, and thus GTN does not need to produce many of them. Instead, it might bene- ï¬t by generating mostly edge cases to establish the decision boundaries, which is why the digits are mostly difï¬cult to recognize. To rephrase this hypothesis in the language of support vector machines, the GTN could be mostly producing the support vectors of each class, instead of more recognizable images well inside of each class (i.e. instead of producing many Platonic images with a high margin from the decision boundary). A prediction of this hypothesis is that the unrecog- nizable GTN-generated images should be closer to the decision boundaries than the recognizable GTN-generated images. To test this hypothesis, we borrow an idea and technique from Toneva et al. (2018), which argues that one way to identify images near (or far) from a decision boundary is to count the number of times that, during the training of a neural network, images in the training set have their classiï¬cation labels change. The intuition is that Platonic images in the center of a class will not have their labels change often across training, whereas images near the boundaries between classes will change labels often as the decision boundaries are updated repeatedly during training. We trained a randomly initialized network on real images (the results are qualitatively the same if the network is trained on the GTN-produced images). After each training step we classify the images in Figure 12 with the network being trained. We then rank the synthetic images from Figure 12 on the frequency that their classiï¬cation changed between adjacent SGD steps. Figure 15 presents these images reordered (in row-major order) according to the number of times the output label for that image changed during training. The recognizable images are all tightly clustered in this analysis, showing that there is a strong relationship between how recognizable an image is and how often its label changes during training. Interestingly, the images are not all the way at one end of the spectrum. However, keep in mind that many images in this sorted list are tied with respect to the number of changes (with ties broken randomly), and the number of ï¬‚ips does not go up linearly with each row of the image. Figure 14 shows the number of label ï¬‚ips vs. the order in this ranked list. The recognizable images on average have 2.0 label ï¬‚ips (Figure 14, orange horizontal line), meaning that they are towards the extreme of images whose labels do not change often. This result is in line with the hypothesis that these are Platonic images well inside the class boundary. However, there are also many unrecognizable images whose labels do not ï¬‚ip often, which is not explained by this hypothesis. Overall, this analysis suggests the discovery of something interesting, although much future work needs to be done to probe this question further. Why are images only realistic at the end of the curriculum? Separate from, but related to, the question of why most images are unrecognizable, is why the recognizable images are only produced at the end of the curriculum. We have come up with a few different hypotheses, but we do not know which is correct. (1) The gradients ï¬‚ow best to those samples, and thus they become the most realistic (in line with Camp 1 of hypotheses above). (2) It helps performance for some reason to have realistic images right at the end of training, but realism does not help (Camp 3) or even hurts (Camp 2) earlier in the curriculum. For example, perhaps the Platonic images are the least likely to change the decision boundaries, allowing them to be used for ï¬nal ï¬ne-tuning of the decision boundaries (akin to an annealed learning rate). In line with this hypothesis is that, when optimization cannot create a deterministic curriculum, realism seems to be higher on average (Figure 13). (3) The effect is produced by the decision to take the batch normalization (Ioffe & Szegedy, 2015) statistics from the ï¬nal batch of training. Batch normalization is a common technique to improve training. Following normal batch norm procedures, during inner-loop training the batch norm statistics (mean and variance) are computed per batch. However, during inner-loop testing/inference, the statistics are instead computed from the training set. In our experiments, we calculate these statistics from the last batch in the curriculum. Thus, if it helps performance on the meta-training test set (the inner loop test set performance the GTN is being optimized for) to have the statistics of that batch match the statistics of the target data set (which contains real images), there could be a pressure for those images to be more realistic. Contrary to this hypothesis, however, is the fact that realism increases in the last two batches of the curriculum, not just the last batch (most visible in Figure 2, which shows sample from each batch in a separate row). Another hypothesis (consistent with Camp 1 and Camp 3), is that producing ï¬rst unrealistic then realistic images might reï¬‚ect how neural networks learn (e.g. ï¬rst learning low-level ï¬lters before moving to more complex examples). However, that hypothesis would presumably predict a gradual increase in realism across the curriculum, instead of realism only sharply increasing in the last few batches. Finally, we did not observe this phenomenon in the CIFAR experiments with a full 21curriculum: the last few batches are not realistic in that experiment (Figure 3b). We do not know why the results on this front are different between MNIST and CIFAR experiments. In short, we do not have a good understanding for why realism increases towards the end of the curriculum. Shedding more light on this issue is an interesting area for future research. Figure 11: Pixel-wise mean per class of all GTN-generated images from the full curriculum treat- ment. 22Figure 12: All images generated by the full-curriculum GTN. The images are shown in the order they are presented to the network, with the ï¬rst batch of images in the curriculum in the top row and the last batch of data in the last row. The batch size does not correspond to the number of samples per row, so batches wrap from the right side of one row to the left side of the row below. 23Figure 13: A sample of images generated by the no-curriculum GTN. 240 1000 2000 3000 4000 Image Index 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0Label Switching Frequency Figure 14: The number of times a class label changes across training for each GTN-generated sample (Y-axis) vs. the rank of that sample when ordered by that same statistic (X-axis). A relatively small fraction of the samples ï¬‚ip labels many times (in line with the idea that they are near the class decision boundaries), whereas most samples change labels only a few times (i.e. once the are learned, they stay learned, in line with them being more canonical examples). The orange line shows the average number of class changes for recognizable images (those in the red box in Figure 15). While not the images with the least number of ï¬‚ips, these recognizable images are towards the end of the spectrum of images whose labels do not change often, in line with the hypothesis that they are more canonical class exemplars. 25Figure 15: All images generated by the full-curriculum GTN ordered by the frequency that their labels change during training. Highlighted is a dense region of realistic images that we manually identiï¬ed. 26",
      "meta_data": {
        "arxiv_id": "1912.07768v1",
        "authors": [
          "Felipe Petroski Such",
          "Aditya Rawal",
          "Joel Lehman",
          "Kenneth O. Stanley",
          "Jeff Clune"
        ],
        "published_date": "2019-12-17T00:57:50Z",
        "pdf_url": "https://arxiv.org/pdf/1912.07768v1.pdf"
      }
    },
    {
      "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
      "abstract": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
      "full_text": "Accelerating Transformer Pre-training with 2:4 Sparsity Yuezhou Hu1 Kang Zhao Weiyu Huang 1 Jianfei Chen 1 Jun Zhu 1 Abstract Training large transformers is slow, but recent innovations on GPU architecture give us an ad- vantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a â€œflip rateâ€ to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through es- timator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the modelâ€™s quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two tech- niques to practically accelerate training: to calcu- late transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar con- vergence to dense training algorithms on several transformer pre-training tasks, while actual ac- celeration can be observed on different shapes of transformer block apparently. Our toolkit is avail- able at https://github.com/huyz2023/ 2by4-pretrain. 1. Introduction Pre-training large-scale transformers is hard, for its intensive computation and time-consuming process (Anthony et al., 2020). To accelerate training, sparsity-based methods have recently emerged as a promising solution, and one of the hardware-friendly sparse patterns is 2:4 sparsity. In a 2:4 sparse matrix, every four consecutive elements contain two 1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University. Correspondence to: Jianfei Chen <jianfeic@tsinghua.edu.cn>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). zeros. Within a tensor core, a 2:4 sparse matrix multiplica- tion (2:4-spMM) could be 2x faster than its dense equivalent on NVIDIA Ampere architecture GPUs. Some works use 2:4 sparsity for accelerating training (Hubara et al., 2021; Lu et al., 2023; McDanel et al., 2022; Chmiel et al., 2023). However, they mainly target on con- volutional neural networks (CNNs) (Hubara et al., 2021; McDanel et al., 2022), whose architecture, optimizer and training procedure are different from transformers. Whether these 2:4 sparse training methods are capable for transform- ers remains under-explored. In practice, we find two bar- riers: 1) Low accuracy. The hyperparameters in some accuracy preserving techniques for transformers vary sig- nificantly from that for CNNs, which is ineffective if trans- planted directly. Remarkably, simply halving the inner di- mensionality of a feed-forward network can also reduce the same amount of computational cost, but provides bet- ter performance than most of proposed 2:4 sparse training methods. 2) Inefficiency. All previous works on 2:4 training stay on simulation, and do not provide actual acceleration results. Besides, they donâ€™t focus on other key operations be- yond matrix multiplication that affect the practical time cost, such as overheads of pruning and activation functions. They usually lead to substantial mismatches between simulation and actual acceleration performance. In this work, we aim to propose an end-to-end acceleration method for pre-training transformers based on 2:4 sparsity. Here are our major contributions: â€¢ We propose three accuracy-preserving techniques (two for masked decay and one for dense fine-tune) for 2:4 training. First, we propose to apply the masked decay on gradients rather than on weight. Second, we show that the feasible masked decay factor on transformers may be very small (100x smaller than it has been reported on CNNs) and devise a method to quickly determine an available decay factor. Besides, our analysis demonstrates that employing a dense fine-tuning stage at the end of pre- training, rather than at the beginning, can enhance the quality of transformers. â€¢ We analyze practical factors affecting the 2:4 training speed of transformers, which is rarely considered by pre- vious works. We identify two speed bottlenecks: prun- ing overhead and gated activation functionsâ€™ overhead. 1 arXiv:2404.01847v3  [cs.LG]  27 Oct 2024Accelerating Transformer Pre-training with 2:4 Sparsity We proposed kernel-level accelerated methods to address each of these bottlenecks. â€¢ To the best of our knowledge, this is the first report on end-to-end acceleration on pre-training transformers (Fig- ure 7, Table 11). Experiments show that transformers pre-trained using our proposed sparse training scheme are comparable or even superior in accuracy to those trained with dense training methods (Table 5, 6). 2. Related Work Existing sparsity-based methods can be classified into two categories: accelerating inference and accelerating training. For training acceleration, they can be further grouped by whether 2:4 sparsity is involved. Sparsity for Inference Acceleration Early methods in- clude one-shot pruning (Han et al., 2015; 2016; Lee et al., 2018; Mishra et al., 2021). Later methods (Evci et al., 2021; Zhou et al., 2021; Lasby et al., 2023) suggest using dynamic sparse training (DST). Particularly, Zhou et al. (2021) pro- poses sparse-refined straight-through estimator (SR-STE) for 2:4 inference. Iterative magnitude-based pruning (IMP) methods (Chen et al., 2020; 2021; You et al., 2022), orig- inated from the winning lottery ticket theory (Frankle & Carbin, 2019; Frankle et al., 2020), can also be viewed as a DST approach. All these methods only speedup the forward pass. They are insufficient to accelerate training. 2:4 Semi-Structured Sparsity for Training Acceleration Accelerating training by 2:4 sparsity is hard, because both the forward and backward passes need to be accelerated. On some GPUs involving sparse tensor cores, 2:4-spMMs perform 2x faster than dense GEMMs (Mishra et al., 2021; BUSATO & POOL). In light of this, (Hubara et al., 2021) firstly proposes a transposable N:M mask to accelerate both output activations and input gradients computation in back- ward pass. Zhang et al. (2023) improve transposable mask to bi-directional mask (Bi-Mask) to further boost mask di- versity. To accelerate calculating weight gradient via 2:4- spMM, an unbiased minimum-variance estimator (MVUE) is introduced (Chmiel et al., 2023). In addition, Xu et al. (2022) also achieve fully sparse training of CNNs using spatial similarity. However, all these works do not report end-to-end training speedups on 2:4 sparse tensor cores, and they are built for CNNs. Practical 2:4 training acceleration on transformers has not been reported so far. Other Structured Sparsity for Training Acceleration Structured sparsity means channel-wise pruning to dense networks. For instance, training a large model and then compressing it to be thinner or shallower seems effective (Li et al., 2020; Zhou et al., 2020), given a fixed accuracy requirement. However, itâ€™s not memory-efficient due to the larger modelâ€™s redundancy. In addition, low-rank adaption proves to be an effective method to reduce fine-tuning costs (Hu et al., 2023), but it canâ€™t accelerate the pre-training. 3. Preliminary In this section, we first present the mathematical formula- tions of dense training and fully sparse training. Afterward, we revisit the related methods which are helpful to achieve fully sparse training with 2:4 sparsity, including SR-STE (Zhou et al., 2021), transposable N: M mask (Hubara et al., 2021), and MVUE (Chmiel et al., 2023). 3.1. Dense Training Problem Formulation Dense training solves an opti- mization problem minw L(w), where L is a loss function, w âˆˆ RD is the collection of dense weights of all layers, flat- tened to a vector. The loss is optimized by gradient descent optimization algorithms such as SGD, Adam (Kingma & Ba, 2017) and AdamW (Loshchilov & Hutter, 2019). GEMMs of a Linear Layer in Dense Training In each training step, a single linear layer performs three general matrix multiplications (GEMMs): Z = XWâŠ¤, âˆ‡X = âˆ‡ZW, âˆ‡W = âˆ‡âŠ¤ ZX, (1) where X, W and Z are input activations, weights, and out- put activations, with shape X, âˆ‡X âˆˆ RpÃ—q, W, âˆ‡W âˆˆ RrÃ—q, and Z, âˆ‡Z âˆˆ RpÃ—r. Here, the three GEMMs com- putes output activations, input activation gradients, and weight gradients, respectively. Without loss of generality, we assume the input X to be a 2D matrix rather than a 3D tensor. In the feed-forward networks of a transformer, this can be done by simply flattening the input tensorsâ€™ first two axes, i.e., axes of batch size and sequence length. 3.2. Fully Sparse Training with 2:4 Sparsity GEMMs can be accelerated with structured sparsity. Partic- ularly, 2:4 sparsity (Mishra et al., 2021) is a semi-structured sparsity pattern supported on NVIDIA Ampere architec- tures. A 2:4 sparse matrix partitions its elements into groups of four numbers, where each group has exactly two zeros. Depending on the direction of partition, there are row-wise 2:4 sparse matrix and column-wise 2:4 sparse matrix; see Appendix A.1. With such sparsity, a GEMM C = AB can be accelerated by 2x with the 2:4-spMM kernel if either A is row-wise 2:4 sparse, or B is column-wise 2:4 sparse. To accelerate training, each GEMM in Equation (1) should have one 2:4 sparse operand. In general, weights and out- put activation gradients are selected to be pruned due to relatively lower pruning-induced loss (Chmiel et al., 2023). 2Accelerating Transformer Pre-training with 2:4 Sparsity That is, Z = XSwt(WâŠ¤), (2) âˆ‡X = âˆ‡ZSw(W), (3) âˆ‡W = Sz(âˆ‡âŠ¤ Z)X. (4) In Equations (2) to (4), Swt, Sw, and Sz represent the prun- ing functions of WâŠ¤, W, and âˆ‡âŠ¤ Z. They take dense matri- ces as input, and outputs 2:4 sparse matrices. By intuition, a pruning function picks out the 2 elements with the max magnitudes in the adjoining 4 elements and zero out the rest. With hardware support, computing Equations (2) to (4) can be theoretically 2x faster than Equation (1). This method use 2:4-spMMs for all matrix multiplications in forward and backward propagation, so we call it fully sparse training (FST). Note that Equation (4) contains a straight-through estimator (STE), which we will explain later. Transposable Masks Hubara et al. (2021) suggest that a weight matrix and its transpose can be simply pruned by multiplying binary masks, i.e., Swt(WâŠ¤) =WâŠ¤ âŠ™ Mwt, S w(W) =W âŠ™ Mw, where Mwt, Mw âˆˆ {0, 1}pÃ—q are 2:4 sparse, and âŠ™ is element-wise product. To utilize 2:4-spMM, the two binary masks should be mutually transposable: Mwt = MâŠ¤ w, (5) which they call as transposable masks (same as our defina- tion in Section 5.1). In this manner, the backward pass share the same sparse weight matrix with the forward pass. The authors also propose a 2-approximation method for generat- ing such masks with claimed low computational complexity. Minimum-Variance Unbiased Estimator Chmiel et al. (2023) propose to calculate the 2:4 sparse masks of neural gradients by MVUE, i.e., Sz(âˆ‡âŠ¤ Z) = MVUE(âˆ‡âŠ¤ Z). (6) Compared to the commonly used minimum square error esti- mation, MVUE guarantees unbiasedness and minimizes the variance of the sparsified gradients, which is more favorable for promoting the convergence of training. 3.3. Optimization Strategies for Sparse Training The optimization of a sparse network is difficult as it has non- differentiable pruning functions. The optimization objective can be formulated as minw L(Ëœ w). The network makes prediction with a sparse weight vector Ëœ w= m(w) âŠ™ w, where the mask m(w) âˆˆ {0, 1}D is the concatenation of masks for each layer. If a layer is not sparsified, then the corresponding mask is an all-one matrix. Computing the gradient is tricky since the mask m is dynamically com- puted based on the dense weight w: by chain rule we have âˆ‡wL(Ëœ w) =âˆ‚ Ëœw âˆ‚w âˆ‡Ëœ wL(Ëœ w), where âˆ‚ Ëœw âˆ‚w is a Jacobian matrix. However, Ëœw is not differentiable with w since it includes a non-differentiable mask-computing-function m(Â·) in it. Thus, it takes some skills to estimate the gradients and up- date the parameters. STE As Ëœw is an approximation of w, a straight-through estimator (STE, Bengio et al. (2013)) directly passes the gradient of Ëœw to w: âˆ‡wL(Ëœ w) â† âˆ‡Ëœ wL(Ëœ w). (7) SR-STE There is a problem with STE: only a portion of the weights in a layer participate in the forward calculation, but all the weights receive gradients. This indicates that the gradients associated with masked weights1 might be inac- curate. To suppress those inaccurate gradients, Zhou et al. (2021) proposes sparse-refined straight-through estimator (SR-STE) which adds a decay term when updating: wt â† wtâˆ’1 âˆ’ Î³(âˆ‡wLt(Ëœ wtâˆ’1) +Î»W (m(wtâˆ’1)) âŠ™ wtâˆ’1), (8) where Î³ stands for the learning rate, Î»W is the decay fac- tor, and m(wtâˆ’1) denotes the logical not operation of m(wtâˆ’1). This decay term alleviates the change of weight mask. With SR-STE, the optimization target becomes min w L(Ëœ w) +Î»W 2 âˆ¥w âŠ™ m(w)âˆ¥2 2. (9) 4. Accuracy Preserving Techniques While the methods reviewed in Section 3 can successfully perform FST on small-scale models such as ResNet and DenseNet, it is not clear whether they can be directly ap- plied to pre-train large transformers. It is challenging for FST to preserve the accuracy of dense training, since the weights and masks need to be learned jointly, which is a non- differentiable, combinatorial optimization problem. More- over, unlike inference acceleration methods, FST has no pre-trained dense model to start with. In this section, we pro- pose three practical techniques to improve the convergence of FST for transformers: transformer-specific masked decay, Fast decay factor determination and dense fine-tuning. 4.1. Flip Rate: Stability of Training Inspired by previous work (Zhou et al., 2021; You et al., 2022), we define a â€œflip rateâ€ to measure how frequently the mask vector changes after one optimizer step. This metric could be used to monitor whether the network connection is stable during training. 3Accelerating Transformer Pre-training with 2:4 Sparsity Figure 1.Flip rates change throughout the training of differentÎ»W on Transformer-base. Note that these models utilize an identical learning rate schedule. Table 1.Training results of different Î»W on Transformer-base. As Î»W increases from 0 to 2e-4, accuracy first rises and then drops, which means that Î»W should be neither too big nor too small to reach the optimal results. Î»W AVG EPOCH LOSS VAL LOSS TEST BLEU DENSE 4.558 3.978 26.15 0 (STE) 4.76 4.164 24.98 6E-7 4.684 4.079 25.68 6E-6 4.626 4.033 25.81 2E-6 4.64 4.041 25.94 2E-5 4.642 4.049 25.74 2E-4 4.662 4.06 25.62 Definition 4.1. Suppose wt is a D-dimensional weight vector at time t, and the flip rate rt is defined as the change in proportion of the mask vector after an optimizer step: rt = âˆ¥m(wt) âˆ’ m(wtâˆ’1)âˆ¥1/D âˆˆ [0, 1]. The larger rt is, the more unstable the network connections become. You et al. (2022) suggest that a sparse neural network acts differently in different training phases. In the early phase of training, it eagerly explores different connection modes, which means the masks vector change rapidly over time. Later, the masks gradually become stable, and the network turns itself to fine-tune weight values. In terms of flip rate, we hypothesize that A healthy training process comes with the flip rate rt rising at the beginning of training and then gradually fading to 0. We measure flip rate change for dense training, STE and SR-STE with different Î»W in Figure 1. For dense training, we compute the flip rate by pruning the dense weight in each iteration, despite the pruned weight is never used for training. In terms of flip rate, dense training is healthy: itsrt exactly increases first before declines. If a training process 1Unlike some relevant literature, we use â€œmasked weightsâ€ and â€œpruned weightsâ€ to denote the weights that are set to 0. consistently has higher flip rate than dense training, which we call as â€œflip rate explosionâ€, it may suffer from a loss in final accuracy due to unstable training; see Table 1. In practice, STE suffers from a flip rate explosion, while SR- STE takes effect by â€œfreezingâ€ masks of weights: by adding a decay term, it decrease the number of flips. This inhibition effect is related to the decay factor of SR-STE: the larger Î»W is, the stronger the inhibition of flips is, and the smaller flip rate goes. In this section, all methods we propose involve our ultimate principle: the peak of the curve should be sufficiently high to fully explore different connection modes, and the tail should be sufficiently low for the optimization process to converge. 4.2. Transformer-Specific Masked Decay Based on our insights on flip rate, we propose a method to suppress the frequent change of masks during FST for transformers, which we call masked decay. Unlike Equation (8) which imposes regularization directly on weights, we propose to add masked decay on gradients, i.e., gt â† âˆ‡wLt(Ëœ wtâˆ’1) +Î»W (m(wtâˆ’1) âŠ™ wtâˆ’1). (10) On SGD, applying decay on weights and on gradients are equivalent, but on popular optimizers like Adam and AdamW they arenâ€™t. Specifically, Adam updates weights by wt â† wtâˆ’1 âˆ’ Î³(Î²1utâˆ’1 + (1âˆ’ Î²1)gt) (1 âˆ’ Î²t 1)(âˆšË†vt + Ïµ) (11) where u and v are the first and second order momentum of w. Compared to Equation (8), the masked decay regu- larization term in Equation (10) would be later normalized by âˆšË†vt + Ïµ in Equation (11), before it is subtracted from weights. In this way, each dimension receives a different intensity of decay (â€œmasked decayâ€). More specifically, weights with larger gradients get smaller decay intensity, and vice versa. In FST, we periodically prune weights by their magnitudes. STE may cause the network to fall into such â€œdilemma pointsâ€, where a portion of pruned weights and unpruned weights have nearly the same L1 norm. Thus, the network consistently oscillate between two possible masks m1 and m2, and is unlikely to jump out the dilemma itself. To illustrate this, we split each weight matrix by small 4 Ã— 4 blocks. We count each blockâ€™s cumulative flip number and measure the â€L1 norm gapâ€ by gi = âˆ¥m1 âŠ™ wiâˆ¥1 âˆ’ âˆ¥m2 âŠ™ wiâˆ¥1, where wi is the i-th 4 Ã— 4 weights, m1 âŠ™ wi and m2 âŠ™ wi have the first and second largest L1-norm among different pruning binary masks. The selected mask is most likely to oscillate between m1 and m2, especially when gi is small. In STE, there exists more 4 Ã— 4 blocks 4Accelerating Transformer Pre-training with 2:4 Sparsity Figure 2.Scatter plots of cumulative flip number and L1 norm gap gi on every 4 Ã— 4 block. All results are selected on Transformer- base, with epoch=20. (a) shows the result of dense model. (b)-(d) shows that of masked decaying on gradients, no decaying, and masked decaying on weights. Also, we do it on purpose to choose an extremely large Î»W for SR-STE. Figure 3.Applying masked decay on weights takes no effect to inhibit flip rate on BERT-base (compared to applying directly on gradient). Table 2.Optimal Î»W for multiple models. MODEL OPTIMAL Î»W RESNET18 (Z HOU ET AL ., 2021) 2 E-4 BERT-BASE 6E-6 TRANSFORMER -BASE 1E-6 DEIT-TINY 2E-3 GPT-2 124M 6 E-5 350M 2 E-4 774M 2 E-4 1558M 6 E-5 with high flip num and low â€L1 norm gapâ€; see Figure 2. This results in overall flip rate explosion of STE. On these occasions, we argue that an evenly masked de- cay applied on weights is insufficient to save the training from such â€œtrapsâ€. The weights donâ€™t differentiate them- selves after an update, so masks may oscillate back. By normalizing the weight gradients with âˆšË†vt + Ïµ, our masked decay amplifies the regularization strength for the dimen- sion with smaller gradient, pushing it towards zero. Then, the regularized dimension can no longer compete with other dimensions. So we effectively break the tie and push the training process out of the trap, towards a â€œhealthierâ€ state. The comparison results between our masked decay defined in Equation (10) and the conventional counterpart in Equa- tion (8) are shown in Figure 3. Results show that applying masked decay on weights takes no effect to inhibit flip rate explosion of STE, while applying on gradients works fine. 4.3. Fast Decay Factor Determination The determination of the decay factor Î»W in Equation (10) is non-trivial: if Î»W is excessively large, then the â€œpeakâ€ of the flip rate curve is not high enough; ifÎ»W is too small, the â€œtailâ€ of the curve is not low enough. Both do not provide a healthy training process. Besides, we find that Î»W values for CNNs and other small-scale networks differ significantly from those for transformers, while on transformers, optimal Î»W can span up to three orders of magnitude (Table 2). As pre-training large transformers is costly, grid searching for Î»W with the final accuracy is impractical, so it is vital to determine a feasible Î»W as quickly as possible. To quickly determine Î»W , here we propose a test-based method: 1) Grid search on the warm-up stage of training. For each Î»W value in a candidate set, sample a corresponding flip rate of the sparse network from a small number of training steps. Note that sampling in early training stage is enough to obtain a representative flip rate specific to a sparse network. 2) Comparison with the dense counterparts. Suppose rt0 to be the standard flip rate on the dense network at time t0 and r â€² t0 to be the sparse networkâ€™s flip rate. Their ratio is Âµ = r â€² t0/rt0 . We suggest that a feasibleÎ»W should have Âµ âˆˆ [0.60, 0.95] and the sparse network may suffer from an accuracy drop if Âµ â‰¥ 1. 4.4. Dense Fine-Tuning To better improve accuracy, we suggest using a â€œdense fine- tuningâ€ procedure at the end of training. Formally, we select a switch point ts. FST is performed while t â‰¤ ts, and dense training is switched to if t > ts. Why Choose Dense Fine-Tuning Instead of Dense Pre- training? While previous work (Han et al., 2017) suggest to switch between sparse and dense training stages, some recent works like STEP (Lu et al., 2023) utilize dense pre- training rather than dense fine-tuning, which means a dense network is initially trained for a period of time before being switched to a sparse one. However, we argue that dense pre- training is meaningless in our FST process. As described in 5Accelerating Transformer Pre-training with 2:4 Sparsity Figure 4.Dense fine-tuning versus dense pre-training on BERT- base Section 4.1, the peak of the flip rate curve should be suffi- ciently high to explore connection modes, so what matters most to the flip rate is the magnitudes of weights, which are the key to determine if connections are built or demol- ished. In this regard, both FST and dense pre-training are capable of delivering proper gradient magnitudes, so dense pre-training is a waste. The precise gradients are generally more necessary in the later stages of training, where the flip rate of the dense network comes to its tail. Figure 4 visual- izes the loss curve of pre-training BERT-base, where dense pre-train obtains nearly the same result as the naive SR-STE method. From this, we propose the following insight: If dense pre-training of tÎ± steps provides slight improve- ment of accuracy, then moving the tÎ± dense steps to the end gives far more improvement than dense pre-training. As for the specific position of the switch point in training, STEP (Lu et al., 2023) suggests that the dense pre-training occupy 10% to 50% of the total steps. Likewise, we deter- mine that our dense fine-tuning takes up the last 1/6 of total steps for balance training efficiency and accuracy. 5. Training Acceleration Techniques For transformers, the forward pass of FST involves prun- ing weights in FFNs with transposable 2:4 masks and then performing normal forward propagation. During backward propagation in FST, the gradients of input activations and weight gradients in FFNs are derived by Equation (3) and (4), respectively. Note that we also utilize MVUE to prune gradients of output activations, i.e., Equation (6). Compared to dense training, our FST replaces all the GEMMs in FFNs with 2:4-spMMs that theoretically perform 2x faster than their dense counterparts on GPUs within sparse tensor cores. In addition to speeding up the most time-consuming GEMMs in FFNs, there are three major operations that also have non-negligible impacts on training speed: 1) Pruning. In FST, pruning includes two steps: finding a mask that satisfies the 2:4 sparse patterns and then enforc- ing the mask to the corresponding dense matrices. In our case, we find that the time cost of finding transposable masks is time-consuming. 2) Activation functions. In transformers, SwiGLU and GEGLU (Shazeer, 2020) are popular. These two acti- vation functions involve a gate mechanism to regulate activations. This mechanism easily induces the GPU L2 cache misses, thus decreasing the computing speed. 3) Updating optimizer states. The excessive update fre- quency can introduce additional time overheads. Below, we show our methods to accelerate these operations, the main workflow of which is shown in Appendix B. 5.1. Fast Computation of Transposable Masks Problem Formulation We aim to find such a mask matrix M âˆˆ {0, 1}rÃ—q for every W âˆˆ RrÃ—q in the FFN layer that 1) each adjoining 4 Ã— 4 block contains 8 non-zero positions; each row and column in the block occupies 2 non-zero elements exactly; 2) maxM âˆ¥M âŠ™ Wâˆ¥1. Then M would be our targeting transposable mask. As described in Equation (5), both a transposable mask itself and its transposition conform to the format of 2:4 sparsity. Previous 2-approximation algorithm (Hubara et al., 2021) consists of two steps: sort elements, and pick elements out of the array. They claim that the procedure has less computational complexity. However, in practice, the sorting and picking process contains too many jumps in its control flow, and may be fatal to modern GPU architecture. To make full use of the GPUsâ€™ parallel computation capability (SIMD and SIMT), we convert the transposable mask-search process into a convolution operation which traverse all the masks to obtain the optimal one in three steps: 1) Create a convolutional kernel in the shape of 4 Ã— 4 Ã— nt, where nt denotes the number of transposable masks. In the case of 2:4 sparsity, mask diversity nt = 90. These mask blocks for 2:4 sparsity can be selected by exhaus- tively inspecting all potential masks offline. 2) Calculate the index matrix via Algorithm 1. The index matrix denotes which 4 Ã— 4 mask in the convolutional kernel is the optimal mask that retains most of the weight norms after being applied to weights. Algorithm 1 transposable mask search Input: mask pattern mâ€², weight matrix W 1. W = abs(W) 2. out = conv2d(W, mâ€², stride= 4, padding= 0) 3. index = argmax(out, dim= 2) return index 3) Replace all the elements in the index matrix by the cor- responding 4 Ã— 4 block, which is the desired mask. 6Accelerating Transformer Pre-training with 2:4 Sparsity Figure 5.Transposable mask search Figure 6.left: adapted method; right: intuitive method Table 3.Throughput of two transposable search kernels on RTX3090 (TB/s). INPUT METHOD 2-APPROX OURS FP16 FP32 FP16 FP32 3072 Ã— 768 18.5 36.4 69.2 104.7 4096 Ã— 1024 22.5 38.4 91.9 131.5 5120 Ã— 1280 22.6 44.4 91 128.2 1024 Ã— 1600 22.8 44.8 95 134.5 8192 Ã— 2048 23 45.1 99.4 142.9 16384 Ã— 4096 23.2 45.4 100.1 144.8 30768 Ã— 8192 23.2 45.5 100.9 145.1 Table 4.Throughput of two GEGLU implementations on RTX3090 with fp16 column-major input tensors (TB/s). INPUT METHOD INTUITIVE OURS 32 Ã— 512 Ã— 768 18.4 55.5 32 Ã— 512 Ã— 1024 19.9 55.7 32 Ã— 512 Ã— 1280 18.2 55.9 32 Ã— 512 Ã— 1600 18.4 55.9 32 Ã— 512 Ã— 2048 19.5 56 32 Ã— 512 Ã— 4096 11.8 56.1 32 Ã— 512 Ã— 8192 12.1 56.2 Notably, step (1) is executed offline. Step (2) and (3) are fre- quently performed during FST. The workflow of our method is shown in Figure 5. Compared to the 2-approximation al- gorithm, our method is up to about 5 times faster (Table 3). 5.2. Acceleration of Gated Activation Functions Activation functions with gated mechanisms are widely used in transformers such as GLM (Du et al., 2022) and LLaMA (Touvron et al., 2023). Typical gated activation functions involve SwiGLU and GEGLU. The bottleneck of such activation functions is that the gate operations easily incur GPU L2 cache miss. Take GEGLU as an example: GEGLU(X, U, V, b, c) = GELU(XUâŠ¤ +b)âŠ™(XVâŠ¤ + c), where X âˆˆ RpÃ—q, U, V âˆˆ RrÃ—q, b, c âˆˆ Rr. In prac- tice, this function is composed of three steps: 1) Concatenate U and V into a new weight matrix W âˆˆ R2rÃ—q, and b, c into a new bias vector d âˆˆ R2r. 2) Directly calculate Z = XWâŠ¤ + d âˆˆ RpÃ—2r as a com- pressed matrix. 3) Split the Z in the second dimension intoZ1, Z2 âˆˆ RpÃ—r. Calculate GELU(Z1) âŠ™ Z2. Different from dense model, where output activations are row-major matrices, in FST, the output activations are column-major; see Appendix A.2. This property results in the third step being extremely time-consuming if conven- tionally Z is accessed along the row dimension. To illustrate, Figure 6 shows that in a column-major matrix Z, accessing along the column accords with array layout. Thus, adjacent elements loaded into the GPU cache can be probably hit. By contrast, accessing along the row does not fully utilize the efficiency of GPU cache. In light of this, we carefully imple- ment a GEGLU kernel where elements are accessed along the column dimension. In this way, GEGLU is performed 5 times faster than the naive counterpart; see Table 4. 5.3. Other Implementation Details Reducing Updating Frequency We find that a 2:4 mask doesnâ€™t change a lot after one optimization step, and it is not necessary to update a mask frequently. For the sake of efficiency, we update the transposable masks of weights every l optimizer steps. We usually take l = 40in practice. Utilities For 2:4-spMMs, we use CUTLASS (Thakkar et al., 2023). Other GPU kernels are implemented in Triton, including transposable mask search kernel, pruning kernel, MVUE kernel, GEGLU kernel, and masked decay kernel. 6. Experiments In this section, we validate the proposed training speedup methods on several transformers, including BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), Transformer- 2Results reported in the original paper; see https: //github.com/facebookresearch/deit/blob/ main/README_deit.md. 3DeiT-base dense model using the original recipe. 7Accelerating Transformer Pre-training with 2:4 Sparsity Table 5.GLUE scores of different 2:4 training methods with BERT. METHODLOSS AVG SCORECOLA MNLI MNLIEXTRAMRPC QNLI QQP RTE SST-2 STS-B DENSE 2.066979.8Â±0.4 45.3Â±1.1 82.6Â±0.2 83.4Â±0.1 78.8Â±1.7/86.1Â±1 89 .3Â±0.2 90.3Â±0.1/87.1Â±0 55.8Â±0.9 91Â±0.5 83 .7Â±1/83.7Â±1HALF 2.128077.9Â±0.4 37.2Â±1.3 82.4Â±0.1 83Â±0.3 75 .1Â±1.4/84.2Â±0.7 88 .8Â±0.3 89.9Â±0.1/86.6Â±0.1 51.2Â±2.4 92.1Â±0.5 82.1Â±0.5/82.3Â±0.4STEP 2.1179 77.7Â±0.1 40.4Â±1.4 82.2Â±0.1 82.8Â±0.1 74.5Â±0.7/83.5Â±0.4 88 .3Â±0.4 90.2Â±0.1/87Â±0.1 50.8Â±2.1 92.3Â±0.3 79.7Â±1.2/80.7Â±0.6BI-MASK2.117677.7Â±0.3 38.3Â±0.7 82.3Â±0.1 83Â±0.1 74 .3Â±0.7/83Â±0.6 88 .3Â±0.3 90.2Â±0.1/86.9Â±0.1 53.1Â±1.4 90.9Â±0.3 80.9Â±0.7/81.7Â±0.4OURS 2.0968 79.6Â±0.6 44.4Â±1.9 82.6Â±0.2 83Â±0.1 80.9Â±0.7/87.4Â±0.4 88.4Â±0.3 90.3Â±0.1/87Â±0.1 54.3Â±1 91.2Â±0.4 82.9Â±2.1/83Â±1.7 Table 6.GLUE scores with different model sizes on GPT-2 models. PARAMSMETHODVAL LOSSAVGSCORECOLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI 124M DENSE 2.907 73.9Â±1.1 44.6Â±0.9 82Â±0.1 78.3Â±1.3/84.8Â±1 88 .4Â±0.2 90Â±0 86 .5Â±0/61.3Â±1.5 91 .9Â±0.2 77.3Â±3.2/77.9Â±2.9 24.3Â±7.1OURS 2.952 74.3Â±0.5 44.8Â±1.3 81.5Â±0.2 77.5Â±1.8/84.2Â±1.3 87.8Â±0.1 89.5Â±0.1 85.9Â±0.1/66Â±1 90.6Â±0.4 80Â±0.8/80.3Â±0.5 23.9Â±6.4 350M DENSE 2.618 76.3Â±0.1 54.3Â±0.4 85.1Â±0.1 80.7Â±1/86.6Â±0.7 90 .7Â±0.1 91Â±0.1 87.8Â±0.1/64.9Â±1.7 93.5Â±0.4 81.7Â±1.2/82.2Â±0.8 17.6Â±3.2OURS 2.688 77.1Â±0.2 51.8Â±1.8 84.3Â±0.1 80.6Â±1.3/86.5Â±0.8 90.4Â±0.2 90.7Â±0.1 87.5Â±0.1/66.7Â±1.3 93.3Â±0.4 83.4Â±1.1/83.5Â±1.1 26.4Â±4 774M DENSE 2.493 76.2Â±0.4 57.5Â±2 86.1Â±0.1 80.3Â±1.3/86.4Â±0.9 91.4Â±0.2 91.1Â±0.1 88Â±0.1/67.7Â±2.6 94 .6Â±0.4 77.3Â±3.3/78.4Â±2.9 15.1Â±2.3OURS 2.564 77.1Â±0.4 55.9Â±0.9 85.6Â±0.2 81.2Â±0.6/87Â±0.4 91.4Â±0.1 91Â±0.1 87.8Â±0.1/71.5Â±0.7 94.2Â±0.4 81.8Â±1.3/82.3Â±1.2 15.8Â±1.2 1558MDENSE 2.399 76.5Â±0.5 55.3Â±2 87 Â±0.1 79 Â±1/85.3Â±0.8 91 .8Â±0.3 91.3Â±0.1 88.3Â±0.1/73.3Â±2 95 .9Â±0.3 78.5Â±2.4/79.2Â±2.5 13Â±1.3OURS 2.489 77.1Â±0.5 56.4Â±3 86.6Â±0.1 80Â±0.4/86.1Â±0.3 91.9Â±0.1 91.4Â±0.1 88.4Â±0.1/75Â±1.8 95.2Â±0.4 80.6Â±1.1/81.1Â±1.3 12.7Â±1.1 Table 7.SQuAD scores on GPT-2 models. PARAMS METHOD EM F1 124M DENSE 67.6 78.8 OURS 67.5 78 .5 350M DENSE 73.2 83.6 OURS 71.9 82 .4 774M DENSE 74.3 84.9 OURS 74.3 84 .6 Table 8.Experimental results for DeiT. SIZE METHOD ACC@1 A CC@5 DEIT-TINY ORIGINAL 2 72.2 91.1 DENSE 3 72.9 91.6 OURS 70.4 90 .1 DEIT-SMALL ORIGINAL 79.9 90.5 DENSE 79.9 94.5 BI-MASK 77.6 - OURS 79.2 94.8 DEIT-BASE ORIGINAL 81.8 95.6 DENSE 81.0 95.0 OURS 81.3 95 .4 Table 9.Experimental results for Transformer-base. METHOD AVG EPOCH LOSS TEST BLEU VAL BLEU VAL LOSS DENSE 4.558 26.15 26.56 3.982 HALF 4.659 26.12 26.36 4.041 STEP 4.692 25.27 25.85 4.082 OURS 4.649 26 .48 26 .78 3 .977 base for machine translation (Vaswani et al., 2023), and DeiT (Touvron et al., 2021b). For BERT, we use Cramming (Geiping & Goldstein, 2022) to pre-train a 16-layer BERT model with the sequence length of 512 on the C4 dataset (Raffel et al., 2019). For GPT-2, we use nanoGPT (Karpathy, 2023) to pre-train GPT-2 124M, 355M, 774M, and 1.5B on OpenWebText (Gokaslan & Cohen, 2019). Both BERT and GPT-2 models are estimated on GLUE (Wang et al., 2018). For DeiT (Touvron et al., 2021a), we pre-train DeiT-tiny on ImageNet-1K dataset (Deng et al., 2009). Besides, we use fairseq (Ott et al., 2019) to train Transformer-base on the WMT 14 En-De dataset (Bojar et al., 2014) and measure the BLEU (Papineni et al., 2002) score of the trained model. Of note, we use n to denote the length of sequences, d to denote the input and output dimensions of each trans- former block, dff to denote the inner dimensions of the FFNs in each transformer block, h to denote the number of heads, and N to denote the micro-batch size on each device. The pre-training and evaluation scripts are pub- licly available at https://github.com/thu-ml/ 2by4-pretrain-acc-examples . 6.1. Accuracy Results To investigate the effect of different 2:4 sparse training meth- ods, we pre-train a sparse BERT-base model on the C4 dataset using two sparse training methods: STEP (Lu et al., 2023) and Bi-Mask (Zhang et al., 2023). Besides, we also pre-train a dense BERT-base and a â€˜Halfâ€™ BERT-base for comparison. Of note, â€˜Halfâ€™ denotes a smaller yet still dense BERT-base model. To create Half model, we simply reduce the dff of each FFN layer in the original BERT-base by half while maintaining the original value of d. Theoretically, this adjustment halves the floating operations (FLOPs) of the original FFN layer as well. Except for the FFN layers, the shapes of the rest layers remain unaltered. All the pre-trained models are measured on GLUE bench- mark (WNLI excluded). Surprisingly, Table 5 shows that despite having identical FLOPs, the 2:4-sparse BERT-base trained with STEP and Bi-Mask shows inferior average scores compared to the Half model. The Half model attains 8Accelerating Transformer Pre-training with 2:4 Sparsity Table 10.Experimental results of masked decay, MVUE, and dense fine-tuning (FT) with BERT-Base. For decay term, we use both techniques in Sections 4.2 and 4.3. MASKED DECAY MVUE D ENSE FT L OSS AVG SCORE % % % 2.1553 77.6 Â± 0.2 ! % % 2.1096 79.2 Â± 0.2 ! ! % 2.1172 78.4 Â± 0.3 ! % ! 2.0896 79.4 Â± 0.2 ! ! ! 2.0968 79 .6 Â±0.6 Table 11.Actual pre-train speed up on the whole network. PARAMETERS BATCH SIZE SPEEDUP 124M 16 1.18 350M 8 1.2 774M 4 1.21 Figure 7.Result of acceleration ratio S of different batch sizes and embedding Sizes. (a) shows the acceleration of a FFN layer. (b)-(d) shows the acceleration of a transformer block when n = 2048, 1024, 512. an average score of 77.9 on GLUE tests, while STEP and Bi-Mask only reach 77.7 due to the weaknesses in MRPC, QNLI, and STSB. By comparison, BERT-base trained in our proposed training method achieves 79.6 on GLUE, which significantly outperforms other sparse training methods and is comparable with the dense baseline, i.e., 79.8. Besides, we pre-train GPT-2 models with proposed meth- ods. Table 6 and 7 shows that our method for model sizes of 124M, 350M, 775M and 1558M achieves lossless scores compared with dense baselines. Similarly, DeiT and Transformer-base trained with our method also reach com- parable results to dense training; see Table 8 and 9. For GPT-2 and BERT, the training loss curves are sketched in Appendix C. Ablation Study We aim to investigate the effect of masked decay, MVUE and dense fine-tuning introduced in Section 4.2, 3.2, and 4.4. The 16-layer BERT-base is used for ablation study. Results in Table 10 show that: 1) The dense fine-tuning procedure helps to improve accuracy on GLUE by 2 points at most ; 2) MVUE leads to insignifi- cant, controllable accuracy loss; 3) By combining all these techniques together, 2:4 sparse training for transformers achieves comparable accuracy results as dense training. 6.2. Speedup Results The training acceleration techniques proposed in Section 5 are evaluated using GPT-2 models and RTX3090 GPUs. FP16 mixed precision training is used on all models. The practical speedups of a single FFN layer, a single trans- former block, and the entire network, compared to their re- spective dense counterparts, are reported. All the measured datum contain both forward and backward propagation. Feed-forward Network Layers For a single FFN layer, we fix n = 2048and change d. Results in Figure 7 show that a FFN layer can be accelerated up to 1.7x faster than its corresponding dense layer. Transformer Block We measure the acceleration ratio of a transformer block when n = 512, 1024, 2048. Results in Figure 7 show that in most cases, a transformer block can be accelerated to 1.3x faster via 2:4 sparsity. To illustrate this, a detailed profile result is given in Appendix D. End-to-end Acceleration Finally, we test the practical speedups of training GPT-2 models. Results in Table 11 show that our training method conducts up to 1.2x faster than the dense training on a single RTX3090. 7. Conclusions In this study, we are the first to propose accelerating the pre-training of transformers by 2:4 sparsity. We analyze the limitations of previous 2:4 training methods, including the impropriety in choosing positions and determining values of the masked decay factor, speed bottleneck incurred by computing transposable masks and gated activation func- tions. We propose a series of techniques to tackle them. Our training method is validated on DeiT, BERT, Transformer- base and GPT-2 models. In particular, we have attained 1.2x end-to-end training acceleration for the GPT-2 774M model without losing its accuracy. 9Accelerating Transformer Pre-training with 2:4 Sparsity Acknowledgements We would like to thank Ziteng Wang, Bingrui Li and Haocheng Xi for valuable discussions and help on the training large transformers. This work was supported by the National Key Research and Development Pro- gram of China (No. 2021ZD0110502), NSFC Projects (Nos. 62376131, 62061136001, 62106123, 62076147, U19A2081, 61972224), Tsinghua Institute for Guo Qiang, and the High Performance Computing Center, Tsinghua University. J.Z is also supported by the XPlorer Prize. Impact Statement Our proposed efficient algorithm can be used to accelerate pre-training large-scale transformers like GLM (Du et al., 2022), LLaMA (Touvron et al., 2023), etc. Recently, large transformers have exhibited remarkable efficacy in various fields such as natural language processing, computer vision, and speech recognition. However, the pre-training stage of large transformers is computationally intensive and time- consuming. For instance, pre-training a GPT-4 can span several months, even using a supercomputer equipped with thousands of GPUs. Thus, acceleration approaches are nec- essary. Our fully sparse training approach of transformers can potentially accelerate the FFN layers of a model by the- oretical 2x faster, without loss of accuracy. Thus, it can be potentially used to save energy and reduce carbon footprint. But this work can also be used to accelerate baleful software, like software that generates malicious contents, which may have a negative impact on human society. References Anthony, L. F. W., Kanding, B., and Selvan, R. Carbon- tracker: Tracking and predicting the carbon footprint of training deep learning models, 2020. Bengio, Y ., LÂ´eonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013. Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint- Amand, H., Soricut, R., Specia, L., and Tamchyna, A. Findings of the 2014 workshop on statistical machine translation. In WMT@ACL, 2014. URL https://api. semanticscholar.org/CorpusID:15535376. BUSATO, F. and POOL, J. Exploiting nvidia ampere struc- tured sparsity with cusparselt [online]. 2020 [visited on 2021-10-10]. Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y ., Wang, Z., and Carbin, M. The lottery ticket hypothesis for pre- trained bert networks, 2020. Chen, X., Cheng, Y ., Wang, S., Gan, Z., Wang, Z., and Liu, J. Earlybert: Efficient bert training via early-bird lottery tickets, 2021. Chmiel, B., Hubara, I., Banner, R., and Soudry, D. Min- imum variance unbiased n:m sparsity for the neural gradients. In The Eleventh International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=vuD2xEtxZcj. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vi- sion and Pattern Recognition, pp. 248â€“255, 2009. doi: 10.1109/CVPR.2009.5206848. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding, 2019. Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J. Glm: General language model pretraining with autoregressive blank infilling, 2022. Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the lottery: Making all tickets winners, 2021. Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks, 2019. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Stabilizing the lottery ticket hypothesis, 2020. Geiping, J. and Goldstein, T. Cramming: Training a lan- guage model on a single gpu in one day, 2022. Gokaslan, A. and Cohen, V . Openwebtext cor- pus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. Han, S., Pool, J., Tran, J., and Dally, W. J. Learning both weights and connections for efficient neural networks, 2015. Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2016. Han, S., Pool, J., Narang, S., Mao, H., Gong, E., Tang, S., Elsen, E., Vajda, P., Paluri, M., Tran, J., Catanzaro, B., and Dally, W. J. Dsd: Dense-sparse-dense training for deep neural networks, 2017. Hu, Z., Lan, Y ., Wang, L., Xu, W., Lim, E.-P., Lee, R. K.-W., Bing, L., and Poria, S. Llm-adapters: An adapter fam- ily for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023. 10Accelerating Transformer Pre-training with 2:4 Sparsity Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, S., and Soudry, D. Accelerated sparse neural training: A provable and efficient method to find n:m transposable masks, 2021. Karpathy, A. nanogpt. https://github.com/ karpathy/nanoGPT/, 2023. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization, 2017. Lasby, M., Golubeva, A., Evci, U., Nica, M., and Ioannou, Y . Dynamic sparse training with structured sparsity, 2023. Lee, N., Ajanthan, T., and Torr, P. H. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018. Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., and Gonzalez, J. Train big, then compress: Rethinking model size for efficient training and inference of trans- formers. In International Conference on machine learn- ing, pp. 5958â€“5968. PMLR, 2020. Loshchilov, I. and Hutter, F. Decoupled weight decay regu- larization, 2019. Lu, Y ., Agrawal, S., Subramanian, S., Rybakov, O., Sa, C. D., and Yazdanbakhsh, A. Step: Learning n:m struc- tured sparsity masks from scratch with precondition, 2023. McDanel, B., Dinh, H., and Magallanes, J. Accelerating dnn training with structured data gradient pruning, 2022. Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G., Yu, C., and Micikevicius, P. Accelerating sparse deep neural networks, 2021. Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL- HLT 2019: Demonstrations, 2019. Papineni, K., Roukos, S., Ward, T., and Zhu, W. J. Bleu: a method for automatic evaluation of machine translation. 10 2002. doi: 10.3115/1073083.1073135. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsu- pervised multitask learners. 2019. URL https: //api.semanticscholar.org/CorpusID: 160025533. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. Shazeer, N. Glu variants improve transformer, 2020. Thakkar, V ., Ramani, P., Cecka, C., Shivam, A., Lu, H., Yan, E., Kosaian, J., Hoemmen, M., Wu, H., Kerr, A., Nicely, M., Merrill, D., Blasig, D., Qiao, F., Majcher, P., Springer, P., Hohnerbach, M., Wang, J., and Gupta, M. CUTLASS, January 2023. URL https://github. com/NVIDIA/cutlass. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image trans- formers & amp; distillation through attention. In Interna- tional Conference on Machine Learning, volume 139, pp. 10347â€“10357, July 2021a. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J Â´egou, H. Training data-efficient image trans- formers & distillation through attention, 2021b. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and anal- ysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018. URL https://api. semanticscholar.org/CorpusID:5034059. Xu, W., He, X., Cheng, K., Wang, P., and Cheng, J. Towards fully sparse training: Information restoration with spatial similarity. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 2929â€“2937, 2022. You, H., Li, C., Xu, P., Fu, Y ., Wang, Y ., Chen, X., Baraniuk, R. G., Wang, Z., and Lin, Y . Drawing early-bird tickets: Towards more efficient training of deep networks, 2022. Zhang, Y ., Luo, Y ., Lin, M., Zhong, Y ., Xie, J., Chao, F., and Ji, R. Bi-directional masks for efficient n:m sparse training, 2023. Zhou, A., Ma, Y ., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H. Learning n:m fine-grained structured sparse neural networks from scratch, 2021. Zhou, D., Ye, M., Chen, C., Meng, T., Tan, M., Song, X., Le, Q., Liu, Q., and Schuurmans, D. Go wide, then narrow: Efficient training of deep thin networks. In In- ternational Conference on Machine Learning, pp. 11546â€“ 11555. PMLR, 2020. 11Accelerating Transformer Pre-training with 2:4 Sparsity A. 2:4-spMM A.1. 2:4 Sparsity Examples of row-wise, column-wise and transposable 2:4 sparse matrix are shown in Figure 8. Note that transposable 2:4 sparsity aligns with both row-wise and column-wise 2:4 sparsity. Figure 8.Row-wise 2:4, column-wise and transposable 2:4 sparse matrix. A.2. Array Layout The array layout of different types of matrix multiplications are listed in Table 12, which explains why output activations and activation gradients are column-major matrices in FST. Table 12.Array layout of MN. Here S denotes that the matrix is in row-wise 2:4 sparsity, R denotes row-major dense matrix, and C denotes column-major dense matrix. M N S S âŠ¤ R C S % % R R SâŠ¤ % % % % R % C R R C % C R R B. Workflow The main workflow of a single linear layer in FST process is depicted in Figure 9. Figure 9.2:4 sparse training iteration for a layer on a single batch. 12Accelerating Transformer Pre-training with 2:4 Sparsity C. Training Loss Curve For BERT-base and GPT-2, we depict training loss curve in Figure 10. Figure 10.Left: train loss of GPT-2; right: train loss of BERT. D. Profiling result To explain how we reach 1.3x block speedup, we profile our code and break down the time costs as shown in the table below; see Table 13. Table 13.Time costs of each part of our network and the dense model in one iteration per layer. m denotes the accumulation steps over micro batches. Our method is evaluated on GPT-2, with batch size 16, sequence length 1024, embedding dimension 1024 and heads number 16. DENSE (MS/EXEC ) SPARSE (MS/EXEC ) ACCELERATION RATIO S FREQUENCY (EXEC /ITER ) FFN LINEAR FWD GEMM 12173.8 7305.78 1.666324472 - BWD GEMM 23295 14080.82 1.654378083 - MVUE+ PRUNE 0 171.4 - - TOTAL 23295 14252.22 1.634482207 - TOTAL 35468.8 21558 1.645273216 - OTHERS 4 FWD 167 118.17 - - BWD 65.5 20.03 - - TOTAL 232.5 138.2 - - TOTAL FWD 12340.8 7423.95 1.662295678 - BWD 23360.5 14272.25 1.636777663 - TOTAL 35701.3 21696.2 1.645509352 - OTHERS FWD 6874.3 7090.55 - - BWD 13920.7 14117.45 - - TOTAL 20795 21208 - - TOTAL FWD 19215.1 14514.5 1.323855455 - BWD 37281.2 28389.7 1.313194574 - TOTAL 56496.3 42904.2 1.316801152 - MASKED DECAY 0 45.2 - 1 m PRUNE WEIGHTS 0 320.3 - 1 m TRANSPOSABLE MASK SEARCH 0 634.8 - 1 40m 4All functions in FFN except linear layers, i.e., activation function and dropout. 13",
      "meta_data": {
        "arxiv_id": "2404.01847v3",
        "authors": [
          "Yuezhou Hu",
          "Kang Zhao",
          "Weiyu Huang",
          "Jianfei Chen",
          "Jun Zhu"
        ],
        "published_date": "2024-04-02T11:12:42Z",
        "venue": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
        "pdf_url": "https://arxiv.org/pdf/2404.01847v3.pdf"
      }
    },
    {
      "title": "Transformer Quality in Linear Time",
      "abstract": "We revisit the design choices in Transformers, and propose methods to address\ntheir weaknesses in handling long sequences. First, we propose a simple layer\nnamed gated attention unit, which allows the use of a weaker single-head\nattention with minimal quality loss. We then propose a linear approximation\nmethod complementary to this new layer, which is accelerator-friendly and\nhighly competitive in quality. The resulting model, named FLASH, matches the\nperplexity of improved Transformers over both short (512) and long (8K) context\nlengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and\n12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on\nC4 for masked language modeling.",
      "full_text": "Transformer Quality in Linear Time Weizhe Hua* 1 2 Zihang Dai * 2 Hanxiao Liu * 2 Quoc V . Le2 Abstract We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head atten- tion with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH3, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9Ã—on Wiki-40B and 12.1Ã— on PG-19 for auto-regressive language modeling, and 4.8Ã—on C4 for masked language modeling. 1. Introduction Transformers (Vaswani et al., 2017) have become the new engine of state-of-the-art deep learning systems, leading to many recent breakthroughs in language (Devlin et al., 2018; Brown et al., 2020) and vision (Dosovitskiy et al., 2020). Although they have been growing in model size, most Trans- formers are still limited to short context size due to their quadratic complexity over the input length. This limitation prevents Transformer models from processing long-term information, a critical property for many applications. Many techniques have been proposed to speedup Transform- ers over extended context via more efï¬cient attention mech- anisms (Child et al., 2019; Dai et al., 2019; Rae et al., 2019; Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Roy et al., 2021; Jaegle et al., 2021). Despite the linear theoretical complexity for some of those methods, vanilla Transformers still remain as the dominant choice in *Equal contribution 1Cornell University 2Google Re- search, Brain Team. Correspondence to: Weizhe Hua <wh399@cornell.edu>, Zihang Dai <zihangd@google.com>, Hanxiao Liu <hanxiaol@google.com>. Proceedings of the39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 0 5 10 15 20 25 30 TPU-core-days -2.8 -3.0 -3.2 Neg. log pplx FLASH TFM+ + TFM 4.9x 25.6x Speedup Length over TFM over TFM++ 512 1.8 Ã— 1.2Ã— 1024 9.0 Ã— 1.3Ã— 2048 8.9 Ã— 1.6Ã— 4096 13.1 Ã— 2.7Ã— 8192 25.6 Ã— 4.9Ã— Figure 1: TPU-v4 training speedup of FLASH relative to the vanilla Transformer (TFM) and an augmented Transformer (TFM++) for auto-regressive language modeling on Wiki- 40B â€” All models are comparable in size at around 110M and trained for 125K steps with 218 tokens per batch. state-of-the-art systems. Here we examine this issue from a practical perspective, and ï¬nd existing efï¬cient attention methods suffer from at least one of the following drawbacks: â€¢ Inferior Quality. Our studies reveal that vanilla Trans- formers, when augmented with several simple tweaks, can be much stronger than the common baselines used in the literature (see Transformer vs. Transformer++ in Figure 1). Existing efï¬cient attention methods often incur signiï¬cant quality drop compared to augmented Trans- formers, and this drop outweighs their efï¬ciency beneï¬ts. â€¢ Overhead in Practice . As efï¬cient attention methods often complicate Transformer layers and require extensive memory re-formatting operations, there can be a nontrivial gap between their theoretical complexity and empirical speed on accelerators such as GPUs or TPUs. â€¢ Inefï¬cient Auto-regressive Training. Most attention lin- earization techniques enjoy fast decoding during infer- ence, but can be extremely slow to train on auto-regressive tasks such as language modeling. This is primarily due to their RNN-style sequential state updates over a large number of steps, making it infeasible to fully leverage the strength of modern accelerators during training. 3FLASH = Fast Linear Attention with a Single Head arXiv:2202.10447v2  [cs.LG]  27 Jun 2022Input Concat V softmax(QK+B) Gated Linear Unit  + Multi-Head Self-Attention Gated Attention Unit (Ours) Input Dense Dense Dense Dense Q K V V relu2(QK+B) Input Dense Dense Dense V Q & K def scale_offset(x): gamma = var(x.shape[=1:]) beta = var(x.shape[=1:]) return x âˆ— gamma + beta def attn(x, v, s=128): z = dense(x, s) q, k = scale_offset(z), scale_offset(z) qk = tf.einsum(/quotesingle.ts1bns,bmsâ†’bnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) âˆ—âˆ— 2 return tf.einsum(/quotesingle.ts1bnm,bmeâ†’bne/quotesingle.ts1, a, v) def gated_attn_unit(x, d=768, e=1536): shortcut, x = x, norm(x) u, v = dense(x, e), dense(x, e) x = u âˆ— attn(x, v) return dense(x, d) + shortcut Figure 2: (a) An augmented Transformer layer which consists of two blocks: Gated Linear Unit (GLU) and Multi-Head Self-Attention (MHSA), (b) Our proposed Gated Attention Unit (GAU), (c) Pseudocode for Gated Attention Unit. Skip connection and input normalization over the residual branch are omitted in (a), (b) for brevity. We address the above issues by developing a new model fam- ily that, for the ï¬rst time, not only achieves parity with fully augmented Transformers in quality, but also truly enjoys lin- ear scalability over the context size on modern accelerators. Unlike existing efï¬cient attention methods which directly aim to approximate the multi-head self-attention (MHSA) in Transformers, we start with a new layer design which nat- urally enables higher-quality approximation. Speciï¬cally, our model, named FLASH, is developed in two steps: First, we propose a new layer that is more desirable for effective approximation. We introduce a gating mechanism to alleviate the burden of self-attention, resulting in the Gated Attention Unit(GAU) in Figure 2. As compared to Transformer layers, each GAU layer is cheaper, and more importantly, its quality relies less on the precision of atten- tion. In fact, GAU with a small single-head, softmax-free attention is as performant as Transformers. While GAU still suffers from quadratic complexity over the context size, it weakens the role of attention hence allows us to carry out approximation later with minimal quality loss. We then propose an efï¬cient method to approximate the quadratic attention in GAU, leading to a layer variant with linear complexity over the context size. The key idea is to ï¬rst group tokens into chunks, then using precise quadratic attention within a chunk and fast linear attention across chunks, as illustrated in Figure 4. We further describe how an accelerator-efï¬cient implementation can be naturally de- rived from this formulation, achieving linear scalability in practice with only a few lines of code change. We conduct extensive experiments to demonstrate the efï¬- cacy of FLASH over a variety of tasks (masked and auto- regressive language modeling), datasets (C4, Wiki-40B, PG- 19) and model scales (110M to 500M). Remarkably, FLASH is competitive with fully-augmented Transformers (Trans- former++) in quality across a wide range of context sizes of practical interest (512â€“8K), while achieving linear scala- bility on modern hardware accelerators. For example, with comparable quality, FLASH achieves a speedup of 1.2Ã—â€“ 4.9Ã—for language modeling on Wiki-40B and a speedup of 1.0Ã—â€“4.8Ã—for masked language modeling on C4 over Transformer++. As we further scale up to PG-19 (Rae et al., 2019), FLASH reduces the training cost of Transformer++ by up to 12.1Ã—and achieves signiï¬cant gain in quality. 2. Gated Attention Unit Here we present Gated Attention Unit (GAU), a simpler yet more performant layer than Transformers. While GAU still has quadratic complexity over the context length, it is more desirable for the approximation method to be presented in Section 3. We start with introducing related layers: Vanilla MLP. Let X âˆˆRTÃ—d be the representations over T tokens. The output for Transformerâ€™s MLP can be formu- lated as O= Ï†(XWu)Wo where Wu âˆˆRdÃ—e, Wo âˆˆReÃ—d. Here ddenotes the model size, edenotes the expanded inter- mediate size, and Ï†is an element-wise activation function. Gated Linear Unit (GLU). This is an improved MLP augmented with gating (Dauphin et al., 2017). GLU has been proven effective in many cases (Shazeer, 2020; Narang et al., 2021) and is used in state-of-the-art Transformer language models (Du et al., 2021; Thoppilan et al., 2022). U = Ï†u(XWu), V = Ï†v(XWv) âˆˆRTÃ—e (1) O= (U âŠ™V)Wo âˆˆRTÃ—d (2) where âŠ™stands for element-wise multiplication. In GLU, each representation ui is gated by another representation vi associated with the same token.0.0 0.2 0.4 Step/sec/TPU-core 3.1 3.0 2.9 2.8 2.7 Neg. log pplx 42M 110M 335M 41M 105M 316M LM (Wiki-40B) MHSA+MLP MHSA+GLU GAU 0.0 0.5 1.0 Steps/sec/TPU-core 1.7 1.6 1.5 1.4 1.3 1.2 Neg. log pplx 42M 110M 335M 41M 105M 316M Masked LM (C4) MHSA+MLP MHSA+GLU GAU Layer Type # of Layers d MHSA+MLP 8+8 512 12+12 768 24+24 1024 MHSA+GLU 8+8 512 12+12 768 24+24 1024 GAU 15 512 22 768 46 1024 Figure 3: GAU vs. Transformers for auto-regressive and masked language modeling on short context length (512). Gated Attention Unit (GAU). The key idea is to formu- late attention and GLU as a uniï¬ed layer and to share their computation as much as possible (Figure 2). This not only results in higher param/compute efï¬ciency, but also natu- rally enables a powerful attentive gating mechanism. Specif- ically, GAU generalizes Eq. (2) in GLU as follows: O= (U âŠ™Ë†V)Wo where Ë†V = AV (3) where AâˆˆRTÃ—T contains token-token attention weights. Unlike GLU which always uses vi to gate ui (both asso- ciated with the same token), our GAU replaces vi with a potentially more relevant representation Ë†vi = âˆ‘ j aijvj â€œre- trievedâ€ from all available tokens using attention. The above will reduce to GLU when Ais an identity matrix. Consistent with the ï¬ndings in Liu et al. (2021), the presence of gating allows the use of a much simpler/weaker attention mechanism than MHSA without quality loss: Z = Ï†z(XWz) âˆˆRTÃ—s (4) A= relu2 ( Q(Z)K(Z)âŠ¤+ b ) âˆˆRTÃ—T (5) Modiï¬cations PPLX (LM/MLM) Params (M) original GAU 16.78 / 4.23 105 relu2 âˆ’ â†’softmax 17.04 / 4.31 105 single-head âˆ’ â†’multi-head 17.76 / 4.48 105 no gating 17.45 / 4.58 131 Table 1: Impact of various modiï¬cations on GAU. where Z is a shared representation ( s â‰ªd)4, Qand K are two cheap transformations that apply per-dim scalars and offsets to Z (similar to the learnable variables in Lay- erNorms), and bis the relative position bias. We also ï¬nd the softmax in MHSA can be simpliï¬ed as a regular activa- tion function in the case of GAU5. The GAU layer and its 4Unless otherwise speciï¬ed, we set s=128 in this work. 5We use squared ReLU (So et al., 2021) throughout this paper, which empirically works well on language tasks. Modiï¬cations PPLX (LM/MLM) Params (M) original MHSA 16.87 / 4.35 110 softmax âˆ’ â†’relu2 17.15 / 4.77 110 multi-head âˆ’ â†’single-head 17.89 / 4.73 110 add gating 17.25 / 4.43 106 Table 2: Impact of various modiï¬cations on MHSA. pseudocode are illustrated in Figure 2. Unlike Transformerâ€™s MHSA which comes with4d2 param- eters, GAUâ€™s attention introduces only a single small dense matrix Wz with dsparameters on top of GLU (scalars and offsets in Qand Kare negligible). By setting e = 2dfor GAU, this compact design allows us to replace each Trans- former block (MLP/GLU + MHSA) with two GAUs while retaining similar model size and training speed. GAU vs. Transformers. Figure 3 shows that GAUs are competitive with Transformers (MSHA + MLP/GLU) on TPUs across different models sizes. Note these experiments are conducted over a relatively short context size (512). We will see later in Section 4 that GAUs are in fact even more performant when the context length is longer, thanks to their reduced capacity in attention. Layer Ablations. In Table 1 & 2 we show that both GAUs and Transformers are locally optimal on their own. 3. Fast Linear Attention with GAU There are two observations from Section 2 that motivate us to extend GAU to modeling long sequences: â€¢ First, the gating mechanism in GAU allows the use of a weaker (single-headed, softmax-free) attention with- out quality loss. If we further adapt this intuition into modeling long sequences with attention, GAU could also boost the effectiveness of approximate (weak) attention mechanisms such as local, sparse and linearized attention.â€¢ In addition, the number of attention modules is naturally doubled with GAU â€” recall MLP+MHSA â‰ˆ2Ã—GAU in terms of cost (Section 2). Since approximate attention usu- ally requires more layers to capture full dependency (Dai et al., 2019; Child et al., 2019), this property also makes GAU more appealing in handling long sequences. With this intuition in mind, we start by reviewing some related work on modeling long sequences with attention, and then show how we enable GAU to achieve Transformer- level quality in linear time on long sequences. 3.1. Existing Linear-Complexity Variants Partial Attention. A popular class of methods tries to approximate the full attention matrix with different partial/s- parse patterns, including local window (Dai et al., 2019; Rae et al., 2019), local+sparse (Child et al., 2019; Li et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), axial (Ho et al., 2019; Huang et al., 2019), learnable patterns through hashing (Kitaev et al., 2020) or clustering (Roy et al., 2021). Though not as effective as full attention, these variants are usually able to enjoy quality gains from scaling to longer sequences. However, the key problem with this class of methods is that they involve extensive irregular or regular memory re-formatting operations such as gather, scatter, slice and concatenation, which are not friendly to modern accelerators of massive parallelism, particularly specialized ASICs like TPU. As a result, their practical beneï¬ts (speed and RAM efï¬ciency), if any, largely depend on the choice of accelerator and usually fall behind the theoretical analysis. Hence, in this work, we deliberately minimize the number of memory re-formatting operations in our model. Linear Attention. Alternatively, another popular line of research linearizes the attention computation by decompos- ing the attention matrix and then re-arranging the order of matrix multiplications (Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021). Schematically, the linear attention can be expressed as Ë†Vlin = Q ( KâŠ¤V ) î´™ î´˜î´— î´š RdÃ—d approx âˆ’âˆ’âˆ’â†’Ë†Vquad = Softmax ( QKâŠ¤) î´™ î´˜î´— î´š RTÃ—T V where Q,K,V âˆˆRTÃ—d are the query, key and value rep- resentations, respectively. Re-arranging the computation reduces the complexity w.r.tT from quadratic to linear. Another desirable property of linear attention is itsconstant6 computation and memory for each auto-regressive decoding step at inference time. To see that, deï¬ne Mt = KâŠ¤ :t V:t and notice that the computation of Mt can be fully incremental: Mt = Mtâˆ’1 + KtVâŠ¤ t (6) 6Constant is with respective to the sequence length T. cumsum cumsum Figure 4: (top) Quadratic attention, (mid) Linear attention, (bottom) Proposed mixed chunk attention with a chunk size (C) of 2 (Cis always greater than or equal to 128 in our ex- periments). Our method signiï¬cantly reduces the compute in quadratic attention (red links), while requiring substan- tially less RNN-style steps (green squares) in conventional linear attention. This means we only need to maintain a cache with constant O(d2) memory and whenever a new input arrives at time stamp t, only constant O(d2) computation is required to accumulate KtVâŠ¤ t into Mtâˆ’1 and get Mt. On the contrary, full quadratic attention requires linear O(Td) computation and memory for each decoding step, as each new input has to attend to all the previous steps. However, on the other hand, re-arranging the computation in linear attention leads to a severe inefï¬ciency during auto- regressive training. As shown in Fig. 4 (mid), due to the causal constraint for auto-regressive training, the query vec- tor at each time step Qt corresponds to a different cache value Mt = KâŠ¤ :t V:t. This requires the model to compute and cache T different values {Mt}T t=1 instead of only one value KâŠ¤V in the non-autoregressive case. In theory, the sequence {Mt}T t=1 can be obtained in O(Td2) by ï¬rst com- puting {KtVâŠ¤ t }T t=1 and then performing a large cumulative sum (cumsum) over T tokens. But in practice, the cumsum introduces an RNN-style sequential dependencyof T steps, where an O(d2) state needs to be processed each step. The sequential dependency not only limits the degree of paral- lelism, but more importantly requires T memory accessin the loop, which usually costs much more time than comput- ing the element-wise addition on modern accelerators. As a result, there exists a considerable gap between the theoreti- cal complexity and actual running time. In practice, we ï¬nd that directly computing the full quadratic attention matrix iseven faster than the re-arranged (linearized) version on both TPUs (Figure 6(a)) and GPUs (Appendix C.1). 3.2. Our Method: Mixed Chunk Attention Based on the strengths and weaknesses of existing linear- complexity attentions, we propose mixed chunk attention, which merges the beneï¬ts from both partial attention and linear attention. The high-level idea is illustrated in Figure 4. Below we reformulate GAU to incorporate this idea. Preparation. The input sequence is ï¬rst chunked into G non-overlapping chunks of size C, i.e. [T] â†’[T/C Ã— C]. Then, Ug âˆˆRCÃ—e, Vg âˆˆRCÃ—e and Zg âˆˆRCÃ—s are produced for each chunk gfollowing the GAU formulation in Eq. (1) and Eq. (4). Next, four types of attention heads Qquad g , Kquad g , Qlin g , Klin g are produced from Zg by applying per-dim scaling and offset (this is very cheap). We will describe how GAUâ€™s attention can be efï¬ciently approximated using a local attention plus a global attention. Note all the major tensors Ug, Vg and Zg are shared be- tween the two components. The only additional parameters introduced over the original GAU are the per-dim scalars and offsets for generating Qlin g and Klin g (4Ã—sparameters). Local Attention per Chunk. First, a local quadratic at- tention is independently applied to each chunk of length C to produce part of the pre-gating state: Ë†Vquad g = relu2 ( Qquad g Kquad g âŠ¤ + b ) Vg. The complexity of this part is O(GÃ—C2 Ã—d) =O(TCd), which is linear in T given that Cremains constant. Global Attention across Chunks. In addition, a global linear attention mechanism is employed to capture long- range interaction across chunks Non-Causal: Ë†Vlin g = Qlin g ( Gâˆ‘ h=1 Klin h âŠ¤ Vh ) , (7) Causal: Ë†Vlin g = Qlin g (gâˆ’1âˆ‘ h=1 Klin h âŠ¤ Vh ) . (8) Note the summations in Eq. (7) and Eq. (8) are performed at the chunk level. For the causal (auto-regressive) case, this reduces the number of elements in the cumsum in token- level linear attention by a factor of C(a typical Cis 256 in our experiments), leading to a signiï¬cant training speedup. Finally, Ë†Vquad g and Ë†Vlin g are added together, followed by gat- ing and a post-attention projection analogous to Eq. (3): Og = [ Ug âŠ™ ( Ë†Vquad g + Ë†Vlin g )] Wo. def _global_linear_attn(q, k, v, causal): if causal: kv = tf.einsum(/quotesingle.ts1bgcs,bgceâ†’bgse/quotesingle.ts1, k, v) kv = tf.cumsum(kv, axis=1, exclusive=True) return tf.einsum(/quotesingle.ts1bgcs,bgseâ†’bgce/quotesingle.ts1, q, kv) else: kv = tf.einsum(/quotesingle.ts1bgcs,bgceâ†’bse/quotesingle.ts1, k, v) return tf.einsum(/quotesingle.ts1bgcs,bseâ†’bgce/quotesingle.ts1, q, kv) def _local_quadratic_attn(q, k, v, causal): qk = tf.einsum(/quotesingle.ts1bgns,bgmsâ†’bgnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) âˆ—âˆ— 2 a = causal_mask(a) if causal else a return tf.einsum(/quotesingle.ts1bgnm,bgmeâ†’bgne/quotesingle.ts1, a, v) def attn(x, v, causal, s=128): # x: [B x G x C x D]; v: [B x G x C x E] z = dense(x, s) v_quad = _local_quadratic_attn( scale_offset(z), scale_offset(z), v, causal) v_lin = _global_linear_attn( scale_offset(z), scale_offset(z), v, causal) return v_quad + v_lin Code 1: Pseudocode for mixed chunk attention. The mixed chunk attention is simple to implement and the corresponding pseudocode is given in Code 1. 3.2.1. D ISCUSSIONS Fast Auto-regressive Training. Importantly, as depicted in Fig. 4 (bottom), thanks to chunking, the sequential de- pendency in the auto-regressive case reduces from T steps in the standard linear attention to G = T/C steps in the chunked version in Eq. (8). Therefore, we observe the auto- regressive training becomes dramatically faster with the chunk size is in {128,256,512}. With the inefï¬ciency of auto-regressive training eliminated, the proposed model still enjoys the constant per-step decoding memory and compu- tation of O(Cd2), where the additional constant C comes from the local quadratic attention. On Non-overlapping Local Attention. Chunks in our method does not overlap with each other. In theory, in- stead of using the non-overlapping local attention, any par- tial attention variant could be used as a substitute while keeping the chunked linear attention ï¬xed. As a concrete example, we explored allowing each chunk to additionally attends to its nearby chunks, which essentially makes the local attention overlapping, similar to Longformer (Belt- agy et al., 2020) and BigBird (Zaheer et al., 2020). While overlapping local attention consistently improves quality, it also introduces many memory re-formatting operations that clearly harm the actual running speed. In our preliminary experiments with language modeling on TPU, we found the cost-beneï¬t trade-off of using overlapping local attention may not be as good as adding more layers in terms of both memory and speed. In general, we believe the optimal par- tial attention variant is task-speciï¬c, while non-overlapping local attention is always a strong candidate when combined with the choice of chunked linear attention.0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 26 27 28 Latency per step (ms) 29 210 211 212 213 Context length (a) Per-step training latency 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (b) Context length = 512 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (c) Context length = 1024 0 5 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (d) Context length = 2048 0 5 10 15 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 -1.5 -2.5 -3.5 -5.5 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 5: Masked language modeling validation-set results on the C4 dataset â€” All models are comparable in size at around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. Connections to Combiner. Similar to our method, Com- biner (Ren et al., 2021) also splits the sequence into non- overlapping chunks and utilizes quadratic local attention within each chunk. The key difference lies in how the long- range information is summarized and combined with the local information (e.g., our mixed chunk attention allows larger effective memory per chunk hence leads to better quality). See Appendix A for detailed discussions. 4. Experiments We focus on two of our models that have different com- plexities with respect to the context length. The quadratic- complexity model FLASH-Quad refers to a stack of GAUs whereas the linear-complexity model named FLASH con- sists of both GAUs and the proposed mixed chunk attention. To demonstrate their efï¬cacy and general applicability, we evaluate them on both bidirectional and auto-regressive se- quence modeling tasks over multiple large-scale datasets. Baselines. First of all, the vanilla Transformer (Vaswani et al., 2017) with GELU activation function (Hendrycks & Gimpel, 2016) is included as a standard baseline for calibra- tion. Despite of being a popular baseline in the literature, we ï¬nd that RoPE (Su et al., 2021) and GLU (Shazeer, 2020) can lead to signiï¬cant performance boosts. We there- fore also include Transformer + RoPE (Transformer+) and Transformer + RoPE + GLU (Transformer++) as two much stronger baselines with quadratic complexity. To demonstrate the advantages of our models on long se- quences, we further compare our models with two notable linear-complexity Transformer variantsâ€”Performer (Choro- manski et al., 2020) and Combiner (Ren et al., 2021), where Performer is a representative linear attention method and Combiner (using a chunked attention design similar to ours) has shown superior cost-beneï¬t trade-off over many other approaches (Ren et al., 2021). To get the best performance, we use the rowmajor-axial variant of Combiner (Combiner- Axial) and the ReLU-kernel variant of Performer. Both models are also augmented with RoPE. For fair comparison, all models are implemented in the same codebase to ensure identical tokenizer and hyper-parameters for training and evaluation. The per-step training latencies of all models are measured using TensorFlow Proï¬ler. See Appendix B for detailed settings and model speciï¬cations. 4.1. Bidirectional Language Modeling In BERT (Devlin et al., 2018), masked language modeling (MLM) reconstructs randomly masked out tokens in the input sequence. We pretrain and evaluate all models on0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 29 210 211 212 213 26 28 210 Context length Latency per step (ms) (a) Per-step training latency 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (b) Context length = 512 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (c) Context length = 1024 0 4 8 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (d) Context length = 2048 0 10 20 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 30 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 6: Auto-regressive language modeling validation-set results on the Wiki-40B dataset â€” All models are sized around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. the C4 dataset (Raffel et al., 2020). We consistently train each model with 218 tokens per batch for 125K steps, while varying the context length on a wide range including 512, 1024, 2048, 4096, and 8192. The quality of each model is reported in perplexity as a proxy metric for the performance on downstream tasks. The training speed of each model (i.e., training latency per step) is measured with 64 TPU-v4 cores, and the total training cost is reported in TPU-v4-core-days. Figure 5(a) shows the latency of each training step for all models at different context lengths. Results for Trans- former+ are omitted for brevity as it lies in between Trans- former and Transformer++. Across all the six models, laten- cies for Combiner, Performer, and FLASH remain roughly constant as the context length increases, demonstrating lin- ear complexity with respect to context length. FLASH-Quad is consistently faster than Transformer and Transformer++ for all context lengths. In particular, FLASH-Quad is 2 Ã— as fast as Transformer++ when the context length increases to 8192. More importantly, as shown in Figures 5(b)-5(f), for all sequence lengths ranging from 512 to 8192, our mod- els always achieve the best quality (i.e., lowest perplexity) under the same computational resource. In particular, if the goal is to match Transformer++â€™s ï¬nal perplexity at step 125K, FLASH-Quad and FLASH can reduce the train- ing cost by 1.1Ã—â€“2.5Ã—and 1.0Ã—â€“4.8Ã—, respectively. It is worth noting that, to the best of our knowledge, FLASH is the only linear-complexity model that achieves perplexity competitive with the fully-augmented Transformers and its quadratic-complexity counterpart. See Appendix C.2 for a detailed quality and speed comparison of all models. 4.2. Auto-regressive Language Modeling For auto-regressive language modeling, we focus on the Wiki-40B (Guo et al., 2020) and PG-19 (Rae et al., 2019) datasets, which consist of clean English Wikipedia pages and books extracted from Project Gutenberg, respectively. It is worth noting that the average document length in PG-19 is 69K words, making it ideal for evaluating model perfor- mance over long context lengths. We train and evaluate all models with 218 tokens per batch for 125K steps, with context lengths ranging from 512 to 8K for Wiki-40B and 1K to 8K for PG-19. We report token-level perplexity for Wiki-40B and word-level perplexity for PG-19. Figure 6(a) shows that FLASH-Quad and FLASH achieve the lowest latency among quadratic and linear complexity models, respectively. We compare the quality and training cost trade-offs of all models on Wiki40-B over increasingTable 3: Auto-regressive language models on the PG-19 dataset â€” Latency (Lat.) is measured with 64 TPU-v4 cores. Model Context Length 1024 2048 4096 8192 PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* Transformer+ 44.45 282 1.00 Ã— 43.14 433 1.00 Ã— 42.80 698 1.00 Ã— 43.27 1292 1.00 Ã— Transformer++ 44.47 292 â€“ 43.18 441 â€“ 43.13 712 â€“ 43.26 1272 1.21 Ã— Combiner 46.04 386 â€“ 44.68 376 â€“ 43.99 374 â€“ 44.12 407 â€“ FLASH-Quad 43.40 231 2.18 Ã— 42.01 273 3.29Ã— 41.46 371 3.59Ã— 41.68 560 5.23Ã— FLASH 44.06 234 1.66Ã— 42.17 237 3.85 Ã— 40.72 234 6.75 Ã— 41.07 250 12.12 Ã— * Measured based on time taken to match Transformer+â€™s ï¬nal quality (at step 125K) on TPU. â€“ Indicates that the speciï¬c model fails to achieve the same perplexity as Transformer+. context lengths in Figures 6(b)-6(f). Similar to the ï¬ndings on MLM tasks, our models dominate all other models in terms of quality-training speed for all sequence lengths. Speciï¬cally, FLASH-Quad reduces the training time of Transformer++ by 1.2Ã—to 2.5Ã—and FLASH cuts the com- pute cost by 1.2Ã—to 4.9Ã—while reaching a similar perplex- ity as Transformer++. Between our own models, FLASH closely tracks the perplexity of FLASH-Quad and starts to achieve a better perplexity-cost trade-off when the con- text length goes beyond 2048. Detailed quality and speed comparisons for all models are included in Appendix C.2. For PG-19, following Rae et al., an increased model scale of roughly 500M parameters (see Table 10) is used for all mod- els in comparison. The results are summarized in Table 3. Compared to the numbers in Wiki-40B, FLASH achieves a more pronounced improvements in perplexity and train- ing time over the augmented Transformers on PG-19. For example, with a context length of 8K, FLASH-Quad and FLASH are able to reach the ï¬nal perplexity (at 125K-step) of Transformer+ in only 55K and 55K steps, yielding 5.23Ã— and 12.12Ã—of speedup, respectively. We hypothesize that the increased gains over Transformer+ arise from the long- range nature of PG-19 (which consists of books). Similar to our previous experiments, FLASH achieves a lower perplex- ity than all of the full-attention Transformer variants while being signiï¬cantly faster, demonstrating the effectiveness of our efï¬cient attention design. 4.3. Fine-tuning To demonstrate the effectiveness of FLASH over down- stream tasks, we ï¬ne-tune our pre-trained models on the TriviaQA dataset (Joshi et al., 2017). Passages in Trivi- aQA can span multiple documents, which challenges the capability of the models in handling long contexts. For a fair and meaningful comparison, we pretrain all models on English Wikipedia (same domain as TriviaQA) with a context length of 4096 and a batch size of 64 for 125k steps. For ï¬ne-tuning, we sweep over three different learn- ing rates, including 1eâˆ’4, 7eâˆ’5, and 5eâˆ’5, and report the best validation-set F1 score across these runs. Table 4: Results on TrivialQA with context length 4096 â€” â€œPTâ€œ stands for pre-training and â€œFTâ€œ stands for ï¬ne-tuning. All models are comparable in size at around 110M. sstands for the head size of the single-head attention. For FLASH, â€œï¬rst-to-allâ€ means that we also let the ï¬rst token in each chunk to attend to the entire sequence using a single-head softmax attention. Latency (Lat.) is measured with 32 TPU-v4 cores. Model PT FT PT / FT PPLX F1 Lat. reduction Transformer+ 3.48 74.2 1.00 Ã—/ 1.00Ã— Combiner 3.51 67.2 2.78Ã—/ 2.75Ã— FLASH-Quads=128 3.24 72.7 1.89 Ã—/ 1.79Ã— FLASH-Quads=512 3.12 74.8 1.76Ã—/ 1.67Ã— FLASHs=512 3.23 73.3 2.61 Ã—/ 2.60Ã— FLASHs=512 + ï¬rst-to-all 3.24 73.9 2.78Ã—/ 2.69Ã— We observe that the ï¬ne-tuning results of the FLASH fam- ily can beneï¬t from several minor changes in the model conï¬guration. As shown in Table 4, increasing the head size of FLASH-Quad from 128 to 512 leads to a signiï¬cant boost of 2.1 point in the F1 score with negligible impact on speed. We further identify several other tweaks that improve the linear FLASH variant speciï¬cally, including using a small chunk size (128), disabling gradient clipping during ï¬netuning, using softmax instead of squared ReLU for the [CLS] token, and (optionally) allowing the ï¬rst to- ken in each chunk to attend to the entire sequence using softmax. With those changes, FLASHs=512 achieves compa- rable quality to Transformer+ (0.3 difference in F1 is within the range of variance) while being 2.8Ã—and 2.7Ã—as fast as Transformer+ in pretraining and ï¬ne-tuning, respectively. 4.4. Ablation Studies Signiï¬cance of quadratic & linear components. To bet- ter understand the efï¬cacy of FLASH, we ï¬rst study how much the local quadratic attention and the global linear atten- tion contribute to the performance individually. To this end,-3.2 -3.0 -2.8 FLASH FLASH (LocalOnly) FLASH (GlobalOnly) MC-TFM++ 0 2 4 6 8 -4.6 -4.4 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 TPU-core-days Neg. log pplx (a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 7: Ablation study of the proposed FLASH architecture. we create FLASH (LocalOnly) and FLASH (GlobalOnly) by only keeping the local quadratic attention and the global linear attention in FLASH, respectively. In FLASH (Glob- alOnly), we reduce the chunk size from 256 to 64 to produce more local summaries for the global linear attention. In Fig- ure 7 we see a signiï¬cant gap between the full model and the two variants, suggesting that the linear and global attention are complementary to each other â€” both are critical to the quality of the proposed mixed chunk attention. Signiï¬cance of GAU. Here we study the importance of using GAU in FLASH. To achieve this,we apply the same idea of mixed chunk attention to Transformer++. We re- fer to this variant as MC-TFM++ (MC stands for mixed chunk) which uses quadratic MHSA within each chunk and multi-head linear attention across chunks. Effectively, MC- TFM++ has the same linear complexity as FLASH, but the core for MC-TFM++ is Transformer++ instead of GAU. Figure 7 shows that FLASH outperforms MC-TFM++ by a large margin (more than 2Ã—speedup when the sequence length is greater than 2048), conï¬rming the importance of GAU in our design. We further look into the perplexity in- crease due to our approximation method in Table 5, showing that the quality loss due to approximation is substantially smaller when going from FLASH-Quad to FLASH than go- ing from TFM++ to MC-TFM++. This indicates that mixed chunk attention is more compatible with GAU than MHSA, which matches our intuition that GAU is more beneï¬cial to weaker/approximate attention mechanisms. Impact of Chunk Size. The choice of chunk size can affect both the quality and the training cost of FLASH. We observe that, in general, larger chunk sizes perform better as the context length increases. For example, setting the chunk size to 512 is clearly preferable to the default chunk size (C=256) when the context length exceeds 1024. In practice, hyperparameter search over the chunk size can be performed to optimize the performance of FLASH further, although we did not explore such option in our experiments. More detailed analysis can be found in Appendix C.3. Table 5: Perplexity increases when mixed chunk attention is applied to GAU (â†’FLASH) or to TFM++ (â†’MC-TFM++) â€” Results are reported for MLM and LM with increasing context lengths from 512 to 8192. MLM on C4 512 1024 2048 4096 8192 FLASH-Quadâ†’FLASH 0.0 0.05 0.06 0.07 0.07 TFM++â†’MC-TFM++ 0.36 0.37 0.49 0.48 0.43 LM on Wiki-40B 512 1024 2048 4096 8192 FLASH-Quadâ†’FLASH -0.05 0.06 0.22 0.30 0.11 TFM++â†’MC-TFM++ 0.54 0.75 0.86 0.90 0.87 5. Conclusion We have presented FLASH, a practical solution to address the quality and empirical speed issues of existing efï¬cient Transformer variants. This is achieved by designing a per- formant layer (gated linear unit) and by combining it with an accelerator-efï¬cient approximation strategy (mixed chunk attention). Experiments on bidirectional and auto-regressive language modeling tasks show that FLASH is as good as fully-augmented Transformers in quality (perplexity), while being substantially faster to train than the state-of-the-art. A future work is to investigate the scaling laws of this new model family and the performance on downstream tasks. Acknowledgements The authors would like to thank Gabriel Bender, John Blitzer, Maarten Bosma, Andrew Brock, Ed Chi, Hanjun Dai, Yann N. Dauphin, Pieter-Jan Kindermans and David So for their useful feedback. Weizhe Hua was supported in part by the Facebook fellowship. References Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Beltagy, I., Peters, M. E., and Cohan, A. Long- former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a ï¬xed-length context. arXiv preprint arXiv:1901.02860, 2019. Dauphin, Y . N., Fan, A., Auli, M., and Grangier, D. Lan- guage modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICMLâ€™17, pp. 933â€“941. JMLR.org, 2017. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint arXiv:2010.11929, 2020. Du, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y ., Krikun, M., Zhou, Y ., Yu, A. W., Firat, O., et al. Glam: Efï¬cient scaling of language models with mixture- of-experts. arXiv preprint arXiv:2112.06905, 2021. Elfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3â€“11, 2018. Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y . N. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICMLâ€™17, pp. 1243â€“1252. JMLR.org, 2017. Guo, M., Dai, Z., Vrandecic, D., and Al-Rfou, R. Wiki-40b: Multilingual language model dataset. In LREC 2020, 2020. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y ., and Liu, W. Ccnet: Criss-cross attention for semantic segmen- tation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 603â€“612, 2019. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with it- erative attention. In International Conference on Machine Learning, pp. 4651â€“4664. PMLR, 2021. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi- aQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601â€“1611, Vancouver, Canada, July 2017. Association for Compu- tational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156â€“5165. PMLR, 2020. Kitaev, N., Kaiser, Å., and Levskaya, A. Reformer: The efï¬cient transformer. arXiv preprint arXiv:2001.04451, 2020. Li, S., Jin, X., Xuan, Y ., Zhou, X., Chen, W., Wang, Y .- X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecast- ing. Advances in Neural Information Processing Systems, 32:5243â€“5253, 2019. Liu, H., Dai, Z., So, D. R., and Le, Q. V . Pay attention to mlps. NeurIPS, 2021. Narang, S., Chung, H. W., Tay, Y ., Fedus, W., Fevry, T., Matena, M., Malkan, K., Fiedel, N., Shazeer, N., Lan, Z., et al. Do transformer modiï¬cations transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention. CoRR, abs/1910.05895, 2019. URL http://arxiv.org/ abs/1910.05895. Peng, H. et al. Random feature attention. In ICLR, 2021. Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Ex- ploring the limits of transfer learning with a uniï¬ed text-to-text transformer. Journal of Machine Learning Research, 21(140):1â€“67, 2020. URL http://jmlr. org/papers/v21/20-074.html. Ramachandran, P., Zoph, B., and Le, Q. V . Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuur- mans, D., and Dai, B. Combiner: Full attention trans- former with sparse computation cost. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=MQQeeDiO5vv. Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efï¬cient content-based sparse attention with routing transform- ers. Transactions of the Association for Computational Linguistics, 9:53â€“68, 2021. Shazeer, N. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/ abs/2002.05202. So, D. R., Ma Â´nke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V . Primer: Searching for efï¬cient transformers for language modeling. NeurIPS, 2021. Su, J., Lu, Y ., Pan, S., Wen, B., and Liu, Y . Roformer: Enhanced transformer with rotary position embedding, 2021. Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul- shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y ., et al. Lamda: Language models for dialog appli- cations. arXiv preprint arXiv:2201.08239, 2022. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998â€“6008, 2017. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al- berti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer se- quences. In NeurIPS, 2020.A. Connections to Combiner To capture long-term information, Combiner (Ren et al., 2021) additionally summarizes each chunk into summary key and value vectors Ksum,V sum âˆˆRT/C Ã—d and concatenate them into the local quadratic attention, i.e. Ë†Vg = Softmax ( Q[Kg; Ksum] ) [Vg; Vsum]. Effectively, Combiner compresses each chunk of C vectors into a single vector of O(d), whereas our chunked linear attention part compresses each chunk into a matrix Klin h âŠ¤ Vh of size O(sd) which is stimes larger. In other words, less compression is done in chunked linear attention, allowing increased memory hence a potential advantage over Combiners. Another difference lies in how the compressed long-term information from different chunks are combined, where Combiner reuses the quadratic attention whereas our chunked linear attention simply performs (cumulative) sum. However, it is straightforward to incorporate what Combiner does in our proposed method by constructing an extra[T/C Ã—T/C] attention matrix to combine the chunk summaries, e.g. Alin = relu2 ( QsumKsumâŠ¤+ bsum ) , Ë†Vlin g = Qlin g [T/Câˆ‘ h=1 alin gh ( Klin h âŠ¤ Vh )] . We indeed brieï¬‚y experimented with this variant and found it helpful. But it clearly complicates the overall model design, and more importantly requires the model to store and attend to all chunk summaries. As a result, the auto-regressive decoding complexity will increase to O((C+ T/C)d2) which is length-dependent and no longer constant. Hence, we do not include this feature in our default conï¬guration. B. Experimental Setup B.1. Hyperparameters Bidirectional Language Modeling. Hyperparameters for the MLM task on C4 are listed in Table 6. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison. Table 6: Hyperparameters for MLM pretraining on C4. MLM Results (Figure 5) Data C4 Sequence length 512 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam Ïµ 1e-6 Adam (Î²1,Î²2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Chunk size 256 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 * Applied to all models except the vanilla Transformer. Auto-regressive Language Modeling. Hyperparameters for the LM tasks on Wiki-40B and PG-19 are listed in Table 7. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison.Table 7: Hyperparameters for LM pretraining on Wiki-40B and PG-19. LM Results (Figure 6) LM Results (Table 3) Data Wiki-40B PG-19 Sequence length 512 - 8192 1024 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam Ïµ 1e-6 Adam (Î²1,Î²2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 Chunk size 256 512 * Applied to all models except the vanilla Transformer. B.2. Model Speciï¬cations Detailed speciï¬cations of all models used in our experiments are summarized in Tables 8, 9, and 10. In the experiments, SiLU/Swish (Elfwing et al., 2018; Hendrycks & Gimpel, 2016; Ramachandran et al., 2017) is used as the nonlinearity for FLASH-Quad and FLASH, as it slightly outperforms GELU (Hendrycks & Gimpel, 2016) in our models. It is also worth noting that we use ScaleNorm for some masked language models because ScaleNorm runs slightly faster than LayerNorm on TPU-v4 without compromising the quality of the model. Table 8: Model conï¬gurations for MLM experiments on the C4 dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type3 ScaleNorm ScaleNorm LayerNorm ScaleNorm ScaleNorm ScaleNorm ScaleNorm Absolute position emb. ScaledSin4 ScaledSin4 Learnable5 ScaledSin4 ScaledSin4 ScaledSin4 ScaledSin4 Relative position emb. RoPE RoPE â€“ RoPE RoPE RoPE RoPE # of layers 24 24 12+12 6 12+126 12+126 12+126 12+126 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size â€“ 256 â€“ â€“ â€“ 256 â€“ Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleNorm and LayerNorm are proposed by Nguyen & Salazar (2019) and Ba et al. (2016), respectively. 4 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 5 The learnable position embedding is proposed by Gehring et al. (2017). 6 The model is consist of 12 attention layers and 12 FFN layers. C. Additional Experimental Results Here, we provide full results on the training speed of different language models using a Nvidia V100 GPU (in Table 11) and the ablation study of chunk size for FLASH (in Figure 8).Table 9: Model conï¬gurations for LM experiments on the Wiki-40B dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 Learnable4 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE â€“ RoPE RoPE RoPE RoPE # of layers 24 24 12+12 5 12+125 12+125 12+125 12+125 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size â€“ 256 â€“ â€“ â€“ 256 â€“ Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The learnable position embedding is proposed by Gehring et al. (2017). 5 The model is consist of 12 attention layers and 12 FFN layers. Table 10: Model conï¬gurations for LM experiments on the PG-19 dataset in Section 4. FLASH-Quad FLASH Transformer+ Transformer++ Combiner # of attention heads 1 1 16 16 16 Attention kernel relu2 relu2 softmax softmax softmax Attention type Quadratic Mixed Chunk Quadratic Quadratic Rowmajor-Axial FFN type GAU1 GAU1 MLP GLU MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE RoPE RoPE RoPE # of layers 72 72 36+36 4 36+364 36+364 Hidden size 1024 1024 1024 1024 1024 Expansion rate 2 2 4 4 4 Chunk size â€“ 512 â€“ â€“ 512 Params (M) 496 496 486 486 562 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The model is consist of 36 attention layers and 36 FFN layers. Table 11: Comparison of latency for each training step of auto-regressive language modeling on Wiki-40B using a single Nvidia Tesla V100 GPU â€” Latency is reported in millisecond. OOM stands for the CUDA out of memory error. Performer-Matmul implements the cumulative sum (cumsum) using matrix multiplication. Context length Ã—Batch size Model 512 Ã—4 1024 Ã—2 2048 Ã—1 4096 Ã—1 Transformer++ 222.4 243.9 315.0 OOM Performer 823.0 827.4 799.8 OOM Performer-Matmul 697.4 701.7 688.9 OOM FLASH 254.4 235.0 242.8 452.9C.1. Auto-regressive Training on GPU We observe that the inefï¬ciency of auto-regressive training is not limited to hardware accelerators such as TPUs. As shown in Table 11, Performer has the largest latency among the three models because it requires to perform cumsum over all tokens sequentially. In contrast, the proposed FLASH achieves the lowest latency when the context length is over 1024, suggesting the effectiveness of the proposed mixed chunk attention mechanism. C.2. Tabular MLM and LM Results We summarize the experimental results of MLM on C4 and LM on Wiki-40B in Tables 12 and 13. Table 12: Bidirectional/masked language models on the C4 dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 4.517 47.7 4.436 63.9 4.196 90.9 4.602 142.5 4.8766 252.7 Transformer+ 4.283 48.8 4.151 64.4 4.032 91.5 3.989 142.9 3.986 252.9 Transformer++ 4.205 47.6 4.058 64.6 3.920 91.6 3.876 143.4 3.933 252.1 Performer 5.897 37.2 6.324 37.6 8.032 39.1 12.622 36.9 102.980 40.9 Combiner 4.449 67.2 4.317 66.4 4.238 66.4 4.195 68.3 4.225 77.3 FLASH-Quad 4.176 43.7 3.964 50.1 3.864 61.7 3.828 84.9 3.830 132.1 FLASH 4.172 51.2 4.015 50.1 3.928 51.4 3.902 50.7 3.897 59.9 Table 13: Auto-regressive language models on the Wiki-40B dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 17.341 54.0 19.808 70.9 18.154 96.3 17.731 149.1 18.254 260.7 Transformer+ 16.907 55.6 15.999 70.3 15.653 96.1 15.515 149.3 15.478 261.9 Transformer++ 16.835 54.7 15.943 70.9 15.489 96.6 15.282 149.2 15.254 261.0 Performer 18.989 1439.7 18.520 1386.9 18.547 1518.9 18.987 1526.7 19.923 1526.8 Combiner 17.338 75.5 16.710 74.4 16.344 71.8 16.171 71.7 16.119 77.9 FLASH-Quad 16.633 54.1 15.879 59.5 15.305 71.3 14.955 96.1 14.998 141.3 FLASH 16.581 57.2 15.935 56.9 15.525 56.7 15.259 57.0 15.109 62.5 C.3. Ablation Study of Chunk Size The choice of chunk size can have an impact on both the quality and the training cost of FLASH. In the extreme case where chunk size equals the context length, FLASH falls back to FLASH-Quad and loses the scalability to long context lengths. In the other extreme case where chunk size is equal to one, the proposed attention module becomes a linear attention, which suffers from inefï¬cient auto-regressive training. Figure 8 shows the tradeoff between the quality and training cost of four different chunk sizes for context lengths from 1K to 8K. D. Pseudocode For FLASH-Quad and FLASH We show the detailed implementation of FLASH-Quad and FLASH in Codes 6 and 8.(a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 8: Ablation study of the chunk size (C) of FLASH for context lengths from 1K to 8K. def _get_scaledsin(embeddings): \"\"\"Create sinusoidal position embedding with a scaling factor.\"\"\" hidden_size = int(embeddings.shape[=1]) pos = tf.range(tf.shape(embeddings)[1]) pos = tf.cast(pos, tf.float32) half_d = hidden_size // 2 freq_seq = tf.cast(tf.range(half_d), tf.float32) / float(half_d) inv_freq = 10000 âˆ—âˆ— =freq_seq sinusoid = tf.einsum(/quotesingle.ts1s,dâ†’sd/quotesingle.ts1, pos, inv_freq) scaledsin = tf.concat([tf.sin(sinusoid), tf.cos(sinusoid)], axis==1) scalar = tf.get_variable( /quotesingle.ts1scaledsin_scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1 / hidden_size âˆ—âˆ— 0.5)) scaledsin âˆ—= scalar return scaledsin Code 2: Pseudocode for ScaledSin absolute position embedding. def rope(x, axis): \"\"\"RoPE position embedding.\"\"\" shape = x.shape.as_list() if isinstance(axis, int): axis = [axis] spatial_shape = [shape[i] for i in axis] total_len = 1 for i in spatial_shape: total_len âˆ—= i position = tf.reshape( tf.cast(tf.range(total_len, delta=1.0), tf.float32), spatial_shape) for i in range(axis[=1] + 1, len(shape) = 1, 1): position = tf.expand_dims(position, axis==1) half_size = shape[=1] // 2 freq_seq = tf.cast(tf.range(half_size), tf.float32)/float(half_size) inv_freq = 10000 âˆ—âˆ— =freq_seq sinusoid = tf.einsum(/quotesingle.ts1...,dâ†’...d/quotesingle.ts1, position, inv_freq) sin = tf.sin(sinusoid) cos = tf.cos(sinusoid) x1, x2 = tf.split(x, 2, axis==1) return tf.concat([x1 âˆ— cos = x2 âˆ— sin, x2 âˆ— cos + x1 âˆ— sin], axis==1) Code 3: Pseudocode for RoPE.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def rel_pos_bias(n): \"\"\"Relative position bias.\"\"\" if n < 512: # Construct Toeplitz matrix directly when the sequence length is less than 512. w = tf.get_variable( /quotesingle.ts1weight/quotesingle.ts1, shape=[2 âˆ— n = 1], dtype=tf.float32, initializer=WEIGHT_INITIALIZER) t = tf.pad(w, [[0, n]]) t = tf.tile(t, [n]) t = t[..., :=n] t = tf.reshape(t, [n, 3 âˆ— n = 2]) r = (2 âˆ— n = 1) // 2 t = t[..., r:=r] else: # Construct Toeplitz matrix using RoPE when the sequence length is over 512. a = tf.get_variable( /quotesingle.ts1a/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) b = tf.get_variable( /quotesingle.ts1b/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) a = rope(tf.tile(a[None, :], [n, 1]), axis=0) b = rope(tf.tile(b[None, :], [n, 1]), axis=0) t = tf.einsum(/quotesingle.ts1mk,nkâ†’mn/quotesingle.ts1, a, b) return t Code 4: Pseudocode for relative position bias. def norm(x, begin_axis==1, eps=1e=5, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1): \"\"\"Normalization layer.\"\"\" shape = x.shape.as_list() axes = list(range(len(shape)))[begin_axis:] if norm_type == /quotesingle.ts1layer_norm/quotesingle.ts1: mean, var = tf.nn.moments(x, axes, keepdims=True) x = (x = mean) âˆ— tf.rsqrt(var + eps) gamma = tf.get_variable( /quotesingle.ts1gamma/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.ones()) beta = tf.get_variable( /quotesingle.ts1beta/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.zeros()) return gamma âˆ— x + beta elif norm_type == /quotesingle.ts1scale_norm/quotesingle.ts1: mean_square =tf.reduce_mean(tf.math.square(x), axes, keepdims=True) x = x âˆ— tf.rsqrt(mean_square + eps) scalar = tf.get_variable(/quotesingle.ts1scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1.0)) return scale âˆ— x Code 5: Pseudocode for LayerNorm and ScaleNorm.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def GAU(x, causal, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"GAU block. Input shape: batch size x sequence length x model size \"\"\" seq_len = tf.shape(x)[1] d = int(x.shape[=1]) e = int(d âˆ— expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 âˆ— e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query (q) and Key (k) from base. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[2, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[2, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hrâ†’...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=1) q, k = tf.unstack(base, axis==2) # Calculate the quadratic attention. qk = tf.einsum(/quotesingle.ts1bnd,bmdâ†’bnm/quotesingle.ts1, q, k) bias = rel_pos_bias(seq_len) kernel = tf.math.square(tf.nn.relu(qk / seq_len + bias)) # Apply the causal mask for auto=regressive tasks. if causal: causal_mask = tf.linalg.band_part( tf.ones([seq_len, seq_len], dtype=x.dtype), num_lower==1, num_upper=0) kernel âˆ—= causal_mask x = u âˆ— tf.einsum(/quotesingle.ts1bnm,bmeâ†’bne/quotesingle.ts1, kernel, v) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 6: Pseudocode for GAU (FLASH-Quad). def segment_ids_to_mask(segment_ids, causal=False): \"\"\"Generate the segment mask from the segment ids. The segment mask is used to remove the attention between tokens in different documents. \"\"\" min_ids, max_ids = tf.reduce_min(segment_ids, axis==1), tf.reduce_max(segment_ids, axis==1) # 1.0 indicates in the same group and 0.0 otherwise mask = tf.logical_and( tf.less_equal(min_ids[:, :, None], max_ids[:, None, :]), tf.greater_equal(max_ids[:, :, None], min_ids[:, None, :])) mask = tf.cast(mask, tf.float32) if causal: g = tf.shape(min_ids)[1] causal_mask = 1.0 = tf.linalg.band_part( tf.ones([g, g], dtype=tf.float32), num_lower=0, num_upper==1) mask âˆ—= causal_mask mask = tf.math.divide_no_nan(mask, tf.reduce_sum(mask, axis==1, keepdims=True)) return mask Code 7: Pseudocode for generating segment mask.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def FLASH(x, causal, segment_ids, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"FLASH block. Input shape: batch size x num chunks x chunk length x model size \"\"\" _, g, n, d = x.shape.as_list() e = int(d âˆ— expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 âˆ— e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query and Key for both quadratic and linear attentions. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[4, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[4, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hrâ†’...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=[1, 2]) quad_q, quad_k, lin_q, lin_k = tf.unstack(base, axis==2) if causal: # Linear attention part. lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgneâ†’bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids, causal=True) cum_lin_kv = tf.einsum(/quotesingle.ts1bhke,bghâ†’bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgkeâ†’bgne/quotesingle.ts1, lin_q, cum_lin_kv) # Quadratic attention part. quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmkâ†’bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu(quad_qk / n + bias)) # Apply the causal mask for auto=regressive tasks. causal_mask = tf.linalg.band_part(tf.ones([n, n], dtype=x.dtype), num_lower==1, num_upper=0) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgmeâ†’bgne/quotesingle.ts1, kernel âˆ— causal_mask, v) else: # Linear attention part lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgneâ†’bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids) lin_kv = tf.einsum(/quotesingle.ts1bhke,bghâ†’bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgkeâ†’bgne/quotesingle.ts1, lin_q, lin_kv) # Quadratic attention part quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmkâ†’bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu((quad_qk / n + bias)) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgmeâ†’bgne/quotesingle.ts1, kernel, v) x = u âˆ— (quadratic + linear) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 8: Pseudocode for FLASH.",
      "meta_data": {
        "arxiv_id": "2202.10447v2",
        "authors": [
          "Weizhe Hua",
          "Zihang Dai",
          "Hanxiao Liu",
          "Quoc V. Le"
        ],
        "published_date": "2022-02-21T18:59:38Z",
        "pdf_url": "https://arxiv.org/pdf/2202.10447v2.pdf"
      }
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafaâˆ— nimrah.mustafa@cispa.de Aleksandar Bojchevskiâ€  a.bojchevski@uni-koeln.de Rebekka Burkholzâˆ— burkholz@cispa.de âˆ—CISPA Helmholtz Center for Information Security, 66123 SaarbrÃ¼cken, Germany â€ University of Cologne, 50923 KÃ¶ln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a nodeâ€™s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a nodeâ€™s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different â€˜effectiveâ€™ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. â€¢ We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. â€¢ This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. â€¢ To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E âŠ† V Ã— V , where the neighborhood of a node v is given as N(v) = {u|(u, v) âˆˆ E}, a GNN layer computes each nodeâ€™s representation by aggregating over its neighborsâ€™ representation. In GATs, this aggregation is weighted by parameterized attention coefficients Î±uv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i âˆˆ [nl] Given input representations hlâˆ’1 v for all nodes v âˆˆ V , a GAT 1 layer l transforms those to: hl v = Ï•( X uâˆˆN(v) Î±l uv Â· Wl shlâˆ’1 u ), where (1) Î±l uv = exp((al)âŠ¤ Â· LeakyReLU(Wl shlâˆ’1 u + Wl t hlâˆ’1 v ))P uâ€²âˆˆN(v) exp((al)âŠ¤ Â· LeakyReLU(Wlshlâˆ’1 uâ€² + Wl t hlâˆ’1v ))) . (2) We consider Ï• to be a positively homogeneous activation functions (i.e Ï•(x) =xÏ•â€²(x) and conse- quently, Ï•(ax) =aÏ•(x) for positive scalars a), such as a ReLU Ï•(x) = max{x, 0} or LeakyReLU Ï•(x) = max{x, 0} + âˆ’Î± max{âˆ’x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 âŠ‚ Rd Ã— Rp for M â‰¤ V , let f : Rd â†’ Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l âˆˆ [L] of size nl has associated parameters: a feature weight matrix Wl âˆˆ RnlÃ—nlâˆ’1 and an attention vector al âˆˆ Rnl, where n0 = d and nL = p. Given a differentiable loss function â„“ : Rd Ã— Rp â†’ R, the loss L = 1/M PM i=1 â„“(f(xm), ym) is used to update model parameters w âˆˆ {Wl, al}L l=1 with learning rate Î³ by gradient descent, i.e., wt+1 = wt âˆ’ Î³âˆ‡wL, where âˆ‡wL = [âˆ‚L/âˆ‚w1, . . . ,âˆ‚L/âˆ‚w|w|] and w0 is set by the initialization scheme. For an infinitesimal Î³ â†’ 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = âˆ’âˆ‡wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i âˆˆ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i âˆˆ [dl], jâˆˆ [dlâˆ’1], and k âˆˆ [dl+1], as depicted in Fig. 1. âŸ¨Â·, Â·âŸ© denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l âˆˆ [L âˆ’ 1] in the network is conserved according to the following law: âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© âˆ’ âŸ¨al[i], âˆ‡al[i]LâŸ© = âŸ¨Wl+1[:, i], âˆ‡Wl+1[:,i]LâŸ©. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation Ï• in Eq. (1), gradient flow in the network adheres to the following invariance for l âˆˆ [L âˆ’ 1]: d dt \u0010\r\rWl[i, :] \r\r2 âˆ’ \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) Î´ with GD  (b) Î´ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show Î´ â‰ˆ 0 (can not empirically be exactly zero due to finite Î³ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c Ì¸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc â‰ˆ 0 as the network becomes slightly unbalanced due to the finite Î³ = 0.1. As both Î´ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 âˆ’ \r\ral[i] \r\r2 âˆ’ \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 âˆ’ \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the â€˜degree of balancednessâ€™ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l âˆˆ [L âˆ’ 1] by summing over i âˆˆ [nl] is: d dt \u0010\r\rWl\r\r2 F âˆ’ \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define Î´ as: Î´ = âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© âˆ’ âŸ¨al, âˆ‡alLâŸ© âˆ’ âŸ¨Wl+1[:, i], âˆ‡Wl+1[:,i]LâŸ© = 0. (7) We observe how the value of Î´ varies during training in Fig. 2 for both GD and Adam optimizers with Î³ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (Î´ â‰ˆ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nLâˆ’1) << 1, where the number of outputs is smaller than the layer width nL << nLâˆ’1. In contrast, E \r\rWLâˆ’1[i, :] \r\r2 = 2nLâˆ’1/(nLâˆ’1 + nLâˆ’2) = 1 and EaLâˆ’1[i]2 = 2/(1 +nLâˆ’1). In consequence, the right-hand side of our derived conservation law nLâˆ’2X j=1 (WLâˆ’1 ij )2 âˆ‡WLâˆ’1 ij L WLâˆ’1 ij âˆ’ (aLâˆ’1 i )2 âˆ‡aLâˆ’1 i L aLâˆ’1 i = nLX k=1 (WL ki)2 âˆ‡WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss â†’ 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L âˆ’1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 âˆ‡W(1) jm L W(1) jm âˆ’ Lâˆ’1X l=1 nlX o=1 a(l) o 2 âˆ‡a(l) o L a(l) o = nLâˆ’1X i=1 nLX k=1 W(L) ki 2 âˆ‡W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWLâˆ’1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL âˆ’ 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l âˆˆ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(wâˆ—âˆ’w0)/wâˆ—|. of trained network parameters (wâˆ—) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (wâˆ— â‰¥ 10âˆ’4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with Î³ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to Î±uv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the â€˜chicken and eggâ€™ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l âˆˆ [L]: \r\rWl[i, :] \r\r2 âˆ’ \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l âˆˆ [L]. 2. Set W1[i, :] = W1[i,:] âˆ¥W1[i,:]âˆ¥ âˆšÎ²i, for i âˆˆ [n1] where Î²i is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] âˆ¥Wl+1[:,i]âˆ¥ \r\rWl[i, :] \r\r for i âˆˆ [nl] and l âˆˆ [L âˆ’ 1] In step 2, Î²i determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set Î²i = 2for i âˆˆ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = Î²i for i âˆˆ [nLâˆ’1]. Balanced Orthogonal Initialization The feature weights Wl for l âˆˆ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let âˆ¥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 âˆ¥ âˆ’U1] where U1 âˆˆ R n1 2 Ã—n0 , Wl = [[Ul âˆ¥ âˆ’Ul] |= [âˆ’Ul âˆ¥ Ul]] where Ul âˆˆ R nl 2 Ã— nlâˆ’1 2 for l = {2, . . . , Lâˆ’ 1}, and WL = [UL |= âˆ’UL] where UL âˆˆ RnLÃ— nlâˆ’1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set Î²i = 2for i âˆˆ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss â‰¤ 10âˆ’4) and select the model state with the highest validation accuracy. For each experiment, the mean Â±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) Â± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 Â± 2.73 58 .40 Â± 2.25 24 .70 Â± 8.90 19 .23 Â± 1.54 18 .72 Â± 1.15 BalX 71.62 Â± 0.80 68 .83 Â± 1.62 64 .13 Â± 1.57 54 .88 Â± 7.95 42 .63 Â± 17.47 BalO 72.02 Â± 0.63 70 .63 Â± 0.60 68 .83 Â± 1.68 68 .70 Â± 1.18 63 .40 Â± 1.43 Pubmed Xav 77.26 Â± 1.39 70 .68 Â± 2.16 67 .32 Â± 10.70 36 .52 Â± 11.50 27 .20 Â± 13.99 BalX 78.02 Â± 0.73 75.66 Â± 1.81 77.60 Â± 1.56 76.44 Â± 1.70 75 .74 Â± 2.94 BalO 77.68 Â± 0.45 76.62 Â± 1.59 77.04 Â± 2.14 78.20 Â± 0.61 77 .80 Â± 1.41 Actor Xav 27.32 Â± 0.59 24 .60 Â± 0.93 24 .08 Â± 0.80 22 .29 Â± 3.26 19 .46 Â± 5.75 BalX 26.00 Â± 0.59 23 .93 Â± 1.42 24.21 Â± 0.78 24 .74 Â± 1.14 23.88 Â± 0.97 BalO 26.59 Â± 1.03 24 .61 Â± 0.77 24.17 Â± 0.62 24 .24 Â± 1.05 23.93 Â± 1.53 Chameleon Xav 52.81 Â± 1.37 54.21 Â± 1.05 30 .31 Â± 5.96 22 .19 Â± 2.04 22 .28 Â± 3.15 BalX 51.18 Â± 1.94 54.21 Â± 0.82 52 .11 Â± 3.72 51.89 Â± 1.89 38 .64 Â± 10.31 BalO 50.00 Â± 3.07 53 .95 Â± 1.81 51 .84 Â± 3.21 52.72 Â± 0.13 44 .30 Â± 1.61 Cornell Xav 42.70 Â± 2.51 41.08 Â± 2.51 42.70 Â± 1.34 25.41 Â± 14.64 22 .70 Â± 13.69 BalX 41.08 Â± 6.84 35 .14 Â± 11.82 41 .08 Â± 2.51 40 .00 Â± 4.93 37.84 Â± 5.62 BalO 42.16 Â± 1.64 43.24 Â± 2.12 36.76 Â± 5.02 35.68 Â± 3.29 36.22 Â± 3.42 Squirrel Xav 35.20 Â± 0.44 40 .96 Â± 0.92 21 .65 Â± 1.52 20 .23 Â± 1.69 19 .67 Â± 0.29 BalX 35.95 Â± 1.69 40.98 Â± 0.87 38.98 Â± 1.49 38.35 Â± 1.07 25 .38 Â± 4.62 BalO 35.83 Â± 0.92 42.52 Â± 1.19 38.85 Â± 1.36 39.15 Â± 0.44 26 .57 Â± 1.83 Texas Xav 60.00 Â± 1.34 60 .54 Â± 3.42 58 .92 Â± 1.34 49 .73 Â± 20.97 17 .84 Â± 26.98 BalX 60.54 Â± 1.64 61 .62 Â± 2.51 61 .62 Â± 2.51 58 .92 Â± 2.51 56.22 Â± 1.34 BalO 60.00 Â± 1.34 57 .30 Â± 1.34 56 .76 Â± 0.00 58 .38 Â± 4.55 57.30 Â± 3.91 Wisconsin Xav 51.37 Â± 6.04 51.37 Â± 8.90 51 .76 Â± 3.64 43 .14 Â± 25.07 31 .76 Â± 31.50 BalX 49.80 Â± 8.79 54.51 Â± 4.19 47.84 Â± 7.16 50.59 Â± 10.49 41.18 Â± 1.54 BalO 49.80 Â± 4.24 55 .69 Â± 3.64 51.76 Â± 5.68 49.02 Â± 4.36 48.24 Â± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as Ï‰GAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249â€“256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and ClÃ©ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias MÃ¼ller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias MÃ¼ller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] PÃ¡l AndrÃ¡s Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar VeliË‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727â€“11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted âŸ¨, âŸ© and âŠ™, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noetherâ€™s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(Î¸) is rescale invariant with respect to disjoint subsets of the parameters Î¸1 and Î¸2 if for every Î» >0 we have L(Î¸) =L((Î»Î¸1, Î»âˆ’1Î¸2, Î¸d)), where Î¸ = (Î¸1, Î¸2, Î¸d). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: âŸ¨Î¸1, âˆ‡Î¸1 LâŸ© âˆ’ âŸ¨Î¸2, âˆ‡Î¸2 LâŸ© = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as Î¸1 = {w | w âˆˆ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as Î¸2 = {w | w âˆˆ Wl+1[: , i]} âˆª {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at Î»Î¸1 and Î»âˆ’1Î¸2 and show that it remains invariant for any choice of Î» >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and Î±l uv. We denote the scaled network components with a tilde resulting in Ëœhl u[i], Ëœhl+1 v [k], and ËœÎ±l uv As we show, parameters of upper layers remain unaffected, as Ëœhl+1 v [k] coincides with its original non-scaled variant Ëœhl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that Ëœal[i] =Î»âˆ’1al[i] and ËœWl[i, j] =Î»Wl[i, j]. This implies that ËœÎ±l uv = exp(el uv)P uâ€²âˆˆN(v) exp(eluv) = Î±l uv , because (11) Ëœel uv = (al)âŠ¤ Â· Ï•2(Wl(hlâˆ’1 u + hlâˆ’1 v )) =el uv, (12) which follows from the positive homogeneity of Ï•2 that allows Ëœel uv = Î»âˆ’1al[i]Ï•2( nlâˆ’1X j Î»Wl[i, j](hlâˆ’1 u [j] +hlâˆ’1 v [j]) + nlX iâ€²Ì¸=i al[i]Ï•2( nlâˆ’1X j Wl[i, j](hlâˆ’1 u [j] +hlâˆ’1 v [j]) (13) = Î»âˆ’1Î»al[i]Ï•2( nlâˆ’1X j Wl[i, j](hlâˆ’1 u [j] +hlâˆ’1 v [j]) + nlX iâ€²Ì¸=i al[i]Ï•2( nlâˆ’1X j Wl[i, j](hlâˆ’1 u [j] +hlâˆ’1 v [j]) (14) = el uv. (15) 15Since ËœÎ±l uv = Î±l uv, it follows that Ëœhu l [i] =Ï•1 ï£« ï£­ X zâˆˆN(u) Î±l zu nlâˆ’1X j Î»Wl[i, j]hlâˆ’1 z [j] ï£¶ ï£¸ = Î»Ï•1 ï£« ï£­ X zâˆˆN(u) Î±l zu nlâˆ’1X j Wl[i, j]hlâˆ’1 z [j] ï£¶ ï£¸ = Î»hl u[i] In the next layer, we therefore have Ëœhl+1 v [k] =Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Î»âˆ’1Wl+1[k, i]Ëœhl u[i] ï£¶ ï£¸ = Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Î»âˆ’1Wl+1[k, i]Î»hl u[i] ï£¶ ï£¸ = Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Wl+1[k, i]hl u[i] ï£¶ ï£¸ = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© âˆ’ âŸ¨al[i], âˆ‡al[i]LâŸ© âˆ’ âŸ¨Wl+1[:, i], âˆ‡Wl+1[:,i]LâŸ© = 0. which can be rearranged to Eq.((2.2): âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© âˆ’ âŸ¨al[i], âˆ‡al[i]LâŸ© = âŸ¨Wl+1[:, i], âˆ‡Wl+1[:,i]LâŸ©. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = âˆ’âˆ‡wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2âŸ¨Wl[i, :], d dtWl[i, :]âŸ© = âˆ’2âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 âˆ’ \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . âˆ’2âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© âˆ’(âˆ’2)âŸ¨al[i], âˆ‡al[i]LâŸ© = âˆ’2âŸ¨Wl+1[:, i], âˆ‡Wl+1[:,i]LâŸ©. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i âˆˆ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l âˆˆ [L âˆ’ 1] in the network are governed by the following law: âŸ¨Wl s[i, :], âˆ‡Wls[i,:]LâŸ© + âŸ¨Wl t [i, :], âˆ‡Wl t [i,:]LâŸ© âˆ’ âŸ¨al[i], âˆ‡al[i]LâŸ© = âŸ¨Wl+1 s [:, i], âˆ‡Wl+1 s [:,i]LâŸ© (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets Î¸1 and Î¸2 of the parameter set Î¸, associated with a neuron i in layer l accordingly, as follows: Î¸1 = {w|w âˆˆ Wl s[i, :]} âˆª {w|w âˆˆ Wl t [i, :]} Î¸2 = {w|w âˆˆ Wl+1 s [:, i]} âˆª {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and Î±l uv. We denote the scaled network components with a tilde resulting in Ëœhl u[i], Ëœhl+1 v [k], and ËœÎ±l uv As we show, parameters of upper layers remain unaffected, as Ëœhl+1 v [k] coincides with its original non-scaled variant Ëœhl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that Ëœal[i] =Î»âˆ’1al[i], ËœWs l [i, j] = Î»Wl s[i, j] and ËœWl t [i, j] =Î»Wl t [i, j]. This implies that ËœÎ±l uv = exp(el uv)P uâ€²âˆˆN(v) exp(eluv) = Î±l uv , because (19) Ëœel uv = (al)âŠ¤ Â· Ï•2(Wl shlâˆ’1 u + Wl t hlâˆ’1 v ) =el uv, (20) which follows from the positive homogeneity of Ï•2 that allows Ëœel uv = Î»âˆ’1al[i]Ï•2( nlâˆ’1X j Î»Wl s[i, j]hlâˆ’1 u [j] +Î»Wl t [i, j]hlâˆ’1 v [j] (21) + nlX iâ€²Ì¸=i al[i]Ï•2( nlâˆ’1X j Wl s[i, j]hlâˆ’1 u [j] +Wl t [i, j]hlâˆ’1 v [j]) (22) = Î»âˆ’1Î»al[i]Ï•2( nlâˆ’1X j Wl s[i, j]hlâˆ’1 u [j] +Wl t [i, j]hlâˆ’1 v [j] (23) + nlX iâ€²Ì¸=i al[i]Ï•2( nlâˆ’1X j Wl s[i, j]hlâˆ’1 u [j] +Wl t [i, j]hlâˆ’1 v [j]) (24) = el uv. (25) Since ËœÎ±l uv = Î±l uv, it follows that Ëœhu l [i] =Ï•1 ï£« ï£­ X zâˆˆN(u) Î±l zu nlâˆ’1X j Î»Wl s[i, j]hlâˆ’1 z [j] ï£¶ ï£¸ = Î»Ï•1 ï£« ï£­ X zâˆˆN(u) Î±l zu nlâˆ’1X j Wl s[i, j]hlâˆ’1 z [j] ï£¶ ï£¸ = Î»hl u[i] 17In the next layer, we therefore have Ëœhv l+1 [k] =Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Î»âˆ’1Wl+1 s [k, i]Ëœhl u[i] ï£¶ ï£¸ = Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Î»âˆ’1Wl+1 s [k, i]Î»hl u[i] ï£¶ ï£¸ = Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Wl+1 s [k, i]hl u[i] ï£¶ ï£¸ = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k âˆˆ [K] in a GAT layer of a L layer network. Then the gradients of layer l âˆˆ [L âˆ’ 1] respect the law: KX k âŸ¨Wl k[i, :], âˆ‡Wl k[i,:]LâŸ© âˆ’ KX k âŸ¨al k[i], âˆ‡al k[i]LâŸ© = KX k âŸ¨Wl+1 k [:, i], âˆ‡Wl+1 k [:,i]LâŸ©. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =âˆ¥K k Ï•(P uâˆˆN(v) Î±kuv l Â· Wl khlâˆ’1 u ) , or Average: hl v = 1 K PK k Ï•(P uâˆˆN(v) Î±kuv l Â· Wl khlâˆ’1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to Î³ = 0.1, Î³ = 0.05 and Î³ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, Î³ = 0.005 and Î³ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(wâˆ—âˆ’w0)/wâˆ—| where w0 is the initialized value and wâˆ— is the value for the model with maximum validation accuracy during training. Absolute change |aâˆ— âˆ’ a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as âˆ¥âˆ‡WlWlâˆ¥F/âˆ¥Wlâˆ¥F and âˆ¥âˆ‡alalâˆ¥/âˆ¥alâˆ¥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w âˆˆ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w âˆˆ Wl and all a âˆˆ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) Â± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 Â± 3.02 76 .96 Â± 2.21 79.48 Â± 0.43 10 25 .48 Â± 18.13 77 .72 Â± 1.49 79.46 Â± 1.34 attention heads = 8 5 73 .56 Â± 2.71 77 .44 Â± 1.54 79.58 Â± 0.53 10 25 .50 Â± 18.18 77 .02 Â± 2.76 79.06 Â± 0.73 activation = ELU 5 75 .68 Â± 1.80 79 .20 Â± 1.07 79.64 Â± 0.36 10 73 .02 Â± 2.27 78.64 Â± 1.72 47.76 Â± 7.39 dropout = 0.6 5 42 .14 Â± 15.97 79 .18 Â± 1.17 81.00 Â± 0.62 10 24 .90 Â± 9.50 30 .94 Â± 1.04 44.40 Â± 1.84 weight decay = 0.0005 5 67 .26 Â± 6.30 77 .36 Â± 1.74 79.56 Â± 0.48 10 18 .78 Â± 11.96 76 .56 Â± 2.91 79.40 Â± 1.15 weight sharing = False 5 70 .80 Â± 7.00 77 .28 Â± 1.45 79.82 Â± 0.63 10 19 .54 Â± 14.03 76 .04 Â± 1.77 79.06 Â± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) Â± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 Â± 1.61 79 .38 Â± 2.24 80.20 Â± 0.57 10 70 .86 Â± 3.99 75 .72 Â± 2.35 79.62 Â± 1.27 attention heads = 8 5 75 .62 Â± 1.74 78 .54 Â± 1.06 79.56 Â± 0.85 10 70 .94 Â± 2.76 75 .48 Â± 2.48 79.74 Â± 1.10 activation = ELU 5 76 .56 Â± 1.72 78 .72 Â± 1.02 79.56 Â± 0.64 10 75 .30 Â± 1.42 78.48 Â± 1.79 76.52 Â± 0.97 dropout = 0.6 5 76 .74 Â± 1.44 78 .42 Â± 1.28 79.92 Â± 0.48 10 32 .48 Â± 6.99 31 .76 Â± 1.33 76.34 Â± 0.95 weight decay = 0.0005 5 75 .10 Â± 2.05 78 .52 Â± 1.41 79.80 Â± 0.63 10 28 .74 Â± 12.04 74 .68 Â± 3.06 79.70 Â± 1.14 weight sharing = False 5 76 .56 Â± 1.72 78 .72 Â± 1.02 79.56 Â± 0.64 10 71 .12 Â± 2.23 73 .32 Â± 1.23 79.46 Â± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) Â± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 Â± 0.52 84.58 Â± 0.65 84.55 Â± 0.47 Pubmed 78.38 Â± 0.77 78 .52 Â± 0.54 78.56 Â± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 Â± 0.9 80 .5 Â± 0.5 78 .0 Â± 0.3 80.9 Â± 0.4 5 73 .2 Â± 3.4 78 .3 Â± 0.8 80.3 Â± 0.8 79.6 Â± 1.0 10 24 .1 Â± 4.5 77 .6 Â± 2.0 80 .0 Â± 1.1 80.0 Â± 0.9 20 14 .4 Â± 11.2 62 .8 Â± 3.6 78 .7 Â± 0.5 78.8 Â± 1.5 40 13 .4 Â± 0.9 65 .9 Â± 7.3 28 .3 Â± 16.2 77.1 Â± 0.9 64 9 .8 Â± 5.4 33 .0 Â± 13.4 27 .3 Â± 12.7 76.7 Â± 1.3 80 12 .4 Â± 19.3 33 .8 Â± 12.9 38 .9 Â± 21.3 77.1 Â± 1.3 Citeseer 2 66 .6 Â± 20.0 71 .3 Â± 1.8 66 .0 Â± 3.2 72.3 Â± 0.9 5 60 .9 Â± 12.3 66 .9 Â± 15.0 69 .0 Â± 6.4 70.1 Â± 1.8 10 23 .8 Â± 36.8 66 .0 Â± 5.9 70.6 Â± 0.9 69.8 Â± 10.9 20 16 .4 Â± 18.2 47 .9 Â± 10.0 67 .0 Â± 8.6 69.7 Â± 4.5 40 13 .9 Â± 56.8 37 .3 Â± 92.8 44 .8 Â± 6.8 64.7 Â± 13.6 64 13 .8 Â± 41.4 29 .5 Â± 15.0 37 .3 Â± 79.6 66.3 Â± 0.5 80 12 .4 Â± 42.7 25 .8 Â± 3.6 30 .1 Â± 21.8 64.1 Â± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, Ï‰GAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of Ï‰GAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: Ï‰GAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 Ã— 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l âˆˆ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      }
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liuâˆ— Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wuâˆ— Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ï¬xed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efï¬ciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiï¬cation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ï¬‚oating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (Lâˆ’1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover âˆ—Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such â€œneighbor explosionâ€ problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above â€œneighbor explosionâ€ problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], qâ‹† ij = Î±ijâˆ¥h(l) j âˆ¥2 âˆ‘ kâˆˆNi Î±ikâˆ¥h(l) k âˆ¥2 for vertex vi, to minimize the variance of the estimator Ë†h(l+1) i involves all its neighborsâ€™ hidden embeddings, i.e.{Ë†h(l) j |vj âˆˆNi}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsÎ±ijâ€™s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulï¬l this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deï¬ne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ï¬rst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi âˆˆV, and edges (vi,vj) âˆˆE. Let the adjacency matrix denote as AâˆˆRNÃ—N. Assuming the feature matrix H(0) âˆˆRNÃ—D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = Ïƒ ( Nâˆ‘ j=1 Î±(vi,vj) h(l) j W(l) ) , l = 0,...,L âˆ’1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, Î±Î±Î±= (Î±(vi,vj)) âˆˆRNÃ—N is a kernel or weight matrix, W(l) âˆˆRD(l)Ã—D(l+1) is the transform parameter on the l-th layer, and Ïƒ(Â·) is the activation function. The weight Î±(vi,vj), or Î±ij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deï¬ne ï¬xed weights as Î±Î±Î±= ËœDâˆ’1 ËœAor Î±Î±Î±= ËœDâˆ’1 2 ËœAËœDâˆ’1 2 respectively, where ËœA= A+I, and ËœDis the diagonal node degree matrix of ËœA. (2) The attentive GNNs [22, 17] deï¬ne a learned weight Î±(vi,vj) by attention functions: Î±(vi,vj) = ËœÎ±(vi,vj;Î¸)âˆ‘ vkâˆˆNi ËœÎ±(vi,vk;Î¸) , where the unnormalized attentions ËœÎ±(vi,vj; Î¸) = exp(ReLU(aT[Whiâˆ¥Whj])), are parameterized by Î¸= {a,W}. Different 2from GCNs, the learned weights Î±ij âˆËœÎ±ij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as Ë†h(l+1) i = Ïƒ ( N(i) Epij [ Ë†h(l) j ] W(l) ) , (2) where pij âˆÎ±ij, and N(i) =âˆ‘ jÎ±ij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = Î±ij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling Ë†h(l+1) i = ÏƒW(l) ( Ë†Âµ(l) i ) = ÏƒW(l) ( Eqij [Î±ij qij Ë†h(l) j ]) , (3) where we use ÏƒW(l) (Â·) to include transform parameter W(l) into the function Ïƒ(Â·) for conciseness. As such, one can ï¬nd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator Ë†Âµ(l) i = 1 k âˆ‘k s=1 Î±ijs qijs Ë†h(l) js , where js âˆ¼qi. Take expectation overqi, we deï¬ne the variance of Ë†Âµ(l) i = Î±ijs qijs Ë†h(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [îµ¹îµ¹îµ¹Ë†Âµ(l) i (t) âˆ’Âµ(l) i (t) îµ¹îµ¹îµ¹ 2] = E [îµ¹îµ¹îµ¹Î±ijs(t) qijs h(l) js (t) âˆ’ âˆ‘ jâˆˆNi Î±ij(t)h(l) j (t) îµ¹îµ¹îµ¹ 2] . (4) Note that Î±ij and h(vj) that are inferred during training may vary over steps tâ€™s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ï¬rst is a function of qi, which we refer to as the effective variance: Ve(qi) = âˆ‘ jâˆˆNi 1 qij Î±2 ijâˆ¥hjâˆ¥2 , (5) while the second does not depend on qi, and we denote it by Vc = îµ¹îµ¹îµ¹âˆ‘ jâˆˆNi Î±ijhj îµ¹îµ¹îµ¹ 2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: qâ‹† ij = Î±ijâˆ¥h(l) j âˆ¥2 âˆ‘ kâˆˆNi Î±ikâˆ¥h(l) k âˆ¥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighborsâ€™ embeddings in the denominator of Eq.(6). Moreover, the Î±ijâ€™s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theÎ±ijâ€™s are ï¬xed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ï¬xed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several â€œlayer samplingâ€ approaches [11, 6, 14, 25] have been proposed to alleviate the â€œneigh- bor explosionâ€ problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efï¬cient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two â€œgraph samplingâ€ approaches [7, 24] ï¬rst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with â€œlayer samplingâ€ approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 â‰¤tâ‰¤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Qâ‹† i = argmin Qi âˆ‘T t=1 Vt e(Qi), such that âˆ‘T t=1 Vt e(Qt i) â‰¤câˆ‘T t=1 Vt e(Qâ‹† i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si âŠ‚Ni where Si âˆ¼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) âˆ’Vt e(Qâ‹† i) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Qâ‹† i we have the upper bound derived on right hand of Eq. (7). We deï¬ne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Qâ‹† i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = âˆ’âˆ‡Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ï¬xed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deï¬ne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs âŠ‚Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =âˆ’âˆ‡qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efï¬cient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si âŠ‚Ni that satisï¬es âˆ‘ Si:jâˆˆSi Qi,Si = qij,âˆ€vj âˆˆNi, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward âˆ’âˆ‡Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ï¬nd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j âˆˆNi else 0, wij(1) = 1if j âˆˆNi else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deï¬ned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect viâ€™sksampled neighbors vj âˆˆSt i, and rewards rt i = {rij(t) :vj âˆˆSt i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ï¬nally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deï¬ned only at the (l+ 1)-th layer, hence we should maintain multiple qiâ€™s at each layer. In practice, we ï¬nd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qiâ€™s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ï¬rst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ï¬rst assume the weights Î±ijâ€™s are ï¬xed, then extend to attentive GNNs that Î±ij(t)â€™s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator Ë†Âµi = 1 k kâˆ‘ s=1 Î±ijs qijs Ë†hjs, js âˆ¼qi. (8) This yields the variance V(qi) = 1 k Eqi [îµ¹îµ¹îµ¹ Î±ijs qijs hjs âˆ’âˆ‘ jâˆˆNi Î±ijhj îµ¹îµ¹îµ¹ 2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =âˆ’âˆ‡qij(t)Vt e(qt i) = Î±2 ij kÂ·qij(t)2 âˆ¥hj(t)âˆ¥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisï¬es âˆ‘ Si:jâˆˆSi Qi,Si = qij,âˆ€vj âˆˆ Ni, where Si âŠ‚Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. Ë†Âµi = âˆ‘ jsâˆˆSi Î±ijs qijs hjs is the unbiased estimator of Âµi = âˆ‘ jâˆˆNi Î±ijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =âˆ‘ SiâŠ‚Ni Qi,Siâˆ¥âˆ‘ jsâˆˆSi Î±ijs qijs hjsâˆ¥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensenâ€™s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) â‰¤âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2. 5Table 1: Dataset summary. â€œsâ€ dontes multi-class task, and â€œmâ€ denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =âˆ‘ jsâˆˆSi Î±ijs qijs(t)2 âˆ¥hjs(t)âˆ¥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = Î±ij qij(t)2 âˆ¥hj(t)âˆ¥2,âˆ€j âˆˆSi. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value Î±ij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ËœÎ±ij. We deï¬ne the adjusted feedback attention values as follows: Î±â€² ij = âˆ‘ jâˆˆSi qij Â· ËœÎ±ijâˆ‘ jâˆˆSi ËœÎ±ij , (10) where ËœÎ±ijâ€™s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use âˆ‘ jâˆˆSi qij as a surrogate of âˆ‘ jâˆˆSi ËœÎ±ij âˆ‘ jâˆˆNi ËœÎ±ij so that we can approximate the truth attention values Î±ij by our adjusted attention values Î±â€² ij. 6 Regret Analysis As we described in section 4, the regret is deï¬ned as âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with Î·= 0.4 and Î´= âˆš (1âˆ’Î·)Î·4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1â‰¤tâ‰¤T, we have Tâˆ‘ t=1 Vt e(Qt i) â‰¤3 Tâˆ‘ t=1 Vt e(Qâ‹† i) + 10 âˆš Tn4 ln(n/k) k3 (11) where T â‰¥ln(n/k)n2(1 âˆ’Î·)/(kÎ·2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability qâ‹† i and using the reward deï¬nition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ï¬xed variance given the speciï¬c estimators, this is the ï¬rst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ï¬x the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(Â±0.014) 0.890(Â±0.002) 0.689(Â±0.005) 0.949(Â±0.001) 0.494(Â±0.001) FastGCN 0.827(Â±0.001) 0.895(Â±0.005) 0.502(Â±0.003) 0.825(Â±0.006) 0.500(Â±0.001) LADIES 0.843(Â±0.003) 0.880(Â±0.006) 0.574(Â±0.003) 0.932(Â±0.001) 0.465(Â±0.007) AS-GCN 0.830(Â±0.001) 0.888(Â±0.006) 0.599(Â±0.004) 0.890(Â±0.013) 0.506(Â±0.012) S-GCN 0.828(Â±0.001) 0.893(Â±0.001) 0.744(Â±0.003) 0.943(Â±0.001) 0.501(Â±0.002) ClusterGCN 0.807(Â±0.006) 0.887(Â±0.001) 0.853(Â±0.001) 0.938(Â±0.002) 0.418(Â±0.002) GraphSAINT 0.815(Â±0.012) 0.899(Â±0.002) 0.787(Â±0.003) 0.965(Â±0.001) 0.507(Â±0.001) GCN-BS 0.855(Â±0.005) 0.903(Â±0.001) 0.905(Â±0.003) 0.957(Â±0.000) 0.513(Â±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(Â±0.001) 0.884(Â±0.003) 0.566(Â±0.002) NA 0.472(Â±0.012) GraphSAINT-GAT0.773(Â±0.036) 0.886(Â±0.016) 0.789(Â±0.001) 0.933(Â±0.012) 0.470(Â±0.002) GAT-BS 0.857(Â±0.003) 0.894(Â±0.001) 0.841(Â±0.001) 0.962(Â±0.001) 0.513(Â±0.001) GAT-BS.M 0.857(Â±0.003) 0.894(Â±0.000) 0.867(Â±0.003) 0.962(Â±0.000) 0.513(Â±0.001) GP-BS 0.811(Â±0.002) 0.890(Â±0.003) 0.958(Â±0.001) 0.964(Â±0.000) 0.507(Â±0.000) GP-BS.M 0.811(Â±0.001) 0.892(Â±0.001) 0.965(Â±0.001) 0.964(Â±0.000) 0.507(Â±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are â€œgraph samplingâ€ techniques that ï¬rst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theâ„“2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ï¬rst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ï¬‚ickr as 200 by doing grid search. We set the architecture of GraphSAINT as â€œ0-1-1â€3 which means MLP layer followed by two graph convolution layers. We use the â€œrwâ€ sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiï¬cantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciï¬ed. As a result, their variances are simply ï¬xed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ï¬nd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighborsâ€™ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48â€“77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efï¬cient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257â€“266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702â€“2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530â€“6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324â€“360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024â€“1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 33, pages 946â€“953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558â€“4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiï¬cation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077â€“2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 33, pages 4424â€“4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593â€“607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiï¬ca- tion in network data. AI magazine, 29(3):93â€“93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375â€“389. Springer, 2010. [22] P. VeliË‡ckoviÂ´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247â€“11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: Î·= 0.4, sample size k, neighbor size n= |Ni|, Î´= âˆš (1 âˆ’Î·)Î·4k5 ln(n/k)/(Tn4). 1: Set Ë†rij(t) =rij(t)/qij(t) if j âˆˆSt i else 0 wij(t+ 1) =wij(t) exp(Î´Ë†rij(t)/n) 2: Set qij(t+ 1)â†(1 âˆ’Î·) wij(t+1)âˆ‘ jâˆˆNi wij(t+1) + Î· n, for j âˆˆNi Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: Î· = 0.4, sample size k, neighbor size n = |Ni|, Î´ = âˆš (1 âˆ’Î·)Î·4k5 ln(n/k)/(Tn4), Ut i = âˆ…. 1: For j âˆˆNi set Ë†rij(t) = {rij(t)/qij(t) if j âˆˆSt i 0 otherwise wij(t+ 1) = {wij(t) exp(Î´Ë†rij(t)/n) if j /âˆˆUt i wij(t) otherwise 2: if maxjâˆˆNi wij(t+ 1)â‰¥( 1 k âˆ’Î· n) âˆ‘ jâˆˆNi wij(t+ 1)/(1 âˆ’Î·) then 3: Decide at so as to satisfy atâˆ‘ wij(t+1)â‰¥at at + âˆ‘ wij(t+1)<at wij(t+ 1)= (1 k âˆ’Î· n)/(1 âˆ’Î·) 4: Set Ut+1 i = {j : wij(t+ 1)â‰¥at} 5: else 6: Set Ut+1 i = âˆ… 7: end if 8: Set wâ€² ij(t+ 1) = {wij(t+ 1) if j âˆˆNi\\Ut+1 i at if j âˆˆUt i 9: Set qij(t+ 1) =k ( (1 âˆ’Î·) wâ€² ij(t+1)âˆ‘ jâˆˆNi wâ€² ij(t+1) + Î· n ) for j âˆˆNi Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with âˆ‘K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set Î² = min{1 âˆ’qi,qj}and Î³ = min{qi,1 âˆ’qj} 6: Update qi and qj as (qi,qj) = { (qi + Î²,qj âˆ’Î²) with probability Î³ Î²+Î³ (qi âˆ’Î³,qj + Î³) with probability Î² Î²+Î³ 7: end while 8: return {i: qi = 1,1 â‰¤iâ‰¤K} 12B Proofs Proposition 1. Ë†Âµi = âˆ‘ jsâˆˆSi Î±ijs qijs hjs is the unbiased estimator of Âµi = âˆ‘ jâˆˆNi Î±ijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si âŠ‚Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy âˆ‘ Si:jâˆˆSi Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[Ë†Âµi] =E ï£® ï£°âˆ‘ jsâˆˆSi Î±ijs qijs hjs ï£¹ ï£» (12) = âˆ‘ SiâŠ‚Ni Qi,Si âˆ‘ jsâˆˆSi Î±ijs qijs hjs (13) = âˆ‘ jâˆˆNi âˆ‘ Si:jâˆˆSi Qi,Si Î±ij qij hj (14) = âˆ‘ jâˆˆNi Î±ij qij hj âˆ‘ Si:jâˆˆSi Qi,Si (15) = âˆ‘ jâˆˆNi Î±ij qij hjqij (16) = âˆ‘ jâˆˆNi Î±ijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) â‰¤âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2. Proof. The variance is V(Qi) =E ï£® ï£¯ï£° îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ jsâˆˆSi Î±ijs qijs hjs âˆ’ âˆ‘ jâˆˆNi Î±ijhj îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2ï£¹ ï£ºï£» = âˆ‘ SiâŠ‚Ni Qi,Si îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ jsâˆˆSi Î±ijs qijs hjs îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2 âˆ’ îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ jâˆˆNi Î±ijhj îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2 . Therefore the effective variance has following upper bound: Ve(Qi) = âˆ‘ SiâŠ‚Ni Qi,Si îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ jsâˆˆSi Î±ijs qijs hjs îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2 â‰¤ âˆ‘ SiâŠ‚Ni Qi,Si âˆ‘ jsâˆˆSi Î±ijs îµ¹îµ¹îµ¹îµ¹ hjs qijs îµ¹îµ¹îµ¹îµ¹ 2 (Jensenâ€²sInequality ) = âˆ‘ jsâˆˆNi âˆ‘ Si:jsâˆˆSi Qi,SiÎ±ijs îµ¹îµ¹îµ¹îµ¹ hjs qijs îµ¹îµ¹îµ¹îµ¹ 2 = âˆ‘ jsâˆˆNi Î±ijs q2 ijs âˆ¥hjsâˆ¥2 âˆ‘ Si:jsâˆˆSi Qi,Si = âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 13Proposition 3. The negative derivative of the approximated effective variance âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =âˆ‘ jsâˆˆSi Î±ijs qijs(t)2 âˆ¥hjs(t)âˆ¥2. Proof. Deï¬ne the upper bound as Ë†Ve(Qi) =âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2, then its derivative is âˆ‡Qi,Si Ë†Ve(Qi) =âˆ‡Qi,Si âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 = âˆ‡Qi,Si âˆ‘ jsâˆˆNi Î±ijsâˆ‘ Sâ€² i:jsâˆˆSâ€² i Qi,Sâ€² i âˆ¥hjsâˆ¥2 = âˆ‡Qi,Si âˆ‘ jsâˆˆSi Î±ijsâˆ‘ Sâ€² i:jsâˆˆSâ€² i Qi,Sâ€² i âˆ¥hjsâˆ¥2 = âˆ’ âˆ‘ jsâˆˆSi Î±js q2 ijs âˆ¥hjsâˆ¥2 (chainrule) Before we give the proof of Theorem 1, we ï¬rst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantÎ·â‰¤1 and any valid distributions Qt i and Qâ‹† i we have (1 âˆ’2Î·)Vt e(Qt i) âˆ’(1 âˆ’Î·)Vt e(Qâ‹† i) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©+ Î·âŸ¨Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ© (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Qâ‹† i we have Vt e(Qt i) âˆ’Vt e(Qâ‹† i) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©. (19) Multiplying both sides of this inequality by 1 âˆ’Î·, we have (1 âˆ’Î·)Vt e(Qt i) âˆ’(1 âˆ’Î·)Vt e(Qâ‹† i) (20) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©âˆ’Î·âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have âŸ¨Qt i,âˆ‡Qt i Vt e(Qt i)âŸ©= âˆ’ âˆ‘ jâˆˆNi qij(t) Î±2 ij kÂ·qij(t)2 âˆ¥hj(t)âˆ¥2 (22) = âˆ’Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective varianceâˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have âŸ¨Qt i,âˆ‡Qt i Vt e(Qt i)âŸ©= âˆ’ âˆ‘ SiâŠ‚Ni Qt i,Si âˆ‘ jsâˆˆSi Î±ijs qijs(t)2 âˆ¥hjsâˆ¥2 (24) = âˆ’ âˆ‘ jsâˆˆNi Î±ijs qijs(t)2 âˆ¥hjsâˆ¥2 âˆ‘ Si:jsâˆˆSi Qt i,Si (25) = âˆ’ âˆ‘ jsâˆˆNi Î±ijs qijs(t)âˆ¥hjsâˆ¥2 (26) = âˆ’Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 âˆ’Î·)Vt e(Qt i) âˆ’(1 âˆ’Î·)Vt e(Qâ‹† i) (28) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©âˆ’Î·âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ© (29) = âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©+ Î·âŸ¨Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©+ Î·Vt e(Qt i). (30) Theorem 1. Using Algorithm 1 with Î·= 0.4 and Î´ = âˆš (1 âˆ’Î·)Î·4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1â‰¤tâ‰¤T, we have Tâˆ‘ t=1 Vt e(Qt i) â‰¤3 Tâˆ‘ t=1 Vt e(Qâ‹† i) + 10 âˆš Tn4 ln(n/k) k3 (31) where T â‰¥ln(n/k)n2(1 âˆ’Î·)/(kÎ·2) and n= |Ni|. Proof. First we explain why condition T â‰¥ln(n/k)n2(1 âˆ’Î·)/(kÎ·2) ensures that Î´Ë†rij(t) â‰¤1, Î´Ë†rij(t) = âˆš (1 âˆ’Î·)Î·4k5 ln(n/k) Tn4 Â·Î±ij(t) q3 ij(t) âˆ¥hj(t)âˆ¥2 (32) â‰¤ âˆš (1 âˆ’Î·)Î·4k5 ln(n/k) Tn4 Â· n3 k3Î·3 (33) â‰¤1 (34) Assuming âˆ¥hj(t)âˆ¥â‰¤ 1, inequality (33) holds because Î±ij(t) â‰¤1 and qij(t) â‰¥kÎ·/n. Then replace T by the condition, we get Î´Ë†rij(t) â‰¤1. Let Wi(t), Wâ€² i(t) denote âˆ‘ jâˆˆNi wij(t), âˆ‘ jâˆˆNi wâ€² ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = âˆ‘ jâˆˆNi\\Ut i wij(t+ 1) Wi(t) + âˆ‘ jâˆˆUt i wij(t+ 1) Wi(t) (35) = âˆ‘ jâˆˆNi\\Ut i wij(t) Wi(t) Â·exp(Î´Ë†rij(t)) + âˆ‘ jâˆˆUt i wij(t) Wi(t) (36) â‰¤ âˆ‘ jâˆˆNi\\Ut i wij(t) Wi(t) [ 1 +Î´Ë†rij(t) + (Î´Ë†rij(t))2] + âˆ‘ jâˆˆUt i wij(t) Wi(t) (37) = 1 +Wâ€² i (t) Wi(t) âˆ‘ jâˆˆNi\\Ut i wij(t) Wâ€² i (t) [ Î´Ë†rij(t) + (Î´Ë†rij(t))2] (38) = 1 +Wâ€² i (t) Wi(t) âˆ‘ jâˆˆNi\\Ut i qij(t)/kâˆ’Î·/n 1 âˆ’Î· [ Î´Ë†rij(t) + (Î´Ë†rij(t))2] (39) â‰¤1 + Î´ k(1 âˆ’Î·) âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†r2 ij(t) (40) Inequality (37) uses ea â‰¤1 +a+ a2 for aâ‰¤1. Equality (39) holds because of update equation of qij(t) deï¬ned in EXP3.M. Inequality (40) holds because Wâ€² i (t) Wi(t) â‰¤1. Since 1 +xâ‰¤ex for xâ‰¥0, we have ln Wi(t+ 1) Wi(t) â‰¤ Î´ k(1 âˆ’Î·) âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†r2 ij(t) (41) 15If we sum, for 1 â‰¤tâ‰¤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = Tâˆ‘ t=1 ln Wi(t+ 1) Wi(t) (42) â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†r2 ij(t) (43) â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)Ë†r2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) â‰¥ln âˆ‘ jâˆˆS wij(T + 1) Wi(1) (45) â‰¥ âˆ‘ jâˆˆS ln wij(T + 1) k âˆ’ln n k (46) â‰¥Î´ k âˆ‘ jâˆˆS âˆ‘ t:j /âˆˆUt i Ë†rij(t) âˆ’ln n k (47) The inequality (46) uses the fact that âˆ‘ jâˆˆS wij(T + 1)â‰¥k( âˆ jâˆˆS wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(Î´ âˆ‘ t:j /âˆˆUt i Ë†rij(t)) From (44) and (47), we get Î´ k âˆ‘ jâˆˆS âˆ‘ t:j /âˆˆUt i Ë†rij(t) âˆ’ln n k â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†r2 ij(t) (48) And we have the following inequality Î´ k âˆ‘ jâˆˆS âˆ‘ t:jâˆˆUt i rij(t) = Î´ k âˆ‘ jâˆˆS âˆ‘ t:jâˆˆUt i qij(t)Ë†rij(t) (49) â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆUt i qij(t)Ë†rij(t) (50) The equality (49) holds beacuse rij(t) =qijË†rij(t) when j âˆˆSt i and Ut i âŠ†St i bacause qt ij = 1for all j âˆˆUt i. Then add inequality (50) in (48) we have Î´ k âˆ‘ jâˆˆS âˆ‘ t:jâˆˆUt i rij(t) +Î´ k âˆ‘ jâˆˆS âˆ‘ t:j /âˆˆUt i Ë†rij(t) âˆ’ln n k (51) â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)Ë†r2 ij(t) (52) Given qij(t) we have E[Ë†r2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that Î´ k Tâˆ‘ t=1 âˆ‘ jâˆˆS rij(t) âˆ’ln n k â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi r2 ij(t) (53) 16By multiplying (53) by Qâ‹† i,S and summing over S, we get Î´ k Tâˆ‘ t=1 âˆ‘ SâŠ‚Ni Qâ‹† i,S âˆ‘ jâˆˆS rij(t) âˆ’ln n k â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi r2 ij(t) (54) As âˆ‘ jâˆˆNi qij(t)rij(t) = âˆ‘ jâˆˆNi âˆ‘ Si:jâˆˆSi Qt i,Sirij(t) (55) = âˆ‘ SiâŠ‚Ni Qt i,Si âˆ‘ jâˆˆSi rij(t) (56) = âˆ’ âˆ‘ SiâŠ‚Ni Qt i,Siâˆ‡Qt i,Si Vt e(Qt i,Si) (57) = âˆ’âŸ¨Qt i,âˆ‡Qt i Vt e(Qt i)âŸ© (58) By plugging (58) in (54) and rearranging it, we ï¬nd Tâˆ‘ t=1 âŸ¨Qt i âˆ’Qâ‹† i ,âˆ‡Qt i Vt e(Qt i)âŸ©+ Î· Tâˆ‘ t=1 âŸ¨Qâ‹† i ,âˆ‡Qt i Vt e(Qt i)âŸ© (59) â‰¤Î´ Tâˆ‘ t=1 âˆ‘ jâˆˆNi r2 ij(t) +(1 âˆ’Î·)k Î´ ln(n/k) Using Lemma 1, we have (1 âˆ’2Î·) Tâˆ‘ t=1 Vt e(Qt i) âˆ’(1 âˆ’Î·) Tâˆ‘ t=1 Vt e(Qâ‹† i ) â‰¤Î´ Tâˆ‘ t=1 âˆ‘ jâˆˆNi r2 ij(t) +(1 âˆ’Î·)k Î´ ln(n/k) (60) Finally, we know that âˆ‘ jâˆˆNi r2 ij(t) = âˆ‘ jâˆˆNi Î±ij(t)2 qij(t)4 (61) â‰¤ âˆ‘ jâˆˆNi Î±ij(t) n4 k4Î·4 (becauseqij(t) â‰¥kÎ·/n) (62) = n4 k4Î·4 (63) By setting Î·= 0.4 and Î´= âˆš (1 âˆ’Î·)Î·4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other â€œlayer samplingâ€ approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between â€œgraph samplingâ€ and â€œlayer samplingâ€ paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with â€œgraph samplingâ€ approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ï¬‚oating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the â€œlayer samplingâ€ paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. â€œGraph samplingâ€ approaches are not applicable to cases where only partial vertices have labels. To summarize, the â€œlayer samplingâ€ approaches are more ï¬‚exible and general compared with â€œgraph samplingâ€ approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ï¬nd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      }
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth",
      "abstract": "Graph Neural Networks (GNNs) have been studied through the lens of expressive\npower and generalization. However, their optimization properties are less well\nunderstood. We take the first step towards analyzing GNN training by studying\nthe gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that\ndespite the non-convexity of training, convergence to a global minimum at a\nlinear rate is guaranteed under mild assumptions that we validate on real-world\ngraphs. Second, we study what may affect the GNNs' training speed. Our results\nshow that the training of GNNs is implicitly accelerated by skip connections,\nmore depth, and/or a good label distribution. Empirical results confirm that\nour theoretical results for linearized GNNs align with the training behavior of\nnonlinear GNNs. Our results provide the first theoretical support for the\nsuccess of GNNs with skip connections in terms of optimization, and suggest\nthat deep GNNs with skip connections would be promising in practice.",
      "full_text": "Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Keyulu Xu * 1 Mozhi Zhang 2 Stefanie Jegelka 1 Kenji Kawaguchi * 3 Abstract Graph Neural Networks (GNNs) have been stud- ied through the lens of expressive power and gen- eralization. However, their optimization proper- ties are less well understood. We take the ï¬rst step towards analyzing GNN training by study- ing the gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that despite the non-convexity of training, convergence to a global minimum at a linear rate is guaranteed under mild assumptions that we validate on real- world graphs. Second, we study what may affect the GNNsâ€™ training speed. Our results show that the training of GNNs is implicitly accelerated by skip connections, more depth, and/or a good label distribution. Empirical results conï¬rm that our theoretical results for linearized GNNs align with the training behavior of nonlinear GNNs. Our results provide the ï¬rst theoretical support for the success of GNNs with skip connections in terms of optimization, and suggest that deep GNNs with skip connections would be promising in practice. 1. Introduction Graph Neural Networks (GNNs) (Gori et al., 2005; Scarselli et al., 2009) are an effective framework for learning with graphs. GNNs learn node representations on a graph by extracting high-level features not only from a node itself but also from a nodeâ€™s surrounding subgraph. Speciï¬- cally, the node representations are recursively aggregated and updated using neighbor representations (Merkwirth & Lengauer, 2005; Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Gilmer et al., 2017; Hamilton et al., 2017; Velickovic et al., 2018; Liao et al., 2020). Recently, there has been a surge of interest in studying the *Equal contribution 1Massachusetts Institute of Technology (MIT) 2The University of Maryland 3Harvard University. Corre- spondence to: Keyulu Xu <keyulu@mit.edu>, Kenji Kawaguchi <kkawaguchi@fas.harvard.edu>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). theoretical aspects of GNNs to understand their success and limitations. Existing works have studied GNNsâ€™ expressive power (Keriven & PeyrÂ´e, 2019; Maron et al., 2019; Chen et al., 2019; Xu et al., 2019; Sato et al., 2019; Loukas, 2020), generalization capability (Scarselli et al., 2018; Du et al., 2019b; Xu et al., 2020; Garg et al., 2020), and extrapolation properties (Xu et al., 2021). However, the understanding of the optimization properties of GNNs has remained lim- ited. For example, researchers working on the fundamental problem of designing more expressive GNNs hope and of- ten empirically observe that more powerful GNNs better ï¬t the training set (Xu et al., 2019; Sato et al., 2020; Vignac et al., 2020). Theoretically, given the non-convexity of GNN training, it is still an open question whether better represen- tational power always translates into smaller training loss. This motivates the more general questions: Can gradient descent ï¬nd a global minimum for GNNs? What affects the speed of convergence in training? In this work, we take an initial step towards answering the questions above by analyzing the trajectory of gradient de- scent, i.e., gradient dynamics or optimization dynamics. A complete understanding of the dynamics of GNNs, and deep learning in general, is challenging. Following prior works on gradient dynamics (Saxe et al., 2014; Arora et al., 2019a; Bartlett et al., 2019), we consider the linearized regime, i.e., GNNs with linear activation. Despite the linearity, key prop- erties of nonlinear GNNs are present: The objective function is non-convex and the dynamics are nonlinear (Saxe et al., 2014; Kawaguchi, 2016). Moreover, we observe the learn- ing curves of linear GNNs and ReLU GNNs are surprisingly similar, both converging to nearly zero training loss at the same linear rate (Figure 1). Similarly, prior works report comparable performance in node classiï¬cation benchmarks even if we remove the non-linearities (Thekumparampil et al., 2018; Wu et al., 2019). Hence, understanding the dynamics of linearized GNNs is a valuable step towards understanding the general GNNs. Our analysis leads to an afï¬rmative answer to the ï¬rst ques- tion. We establish that gradient descent training of a lin- earized GNN with squared loss converges to a global mini- mum at a linear rate. Experiments conï¬rm that the assump- arXiv:2105.04550v2  [cs.LG]  26 May 2021Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 3000 7000 10000 Iteration 10 3 10 2 10 1 100 Training Loss linear GIN ReLU GIN 0 3000 7000 10000 Iteration 10 2 10 1 100 Training Loss linear GCN ReLU GCN Figure 1.Training curves of linearized GNNs vs. ReLU GNNs on the Cora node classiï¬cation dataset. tions of our theoretical results for global convergence hold on real-world datasets. The most signiï¬cant contribution of our convergence analysis is on multiscale GNNs, i.e., GNN architectures that use skip connections to combine graph features at various scales (Xu et al., 2018; Li et al., 2019; Abu-El-Haija et al., 2020; Chen et al., 2020; Li et al., 2020). The skip connections introduce complex interactions among layers, and thus the resulting dynamics are more intricate. To our knowledge, our results are the ï¬rst convergence re- sults for GNNs with more than one hidden layer, with or without skip connections. We then study what may affect the training speed of GNNs. First, for any ï¬xed depth, GNNs with skip connections train faster. Second, increasing the depth further accelerates the training of GNNs. Third, faster training is obtained when the labels are more correlated with the graph features, i.e., labels contain â€œsignalâ€ instead of â€œnoiseâ€. Overall, experiments for nonlinear GNNs agree with the prediction of our theory for linearized GNNs. Our results provide the ï¬rst theoretical justiï¬cation for the empirical success of multiscale GNNs in terms of optimiza- tion, and suggest that deeper GNNs with skip connections may be promising in practice. In the GNN literature, skip connections are initially motivated by the â€œover-smoothingâ€ problem (Xu et al., 2018): via the recursive neighbor aggre- gation, node representations of a deep GNN on expander- like subgraphs would be mixing features from almost the entire graph, and may thereby â€œwash outâ€ relevant local in- formation. In this case, shallow GNNs may perform better. Multiscale GNNs with skip connections can combine and adapt to the graph features at various scales, i.e., the out- put of intermediate GNN layers, and such architectures are shown to help with this over-smoothing problem (Xu et al., 2018; Li et al., 2019; 2020; Abu-El-Haija et al., 2020; Chen et al., 2020). However, the properties of multiscale GNNs have mostly been understood at a conceptual level. Xu et al. (2018) relate the learned representations to random walk distributions and Oono & Suzuki (2020) take a boosting view, but they do not consider the optimization dynamics. We give an explanation from the lens of optimization. The training losses of deeper GNNs may be worse due to over- smoothing. In contrast, multiscale GNNs can express any shallower GNNs and fully exploit the power by converging to a global minimum. Hence, our results suggest that deeper GNNs with skip connections are guaranteed to train faster with smaller training losses. We present our results on global convergence in Section 3, after introducing relevant background (Section 2). In Sec- tion 4, we compare the training speed of GNNs as a function of skip connections, depth, and the label distribution. All proofs are deferred to the Appendix. 2. Preliminaries 2.1. Notation and Background We begin by introducing our notation. Let G= (V,E) be a graph with nvertices V = {v1,v2,Â·Â·Â· ,vn}. Its adjacency matrix A âˆˆRnÃ—n has entries Aij = 1 if (vi,vj) âˆˆE and 0 otherwise. The degree matrix associated with Ais D = diag (d1,d2,...,d n) with di = âˆ‘ n j=1 Aij. For any matrix M âˆˆRmÃ—mâ€² , we denote its j-th column vector by Mâˆ—j âˆˆRm, its i-th row vector by Miâˆ— âˆˆRmâ€² , and its largest and smallest (i.e., min(m,mâ€²)-th largest) singular values by Ïƒmax(M) and Ïƒmin(M), respectively. The data matrix X âˆˆRmxÃ—n has columns Xâˆ—j corresponding to the feature vector of node vj, with input dimension mx. The task of interest is node classiï¬cation or regression. Each node vi âˆˆV has an associated label yi âˆˆRmy . In the transductive (semi-supervised) setting, we have access to training labels for only a subset IâŠ† [n] of nodes on G, and the goal is to predict the labels for the other nodes in [n] \\I. Our problem formulation easily extends to the inductive setting by letting I= [n], and we can use the trained model for prediction on unseen graphs. Hence, we have access to Â¯n= |I|â‰¤ ntraining labels Y = [yi]iâˆˆIâˆˆRmyÃ—Â¯n, and we train the GNN using X,Y,G . Additionally, for any M âˆˆ RmÃ—mâ€² , Imay index sub-matrices Mâˆ—I = [ Mâˆ—i]iâˆˆI âˆˆ RmÃ—Â¯n (when mâ€² â‰¥n) and MIâˆ— = [ Miâˆ—]iâˆˆI âˆˆRÂ¯nÃ—m (when mâ‰¥n). Graph Neural Networks (GNNs) use the graph structure and node features to learn representations of nodes (Scarselli et al., 2009). GNNs maintain hidden representations hv (l) âˆˆ Rml for each node v, where ml is the hidden dimension on the l-th layer. We let X(l) = [ h1 (l),h2 (l),Â·Â·Â· ,hn (l) ] âˆˆ RmlÃ—n, and set X(0) as the input features X. The node hidden representations X(l) are updated by aggregating and transforming the neighbor representations: X(l) = Ïƒ ( B(l)X(lâˆ’1)S ) âˆˆRmlÃ—n, (1) where Ïƒis a nonlinearity such as ReLU, B(l) âˆˆRmlÃ—mlâˆ’1 is the weight matrix, and S âˆˆRnÃ—n is the GNN aggrega- tion matrix, whose formula depends on the exact variant of GNN. In Graph Isomorphism Networks (GIN) (Xu et al.,Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Cora CiteSeer 10 20 10 13 10 5 103 GCN GIN (a) Graph Ïƒ2 min(X(SH)âˆ—I) 0 1500 3000 Iteration (T) 5.07 5.08T 1e 3  (b) Time-dependent Î»(H) T 10 6  10 5  10 4  10 3  10 2 T  (c) limTâ†’âˆÎ»(H) T across training settings Figure 2.Empirical validation of assumptions for global convergence of linear GNNs. Left panel conï¬rms the graph condition Ïƒ2 min(X(SH)âˆ—I) >0 for datasets Cora and Citeseer, and for models GCN and GIN. Middle panel shows the time-dependent Î»(H) T for one training setting (linear GCN on Cora). Each point in right panel is Î»(H) T >0 at the last iteration for different training settings. 2019), S = A+ In is the adjacency matrix of Gwith self- loop, where In âˆˆRnÃ—n is an identity matrix. In Graph Convolutional Networks (GCN) (Kipf & Welling, 2017), S = Ë†Dâˆ’1 2 (A+In) Ë†Dâˆ’1 2 is the normalized adjacency matrix, where Ë†Dis the degree matrix of A+ In. 2.2. Problem Setup We ï¬rst formally deï¬ne linearized GNNs. Deï¬nition 1. (Linear GNN). Given data matrix X âˆˆ RmxÃ—n, aggregation matrix S âˆˆ RnÃ—n, weight matri- ces W âˆˆRmyÃ—mH , B(l) âˆˆRmlÃ—mlâˆ’1, and their collec- tion B = (B(1),...,B (H)), a linear GNN with H layers f(X,W,B ) âˆˆRmyÃ—n is deï¬ned as f(X,W,B ) = WX(H), X (l) = B(l)X(lâˆ’1)S. (2) Throughout this paper, we refer multiscale GNNs to the commonly used Jumping Knowledge Network (JK-Net) (Xu et al., 2018), which connects the output of all intermediate GNN layers to the ï¬nal layer with skip connections: Deï¬nition 2. (Multiscale linear GNN). Given data X âˆˆ RmxÃ—n, aggregation matrix S âˆˆ RnÃ—n, weight matri- ces W(l) âˆˆ RmyÃ—ml, B(l) âˆˆ RmlÃ—mlâˆ’1 with W = (W(0),W(1),...,W (H)), a multiscale linear GNN with H layers f(X,W,B ) âˆˆRmyÃ—n is deï¬ned as f(X,W,B ) = Hâˆ‘ l=0 W(l)X(l), (3) X(l) = B(l)X(lâˆ’1)S. (4) Given a GNN f(Â·) and a loss function â„“(Â·,Y ), we can train the GNN by minimizing the training loss L(W,B): L(W,B) = â„“ ( f(X,W,B )âˆ—I,Y ) , (5) where f(X,W,B )âˆ—Icorresponds to the GNNâ€™s predictions on nodes that have training labels and thus incur training losses. The pair (W,B) represents the trainable weights: L(W,B) = L(W(1),...,W (H),B(1),...,B (H)) For completeness, we deï¬ne the global minimum of GNNs. Deï¬nition 3. (Global minimum). For any H âˆˆN0, Lâˆ— H is the global minimum value of the H-layer linear GNN f: Lâˆ— H = inf W,B â„“ ( f(X,W,B )âˆ—I,Y ) . (6) Similarly, we deï¬ne Lâˆ— 1:H as the global minimum value of the multiscale linear GNN f with H layers. We are ready to present our main results on global conver- gence for linear GNNs and multiscale linear GNNs. 3. Convergence Analysis In this section, we show that gradient descent training a linear GNN with squared loss, with or without skip connec- tions, converges linearly to a global minimum. Our condi- tions for global convergence hold on real-world datasets and provably hold under assumptions, e.g., initialization. In linearized GNNs, the loss L(W,B) is non-convex (and non-invex) despite the linearity. The graph aggregation S creates interaction among the data and poses additional chal- lenges in the analysis. We show a ï¬ne-grained analysis of the GNNâ€™s gradient dynamics can overcome these chal- lenges. Following previous works on gradient dynamics (Saxe et al., 2014; Huang & Yau, 2020; Ji & Telgarsky, 2020; Kawaguchi, 2021), we analyze the GNN learning process via the gradient ï¬‚ow, i.e., gradient descent with inï¬nitesimal steps: âˆ€tâ‰¥0,the network weights evolve as d dtWt = âˆ’âˆ‚L âˆ‚W(Wt,Bt), d dtBt = âˆ’âˆ‚L âˆ‚B(Wt,Bt), (7) where (Wt,Bt) represents the trainable parameters at time twith initialization (W0,B0).Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 3.1. Linearized GNNs Theorem 1 states our result on global convergence for lin- earized GNNs without skip connections. Theorem 1. Let f be an H-layer linear GNN and â„“(q,Y ) = âˆ¥q âˆ’Yâˆ¥2 F where q,Y âˆˆRmyÃ—Â¯n. Then, for any T >0, L(WT,BT) âˆ’Lâˆ— H (8) â‰¤(L(W0,B0) âˆ’Lâˆ— H)eâˆ’4Î»(H) T Ïƒ2 min(X(SH)âˆ—I)T, where Î»(H) T is the smallest eigenvalue Î»(H) T := inftâˆˆ[0,T] Î»min(( Â¯B(1:H) t )âŠ¤Â¯B(1:H) t ) and Â¯B(1:l) := B(l)B(lâˆ’1) Â·Â·Â·B(1) for any l âˆˆ {0,...,H } with Â¯B(1:0) := I. Proof. (Sketch) We decompose the gradient dynamics into three components: the graph interaction, non-convex fac- tors, and convex factors. We then bound the effects of the graph interaction and non-convex factors through Ïƒ2 min(X(SH)âˆ—I) and Î»min(( Â¯B(1:H) t )âŠ¤Â¯B(1:H) t ) respectively. The complete proof is in Appendix A.1. Theorem 1 implies that convergence to a global minimum at a linear rate is guaranteed if Ïƒ2 min(X(SH)âˆ—I) > 0 and Î»T > 0. The ï¬rst condition on the product of X and SH indexed by Ionly depends on the node features X and the GNN aggregation matrix S. It is satisï¬ed if rank(X(SH)âˆ—I) = min(mx,Â¯n), because Ïƒmin(X(SH)âˆ—I) is the min(mx,Â¯n)-th largest singular value of X(SH)âˆ—Iâˆˆ RmxÃ—Â¯n. The second condition Î»(H) T >0 is time-dependent and requires a more careful treatment. Linear convergence is implied as long as Î»min(( Â¯B(1:H) t )âŠ¤Â¯B(1:H) t ) â‰¥Ïµ> 0 for all times tbefore stopping. Empirical validation of conditions. We verify both the graph condition Ïƒ2 min(X(SH)âˆ—I) > 0 and the time- dependent condition Î»(H) T > 0 for (discretized) T > 0. First, on the popular graph datasets, Cora and Cite- seer (Sen et al., 2008), and the GNN models, GCN (Kipf & Welling, 2017) and GIN (Xu et al., 2019), we have Ïƒ2 min(X(SH)âˆ—I) > 0 (Figure 2a). Second, we train lin- ear GCN and GIN on Cora and Citeseer to plot an exam- ple of how the Î»(H) T = inf tâˆˆ[0,T] Î»min(( Â¯B(1:H) t )âŠ¤Â¯B(1:H) t ) changes with respect to time T (Figure 2b). We further con- ï¬rm that Î»(H) T > 0 until convergence, limTâ†’âˆÎ»(H) T > 0 across different settings, e.g., datasets, depths, models (Fig- ure 2c). Our experiments use the squared loss, random initialization, learning rate 1e-4, and set the hidden dimen- sion to the input dimension (note that Theorem 1 assumes the hidden dimension is at least the input dimension). Fur- ther experimental details are in Appendix C. Along with Theorem 1, we conclude that linear GNNs converge linearly to a global minimum. Empirically, we indeed see both linear and ReLU GNNs converging at the same linear rate to nearly zero training loss in node classiï¬cation tasks (Figure 1). Guarantee via initialization. Besides the empirical ver- iï¬cation, we theoretically show that a good initialization guarantees the time-dependent condition Î»T > 0 for any T >0. Indeed, like other neural networks, GNNs do not converge to a global optimum with certain initializations: e.g., initializing all weights to zero leads to zero gradients and Î»(H) T = 0 for all T, and hence no learning. We intro- duce a notion of singular margin and say an initialization is good if it has a positive singular margin. Intuitively, a good initialization starts with an already small loss. Deï¬nition 4. (Singular margin). The initialization (W0,B0) is said to have singular margin Î³ >0 with respect to a layer lâˆˆ{1,...,H }if Ïƒmin(B(l)B(lâˆ’1) Â·Â·Â·B(1)) â‰¥Î³ for all (W,B) such that L(W,B) â‰¤L(W0,B0). Proposition 1 then states that an initialization with positive singular margin Î³guarantees Î»(H) T â‰¥Î³2 >0 for all T: Proposition 1. Let f be a linear GNN with H layers and â„“(q,Y ) = âˆ¥q âˆ’Yâˆ¥2 F. If the initialization (W0,B0) has singular margin Î³ > 0 with respect to the layer H and mH â‰¥mx, then Î»(H) T â‰¥Î³2 for all T âˆˆ[0,âˆ). Proposition 1 follows since L(Wt,Bt) is non-increasing with respect to time t(proof in Appendix A.2). Relating to previous works, our singular margin is a general- ized variant of the deï¬ciency margin of linear feedforward networks (Arora et al., 2019a, Deï¬nition 2 and Theorem 1): Proposition 2. (Informal) If initialization (W0,B0) has deï¬ciency margin c> 0, then it has singular margin Î³ >0. The formal version of Proposition 2 is in Appendix A.3. To summarize, Theorem 1 along with Proposition 1 implies that we have a prior guarantee of linear convergence to a global minimum for any graph with rank(X(SH)âˆ—I) = min(mx,Â¯n) and initialization (W0,B0) with singular mar- gin Î³ > 0: i.e., for any desired Ïµ > 0, we have that L(WT,BT) âˆ’Lâˆ— H â‰¤Ïµfor any T such that T â‰¥ 1 4Î³2Ïƒ2 min(X(SH)âˆ—I) log L(A0,B0) âˆ’Lâˆ— H Ïµ . (9) While the margin condition theoretically guarantees linear convergence, empirically, we have already seen that the convergence conditions of across different training settings for widely used random initialization. Theorem 1 suggests that the convergence rate depends on a combination of data features X, the GNN architecture and graph structure via Sand H, the label distribution and initialization via Î»T. For example, GIN has better such con- stants than GCN on the Cora dataset with everything elseOptimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth held equal (Figure 2a). Indeed, in practice, GIN converges faster than GCN on Cora (Figure 1). In general, the com- putation and comparison of the rates given by Theorem 1 requires computation such as those in Figure 2. In Section 4, we will study an alternative way of comparing the speed of training by directly comparing the gradient dynamics. 3.2. Multiscale Linear GNNs Without skip connections, the GNNs under linearization still behave like linear feedforward networks with augmented graph features. With skip connections, the dynamics and analysis become much more intricate. The expressive power of multiscale linear GNNs changes signiï¬cantly as depth increases. Moreover, the skip connections create complex interactions among different layers and graph structures of various scales in the optimization dynamics. Theorem 2 states our convergence results for multiscale linear GNNs in three cases: (i) a general form; (ii) a weaker condition for boundary cases that uses Î»Hâ€² T instead of Î»1:H T ; (iii) a faster rate if we have monotonic expressive power as depth increases. Theorem 2. Let f be a multiscale linear GNN with H layers and â„“(q,Y ) = âˆ¥qâˆ’Yâˆ¥2 F where q,Y âˆˆRmyÃ—Â¯n. Let Î»(1:H) T := min0â‰¤lâ‰¤H Î»(l) T . For any T >0, the following hold: (i) (General). Let GH := [XâŠ¤,(XS)âŠ¤,..., (XSH)âŠ¤]âŠ¤ âˆˆR(H+1)mxÃ—n. Then L(WT,BT) âˆ’Lâˆ— 1:H (10) â‰¤(L(W0,B0) âˆ’Lâˆ— 1:H)eâˆ’4Î»(1:H) T Ïƒ2 min((GH)âˆ—I)T. (ii) (Boundary cases). For any Hâ€²âˆˆ{0,1,...,H }, L(WT,BT) âˆ’Lâˆ— Hâ€² (11) â‰¤(L(W0,B0) âˆ’Lâˆ— Hâ€²)eâˆ’4Î»(Hâ€²) T Ïƒ2 min(X(SHâ€² )âˆ—I)T. (iii) (Monotonic expressive power). If there exist l,lâ€² âˆˆ {0,...,H }with l <lâ€²such that Lâˆ— l â‰¥Lâˆ— l+1 â‰¥Â·Â·Â·â‰¥ Lâˆ— lâ€² or Lâˆ— l â‰¤Lâˆ— l+1 â‰¤Â·Â·Â·â‰¤ Lâˆ— lâ€², then L(WT,BT) âˆ’Lâˆ— lâ€²â€² (12) â‰¤(L(W0,B0) âˆ’Lâˆ— lâ€²â€²)eâˆ’4 âˆ‘lâ€² k=l Î»(k) T Ïƒ2 min(X(Sk)âˆ—I)T, where lâ€²â€²= lif Lâˆ— l â‰¥Lâˆ— l+1 â‰¥Â·Â·Â·â‰¥ Lâˆ— lâ€², and lâ€²â€²= lâ€²if Lâˆ— l â‰¤Lâˆ— l+1 â‰¤Â·Â·Â·â‰¤ Lâˆ— lâ€². Proof. (Sketch) A key observation in our proof is that the interactions of different scales cancel out to point towards a speciï¬c direction in the gradient dynamics induced in a space of the loss value. The complete proof is in Ap- pendix A.4. Similar to Theorem 1 for linear GNNs, the most general form (i) of Theorem 2 implies that convergence to the global minimum value of the entire multiscale linear GNN Lâˆ— 1:H at linear rate is guaranteed when Ïƒ2 min((GH)âˆ—I) > 0 and Î»(1:H) T >0. The graph condition Ïƒ2 min((GH)âˆ—I) >0 is sat- isï¬ed if rank((GH)âˆ—I) = min(mx(H+ 1),Â¯n). The time- dependent condition Î»(1:H) T >0 is guaranteed if the initial- ization (W0,B0) has singular margin Î³ >0 with respect to every layer (Proposition 3 is proved in Appendix A.5): Proposition 3. Let f be a multiscale linear GNN and â„“(q,Y ) = âˆ¥qâˆ’Yâˆ¥2 F. If the initialization (W0,B0) has sin- gular margin Î³ >0 with respect to every layer lâˆˆ[H] and ml â‰¥mx for lâˆˆ[H], then Î»(1:H) T â‰¥Î³2 for all T âˆˆ[0,âˆ). We demonstrate that the conditions of Theorem 2 (i) hold for real-world datasets, suggesting in practice multiscale linear GNNs converge linearly to a global minimum. Empirical validation of conditions. On datasets Cora and Citeseer and for GNN models GCN and GIN, we conï¬rm that Ïƒ2 min((GH)âˆ—I) > 0 (Figure 3a). Moreover, we train multiscale linear GCN and GIN on Cora and Citeseer to plot an example of how the Î»(1:H) T changes with respect to time T (Figure 3b), and we conï¬rm that at convergence,Î»(1:H) T > 0 across different settings (Figure 3c). Experimental details are in Appendix C. Boundary cases. Because the global minimum value of multiscale linear GNNs Lâˆ— 1:H can be smaller than that of linear GNNs Lâˆ— H, the conditions in Theorem 2(i) may some- times be stricter than those of Theorem 1. For example, in Theorem 2(i), we require Î»(1:H) T := min0â‰¤lâ‰¤H Î»(l) T rather than Î»(H) T to be positive. If Î»(l) T = 0 for some l, then Theo- rem 2(i) will not guarantee convergence to Lâˆ— 1:H. Although the boundary cases above did not occur on the tested real-world graphs (Figure 3), for theoretical interest, Theorem 2(ii) guarantees that in such cases, multiscale lin- ear GNNs still converge to a value no worse than the global minimum value of non-multiscale linear GNNs. For any intermediate layer Hâ€², assuming Ïƒ2 min(X(SHâ€² )âˆ—I) >0 and Î»(Hâ€²) T >0, Theorem 2(ii) bounds the loss of the multiscale linear GNN L(WT,BT) at convergence by the global mini- mum value Lâˆ— Hâ€² of the corresponding linear GNN with Hâ€² layers. Faster rate under monotonic expressive power.Theorem 2(iii) considers a special case that is likely in real graphs: the global minimum value of the non-multiscale linear GNN Lâˆ— Hâ€² is monotonic as Hâ€²increases. Then (iii) gives a faster rate than (ii) and linear GNNs. For example, if the globally optimal value decreases as linear GNNs get deeper. i.e., Lâˆ— 0 â‰¥Lâˆ— 1 â‰¥Â·Â·Â·â‰¥ Lâˆ— H, or vice versa, Lâˆ— 0 â‰¤Lâˆ— 1 â‰¤Â·Â·Â·â‰¤Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Cora CiteSeer 10 20 10 13 10 5 103 GCN GIN (a) Graph Ïƒ2 min((GH)âˆ—I) 0 1500 3000 Iteration (T) 4.9 5.0 5.1T 1e 3  (b) Time-dependent Î»(1:H) T 10 6  10 5  10 4  10 3  10 2 T  (c) limTâ†’âˆÎ»(1:H) T across training settings Figure 3.Empirical validation of assumptions for global convergence of multiscale linear GNNs. Left panel conï¬rms the graph condition Ïƒ2 min((GH)âˆ—I) > 0 for Cora and Citeseer, and for GCN and GIN. Middle panel shows the time-dependent Î»(1:H) T for one training setting (multiscale linear GCN on Cora). Each point in right panel is Î»(1:H) T >0 at the last iteration for different training settings. Lâˆ— H, then Theorem 2 (i) implies that L(WT,BT) âˆ’Lâˆ— l (13) â‰¤(L(W0,B0) âˆ’Lâˆ— l)eâˆ’4 âˆ‘H k=0 Î»(k) T Ïƒ2 min(X(Sk)âˆ—I)T, where l = 0 if Lâˆ— 0 â‰¥Lâˆ— 1 â‰¥ Â·Â·Â· â‰¥Lâˆ— H, and l = H if Lâˆ— 0 â‰¤Lâˆ— 1 â‰¤Â·Â·Â·â‰¤ Lâˆ— H. Moreover, if the globally optimal value does not change with respect to the depth as Lâˆ— 1:H = Lâˆ— 1 = Lâˆ— 2 = Â·Â·Â· = Lâˆ— H, then we have L(WT,BT) âˆ’Lâˆ— 1:H (14) â‰¤(L(W0,B0) âˆ’Lâˆ— 1:H)eâˆ’4 âˆ‘H k=0 Î»(k) T Ïƒ2 min(X(Sk)âˆ—I)T. We obtain a faster rate for multiscale linear GNNs than for linear GNNs, as eâˆ’4 âˆ‘H k=0 Î»(k) T Ïƒ2 min(X(Sk)âˆ—I)T â‰¤ eâˆ’4Î»(H) T Ïƒ2 min(X(SH)âˆ—I)T. Interestingly, unlike linear GNNs, multiscale linear GNNs in this case do not require any condition on initialization to obtain a prior guarantee on global convergence since eâˆ’4 âˆ‘H k=0 Î»(k) T Ïƒ2 min(X(Sk)âˆ—I)T â‰¤ eâˆ’4Î»(0) T Ïƒ2 min(X(S0)âˆ—I)T with Î»(0) T = 1 and X(S0)âˆ—I= Xâˆ—I. To summarize, we prove global convergence rates for multi- scale linear GNNs (Thm. 2(i)) and experimentally validate the conditions. Part (ii) addresses boundary cases where the conditions of Part (i) do not hold. Part (iii) gives faster rates assuming monotonic expressive power with respect to depth. So far, we have shown multiscale linear GNNs converge faster than linear GNNs in the case of (iii). Next, we compare the training speed for more general cases. 4. Implicit Acceleration In this section, we study how the skip connections, depth of GNN, and label distribution may affect the speed of training for GNNs. Similar to previous works (Arora et al., 2018), we compare the training speed by comparing the per step loss reduction d dtL(Wt,Bt) for arbitrary differentiable loss functions â„“(Â·,Y ) : Rmy â†’R. Smaller d dtL(Wt,Bt) im- plies faster training. Loss reduction offers a complementary view to the convergence rates in Section 3, since it is instant and not an upper bound. We present an analytical form of the loss reduction d dtL(Wt,Bt) for linear GNNs and multiscale linear GNNs. The comparison of training speed then follows from our for- mula for d dtL(Wt,Bt). For better exposition, we ï¬rst intro- duce several notations. We let Â¯B(lâ€²:l) = B(l)B(lâˆ’1) Â·Â·Â·B(lâ€²) for all lâ€²and lwhere Â¯B(lâ€²:l) = I if lâ€²>l. We also deï¬ne J(i,l),t := [ Â¯B(1:iâˆ’1) t âŠ—(W(l),t Â¯B(i+1:l) t )âŠ¤], F(l),t := [( Â¯B(1:l) t )âŠ¤Â¯B(1:l) t âŠ—Imy ] âª°0, Vt := âˆ‚L(Wt,Bt) âˆ‚Ë†Yt , where Ë†Yt := f(X,Wt,Bt)âˆ—I. For any vector v âˆˆ Rm and positive semideï¬nite matrix M âˆˆ RmÃ—m, we use âˆ¥vâˆ¥2 M := vâŠ¤Mv.1 Intuitively, Vt represents the deriva- tive of the loss L(Wt,Bt) with respect to the model output Ë†Y = f(X,Wt,Bt)âˆ—I. J(i,l),t and F(l),t represent matri- ces that describe how the errors are propagated through the weights of the networks. Theorem 3, proved in Appendix A.6, gives an analytical formula of loss reduction for linear GNNs and multiscale linear GNNs. Theorem 3. For any differentiable loss function q â†¦â†’ â„“(q,Y ), the following hold for any H â‰¥0 and tâ‰¥0: (i) (Non-multiscale) For f as in Deï¬nition 1: d dtL1(Wt,Bt) = âˆ’ îµ¹îµ¹vec[Vt(X(SH)âˆ—I)âŠ¤]îµ¹îµ¹2 F(H),t (15) âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H),tvec[Vt(X(SH)âˆ—I)âŠ¤]îµ¹îµ¹2 2 . 1We use this Mahalanobis norm notation for conciseness with- out assuming it to be a norm, since M may be low rank.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 2000 4000 6000 Iteration 10 1 100 Training Loss multiscale non-multiscale (a) Multiscale vs. non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (b) Depth. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (c) Signal vs. noise. Figure 4.Comparison of the training speed of GNNs. Left: Multiscale GNNs train faster than non-multiscale GNNs. Middle: Deeper GNNs train faster. Right: GNNs train faster when the labels have signals instead of random noise. The patterns above hold for both ReLU and linear GNNs. Additional results are in Appendix B. (ii) (Multiscale) For f as in Deï¬nition 2: d dtL2(Wt,Bt) = âˆ’ Hâˆ‘ l=0 îµ¹îµ¹vec[Vt(X(Sl)âˆ—I)âŠ¤]îµ¹îµ¹2 F(l),t (16) âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l),tvec[Vt(X(Sl)âˆ—I)âŠ¤] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 . In what follows, we apply Theorem 3 to predict how differ- ent factors affect the training speed of GNNs. 4.1. Acceleration with Skip Connections We ï¬rst show that multiscale linear GNNs tend to achieve faster loss reduction d dtL2(Wt,Bt) compared to the corresponding linear GNN without skip connections, d dtL1(Wt,Bt). It follows from Theorem 3 that d dtL2(Wt,Bt) âˆ’ d dtL1(Wt,Bt) (17) â‰¤âˆ’ Hâˆ’1âˆ‘ l=0 îµ¹îµ¹vec [ Vt(X(Sl)âˆ—I)âŠ¤]îµ¹îµ¹2 F(l),t , if âˆ‘ H i=1(âˆ¥aiâˆ¥2 2 + 2 bâŠ¤ i ai) â‰¥ 0, where ai =âˆ‘ Hâˆ’1 l=i J(i,l),tvec[Vt(X(Sl)âˆ—I)âŠ¤], and bi = J(i,H),tvec[ Vt(X(SH)âˆ—I)âŠ¤]. The assumption of âˆ‘ H i=1(âˆ¥aiâˆ¥2 2 +2bâŠ¤ i ai) â‰¥0 is satisï¬ed in various ways: for example, it is satisï¬ed if the last layerâ€™s term bi and the other layersâ€™ terms ai are aligned as bâŠ¤ i ai â‰¥0, or if the last layerâ€™s term bi is dominated by the other layersâ€™ termsai as 2âˆ¥biâˆ¥2 â‰¤âˆ¥aiâˆ¥2. Then equation (17) shows that the multiscale linear GNN decreases the loss value with strictly many more negative terms, suggesting faster training. Empirically, we indeed observe that multiscale GNNs train faster (Figure 4a), both for (nonlinear) ReLU and linear GNNs. We verify this by training multiscale and non- multiscale, ReLU and linear GCNs on the Cora and Citeseer datasets with cross-entropy loss, learning rate 5e-5, and hidden dimension 32. Results are in Appendix B. 4.2. Acceleration with More Depth Our second ï¬nding is that deeper GNNs, with or without skip connections, train faster. For any differentiable loss function qâ†¦â†’â„“(q,Y ), Theorem 3 states that the loss of the multiscale linear GNN decreases as d dtL(Wt,Bt) = âˆ’ Hâˆ‘ l=0 îµ¹îµ¹vec[Vt(X(Sl)âˆ—I)âŠ¤]îµ¹îµ¹2 F(l),t î´™ î´˜î´— î´š â‰¥0 î´™ î´˜î´— î´š further improvement as depthHincreases (18) âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l),tvec[Vt(X(Sl)âˆ—I)âŠ¤] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 . î´™ î´˜î´— î´š â‰¥0 î´™ î´˜î´— î´š further improvement as depthHincreases In equation (18), we can see that the multiscale linear GNN achieves faster loss reduction as depth H increases. A simi- lar argument applies to non-multiscale linear GNNs. Empirically too, deeper GNNs train faster (Figure 4b). Again, the acceleration applies to both (nonlinear) ReLU GNNs and linear GNNs. We verify this by training mul- tiscale and non-multiscale, ReLU and linear GCNs with 2, 4, and 6 layers on the Cora and Citeseer datasets with learning rate 5e-5, hidden dimension 32, and cross-entropy loss. Results are in Appendix B. 4.3. Label Distribution: Signal vs. Noise Finally, we study how the labels affect the training speed. For the loss reduction (15) and (16), we argue that the norm of Vt(X(Sl)âˆ—I)âŠ¤tends to be larger for labels Y that are more correlated with the graph features X(Sl)âˆ—I, e.g., la- bels are signals instead of â€œnoiseâ€. Without loss of generality, we assume Y is normalized, e.g., one-hot labels. Here, Vt = âˆ‚L(At,Bt) âˆ‚Ë†Yt is the derivative of the loss with respect to the model output, e.g., Vt = 2( Ë†Yt âˆ’Y) for squared loss. If the rows of Y are random noise vectors,Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 1000 Iteration 0.00 0.01 0.02 signal first signal second noise first noise second Figure 5.The scale of the ï¬rst term dominates the second term of the loss reduction d dt L(Wt,Bt) for linear GNNs trained with the original labels vs. random labels on Cora. then so are the rows ofVt, and they are expected to get more orthogonal to the columns of (X(Sl)âˆ—I)âŠ¤as nincreases. In contrast, if the labels Y are highly correlated with the graph features (X(Sl)âˆ—I)âŠ¤, i.e., the labels have signal, then the norm of Vt(X(Sl)âˆ—I)âŠ¤will be larger, implying faster training. Our argument above focuses on the ï¬rst term of the loss reduction, âˆ¥Vt(X(Sl)âˆ—I)âŠ¤âˆ¥2 F. We empiri- cally demonstrate that the scale of the second term,îµ¹îµ¹îµ¹âˆ‘ H l=iJ(i,l),tvec [ Vt(X(Sl)âˆ—I)âŠ¤]îµ¹îµ¹îµ¹ 2 2 , is dominated by that of the ï¬rst term (Figure 5). Thus, we can expect GNNs to train faster with signals than noise. We train GNNs with the original labels of the dataset and random labels (i.e., selecting a class with uniform probabil- ity), respectively. The prediction of our theoretical analysis aligns with practice: training is much slower for random labels (Figure 4c). We verify this for mutliscale and non- multiscale, ReLU and linear GCNs on the Cora and Citseer datasets with learning rate 1e-4, hidden dimension 32, and cross-entropy loss. Results are in Appendix B. 5. Related Work Theoretical analysis of linearized networks. The theoret- ical study of neural networks with some linearized com- ponents has recently drawn much attention. Tremendous efforts have been made to understand linearfeedforward net- works, in terms of their loss landscape (Kawaguchi, 2016; Hardt & Ma, 2017; Laurent & Brecht, 2018) and optimiza- tion dynamics (Saxe et al., 2014; Arora et al., 2019a; Bartlett et al., 2019; Du & Hu, 2019; Zou et al., 2020). Recent works prove global convergence rates for deep linear networks un- der certain conditions (Bartlett et al., 2019; Du & Hu, 2019; Arora et al., 2019a; Zou et al., 2020). For example, Arora et al. (2019a) assume the data to be whitened. Zou et al. (2020) ï¬x the weights of certain layers during training. Our work is inspired by these works but differs in that our anal- ysis applies to all learnable weights and does not require these speciï¬c assumptions, and we study the more complex GNN architecture with skip connections. GNNs consider the interaction of graph structures via the recursive message passing, but such structured, locally varying interaction is not present in feedforward networks. Furthermore, linear feedforward networks, even with skip connections, have the same expressive power as shallow linear models, a crucial condition in previous proofs (Bartlett et al., 2019; Du & Hu, 2019; Arora et al., 2019a; Zou et al., 2020). In contrast, the expressive power of multiscale linear GNNs can change signiï¬cantly as depth increases. Accordingly, our proofs signiï¬cantly differ from previous studies. Another line of works studies the gradient dynamics of neu- ral networks in the neural tangent kernel (NTK) regime (Ja- cot et al., 2018; Li & Liang, 2018; Allen-Zhu et al., 2019; Arora et al., 2019b; Chizat et al., 2019; Du et al., 2019a;c; Kawaguchi & Huang, 2019; Nitanda & Suzuki, 2021). With over-parameterization, the NTK remains almost constant during training. Hence, the corresponding neural network is implicitly linearized with respect to random features of the NTK at initialization (Lee et al., 2019; Yehudai & Shamir, 2019; Liu et al., 2020). On the other hand, our work needs to address nonlinear dynamics and changing expressive power. Learning dynamics and optimization of GNNs. Closely related to our work, Du et al. (2019b); Xu et al. (2021) study the gradient dynamics of GNNs via the Graph NTK but focus on GNNsâ€™ generalization and extrapolation properties. We instead analyze optimization. Only Zhang et al. (2020) also prove global convergence for GNNs, but for the one- hidden-layer case, and they assume a specialized tensor initialization and training algorithms. In contrast, our results work for any ï¬nite depth with no assumptions on specialized training. Other works aim to accelerate and stabilize the training of GNNs through normalization techniques (Cai et al., 2020) and importance sampling (Chen et al., 2018a;b; Huang et al., 2018; Chiang et al., 2019; Zou et al., 2019). Our work complements these practical works with a better theoretical understanding of GNN training. 6. Conclusion This work studies the training properties of GNNs through the lens of optimization dynamics. For linearized GNNs with or without skip connections, despite the non-convex objective, we show that gradient descent training is guar- anteed to converge to a global minimum at a linear rate. The conditions for global convergence are validated on real- world graphs. We further ï¬nd out that skip connections, more depth, and/or a good label distribution implicitly ac- celerate the training of GNNs. Our results suggest deeper GNNs with skip connections may be promising in practice, and serve as a ï¬rst foundational step for understanding the optimization of general GNNs.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Acknowledgements KX and SJ were supported by NSF CAREER award 1553284 and NSF III 1900933. MZ was supported by ODNI, IARPA, via the BETTER Program contract 2019- 19051600005. The research of KK was partially supported by the Center of Mathematical Sciences and Applications at Harvard University. The views, opinions, and/or ï¬nd- ings contained in this article are those of the author and should not be interpreted as representing the ofï¬cial views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency, the Department of Defense, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. References Abu-El-Haija, S., Kapoor, A., Perozzi, B., and Lee, J. N-gcn: Multi-scale graph convolution for semi-supervised node classiï¬cation. In Uncertainty in Artiï¬cial Intelligence, pp. 841â€“851. PMLR, 2020. Allen-Zhu, Z., Li, Y ., and Song, Z. A convergence theory for deep learning via over-parameterization. In International Conference on Machine Learning, pp. 242â€“252, 2019. Arora, S., Cohen, N., and Hazan, E. On the optimization of deep networks: Implicit acceleration by overparameteri- zation. In International Conference on Machine Learning, pp. 244â€“253. PMLR, 2018. Arora, S., Cohen, N., Golowich, N., and Hu, W. A conver- gence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Rep- resentations, 2019a. Arora, S., Du, S., Hu, W., Li, Z., and Wang, R. Fine-grained analysis of optimization and generalization for overpa- rameterized two-layer neural networks. In International Conference on Machine Learning, pp. 322â€“332, 2019b. Bartlett, P. L., Helmbold, D. P., and Long, P. M. Gradi- ent descent with identity initialization efï¬ciently learns positive-deï¬nite linear transformations by deep residual networks. Neural computation, 31(3):477â€“502, 2019. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-y., and Wang, L. Graphnorm: A principled approach to acceler- ating graph neural network training. arXiv preprint arXiv:2009.03294, 2020. Chen, J., Ma, T., and Xiao, C. FastGCN: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations, 2018a. Chen, J., Zhu, J., and Song, L. Stochastic training of graph convolutional networks with variance reduction. In Inter- national Conference on Machine Learning, pp. 942â€“950, 2018b. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Sim- ple and deep graph convolutional networks. In Interna- tional Conference on Machine Learning, pp. 1725â€“1735. PMLR, 2020. Chen, Z., Villar, S., Chen, L., and Bruna, J. On the equiv- alence between graph isomorphism testing and function approximation with gnns. In Advances in Neural Infor- mation Processing Systems, pp. 15894â€“15902, 2019. Chiang, W.-L., Liu, X., Si, S., Li, Y ., Bengio, S., and Hsieh, C.-J. Cluster-gcn: An efï¬cient algorithm for training deep and large graph convolutional networks. In Proceed- ings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 257â€“266, 2019. Chizat, L., Oyallon, E., and Bach, F. On lazy training in differentiable programming. In Advances in Neural Information Processing Systems, pp. 2937â€“2947, 2019. Defferrard, M., Bresson, X., and Vandergheynst, P. Con- volutional neural networks on graphs with fast localized spectral ï¬ltering. In Advances in Neural Information Processing Systems, pp. 3844â€“3852, 2016. Du, S. and Hu, W. Width provably matters in optimiza- tion for deep linear neural networks. In International Conference on Machine Learning, pp. 1655â€“1664, 2019. Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. Gradient descent ï¬nds global minima of deep neural networks. In International Conference on Machine Learning , pp. 1675â€“1685, 2019a. Du, S. S., Hou, K., Salakhutdinov, R. R., Poczos, B., Wang, R., and Xu, K. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Advances in Neural Information Processing Systems, pp. 5724â€“5734, 2019b. Du, S. S., Zhai, X., Poczos, B., and Singh, A. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Rep- resentations, 2019c. Duvenaud, D. K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru-Guzik, A., and Adams, R. P. Con- volutional networks on graphs for learning molecular ï¬n- gerprints. In Advances in neural information processing systems, pp. 2224â€“2232, 2015.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Fey, M. and Lenssen, J. E. Fast graph representation learning with pytorch geometric.arXiv preprint arXiv:1903.02428, 2019. Garg, V . K., Jegelka, S., and Jaakkola, T. Generalization and representational limits of graph neural networks. In International Conference on Machine Learning, 2020. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chem- istry. In International Conference on Machine Learning, pp. 1273â€“1272, 2017. Gori, M., Monfardini, G., and Scarselli, F. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pp. 729â€“734. IEEE, 2005. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pp. 1025â€“1035, 2017. Hardt, M. and Ma, T. Identity matters in deep learning. In International Conference on Learning Representations, 2017. Huang, J. and Yau, H.-T. Dynamics of deep neural networks and neural tangent hierarchy. In International conference on machine learning, 2020. Huang, W., Zhang, T., Rong, Y ., and Huang, J. Adap- tive sampling towards fast graph representation learning. Advances in neural information processing systems, 31: 4558â€“4567, 2018. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pp. 8571â€“8580, 2018. Ji, Z. and Telgarsky, M. Directional convergence and align- ment in deep learning. arXiv preprint arXiv:2006.06657, 2020. Kawaguchi, K. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586â€“594, 2016. Kawaguchi, K. On the theory of implicit deep learning: Global convergence with implicit layers. In International Conference on Learning Representations (ICLR), 2021. Kawaguchi, K. and Huang, J. Gradient descent ï¬nds global minima for generalizable deep neural networks of prac- tical sizes. In 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 92â€“99. IEEE, 2019. Kearnes, S., McCloskey, K., Berndl, M., Pande, V ., and Riley, P. Molecular graph convolutions: moving beyond ï¬ngerprints. Journal of computer-aided molecular design, 30(8):595â€“608, 2016. Keriven, N. and PeyrÂ´e, G. Universal invariant and equiv- ariant graph neural networks. In Advances in Neural Information Processing Systems, pp. 7092â€“7101, 2019. Kipf, T. N. and Welling, M. Semi-supervised classiï¬ca- tion with graph convolutional networks. In International Conference on Learning Representations, 2017. Laurent, T. and Brecht, J. Deep linear networks with arbi- trary loss: All local minima are global. In International conference on machine learning, pp. 2902â€“2907. PMLR, 2018. Lee, J., Xiao, L., Schoenholz, S., Bahri, Y ., Novak, R., Sohl- Dickstein, J., and Pennington, J. Wide neural networks of any depth evolve as linear models under gradient descent. In Advances in neural information processing systems , pp. 8572â€“8583, 2019. Li, G., Muller, M., Thabet, A., and Ghanem, B. Deepgcns: Can gcns go as deep as cnns? In Proceedings of the IEEE International Conference on Computer Vision, pp. 9267â€“9276, 2019. Li, G., Xiong, C., Thabet, A., and Ghanem, B. Deep- ergcn: All you need to train deeper gcns. arXiv preprint arXiv:2006.07739, 2020. Li, Y . and Liang, Y . Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157â€“8166, 2018. Liao, P., Zhao, H., Xu, K., Jaakkola, T., Gordon, G., Jegelka, S., and Salakhutdinov, R. Graph adversarial networks: Protecting information against adversarial attacks. arXiv preprint arXiv:2009.13504, 2020. Liu, C., Zhu, L., and Belkin, M. On the linearity of large non-linear models: when and why the tangent kernel is constant. Advances in Neural Information Processing Systems, 33, 2020. Loukas, A. How hard is to distinguish graphs with graph neural networks? In Advances in neural information processing systems, 2020. Maron, H., Ben-Hamu, H., Serviansky, H., and Lipman, Y . Provably powerful graph networks. In Advances in Neural Information Processing Systems, pp. 2156â€“2167, 2019.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Merkwirth, C. and Lengauer, T. Automatic generation of complementary descriptors with molecular graph net- works. J. Chem. Inf. Model., 45(5):1159â€“1168, 2005. Nitanda, A. and Suzuki, T. Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime. In International Conference on Learning Repre- sentations, 2021. Oono, K. and Suzuki, T. Optimization and generalization analysis of transduction through gradient boosting and ap- plication to multi-scale graph neural networks. Advances in Neural Information Processing Systems, 33, 2020. Sato, R., Yamada, M., and Kashima, H. Approximation ra- tios of graph neural networks for combinatorial problems. In Advances in Neural Information Processing Systems, pp. 4083â€“4092, 2019. Sato, R., Yamada, M., and Kashima, H. Random fea- tures strengthen graph neural networks. arXiv preprint arXiv:2002.03155, 2020. Saxe, A. M., McClelland, J. L., and Ganguli, S. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61â€“80, 2009. Scarselli, F., Tsoi, A. C., and Hagenbuchner, M. The vapnikâ€“ chervonenkis dimension of graph and recursive neural networks. Neural Networks, 108:248â€“259, 2018. Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., and Eliassi-Rad, T. Collective classiï¬cation in network data. AI magazine, 29(3):93, 2008. Thekumparampil, K. K., Wang, C., Oh, S., and Li, L.- J. Attention-based graph neural network for semi- supervised learning. arXiv preprint arXiv:1803.03735, 2018. Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations, 2018. Vignac, C., Loukas, A., and Frossard, P. Building power- ful and equivariant graph neural networks with message- passing. Advances in neural information processing sys- tems, 2020. Wu, F., Souza, A., Fifty, C., Yu, T., and Weinberger, K. Sim- plifying graph convolutional networks. In International Conference on Machine Learning, 2019. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Confer- ence on Machine Learning, pp. 5453â€“5462, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. Xu, K., Li, J., Zhang, M., Du, S. S., ichi Kawarabayashi, K., and Jegelka, S. What can neural networks reason about? In International Conference on Learning Representations, 2020. Xu, K., Zhang, M., Li, J., Du, S. S., Kawarabayashi, K.-I., and Jegelka, S. How neural networks extrapolate: From feedforward to graph neural networks. In International Conference on Learning Representations, 2021. Yehudai, G. and Shamir, O. On the power and limitations of random features for understanding neural networks. In Advances in Neural Information Processing Systems, pp. 6598â€“6608, 2019. Zhang, S., Wang, M., Liu, S., Chen, P.-Y ., and Xiong, J. Fast learning of graph neural networks with guaranteed gener- alizability: One-hidden-layer case. In International Con- ference on Machine Learning, pp. 11268â€“11277, 2020. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems , pp. 11249â€“ 11259, 2019. Zou, D., Long, P. M., and Gu, Q. On the global conver- gence of training deep linear resnets. In International Conference on Learning Representations, 2020.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth A. Proofs In this section, we complete the proofs of our theoretical results. We show the proofs of Theorem 1 in Appendix A.1, Proposition 1 in Appendix A.2, Proposition 2 in Appendix A.3, Theorem 2 in Appendix A.4, Proposition 3 in Appendix A.5, and Theorem 3 in Appendix A.6. Before starting our proofs, we ï¬rst introduce additional notation used in the proofs. We deï¬ne the corner cases on the products of Bas: B(H)B(Hâˆ’1) Â·Â·Â·B(l+1) := Iml if H = l (19) B(H)B(Hâˆ’1) ...B (1) := Imx if H = 0 (20) B(lâˆ’1)B(lâˆ’2) ...B (1) := Imx if l= 1 (21) Similarly, for any matrices M(l), we deï¬ne M(l)M(lâˆ’1) Â·Â·Â·M(k) := Im if l < k,and M(l)M(lâˆ’1) Â·Â·Â·M(k) := M(k) = M(l) if l= k.Given a scalar-valued variable aâˆˆR and a matrix M âˆˆRdÃ—dâ€² , we deï¬ne âˆ‚a âˆ‚M = ï£® ï£¯ï£¯ï£° âˆ‚a âˆ‚M11 Â·Â·Â· âˆ‚a âˆ‚M1dâ€² ... ... ... âˆ‚a âˆ‚Md1 Â·Â·Â· âˆ‚a âˆ‚Mddâ€² ï£¹ ï£ºï£ºï£»âˆˆRdÃ—dâ€² , (22) where Mij represents the (i,j)-th entry of the matrix M. Given a vector-valued variable a âˆˆRd and a column vector bâˆˆRdâ€² , we let âˆ‚a âˆ‚b = ï£® ï£¯ï£¯ï£° âˆ‚a1 âˆ‚b1 Â·Â·Â· âˆ‚a1 âˆ‚bdâ€² ... ... ... âˆ‚ad âˆ‚b1 Â·Â·Â· âˆ‚ad âˆ‚bdâ€² ï£¹ ï£ºï£ºï£»âˆˆRdÃ—dâ€² , (23) where bi represents the i-th entry of the column vector b. Similarly, given a vector-valued variable aâˆˆRd and a row vector bâˆˆR1Ã—dâ€² , we write âˆ‚a âˆ‚b = ï£® ï£¯ï£¯ï£° âˆ‚a1 âˆ‚b11 Â·Â·Â· âˆ‚a1 âˆ‚b1dâ€² ... ... ... âˆ‚ad âˆ‚b11 Â·Â·Â· âˆ‚ad âˆ‚b1dâ€² ï£¹ ï£ºï£ºï£»âˆˆRdÃ—dâ€² , (24) where b1i represents the i-th entry of the row vector b. Finally, we recall the deï¬nition of the Kronecker product product of two matrices: for matrices M âˆˆRdMÃ—dâ€² M and Â¯M âˆˆRdÂ¯MÃ—dâ€² Â¯M , M âŠ— Â¯M = ï£® ï£¯ï£° M11 Â¯M Â·Â·Â· M1dâ€² M Â¯M ... ... ... MdM1 Â¯M Â·Â·Â· MdMdâ€² M Â¯M ï£¹ ï£ºï£»âˆˆRdMdÂ¯MÃ—dâ€² Mdâ€² Â¯M . (25) A.1. Proof of Theorem 1 We begin with a proof overview of Theorem 1. We ï¬rst relate the gradients âˆ‡W(H)Land âˆ‡B(l)Lto the gradient âˆ‡(H)L, which is deï¬ned by âˆ‡(H)L(W,B) := âˆ‚L(W,B) âˆ‚Ë†Y (X(SH)âˆ—I)âŠ¤âˆˆRmyÃ—mx. Using the proven relation of (âˆ‡W(H)L,âˆ‡B(l)L) and âˆ‡(H)L, we ï¬rst analyze the dynamics induced in the space of W(l)B(l)B(lâˆ’1) Â·Â·Â·B(1) in Appendix A.1.1, and then the dynamics induced int the space of loss valueL(W,B) in Appendix A.1.2. Finally, we complete the proof by using the assumption of employing the square loss in Appendix A.1.3. Let W(H) = W (during the proof of Theorem 1). We ï¬rst prove the relationship of the gradients âˆ‡W(H)L, âˆ‡B(l)Land âˆ‡(H)Lin the following lemma:Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Lemma 1. Let f be an H-layer linear GNN and â„“(q,Y ) = âˆ¥qâˆ’Yâˆ¥2 F where q,Y âˆˆRmyÃ—Â¯n. Then, for any (W,B), âˆ‡W(H)L(W,B) = âˆ‡(H)L(W,B)(B(H)B(Hâˆ’1) ...B (1))âŠ¤âˆˆRmyÃ—ml, (26) and âˆ‡B(l)L(W,B) = (W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‡(H)L(W,B)(B(lâˆ’1)B(lâˆ’2) ...B (1))âŠ¤âˆˆRmlÃ—mlâˆ’1, (27) Proof of Lemma 1. From Deï¬nition 1, we have Ë†Y = f(X,W,B )âˆ—I= W(H)(X(H))âˆ—Iwhere X(l) = B(l)X(lâˆ’1)S. Using this deï¬nition, we can derive the formula of âˆ‚vec[Ë†Y] âˆ‚vec[W(H)] âˆˆRmynÃ—mymÂ¯H as: âˆ‚vec[Ë†Y] âˆ‚vec[W(H)] = âˆ‚ âˆ‚vec[W(H)] vec[W(H)(X(H))âˆ—I] = âˆ‚ âˆ‚vec[W(H)][((X(H))âˆ—I)âŠ¤âŠ—Imy ] vec[W(H)] = [((X(H))âˆ—I)âŠ¤âŠ—Imy ] âˆˆRmynÃ—mymÂ¯H (28) We will now derive the formula of âˆ‚vec[Ë†Y] âˆ‚vec[B(l)] âˆˆRmynÃ—mlmlâˆ’1: âˆ‚vec[Ë†Y] âˆ‚vec[B(l)] = âˆ‚ âˆ‚vec[B(l)] vec[W(H)(X(H))âˆ—I] = âˆ‚ âˆ‚vec[B(l)][In âŠ—W(H)] vec[(X(H))âˆ—I] = [In âŠ—W(H)]âˆ‚vec[(X(H))âˆ—I] âˆ‚vec[B(l)] = [In âŠ—W(H)]âˆ‚vec[(X(H))âˆ—I] âˆ‚vec[X(l)] âˆ‚vec[X(l)] âˆ‚vec[B(l)] = [In âŠ—W(H)]âˆ‚vec[(X(H))âˆ—I] âˆ‚vec[X(l)] âˆ‚vec[B(l)X(lâˆ’1)S] âˆ‚vec[B(l)] = [In âŠ—W(H)]âˆ‚vec[(X(H))âˆ—I] âˆ‚vec[X(l)] âˆ‚[(X(lâˆ’1)S)âŠ¤âŠ—Iml] vec[B(l)] âˆ‚vec[B(l)] = [In âŠ—W(H)]âˆ‚vec[(X(H))âˆ—I] âˆ‚vec[X(l)] [(X(lâˆ’1)S)âŠ¤âŠ—Iml] (29) Here, we have that vec[(X(H))âˆ—I] = vec[B(H)X(Hâˆ’1)Sâˆ—I] = vec[(SâŠ¤)Iâˆ—âŠ—B(H)] vec[X(Hâˆ’1)]. (30) and vec[X(H)] = vec[B(H)X(Hâˆ’1)Sâˆ—I] = vec[SâŠ—B(H)] vec[X(Hâˆ’1)]. (31) By recursively applying (31), we have that vec[(X(H))âˆ—I] = vec[(SâŠ¤)Iâˆ—âŠ—B(H)] vec[SâŠ¤âŠ—B(Hâˆ’1)] Â·Â·Â·vec[SâŠ¤âŠ—B(l+1)] vec[X(l)] = vec[((SHâˆ’l)âŠ¤)Iâˆ—âŠ—B(H)B(Hâˆ’1) Â·Â·Â·B(l+1)] vec[X(l)], where B(H)B(Hâˆ’1) Â·Â·Â·B(l+1) := Iml if H = l. Therefore,Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth âˆ‚vec[(X(H))âˆ—I] âˆ‚vec[X(l)] = vec[((SHâˆ’l)âŠ¤)Iâˆ—âŠ—B(H)B(Hâˆ’1) Â·Â·Â·B(l+1)]. (32) Combining (29) and (32) yields âˆ‚vec[Ë†Y] âˆ‚vec[B(l)] = [In âŠ—W(H)]âˆ‚vec[(X(H))âˆ—I] âˆ‚vec[X(l)] [(X(lâˆ’1)S)âŠ¤âŠ—Iml] = [In âŠ—W(H)] vec[((SHâˆ’l)âŠ¤)Iâˆ—âŠ—B(H)B(Hâˆ’1) Â·Â·Â·B(l+1)][(X(lâˆ’1)S)âŠ¤âŠ—Iml] = [(X(lâˆ’1)(SHâˆ’l+1)âˆ—I)âŠ¤âŠ—W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(l+1)] âˆˆRmynÃ—mlmlâˆ’1. (33) Using (28), we will now derive the formula of âˆ‡W(H)L(W,B) âˆˆRmyÃ—mH : âˆ‚L(W,B) âˆ‚vec[W(H)] = âˆ‚L(W,B) âˆ‚vec[Ë†Y] âˆ‚vec[Ë†Y] âˆ‚vec[W(H)] = âˆ‚L(W,B) âˆ‚vec[Ë†Y] [(X(H))âŠ¤ âˆ—IâŠ—Imy ] Thus, with âˆ‚L(W,B) âˆ‚Ë†Y âˆˆRmyÃ—n, âˆ‡vec[W(H)]L(W,B) = ( âˆ‚L(W,B) âˆ‚vec[W(H)] )âŠ¤ = [(X(H))âˆ—IâŠ—Imy ] ( âˆ‚L(W,B) âˆ‚vec[Ë†Y] )âŠ¤ = [(X(H))âˆ—IâŠ—Imy ] vec [âˆ‚L(W,B) âˆ‚Ë†Y ] = vec [âˆ‚L(W,B) âˆ‚Ë†Y (X(H))âŠ¤ âˆ—I ] âˆˆRmymH . Therefore, âˆ‡W(H)L(W,B) = âˆ‚L(W,B) âˆ‚Ë†Y (X(H))âŠ¤ âˆ—IâˆˆRmyÃ—mH . (34) Using (33), we will now derive the formula of âˆ‡B(l)L(W,B) âˆˆRmlÃ—mlâˆ’1: âˆ‚L(W,B) âˆ‚vec[B(l)] = âˆ‚L(W,B) âˆ‚vec[Ë†Y] âˆ‚vec[Ë†Y] âˆ‚vec[B(l)] = âˆ‚L(W,B) âˆ‚vec[Ë†Y] [(X(lâˆ’1)(SHâˆ’l+1)âˆ—I)âŠ¤âŠ—W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(l+1)]. Thus, with âˆ‚L(W,B) âˆ‚Ë†Y âˆˆRmyÃ—n, âˆ‡vec[B(l)]L(W,B) = (âˆ‚L(W,B) âˆ‚vec[B(l)] )âŠ¤ = [X(lâˆ’1)(SHâˆ’l+1)âˆ—IâŠ—(W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(l+1))âŠ¤] ( âˆ‚L(W,B) âˆ‚vec[Ë†Y] )âŠ¤ = [X(lâˆ’1)(SHâˆ’l+1)âˆ—IâŠ—(W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(l+1))âŠ¤] vec [âˆ‚L(W,B) âˆ‚Ë†Y ] = vec [ (W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‚L(W,B) âˆ‚Ë†Y (X(lâˆ’1)(SHâˆ’l+1)âˆ—I)âŠ¤ ] âˆˆRmlmlâˆ’1.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Therefore, âˆ‡B(l)L(W,B) = (W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‚L(W,B) âˆ‚Ë†Y (X(lâˆ’1)(SHâˆ’l+1)âˆ—I)âŠ¤âˆˆRmlÃ—mlâˆ’1. (35) With (34) and (35), we are now ready to prove the statement of this lemma by introducing the following notation: âˆ‡(l)L(W,B) := âˆ‚L(W,B) âˆ‚Ë†Y (X(Sl)âˆ—I)âŠ¤âˆˆRmyÃ—mx. Using this notation along with (34) âˆ‡W(H)L(W,B) = âˆ‚L(W,B) âˆ‚Ë†Y (X(H))âŠ¤ âˆ—I = âˆ‚L(W,B) âˆ‚Ë†Y (B(H)X(Hâˆ’1)(S)âˆ—I)âŠ¤ = âˆ‚L(W,B) âˆ‚Ë†Y (B(H)B(Hâˆ’1) ...B (1)X(SH)âˆ—I)âŠ¤ = âˆ‡(H)L(W,B)(B(H)B(Hâˆ’1) ...B (1))âŠ¤, Similarly, using (35), âˆ‡B(l)L(W,B) = (W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‚L(W,B) âˆ‚Ë†Y (X(lâˆ’1)(SHâˆ’l+1)âˆ—I)âŠ¤ = (W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‚L(W,B) âˆ‚Ë†Y (B(lâˆ’1)B(lâˆ’2) ...B (1)X(Slâˆ’1SHâˆ’l+1)âˆ—I)âŠ¤ = (W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‚L(W,B) âˆ‚Ë†Y (B(lâˆ’1)B(lâˆ’2) ...B (1)X(SH)âˆ—I)âŠ¤ = (W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‡(H)L(W,B)(B(lâˆ’1)B(lâˆ’2) ...B (1))âŠ¤ where B(lâˆ’1)B(lâˆ’2) ...B (1) := Imx if l= 1. By using Lemma 1, we complete the proof of Theorem 1 in the following. A.1.1. D YNAMICS INDUCED IN THE SPACE OF W(l)B(l)B(lâˆ’1) Â·Â·Â·B(1) We now consider the dynamics induced in the space of W(l)B(l)B(lâˆ’1) Â·Â·Â·B(1). We ï¬rst consider the following discrete version of the dynamics: Wâ€² (H) = W(H) âˆ’Î±âˆ‡W(H)L(W,B) Bâ€² (l) = B(l) âˆ’Î±âˆ‡B(l)L(W,B). This dynamics induces the following dynamics: Wâ€² (H)Bâ€² (H)Bâ€² (Hâˆ’1) Â·Â·Â·Bâ€² (1) = (W(H) âˆ’Î±âˆ‡W(H)L(W,B))(B(H) âˆ’Î±âˆ‡B(H)L(W,B)) Â·Â·Â·(B(1) âˆ’Î±âˆ‡B(1)L(W,B)). Deï¬ne Z(H) := W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(1), and Zâ€² (H) := Wâ€² (H)Bâ€² (H)Bâ€² (Hâˆ’1) Â·Â·Â·Bâ€² (1). Then, we can rewrite Zâ€² (H) = (W(H) âˆ’Î±âˆ‡W(H)L(W,B))(B(H) âˆ’Î±âˆ‡B(H)L(W,B)) Â·Â·Â·(B(1) âˆ’Î±âˆ‡B(1)L(W,B)).Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth By expanding the multiplications, this can be written as: Zâ€² (H) = Z(H) âˆ’Î±âˆ‡W(H)L(W,B)B(H) Â·Â·Â·B(1) âˆ’Î± Hâˆ‘ i=1 W(H)B(H) Â·Â·Â·B(i+1)âˆ‡B(i)L(W,B)B(iâˆ’1) Â·Â·Â·B(1) + O(Î±2). By vectorizing both sides, vec[Zâ€² (H)] âˆ’vec[Z(H)] = âˆ’Î±vec[âˆ‡W(H)L(W,B)B(H) Â·Â·Â·B(1)] âˆ’Î± Hâˆ‘ i=1 vec[W(H)B(H) Â·Â·Â·B(i+1)âˆ‡B(i)L(W,B)B(iâˆ’1) Â·Â·Â·B(1)] + O(Î±2). Here, using the formula of âˆ‡W(H)L(W,B) and âˆ‡B(H)L(W,B), we have that vec[âˆ‡W(H)L(W,B)B(H) Â·Â·Â·B(1)] = vec[âˆ‡(H)L(W,B)(B(H) ...B (1))âŠ¤B(H) Â·Â·Â·B(1)] = [(B(H) ...B (1))âŠ¤B(H) Â·Â·Â·B(1) âŠ—Imy ] vec[âˆ‡(H)L(W,B)], and Hâˆ‘ i=1 vec[W(H)B(H) Â·Â·Â·B(i+1)âˆ‡B(i)L(W,B)B(iâˆ’1) Â·Â·Â·B(1)] = Hâˆ‘ i=1 vec [ W(H)B(H) Â·Â·Â·B(i+1)(W(H)B(H) Â·Â·Â·B(i+1))âŠ¤âˆ‡(H)L(W,B)(B(iâˆ’1) ...B (1))âŠ¤B(iâˆ’1) Â·Â·Â·B(1) ] = Hâˆ‘ i=1 [(B(iâˆ’1) ...B (1))âŠ¤B(iâˆ’1) Â·Â·Â·B(1) âŠ—W(H)B(H) Â·Â·Â·B(i+1)(W(H)B(H) Â·Â·Â·B(i+1))âŠ¤] vec [ âˆ‡(H)L(W,B) ] . Summarizing above, vec[Zâ€² (H)] âˆ’vec[Z(H)] = âˆ’Î±[(B(H) ...B (1))âŠ¤B(H) Â·Â·Â·B(1) âŠ—Imy ] vec[âˆ‡(H)L(W,B)] âˆ’Î± Hâˆ‘ i=1 [(B(iâˆ’1) ...B (1))âŠ¤B(iâˆ’1) Â·Â·Â·B(1) âŠ—W(H)B(H) Â·Â·Â·B(i+1)(W(H)B(H) Â·Â·Â·B(i+1))âŠ¤] vec [ âˆ‡(H)L(W,B) ] + O(Î±2) Therefore, the induced continuous dynamics of Z(H) = W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(1) is d dtvec[Z(H)] = âˆ’F(H) vec[âˆ‡(H)L(W,B)] âˆ’ (Hâˆ‘ i=1 JâŠ¤ (i,H)J(i,H) ) vec [ âˆ‡(H)L(W,B) ] , where F(H) = [(B(H) ...B (1))âŠ¤B(H) Â·Â·Â·B(1) âŠ—Imy ], and J(i,H) = [B(iâˆ’1) ...B (1) âŠ—(W(H)B(H) Â·Â·Â·B(i+1))âŠ¤]. This is because JâŠ¤ (i,H)J(i,H) = [(B(iâˆ’1) ...B (1))âŠ¤âŠ—W(H)B(H) Â·Â·Â·B(i+1)][B(iâˆ’1) ...B (1) âŠ—(W(H)B(H) Â·Â·Â·B(i+1))âŠ¤] = [(B(iâˆ’1) ...B (1))âŠ¤B(iâˆ’1) ...B (1) âŠ—W(H)B(H) Â·Â·Â·B(i+1)(W(H)B(H) Â·Â·Â·B(i+1))âŠ¤].Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth A.1.2. D YNAMICS INDUCED INT THE SPACE OF LOSS VALUE L(W,B) We now analyze the dynamics induced int the space of loss valueL(W,B). Using chain rule, d dtL(W,B) = d dtL0(Z(H)) = âˆ‚L0(Z(H)) âˆ‚vec[Z(H)] dvec[Z(H)] dt , where L0(Z(H)) = â„“(f0(X,Z(H))âˆ—I,Y ), f0(X,Z(H)) = Z(H)XSH, and Z(H) = W(H)B(H)B(Hâˆ’1) Â·Â·Â·B(1). Since f0(X,Z(H)) = f(X,W,B ) = Ë†Y and L0(Z(H)) = L(W,B), we have that (âˆ‚L0(Z(H)) âˆ‚vec[Z(H)] )âŠ¤ = ( âˆ‚L(W,B) âˆ‚vec[Ë†Y] âˆ‚vec[Ë†Y] âˆ‚vec[Z(H)] )âŠ¤ = ( âˆ‚L(W,B) âˆ‚vec[Ë†Y] ( âˆ‚ âˆ‚vec[Z(H)][(X(SH)âˆ—I)âŠ¤âŠ—Imy ] vec[Z(H)] ))âŠ¤ = [X(SH)âˆ—IâŠ—Imy ] vec [âˆ‚L(W,B) âˆ‚Ë†Y ] = vec [âˆ‚L(W,B) âˆ‚Ë†Y (X(SH)âˆ—I)âŠ¤ ] = vec[âˆ‡(H)L(W,B)] Combining these, d dtL(W,B) = vec[âˆ‡(H)L(W,B)]âŠ¤dvec[Z(H)] dt = âˆ’vec[âˆ‡(H)L(W,B)]âŠ¤F(H) vec[âˆ‡(H)L(W,B)] âˆ’ Hâˆ‘ i=1 vec[âˆ‡(H)L(W,B)]âŠ¤JâŠ¤ (i,H)J(i,H) vec [ âˆ‡(H)L(W,B) ] = âˆ’vec[âˆ‡(H)L(W,B)]âŠ¤F(H) vec[âˆ‡(H)L(W,B)] âˆ’ Hâˆ‘ i=1 âˆ¥J(i,H) vec [ âˆ‡(H)L(W,B) ] âˆ¥2 2 Therefore, d dtL(W,B) = âˆ’vec[âˆ‡(H)L(W,B)]âŠ¤F(H) vec[âˆ‡(H)L(W,B)] âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 (36) Since F(H) is real symmetric and positive semideï¬nite, d dtL(W,B) â‰¤âˆ’Î»min(F(H))âˆ¥vec[âˆ‡(H)L(W,B)]âˆ¥2 2 âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 . With Î»W,B = Î»min(F(H)), d dtL(W,B) â‰¤âˆ’Î»W,Bâˆ¥vec[âˆ‡(H)L(W,B)]âˆ¥2 2 âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 (37)Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth A.1.3. C OMPLETING THE PROOF BY USING THE ASSUMPTION OF THE SQUARE LOSS Using the assumption that L(W,B) = â„“(f(X,W,B )âˆ—I,Y ) = âˆ¥f(X,W,B )âˆ—Iâˆ’Yâˆ¥2 F with Ë†Y = f(X,W,B )âˆ—I, we have âˆ‚L(W,B) âˆ‚Ë†Y = âˆ‚ âˆ‚Ë†Y âˆ¥Ë†Y âˆ’Yâˆ¥2 F = 2( Ë†Y âˆ’Y) âˆˆRmyÃ—n, and vec[âˆ‡(H)L(W,B)] = vec [âˆ‚L(W,B) âˆ‚Ë†Y (X(SH)âˆ—I)âŠ¤ ] = 2 vec [ ( Ë†Y âˆ’Y)(X(SH)âˆ—I)âŠ¤ ] = 2[X(SH)âˆ—IâŠ—Imy ] vec[Ë†Y âˆ’Y]. Therefore, âˆ¥vec[âˆ‡(H)L(W,B)]âˆ¥2 2 = 4 vec[Ë†Y âˆ’Y]âŠ¤[(X(SH)âˆ—I)âŠ¤X(SH)âˆ—IâŠ—Imy ] vec[Ë†Y âˆ’Y] (38) Using (37) and (38), d dtL(W,B) â‰¤âˆ’Î»W,Bâˆ¥vec[âˆ‡(H)L(W,B)]âˆ¥2 2 âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 â‰¤âˆ’4Î»W,Bvec[Ë†Y âˆ’Y]âŠ¤[(X(SH)âˆ—I)âŠ¤X(SH)âˆ—IâŠ—Imy ] vec[Ë†Y âˆ’Y] âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 = âˆ’4Î»W,Bvec[Ë†Y âˆ’Y]âŠ¤ [ ËœGâŠ¤ H ËœGH âŠ—Imy ] vec[Ë†Y âˆ’Y] âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 where the last line follows from the following deï¬nition: ËœGH := X(SH)âˆ—I. Decompose vec[Ë†Yâˆ’Y] as vec[Ë†Yâˆ’Y] = v+vâŠ¥, where v= PËœGâŠ¤ HâŠ—Imy vec[Ë†Yâˆ’Y], vâŠ¥= (Imynâˆ’PËœGâŠ¤ HâŠ—Imy ) vec[Ë†Yâˆ’Y], and PËœGâŠ¤ HâŠ—Imy âˆˆRmynÃ—myn represents the orthogonal projection onto the column space of ËœGâŠ¤ H âŠ—Imy âˆˆRmynÃ—mymx. Then, vec[Ë†Y âˆ’Y]âŠ¤ [ ËœGâŠ¤ H ËœGH âŠ—Imy ] vec[Ë†Y âˆ’Y] = (v+ vâŠ¥)âŠ¤ [ ËœGâŠ¤ H âŠ—Imy ][ ËœGH âŠ—Imy ] (v+ vâŠ¥) = vâŠ¤ [ ËœGâŠ¤ H âŠ—Imy ][ ËœGH âŠ—Imy ] v â‰¥Ïƒ2 min( ËœGH)âˆ¥PËœGâŠ¤ HâŠ—Imy vec[Ë†Y âˆ’Y]âˆ¥2 2 = Ïƒ2 min( ËœGH)âˆ¥PËœGâŠ¤ HâŠ—Imy vec[Ë†Y] âˆ’PËœGâŠ¤ HâŠ—Imy vec[Y]âˆ¥2 2 = Ïƒ2 min( ËœGH)âˆ¥vec[Ë†Y] âˆ’PËœGâŠ¤ HâŠ—Imy vec[Y] Â±vec[Y]âˆ¥2 2 = Ïƒ2 min( ËœGH)âˆ¥vec[Ë†Y] âˆ’vec[Y] + (Imyn âˆ’PËœGâŠ¤ HâŠ—Imy ) vec[Y]âˆ¥2 2 â‰¥Ïƒ2 min( ËœGH)(âˆ¥vec[Ë†Y âˆ’Y]âˆ¥2 âˆ’âˆ¥(Imyn âˆ’PËœGâŠ¤ HâŠ—Imy ) vec[Y]âˆ¥2)2 â‰¥Ïƒ2 min( ËœGH)(âˆ¥vec[Ë†Y âˆ’Y]âˆ¥2 2 âˆ’âˆ¥(Imyn âˆ’PËœGâŠ¤ HâŠ—Imy ) vec[Y]âˆ¥2 2, where we used the fact that the singular values of [ ËœGâŠ¤ H âŠ—Imy ] are products of singular values of ËœGH and Imy . By noticing that L(W,B) = âˆ¥vec[Ë†Y âˆ’Y]âˆ¥2 2 and Lâˆ— H = âˆ¥(Imyn âˆ’PËœGâŠ¤ HâŠ—Imy ) vec[Y]âˆ¥2 2 , vec[Ë†Y âˆ’Y]âŠ¤ [ ËœGâŠ¤ H ËœGH âŠ—Imy ] vec[Ë†Y âˆ’Y] â‰¥Ïƒ2 min( ËœGH)(L(W,B) âˆ’Lâˆ— H).Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Therefore, d dtL(W,B) â‰¤âˆ’4Î»W,Bvec[Ë†Y âˆ’Y]âŠ¤ [ ËœGâŠ¤ H ËœGH âŠ—Imy ] vec[Ë†Y âˆ’Y] âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 â‰¤âˆ’4Î»W,BÏƒ2 min( ËœGH)(L(W,B) âˆ’Lâˆ— H) âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 Since d dtLâˆ— H = 0, d dt(L(W,B) âˆ’Lâˆ— H) â‰¤âˆ’4Î»W,BÏƒ2 min( ËœGH)(L(W,B) âˆ’Lâˆ— H) âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 By deï¬ning L = L(W,B) âˆ’Lâˆ— H, dL dt â‰¤âˆ’4Î»W,BÏƒ2 min( ËœGH)L âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 (39) Since d dtL â‰¤0 and L â‰¥0, if L = 0 at some time Â¯t, then L = 0 for any time tâ‰¥Â¯t. Therefore, if L = 0 at some time Â¯t, then we have the desired statement of this theorem for any time tâ‰¥Â¯t. Thus, we can focus on the time interval [0,Â¯t] such that L >0 for any time tâˆˆ[0,Â¯t] (here, it is allowed to have Â¯t= âˆ). Thus, focusing on the time interval with L >0 , equation (39) implies that 1 L dL dt â‰¤âˆ’4Î»W,BÏƒ2 min( ËœGH) âˆ’1 L Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 By taking integral over time âˆ« T 0 1 L dL dtdtâ‰¤âˆ’ âˆ« T 0 4Î»W,BÏƒ2 min( ËœGH)dtâˆ’ âˆ« T 0 1 L Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 dt By using the substitution rule for integrals, âˆ«T 0 1 L dL dtdt= âˆ«LT L0 1 LdL = log(LT) âˆ’log(L0), where L0 = L(W0,B0) âˆ’Lâˆ— and LT = L(WT,BT) âˆ’Lâˆ— H. Thus, log(LT) âˆ’log(L0) â‰¤âˆ’4Ïƒ2 min( ËœGH) âˆ« T 0 Î»W,Bdtâˆ’ âˆ« T 0 1 L Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 dt which implies that LT â‰¤elog(L0)âˆ’4Ïƒ2 min( ËœGH) âˆ«T 0 Î»W,Bdtâˆ’ âˆ«T 0 1 L âˆ‘H i=1âˆ¥J(i,H) vec[âˆ‡(H)L(W,B)]âˆ¥ 2 2dt = L0eâˆ’4Ïƒ2 min( ËœGH) âˆ«T 0 Î»W,Bdtâˆ’ âˆ«T 0 1 L âˆ‘H i=1âˆ¥J(i,H) vec[âˆ‡(H)L(W,B)]âˆ¥ 2 2dt By recalling the deï¬nition of L = L(W,B) âˆ’Lâˆ— H and that d dtL â‰¤0, we have that if L(WT,BT) âˆ’Lâˆ— H > 0, then L(Wt,Bt) âˆ’Lâˆ— H >0 for all tâˆˆ[0,T], and L(WT,BT) âˆ’Lâˆ— H â‰¤(L(W0,B0) âˆ’Lâˆ— H)e âˆ’4Ïƒ2 min( ËœGH) âˆ«T 0 Î»Wt,Btdtâˆ’ âˆ«T 0 1 L(Wt,Bt)âˆ’Lâˆ— H âˆ‘H i=1âˆ¥J(i,H) vec[âˆ‡(H)L(Wt,Bt)]âˆ¥ 2 2dt . (40) Using the property of Kronecker product, Î»min([(B(H),t...B (1),t)âŠ¤B(H),tÂ·Â·Â·B(1),t âŠ—Imy ]) = Î»min((B(H),t...B (1),t)âŠ¤B(H),tÂ·Â·Â·B(1),t),Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth which implies that Î»(H) T = inf tâˆˆ[0,T] Î»Wt,Bt. Thus, by noticing thatâˆ«T 0 1 L(Wt,Bt)âˆ’Lâˆ— H âˆ‘H i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(Wt,Bt)] îµ¹îµ¹2 2 dtâ‰¥0, equation (40) implies that L(WT,BT) âˆ’Lâˆ— H â‰¤(L(W0,B0) âˆ’Lâˆ— H)e âˆ’4Î»(H) T Ïƒ2 min( ËœGH)Tâˆ’ âˆ«T 0 1 L(Wt,Bt)âˆ’Lâˆ— H âˆ‘H i=1âˆ¥J(i,H) vec[âˆ‡(H)L(Wt,Bt)]âˆ¥ 2 2dt â‰¤(L(W0,B0) âˆ’Lâˆ— H)eâˆ’4Î»(H) T Ïƒ2 min( ËœGH)T = (L(W0,B0) âˆ’Lâˆ— H)eâˆ’4Î»(H) T Ïƒ2 min(X(SH)âˆ—I)T A.2. Proof of Proposition 1 From Deï¬nition 4, we have that Ïƒmin( Â¯B(1:H)) = Ïƒmin(B(H)B(Hâˆ’1) Â·Â·Â·B(1)) â‰¥Î³ for all (W,B) such that L(W,B) â‰¤ L(W0,B0). From equation (37) in the proof of Theorem 1, it holds that d dtL(Wt,Bt) â‰¤0 for all t. Thus, we have that L(Wt,Bt) â‰¤L(W0,B0) and hence Ïƒmin( Â¯B(1:H) t ) â‰¥Î³for all t. Under this problem setting (mH â‰¥mx), this implies that Î»min(( Â¯B(1:H) t )âŠ¤Â¯B(1:H) t ) â‰¥Î³2 for all tand thus Î»(H) T â‰¥Î³2. A.3. Proof of Proposition 2 We ï¬rst give the complete version of Proposition 2. Proposition 4 is the formal version of Proposition 2 and shows that our singular margin generalizes deï¬ciency margin proposed in Arora et al. (2019a). Using the deï¬ciency margin assumption, Arora et al. (2019a) analyzed the following optimization problem: minimize ËœW ËœL( ËœW(1),..., ËœW(H+1)) : = 1 2âˆ¥ËœW(H+1) ËœW(H) Â·Â·Â· ËœW(1) âˆ’ËœÎ¦âˆ¥2 F (41) = 1 2âˆ¥ËœWâŠ¤ (1) ËœWâŠ¤ (2) Â·Â·Â· ËœWâŠ¤ (H+1) âˆ’ËœÎ¦âŠ¤âˆ¥2 F, (42) where ËœÎ¦ âˆˆRËœmyÃ—Ëœmx is a target matrix and the last equality follows from âˆ¥Mâˆ¥F = âˆ¥MâŠ¤âˆ¥F for any matrix M by the deï¬nition of the Frobenius norm. Therefore, this optimization problem (41) from the previous work is equivalent to the following optimization problem in our notation: minimize W,B L(W,B) := 1 2âˆ¥WB(H)B(Hâˆ’1) Â·Â·Â·B(1) âˆ’Î¦âˆ¥2 F, (43) where WB(H)B(Hâˆ’1) Â·Â·Â·B(1) = ËœW(H+1) ËœW(H) Â·Â·Â· ËœW(1) (i.e., W = ËœW(H+1) with B(l) = ËœW(l)) and Î¦ = ËœÎ¦ if Ëœmy â‰¥Ëœmx, and WB(H)B(Hâˆ’1) Â·Â·Â·B(1) = ËœWâŠ¤ (1) ËœWâŠ¤ (2) Â·Â·Â· ËœWâŠ¤ (H+1) (i.e., W = ËœWâŠ¤ (1) with B(l) = ËœWâŠ¤ (H+2âˆ’l)) and Î¦ = ËœÎ¦âŠ¤if Ëœmy < Ëœmx. That is, we have Î¦ âˆˆRmyÃ—mx where my = Ëœmy with mx = Ëœmx if Ëœmy â‰¥Ëœmx, and my = Ëœmx with mx = Ëœmy if Ëœmy < Ëœmx. Therefore, our general problem framework with graph structures can be reduced and applicable to the previous optimization problem without graph structures by setting 1 nXXâŠ¤= I, S = I, I= [n], f(X,W,B ) = WB(H)B(Hâˆ’1) Â·Â·Â·B(1), and â„“(q,Î¦) = 1 2 âˆ¥qâˆ’Î¦âˆ¥2 F where Î¦ âˆˆRmyÃ—mx is a target matrix with my â‰¥mx without loss of generality. An initialization (W0,B0) is said to have deï¬ciency margin c >0 if the end-to-end matrix W0 Â¯B(1:H) 0 of the initialization (W0,B0) has deï¬ciency margin c> 0 with respect to the target Î¦ (Arora et al., 2019a, Deï¬nition 2): i.e., Arora et al. (2019a) assumed that the initialization (W0,B0) has deï¬ciency margin c> 0 (as it is also invariant to the transpose of ËœW(H+1) ËœW(H) Â·Â·Â· ËœW(1) âˆ’ËœÎ¦). Proposition 4. Consider the optimization problem in (Arora et al., 2019a) by setting 1 nXXâŠ¤ = I, S = I, I = [ n], f(X,W,B ) = WB(H)B(Hâˆ’1) Â·Â·Â·B(1), and â„“(q,Î¦) = 1 2 âˆ¥qâˆ’Î¦âˆ¥2 F where Î¦ âˆˆRmyÃ—mx is a target matrix with my â‰¥mx without loss of generality (since the transpose of these two dimensions leads to the equivalent optimization problem under this setting: see above). Then, if an initialization (W0,B0) has deï¬ciency margin c> 0, it has singular margin Î³ >0. Proof of Proposition 4. By the deï¬nition of the deï¬ciency margin (Arora et al., 2019a, Deï¬nition 2) and its consequence (Arora et al., 2019a, Claim 1), if an initialization (W0,B0) has deï¬ciency margin c> 0, then any pair (W,B) for which L(W,B) â‰¤L(W0,B0) satisï¬es Ïƒmin(WB(H)B(Hâˆ’1) Â·Â·Â·B(1)) â‰¥c> 0. Since the number of nonzero singular values isOptimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth equal to the matrix rank, this implies that rank(WB(H)B(Hâˆ’1) Â·Â·Â·B(1)) â‰¥min(my,mx) for any pair (W,B) for which L(W,B) â‰¤L(W0,B0). Since rank(MMâ€²) â‰¤min(rank(M),rank(Mâ€²)), this implies that mH â‰¥min(my,mx) = mx, (44) (as well as ml â‰¥min(my,mx) for all l), and that for any pair (W,B) for which L(W,B) â‰¤L(W0,B0), mx = min(my,mx) â‰¤rank(WB(H)B(Hâˆ’1) Â·Â·Â·B(1)) â‰¤min(rank(W),rank(B(H)B(Hâˆ’1) Â·Â·Â·B(1))) (45) â‰¤rank(B(H)B(Hâˆ’1) Â·Â·Â·B(1)) â‰¤mx. (46) This shows that rank(B(H)B(Hâˆ’1) Â·Â·Â·B(1)) = mx for any pair (W,B) for which L(W,B) â‰¤ L(W0,B0). Since mH â‰¥ mx from (44) and the number of nonzero singular values is equal to the matrix rank, this implies that Ïƒmin(B(H)B(Hâˆ’1) Â·Â·Â·B(1)) â‰¥Î³ for some Î³ >0 for any pair (W,B) for which L(W,B) â‰¤L(W0,B0). Thus, if an initialization (W0,B0) has deï¬ciency margin c> 0, then it has singular margin Î³ >0. A.4. Proof of Theorem 2 This section completes the proof of Theorem 2. We compute the derivatives of the output of multiscale linear GNN with respect to the parameters W(l) and B(l) in Appendix A.4.1. Then using these derivatives, we compute the gradient of the loss with respect to W(l) in Appendix A.4.2 and B(l) in Appendix A.4.3. We then rearrange the formula of the gradients such that they are related to the formula of âˆ‡(l)L(W,B) in Appendices A.4.4. Using the proven relation, we ï¬rst analyze the dynamics induced in the space of W(l)B(l)B(lâˆ’1) Â·Â·Â·B(1) in Appendix A.4.5, and then the dynamics induced int the space of loss value L(W,B) in Appendix A.4.6. Finally, we complete the proof by using the assumption of using the square loss in Appendices A.4.7â€“A.4.10. In the following, we ï¬rst prove the statement for the case of I= [n] for the simplicity of notation and then prove the statement for the general case afterwards. A.4.1. D ERIVATION OF FORMULA FOR âˆ‚vec[Ë†Y] âˆ‚vec[W(l)] âˆˆRmynÃ—myml AND âˆ‚vec[Ë†Y] âˆ‚vec[B(l)] âˆˆRmynÃ—mlmlâˆ’1 We can easily compute âˆ‚vec[Ë†Y] âˆ‚vec[W(l)] by using the property of the Kronecker product as follows: âˆ‚vec[Ë†Y] âˆ‚vec[W(l)] = âˆ‚ âˆ‚vec[W(l)] Hâˆ‘ k=0 vec[W(k)X(k)] = âˆ‚ âˆ‚vec[W(l)] Hâˆ‘ k=0 [XâŠ¤ (k) âŠ—Imy ] vec[W(k)] = [XâŠ¤ (l) âŠ—Imy ] âˆˆRmynÃ—myml (47) We now compute âˆ‚vec[Ë†Y] âˆ‚vec[B(l)] by using the chain rule and the property of the Kronecker product as follows: âˆ‚vec[Ë†Y] âˆ‚vec[B(l)] = âˆ‚ âˆ‚vec[B(l)] Hâˆ‘ k=0 vec[W(k)X(k)] = âˆ‚ âˆ‚vec[B(l)] Hâˆ‘ k=0 [In âŠ—W(k)] vec[X(k)] = Hâˆ‘ k=0 [In âŠ—W(k)]âˆ‚vec[X(k)] âˆ‚vec[B(l)] = Hâˆ‘ k=l [In âŠ—W(k)]âˆ‚vec[X(k)] âˆ‚vec[X(l)] âˆ‚vec[X(l)] âˆ‚vec[B(l)] = Hâˆ‘ k=l [In âŠ—W(k)]âˆ‚vec[X(k)] âˆ‚vec[X(l)] âˆ‚vec[B(l)X(lâˆ’1)S] âˆ‚vec[B(l)]Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth = Hâˆ‘ k=l [In âŠ—W(k)]âˆ‚vec[X(k)] âˆ‚vec[X(l)] âˆ‚[(X(lâˆ’1)S)âŠ¤âŠ—Iml] vec[B(l)] âˆ‚vec[B(l)] = Hâˆ‘ k=l [In âŠ—W(k)]âˆ‚vec[X(k)] âˆ‚vec[X(l)] [(X(lâˆ’1)S)âŠ¤âŠ—Iml] Here, for any kâ‰¥1, vec[X(k)] = vec[B(k)X(kâˆ’1)S] = vec[SâŠ¤âŠ—B(k)] vec[X(kâˆ’1)]. By recursively applying this, we have that for any kâ‰¥l, vec[X(k)] = vec[SâŠ¤âŠ—B(k)] vec[SâŠ¤âŠ—B(kâˆ’1)] Â·Â·Â·vec[SâŠ¤âŠ—B(l+1)] vec[X(l)] = vec[(Skâˆ’l)âŠ¤âŠ—B(k)B(kâˆ’1) Â·Â·Â·B(l+1)] vec[X(l)], where S0 := In and B(k)B(kâˆ’1) Â·Â·Â·B(l+1) := Iml if k= l. Therefore, âˆ‚vec[X(k)] âˆ‚vec[X(l)] = vec[(Skâˆ’l)âŠ¤âŠ—B(k)B(kâˆ’1) Â·Â·Â·B(l+1)]. Combining the above equations yields âˆ‚vec[Ë†Y] âˆ‚vec[B(l)] = Hâˆ‘ k=l [In âŠ—W(k)]âˆ‚vec[X(k)] âˆ‚vec[X(l)] [(X(lâˆ’1)S)âŠ¤âŠ—Iml] = Hâˆ‘ k=l [In âŠ—W(k)] vec[(Skâˆ’l)âŠ¤âŠ—B(k)B(kâˆ’1) Â·Â·Â·B(l+1)][(X(lâˆ’1)S)âŠ¤âŠ—Iml] = Hâˆ‘ k=l [(X(lâˆ’1)Skâˆ’l+1)âŠ¤âŠ—W(k)B(k)B(kâˆ’1) Â·Â·Â·B(l+1)] âˆˆRmynÃ—mlmlâˆ’1. (48) A.4.2. D ERIVATION OF A FORMULA OF âˆ‡W(l)L(W,B) âˆˆRmyÃ—ml Using the chain rule and (47), we have that âˆ‚L(W,B) âˆ‚vec[W(l)] = âˆ‚L(W,B) âˆ‚vec[Ë†Y] âˆ‚vec[Ë†Y] âˆ‚vec[W(l)] = âˆ‚L(W,B) âˆ‚vec[Ë†Y] [XâŠ¤ (l) âŠ—Imy ]. Thus, with âˆ‚L(W,B) âˆ‚Ë†Y âˆˆRmyÃ—n, by using âˆ‡vec[W(l)]L(W,B) = (âˆ‚L(W,B) âˆ‚vec[W(l)] )âŠ¤ = [X(l) âŠ—Imy ] ( âˆ‚L(W,B) âˆ‚vec[Ë†Y] )âŠ¤ = [X(l) âŠ—Imy ] vec [âˆ‚L(W,B) âˆ‚Ë†Y ] = vec [âˆ‚L(W,B) âˆ‚Ë†Y XâŠ¤ (l) ] âˆˆRmyml. Therefore, âˆ‡W(l)L(W,B) = âˆ‚L(W,B) âˆ‚Ë†Y XâŠ¤ (l) âˆˆRmyÃ—ml. (49)Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth A.4.3. D ERIVATION OF A FORMULA OF âˆ‡B(l)L(W,B) âˆˆRmlÃ—mlâˆ’1 Using the chain rule and (48), we have that âˆ‚L(W,B) âˆ‚vec[B(l)] = âˆ‚L(W,B) âˆ‚vec[Ë†Y] âˆ‚vec[Ë†Y] âˆ‚vec[B(l)] = âˆ‚L(W,B) âˆ‚vec[Ë†Y] Hâˆ‘ k=l [(X(lâˆ’1)Skâˆ’l+1)âŠ¤âŠ—W(k)B(k)B(kâˆ’1) Â·Â·Â·B(l+1)]. Thus, with âˆ‚L(W,B) âˆ‚Ë†Y âˆˆRmyÃ—n, âˆ‡vec[B(l)]L(W,B) = (âˆ‚L(W,B) âˆ‚vec[B(l)] )âŠ¤ = Hâˆ‘ k=l [X(lâˆ’1)Skâˆ’l+1 âŠ—(W(k)B(k)B(kâˆ’1) Â·Â·Â·B(l+1))âŠ¤] ( âˆ‚L(W,B) âˆ‚vec[Ë†Y] )âŠ¤ = Hâˆ‘ k=l [X(lâˆ’1)Skâˆ’l+1 âŠ—(W(k)B(k)B(kâˆ’1) Â·Â·Â·B(l+1))âŠ¤] vec [âˆ‚L(W,B) âˆ‚Ë†Y ] = Hâˆ‘ k=l vec [ (W(k)B(k)B(kâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‚L(W,B) âˆ‚Ë†Y (X(lâˆ’1)Skâˆ’l+1)âŠ¤ ] âˆˆRmlmlâˆ’1. Therefore, âˆ‡B(l)L(W,B) = Hâˆ‘ k=l (W(k)B(k)B(kâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‚L(W,B) âˆ‚Ë†Y (X(lâˆ’1)Skâˆ’l+1)âŠ¤âˆˆRmlÃ—mlâˆ’1. (50) A.4.4. R ELATING GRADIENTS TO âˆ‡(l)L We now relate the gradients of the loss to âˆ‡(l)L, which is deï¬ned by âˆ‡(l)L(W,B) := âˆ‚L(W,B) âˆ‚Ë†Y (XSl)âŠ¤âˆˆRmyÃ—mx. By using this deï¬nition and (49), we have that âˆ‡W(l)L(W,B) = âˆ‚L(W,B) âˆ‚Ë†Y XâŠ¤ (l) = âˆ‚L(W,B) âˆ‚Ë†Y (B(l)X(lâˆ’1)S)âŠ¤ = âˆ‚L(W,B) âˆ‚Ë†Y (B(l)B(lâˆ’1) ...B (1)XSl)âŠ¤ = âˆ‡(l)L(W,B)(B(l)B(lâˆ’1) ...B (1))âŠ¤, where B(l)B(lâˆ’1) ...B (1) := Imx if l= 0. Similarly, by using the deï¬nition and (50), âˆ‡B(l)L(W,B) = Hâˆ‘ k=l (W(k)B(k)B(kâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‚L(W,B) âˆ‚Ë†Y (X(lâˆ’1)Skâˆ’l+1)âŠ¤ = Hâˆ‘ k=l (W(k)B(k)B(kâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‚L(W,B) âˆ‚Ë†Y (B(lâˆ’1)B(lâˆ’2) ...B (1)XSlâˆ’1Skâˆ’l+1)âŠ¤ = Hâˆ‘ k=l (W(k)B(k)B(kâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‚L(W,B) âˆ‚Ë†Y (B(lâˆ’1)B(lâˆ’2) ...B (1)XSk)âŠ¤Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth = Hâˆ‘ k=l (W(k)B(k)B(kâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‡(k)L(W,B)(B(lâˆ’1)B(lâˆ’2) ...B (1))âŠ¤ where B(lâˆ’1)B(lâˆ’2) ...B (1) := Imx if l= 1. In summary thus far, we have that âˆ‡W(l)L(W,B) = âˆ‡(l)L(W,B)(B(l)B(lâˆ’1) ...B (1))âŠ¤âˆˆRmyÃ—ml, (51) and âˆ‡B(l)L(W,B) = Hâˆ‘ k=l (W(k)B(k)B(kâˆ’1) Â·Â·Â·B(l+1))âŠ¤âˆ‡(k)L(W,B)(B(lâˆ’1)B(lâˆ’2) ...B (1))âŠ¤âˆˆRmlÃ—mlâˆ’1, (52) where âˆ‡(l)L(W,B) := âˆ‚L(W,B) âˆ‚Ë†Y (XSl)âŠ¤âˆˆRmyÃ—mx, B(k)B(kâˆ’1) Â·Â·Â·B(l+1) := Iml if k= l, B(l)B(lâˆ’1) ...B (1) := Imx if l= 0, and B(lâˆ’1)B(lâˆ’2) ...B (1) := Imx if l= 1. A.4.5. D YNAMICS INDUCED IN THE SPACE OF W(l)B(l)B(lâˆ’1) Â·Â·Â·B(1) We now consider the Dynamics induced in the space ofW(l)B(l)B(lâˆ’1) Â·Â·Â·B(1). We ï¬rst consider the following discrete version of the dynamics: Wâ€² (l) = W(l) âˆ’Î±âˆ‡W(l)L(W,B) Bâ€² (l) = B(l) âˆ’Î±âˆ‡B(l)L(W,B). This dynamics induces the following dynamics: Wâ€² (l)Bâ€² (l)Bâ€² (lâˆ’1) Â·Â·Â·Bâ€² (1) = (W(l) âˆ’Î±âˆ‡W(l)L(W,B))(B(l) âˆ’Î±âˆ‡B(l)L(W,B)) Â·Â·Â·(B(1) âˆ’Î±âˆ‡B(1)L(W,B)). Deï¬ne Z(l) := W(l)B(l)B(lâˆ’1) Â·Â·Â·B(1) and Zâ€² (l) := Wâ€² (l)Bâ€² (l)Bâ€² (lâˆ’1) Â·Â·Â·Bâ€² (1). Then, we can rewrite Zâ€² (l) = (W(l) âˆ’Î±âˆ‡W(l)L(W,B))(B(l) âˆ’Î±âˆ‡B(l)L(W,B)) Â·Â·Â·(B(1) âˆ’Î±âˆ‡B(1)L(W,B)). By expanding the multiplications, this can be written as: Zâ€² (l) = Z(l) âˆ’Î±âˆ‡W(l)L(W,B)B(l) Â·Â·Â·B(1) âˆ’Î± lâˆ‘ i=1 W(l)B(l) Â·Â·Â·B(i+1)âˆ‡B(i)L(W,B)B(iâˆ’1) Â·Â·Â·B(1) + O(Î±2) By vectorizing both sides, vec[Zâ€² (l)] âˆ’vec[Z(l)] = âˆ’Î±vec[âˆ‡W(l)L(W,B)B(l) Â·Â·Â·B(1)] âˆ’Î± lâˆ‘ i=1 vec[W(l)B(l) Â·Â·Â·B(i+1)âˆ‡B(i)L(W,B)B(iâˆ’1) Â·Â·Â·B(1)] + O(Î±2) Here, using the formula of âˆ‡W(l)L(W,B) and âˆ‡B(l)L(W,B), we have that vec[âˆ‡W(l)L(W,B)B(l) Â·Â·Â·B(1)] = vec[âˆ‡(l)L(W,B)(B(l) ...B (1))âŠ¤B(l) Â·Â·Â·B(1)] = [(B(l) ...B (1))âŠ¤B(l) Â·Â·Â·B(1) âŠ—Imy ] vec[âˆ‡(l)L(W,B)], and lâˆ‘ i=1 vec[W(l)B(l) Â·Â·Â·B(i+1)âˆ‡B(i)L(W,B)B(iâˆ’1) Â·Â·Â·B(1)]Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth = lâˆ‘ i=1 vec [ W(l)B(l) Â·Â·Â·B(i+1) Hâˆ‘ k=i (W(k)B(k) Â·Â·Â·B(i+1))âŠ¤âˆ‡(k)L(W,B)(B(iâˆ’1) ...B (1))âŠ¤B(iâˆ’1) Â·Â·Â·B(1) ] = lâˆ‘ i=1 Hâˆ‘ k=i vec [ W(l)B(l) Â·Â·Â·B(i+1)(W(k)B(k) Â·Â·Â·B(i+1))âŠ¤âˆ‡(k)L(W,B)(B(iâˆ’1) ...B (1))âŠ¤B(iâˆ’1) Â·Â·Â·B(1) ] = lâˆ‘ i=1 Hâˆ‘ k=i [(B(iâˆ’1) ...B (1))âŠ¤B(iâˆ’1) Â·Â·Â·B(1) âŠ—W(l)B(l) Â·Â·Â·B(i+1)(W(k)B(k) Â·Â·Â·B(i+1))âŠ¤] vec [ âˆ‡(k)L(W,B) ] . Summarizing above, vec[Zâ€² (l)] âˆ’vec[Z(l)] = âˆ’Î±[(B(l) ...B (1))âŠ¤B(l) Â·Â·Â·B(1) âŠ—Imy ] vec[âˆ‡(l)L(W,B)] âˆ’Î± lâˆ‘ i=1 Hâˆ‘ k=i [(B(iâˆ’1) ...B (1))âŠ¤B(iâˆ’1) Â·Â·Â·B(1) âŠ—W(l)B(l) Â·Â·Â·B(i+1)(W(k)B(k) Â·Â·Â·B(i+1))âŠ¤] vec [ âˆ‡(k)L(W,B) ] + O(Î±2) Therefore, the induced continuous dynamics of Z(l) = W(l)B(l)B(lâˆ’1) Â·Â·Â·B(1) is d dtvec[Z(l)] = âˆ’F(l) vec[âˆ‡(l)L(W,B)] âˆ’ lâˆ‘ i=1 Hâˆ‘ k=i JâŠ¤ (i,l)J(i,k) vec [ âˆ‡(k)L(W,B) ] where F(l) = [(B(l) ...B (1))âŠ¤B(l) Â·Â·Â·B(1) âŠ—Imy ], and J(i,l) = [B(iâˆ’1) ...B (1) âŠ—(W(l)B(l) Â·Â·Â·B(i+1))âŠ¤]. This is because JâŠ¤ (i,k)J(i,k) = [(B(iâˆ’1) ...B (1))âŠ¤âŠ—W(l)B(l) Â·Â·Â·B(i+1)][B(iâˆ’1) ...B (1) âŠ—(W(k)B(k) Â·Â·Â·B(i+1))âŠ¤] = [(B(iâˆ’1) ...B (1))âŠ¤B(iâˆ’1) ...B (1) âŠ—W(l)B(l) Â·Â·Â·B(i+1)(W(k)B(k) Â·Â·Â·B(i+1))âŠ¤]. A.4.6. D YNAMICS INDUCED INT THE SPACE OF LOSS VALUE L(W,B) We now analyze the dynamics induced int the space of loss valueL(W,B). Deï¬ne L(W,B) := â„“(f(X,W,B ),Y ), where â„“is chosen later. Using chain rule, d dtL(W,B) = d dtL0(Z(H),...,Z (0)) = Hâˆ‘ l=0 âˆ‚L0(Z(l),...,Z (0)) âˆ‚vec[Z(l)] dvec[Z(l)] dt , where L0(Z(H),...,Z (0)) = â„“(f0(X,Z),Y ), f0(X,Z) = Hâˆ‘ l=0 Z(l)XSl, and Z(l) = W(l)B(l)B(lâˆ’1) Â·Â·Â·B(1). Since f0(X,Z) = f(X,W,B ) = Ë†Y and L0(Z(H),...,Z (0)) = L(W,B), (âˆ‚L0(Z(l),...,Z (0)) âˆ‚vec[Z(l)] )âŠ¤ = ( âˆ‚L(W,B) âˆ‚vec[Ë†Y] âˆ‚vec[Ë†Y] âˆ‚vec[Z(l)] )âŠ¤Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth = ( âˆ‚L(W,B) âˆ‚vec[Ë†Y] ( âˆ‚ âˆ‚vec[Z(l)] Hâˆ‘ k=0 [(XSk)âŠ¤âŠ—Imy ] vec[Z(k)] ))âŠ¤ = [XSl âŠ—Imy ] vec [âˆ‚L(W,B) âˆ‚Ë†Y ] = vec [âˆ‚L(W,B) âˆ‚Ë†Y (XSl)âŠ¤ ] = vec[âˆ‡(l)L(W,B)] Therefore, d dtL(W,B) = Hâˆ‘ l=0 vec[âˆ‡(l)L(W,B)]âŠ¤dvec[Z(l)] dt = âˆ’ Hâˆ‘ l=0 vec[âˆ‡(l)L(W,B)]âŠ¤F(l) vec[âˆ‡(l)L(W,B)] âˆ’ Hâˆ‘ l=1 lâˆ‘ i=1 Hâˆ‘ k=i vec[âˆ‡(l)L(W,B)]âŠ¤JâŠ¤ (i,l)J(i,k) vec [ âˆ‡(k)L(W,B) ] To simplify the second term, deï¬ne M(l,i) = âˆ‘H k=ivec[âˆ‡(l)L(W,B)]âŠ¤JâŠ¤ (i,l)J(i,k) vec [ âˆ‡(k)L(W,B) ] and note that we can expand the double sums and regroup terms as follows: Hâˆ‘ l=1 lâˆ‘ i=1 M(l,i) = Hâˆ‘ l=1 M(l,1) + Hâˆ‘ l=2 M(l,2) + Â·Â·Â· + Hâˆ‘ l=H M(l,H) = Hâˆ‘ i=1 Hâˆ‘ l=i M(l,i). Moreover, for each iâˆˆ{1,...,H }, Hâˆ‘ l=i M(l,i) = Hâˆ‘ l=i Hâˆ‘ k=i vec[âˆ‡(l)L(W,B)]âŠ¤JâŠ¤ (i,l)J(i,k) vec [ âˆ‡(k)L(W,B) ] = (Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] )âŠ¤(Hâˆ‘ k=i J(i,k) vec [ âˆ‡(k)L(W,B) ] ) = îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 Using these facts, the second term can be simpliï¬ed as Hâˆ‘ l=1 lâˆ‘ i=1 Hâˆ‘ k=i vec[âˆ‡(l)L(W,B)]âŠ¤JâŠ¤ (i,l)J(i,k) vec [ âˆ‡(k)L(W,B) ] = Hâˆ‘ l=1 lâˆ‘ i=1 M(l,i) = Hâˆ‘ i=1 Hâˆ‘ l=i M(l,i) = Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 Combining these, d dtL(W,B) = âˆ’ Hâˆ‘ l=0 vec[âˆ‡(l)L(W,B)]âŠ¤F(l) vec[âˆ‡(l)L(W,B)] âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 (53)Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth Since F(l) is real symmetric and positive semideï¬nite, d dtL(W,B) â‰¤âˆ’ Hâˆ‘ l=0 Î»min(F(l))âˆ¥vec[âˆ‡(l)L(W,B)]âˆ¥2 2 âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 (54) A.4.7. C OMPLETING THE PROOF BY USING THE ASSUMPTION OF THE SQUARE LOSS Using the assumption that L(W,B) = â„“(f(X,W,B ),Y ) = âˆ¥f(X,W,B ) âˆ’Yâˆ¥2 F with Ë†Y = f(X,W,B ), we have âˆ‚L(W,B) âˆ‚Ë†Y = âˆ‚ âˆ‚Ë†Y âˆ¥Ë†Y âˆ’Yâˆ¥2 F = 2( Ë†Y âˆ’Y) âˆˆRmyÃ—n, and hence vec[âˆ‡(l)L(W,B)] = vec [âˆ‚L(W,B) âˆ‚Ë†Y (XSl)âŠ¤ ] = 2 vec [ ( Ë†Y âˆ’Y)(XSl)âŠ¤ ] = 2[XSl âŠ—Imy ] vec[Ë†Y âˆ’Y]. Therefore, âˆ¥vec[âˆ‡(l)L(W,B)]âˆ¥2 2 = 4 vec[Ë†Y âˆ’Y]âŠ¤[(XSl)âŠ¤XSl âŠ—Imy ] vec[Ë†Y âˆ’Y]. (55) We are now ready to complete the proof of Theorem 2 for each cases (i), (ii) and (iii). A.4.8. C ASE (I): C OMPLETING THE PROOF OF THEOREM 2 (I) Using equation (54) and (55) with Î»W,B = min0â‰¤lâ‰¤H Î»min(F(l)), we have that d dtL(W,B) â‰¤âˆ’Î»W,B Hâˆ‘ l=0 âˆ¥vec[âˆ‡(l)L(W,B)]âˆ¥2 2 âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 â‰¤âˆ’4Î»W,B Hâˆ‘ l=0 vec[Ë†Y âˆ’Y]âŠ¤[(XSl)âŠ¤XSl âŠ—Imy ] vec[Ë†Y âˆ’Y] âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 â‰¤âˆ’4Î»W,Bvec[Ë†Y âˆ’Y]âŠ¤ [(Hâˆ‘ l=0 (XSl)âŠ¤XSl ) âŠ—Imy ] vec[Ë†Y âˆ’Y] âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 = âˆ’4Î»W,Bvec[Ë†Y âˆ’Y]âŠ¤[ GâŠ¤ HGH âŠ—Imy ] vec[Ë†Y âˆ’Y] âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 where the last line follows from the following fact: GâŠ¤ HGH = ï£® ï£¯ï£¯ï£¯ï£° X XS ... XSH ï£¹ ï£ºï£ºï£ºï£» âŠ¤ï£® ï£¯ï£¯ï£¯ï£° X XS ... XSH ï£¹ ï£ºï£ºï£ºï£»= Hâˆ‘ l=0 (XSl)âŠ¤XSl. Decompose vec[Ë†Yâˆ’Y] as vec[Ë†Yâˆ’Y] = v+vâŠ¥, where v= PGâŠ¤ HâŠ—Imy vec[Ë†Yâˆ’Y], vâŠ¥= (Imynâˆ’PGâŠ¤ HâŠ—Imy ) vec[Ë†Yâˆ’Y], and PGâŠ¤ HâŠ—Imy âˆˆRmynÃ—mynrepresents the orthogonal projection onto the column space ofGâŠ¤ HâŠ—Imy âˆˆRmynÃ—(H+1)mymx. Then, vec[Ë†Y âˆ’Y]âŠ¤[ GâŠ¤ HGH âŠ—Imy ] vec[Ë†Y âˆ’Y] = (v+ vâŠ¥)âŠ¤[ GâŠ¤ H âŠ—Imy ][ GH âŠ—Imy ] (v+ vâŠ¥) = vâŠ¤[ GâŠ¤ H âŠ—Imy ][ GH âŠ—Imy ] v â‰¥Ïƒ2 min(GH)âˆ¥PGâŠ¤ HâŠ—Imy vec[Ë†Y âˆ’Y]âˆ¥2 2Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth = Ïƒ2 min(GH)âˆ¥PGâŠ¤ HâŠ—Imy vec[Ë†Y] âˆ’PGâŠ¤ HâŠ—Imy vec[Y]âˆ¥2 2 = Ïƒ2 min(GH)âˆ¥vec[Ë†Y] âˆ’PGâŠ¤ HâŠ—Imy vec[Y] Â±vec[Y]âˆ¥2 2 = Ïƒ2 min(GH)âˆ¥vec[Ë†Y] âˆ’vec[Y] + (Imyn âˆ’PGâŠ¤ HâŠ—Imy ) vec[Y]âˆ¥2 2 â‰¥Ïƒ2 min(GH)(âˆ¥vec[Ë†Y âˆ’Y]âˆ¥2 âˆ’âˆ¥(Imyn âˆ’PGâŠ¤ HâŠ—Imy ) vec[Y]âˆ¥2)2 â‰¥Ïƒ2 min(GH)(âˆ¥vec[Ë†Y âˆ’Y]âˆ¥2 2 âˆ’âˆ¥(Imyn âˆ’PGâŠ¤ HâŠ—Imy ) vec[Y]âˆ¥2 2, where we used the fact that the singular values of [ GâŠ¤ H âŠ—Imy ] are products of singular values of GH and Imy . By noticing that L(W,B) = âˆ¥vec[Ë†Y âˆ’Y]âˆ¥2 2 and Lâˆ— 1:H = âˆ¥(Imyn âˆ’PGâŠ¤ HâŠ—Imy ) vec[Y]âˆ¥2 2 , vec[Ë†Y âˆ’Y]âŠ¤[ GâŠ¤ HGH âŠ—Imy ] vec[Ë†Y âˆ’Y] â‰¥Ïƒ2 min(GH)(L(W,B) âˆ’Lâˆ— 1:H). Therefore, d dtL(W,B) â‰¤âˆ’4Î»W,Bvec[Ë†Y âˆ’Y]âŠ¤[ GâŠ¤ HGH âŠ—Imy ] vec[Ë†Y âˆ’Y] âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 â‰¤âˆ’4Î»W,BÏƒ2 min(GH)(L(W,B) âˆ’Lâˆ— 1:H) âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 Since d dtLâˆ— 1:H = 0, d dt(L(W,B) âˆ’Lâˆ— 1:H) â‰¤âˆ’4Î»W,BÏƒ2 min(GH)(L(W,B) âˆ’Lâˆ— 1:H) âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 By deï¬ning L = L(W,B) âˆ’Lâˆ— 1:H, dL dt â‰¤âˆ’4Î»W,BÏƒ2 min(GH)L âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 (56) Since d dtL â‰¤0 and L â‰¥0, if L = 0 at some time Â¯t, then L = 0 for any time tâ‰¥Â¯t. Therefore, if L = 0 at some time Â¯t, then we have the desired statement of this theorem for any time tâ‰¥Â¯t. Thus, we can focus on the time interval [0,Â¯t] such that L >0 for any time tâˆˆ[0,Â¯t] (here, it is allowed to have Â¯t= âˆ). Thus, focusing on the time interval with L >0 , equation (56) implies that 1 L dL dt â‰¤âˆ’4Î»W,BÏƒ2 min(GH) âˆ’1 L Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 By taking integral over time âˆ« T 0 1 L dL dtdtâ‰¤âˆ’ âˆ« T 0 4Î»W,BÏƒ2 min(GH)dtâˆ’ âˆ« T 0 1 L Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 dt By using the substitution rule for integrals, âˆ«T 0 1 L dL dtdt= âˆ«LT L0 1 LdL = log(LT) âˆ’log(L0), where L0 = L(W0,B0) âˆ’Lâˆ— 1:H and LT = L(WT,BT) âˆ’Lâˆ— 1:H. Thus, log(LT) âˆ’log(L0) â‰¤âˆ’4Ïƒ2 min(GH) âˆ« T 0 Î»W,Bdtâˆ’ âˆ« T 0 1 L Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 dtOptimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth which implies that LT â‰¤elog(L0)âˆ’4Ïƒ2 min(GH) âˆ«T 0 Î»W,Bdtâˆ’ âˆ«T 0 1 L âˆ‘H i=1âˆ¥ âˆ‘H l=i J(i,l) vec[âˆ‡(l)L(W,B)]âˆ¥ 2 2dt = L0eâˆ’4Ïƒ2 min(GH) âˆ«T 0 Î»W,Bdtâˆ’ âˆ«T 0 1 L âˆ‘H i=1âˆ¥ âˆ‘H l=i J(i,l) vec[âˆ‡(l)L(W,B)]âˆ¥ 2 2dt By recalling the deï¬nition of L = L(W,B) âˆ’Lâˆ— 1:H and that d dtL â‰¤0, we have that if L(WT,BT) âˆ’Lâˆ— 1:H > 0, then L(Wt,Bt) âˆ’Lâˆ— 1:H >0 for all tâˆˆ[0,T], and L(WT,BT) âˆ’Lâˆ— 1:H â‰¤(L(W0,B0) âˆ’Lâˆ— 1:H)eâˆ’4Ïƒ2 min(GH) âˆ«T 0 Î»Wt,Btdtâˆ’ âˆ«T 0 1 L(Wt,Bt)âˆ’Lâˆ— âˆ‘H i=1âˆ¥ âˆ‘H l=i J(i,l) vec[âˆ‡(l)L(Wt,Bt)]âˆ¥ 2 2dt. By noticing that Î»(1:H) T = inf tâˆˆ[0,T] Î»Wt,Bt and that âˆ«T 0 1 L(Wt,Bt)âˆ’Lâˆ— âˆ‘H i=1 îµ¹îµ¹îµ¹âˆ‘H l=iJ(i,l) vec[âˆ‡(l)L(Wt,Bt)] îµ¹îµ¹îµ¹ 2 2 dt â‰¥0, this implies that L(WT,BT) âˆ’Lâˆ— 1:H â‰¤(L(W0,B0) âˆ’Lâˆ— 1:H)eâˆ’4Î»(1:H) T Ïƒ2 min(GH)Tâˆ’ âˆ«T 0 1 L(Wt,Bt)âˆ’Lâˆ— âˆ‘H i=1âˆ¥ âˆ‘H l=i J(i,l) vec[âˆ‡(l)L(Wt,Bt)]âˆ¥ 2 2dt â‰¤(L(W0,B0) âˆ’Lâˆ— 1:H)eâˆ’4Î»(1:H) T Ïƒ2 min(GH)T. This completes the proof of Theorem 2 (i) for the case of I= [n]. Since every step in this proof is valid when we replace f(X,W,B ) by f(X,W,B )âˆ—Iand XSl by X(Sl)âˆ—Iwithout using any assumption on Sor the relation between Slâˆ’1 and S, our proof also yields for the general case of Ithat L(WT,BT) âˆ’Lâˆ— 1:H â‰¤(L(W0,B0) âˆ’Lâˆ— 1:H)eâˆ’4Î»(1:H) T Ïƒ2 min((GH)âˆ—I)T. A.4.9. C ASE (II): C OMPLETING THE PROOF OF THEOREM 2 (II) Using equation (54) and (55) , we have that for any Hâ€²âˆˆ{0,1,...,H }, d dtL(W,B) â‰¤âˆ’Î»min(F(Hâ€²))âˆ¥vec[âˆ‡(Hâ€²)L(W,B)]âˆ¥2 2 â‰¤âˆ’4Î»min(F(Hâ€²)) vec[Ë†Y âˆ’Y]âŠ¤[(XSHâ€² )âŠ¤XSHâ€² âŠ—Imy ] vec[Ë†Y âˆ’Y] = âˆ’4Î»W,Bvec[Ë†Y âˆ’Y]âŠ¤ [ ËœGâŠ¤ Hâ€²ËœGHâ€²âŠ—Imy ] vec[Ë†Y âˆ’Y], where Î»W,B := Î»min(F(Hâ€²)), and ËœGHâ€² := XSHâ€² . Decompose vec[Ë†Yâˆ’Y] as vec[Ë†Yâˆ’Y] = v+vâŠ¥, where v= PËœGâŠ¤ Hâ€²âŠ—Imy vec[Ë†Yâˆ’Y], vâŠ¥= (Imynâˆ’PËœGâŠ¤ Hâ€²âŠ—Imy ) vec[Ë†Yâˆ’Y], and PËœGâŠ¤ Hâ€²âŠ—Imy âˆˆRmynÃ—myn represents the orthogonal projection onto the column space of ËœGâŠ¤ Hâ€²âŠ—Imy âˆˆRmynÃ—mymx. Then, vec[Ë†Y âˆ’Y]âŠ¤ [ ËœGâŠ¤ Hâ€²ËœGHâ€²âŠ—Imy ] vec[Ë†Y âˆ’Y] = (v+ vâŠ¥)âŠ¤ [ ËœGâŠ¤ Hâ€²âŠ—Imy ][ ËœGHâ€²âŠ—Imy ] (v+ vâŠ¥) = vâŠ¤ [ ËœGâŠ¤ Hâ€²âŠ—Imy ][ ËœGHâ€²âŠ—Imy ] v â‰¥Ïƒ2 min( ËœGHâ€²)âˆ¥PËœGâŠ¤ Hâ€²âŠ—Imy vec[Ë†Y âˆ’Y]âˆ¥2 2 = Ïƒ2 min( ËœGHâ€²)âˆ¥PËœGâŠ¤ Hâ€²âŠ—Imy vec[Ë†Y] âˆ’PËœGâŠ¤ Hâ€²âŠ—Imy vec[Y]âˆ¥2 2 = Ïƒ2 min( ËœGHâ€²)âˆ¥vec[Ë†Y] âˆ’PËœGâŠ¤ Hâ€²âŠ—Imy vec[Y] Â±vec[Y]âˆ¥2 2 = Ïƒ2 min( ËœGHâ€²)âˆ¥vec[Ë†Y] âˆ’vec[Y] + (Imyn âˆ’PËœGâŠ¤ Hâ€²âŠ—Imy ) vec[Y]âˆ¥2 2Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth â‰¥Ïƒ2 min( ËœGHâ€²)(âˆ¥vec[Ë†Y âˆ’Y]âˆ¥2 âˆ’âˆ¥(Imyn âˆ’PËœGâŠ¤ Hâ€²âŠ—Imy ) vec[Y]âˆ¥2)2 â‰¥Ïƒ2 min( ËœGHâ€²)(âˆ¥vec[Ë†Y âˆ’Y]âˆ¥2 2 âˆ’âˆ¥(Imyn âˆ’PËœGâŠ¤ Hâ€²âŠ—Imy ) vec[Y]âˆ¥2 2, where we used the fact that the singular values of [ ËœGâŠ¤ Hâ€²âŠ—Imy ] are products of singular values of ËœGHâ€² and Imy . By noticing that L(W,B) = âˆ¥vec[Ë†Y âˆ’Y]âˆ¥2 2 and Lâˆ— Hâ€² = âˆ¥(Imyn âˆ’PËœGâŠ¤ Hâ€²âŠ—Imy ) vec[Y]âˆ¥2 2 , we have that for any Hâ€² âˆˆ {0,1,...,H }, vec[Ë†Y âˆ’Y]âŠ¤ [ ËœGâŠ¤ Hâ€²ËœGHâ€²âŠ—Imy ] vec[Ë†Y âˆ’Y] â‰¥Ïƒ2 min( ËœGHâ€²)(L(W,B) âˆ’Lâˆ— Hâ€²). (57) Therefore, d dtL(W,B) â‰¤âˆ’4Î»W,Bvec[Ë†Y âˆ’Y]âŠ¤ [ ËœGâŠ¤ Hâ€²ËœGHâ€²âŠ—Imy ] vec[Ë†Y âˆ’Y] â‰¤âˆ’4Î»W,BÏƒ2 min( ËœGHâ€²)(L(W,B) âˆ’Lâˆ— Hâ€²) Since d dtLâˆ— Hâ€² = 0, d dt(L(W,B) âˆ’Lâˆ— Hâ€²) â‰¤âˆ’4Î»W,BÏƒ2 min( ËœGHâ€²)(L(W,B) âˆ’Lâˆ— Hâ€²) By deï¬ning L = L(W,B) âˆ’Lâˆ— Hâ€², dL dt â‰¤âˆ’4Î»W,BÏƒ2 min( ËœGHâ€²)L (58) Since d dtL â‰¤0 and L â‰¥0, if L = 0 at some time Â¯t, then L = 0 for any time tâ‰¥Â¯t. Therefore, if L = 0 at some time Â¯t, then we have the desired statement of this theorem for any time tâ‰¥Â¯t. Thus, we can focus on the time interval [0,Â¯t] such that L >0 for any time tâˆˆ[0,Â¯t] (here, it is allowed to have Â¯t= âˆ). Thus, focusing on the time interval with L >0 , equation (58) implies that 1 L dL dt â‰¤âˆ’4Î»W,BÏƒ2 min( ËœGHâ€²) By taking integral over time âˆ« T 0 1 L dL dtdtâ‰¤âˆ’ âˆ« T 0 4Î»W,BÏƒ2 min( ËœGHâ€²)dt By using the substitution rule for integrals, âˆ«T 0 1 L dL dtdt= âˆ«LT L0 1 LdL = log(LT) âˆ’log(L0), where L0 = L(W0,B0) âˆ’Lâˆ— and LT = L(WT,BT) âˆ’Lâˆ— Hâ€². Thus, log(LT) âˆ’log(L0) â‰¤âˆ’4Ïƒ2 min( ËœGHâ€²) âˆ« T 0 Î»W,Bdt which implies that LT â‰¤elog(L0)âˆ’4Ïƒ2 min( ËœGHâ€²) âˆ«T 0 Î»W,Bdt = L0eâˆ’4Ïƒ2 min( ËœGHâ€²) âˆ«T 0 Î»W,Bdt By recalling the deï¬nition of L = L(W,B) âˆ’Lâˆ— Hâ€² and that d dtL â‰¤0, we have that if L(WT,BT) âˆ’Lâˆ— Hâ€² > 0, then L(Wt,Bt) âˆ’Lâˆ— Hâ€² >0 for all tâˆˆ[0,T], and L(WT,BT) âˆ’Lâˆ— Hâ€² â‰¤(L(W0,B0) âˆ’Lâˆ— Hâ€²)eâˆ’4Ïƒ2 min( ËœGHâ€²) âˆ«T 0 Î»Wt,Btdt. By noticing that Î»(Hâ€²) T = inftâˆˆ[0,T] Î»Wt,Bt, this implies that for any Hâ€²âˆˆ{0,1,...,H }, L(WT,BT) âˆ’Lâˆ— Hâ€² â‰¤(L(W0,B0) âˆ’Lâˆ— Hâ€²)eâˆ’4Î»(Hâ€²) T Ïƒ2 min( ËœGHâ€²)TOptimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth = (L(W0,B0) âˆ’Lâˆ— Hâ€²)eâˆ’4Î»(H) T Ïƒ2 min(XSHâ€² )T This completes the proof of Theorem 2 (ii) for the case of I= [n]. Since every step in this proof is valid when we replace f(X,W,B ) by f(X,W,B )âˆ—Iand XSl by X(Sl)âˆ—Iwithout using any assumption on Sor the relation between Slâˆ’1 and S, our proof also yields for the general case of Ithat L(WT,BT) âˆ’Lâˆ— Hâ€² â‰¤(L(W0,B0) âˆ’Lâˆ— Hâ€²)eâˆ’4Î»(H) T Ïƒ2 min(X(SHâ€² )âˆ—I)T A.4.10. C ASE (III ): C OMPLETING THE PROOF OF THEOREM 2 (III ) In this case, we have the following assumption: there exist l,lâ€²âˆˆ{0,...,H }with l<l â€²such that Lâˆ— l â‰¥Lâˆ— l+1 â‰¥Â·Â·Â·â‰¥ Lâˆ— lâ€² or Lâˆ— l â‰¤Lâˆ— l+1 â‰¤Â·Â·Â·â‰¤ Lâˆ— lâ€². Using equation (54) and (55) with ËœGl = XSl, we have that d dtL(W,B) â‰¤âˆ’ Hâˆ‘ l=0 Î»min(F(l))âˆ¥vec[âˆ‡(l)L(W,B)]âˆ¥2 2 â‰¤âˆ’4 Hâˆ‘ l=0 Î»min(F(l)) vec[Ë†Y âˆ’Y]âŠ¤[(XSl)âŠ¤XSl âŠ—Imy ] vec[Ë†Y âˆ’Y] = âˆ’4 Hâˆ‘ l=0 Î»min(F(l)) vec[Ë†Y âˆ’Y]âŠ¤[ ËœGâŠ¤ l ËœGl âŠ—Imy ] vec[Ë†Y âˆ’Y] Using (57), since vec[Ë†Y âˆ’Y]âŠ¤ [ ËœGâŠ¤ l ËœGl âŠ—Imy ] vec[Ë†Y âˆ’Y] â‰¥Ïƒ2 min( ËœGl)(L(W,B) âˆ’Lâˆ— l) for any lâˆˆ{0,1,...,H }, d dtL(W,B) â‰¤âˆ’4 Hâˆ‘ l=0 Î»min(F(l))Ïƒ2 min( ËœGl)(L(W,B) âˆ’Lâˆ— l). (59) Let lâ€²â€² = lif Lâˆ— l â‰¥Lâˆ— l+1 â‰¥Â·Â·Â·â‰¥ Lâˆ— lâ€², and lâ€²â€² = lâ€²if Lâˆ— l â‰¤Lâˆ— l+1 â‰¤Â·Â·Â·â‰¤ Lâˆ— lâ€². Then, using (59) and the assumption of Lâˆ— l â‰¥Lâˆ— l+1 â‰¥Â·Â·Â·â‰¥ Lâˆ— lâ€² or Lâˆ— l â‰¤Lâˆ— l+1 â‰¤Â·Â·Â·â‰¤ Lâˆ— lâ€² for some l,lâ€²âˆˆ{0,...,H }, we have that d dtL(W,B) â‰¤âˆ’4(L(W,B) âˆ’Lâˆ— lâ€²â€²) lâ€² âˆ‘ k=l Î»min(F(k))Ïƒ2 min( ËœGk). (60) Since d dtLâˆ— lâ€²â€² = 0, d dt(L(W,B) âˆ’Lâˆ— lâ€²â€²) â‰¤âˆ’4(L(W,B) âˆ’Lâˆ— lâ€²â€²) lâ€² âˆ‘ k=l Î»min(F(k))Ïƒ2 min( ËœGk). By taking integral over time in the same way as that in the proof for the case of (i) and (ii), we have that L(WT,BT) âˆ’Lâˆ— lâ€²â€² â‰¤(L(W0,B0) âˆ’Lâˆ— lâ€²â€²)eâˆ’4 âˆ‘lâ€² k=l Ïƒ2 min( ËœGk) âˆ«T 0 Î»min(F(k),t)dt (61) Using the property of Kronecker product, Î»min(F(l),t) = Î»min([(B(l),t...B (1),t)âŠ¤B(l),tÂ·Â·Â·B(1),t âŠ—Imy ]) = Î»min((B(l),t...B (1),t)âŠ¤B(l),tÂ·Â·Â·B(1),t), which implies that Î»(k) T = inftâˆˆ[0,T] Î»min(F(k),t). Therefore, equation (61) with Î»(k) T = inftâˆˆ[0,T] Î»min(F(k),t) yields that L(WT,BT) âˆ’Lâˆ— lâ€²â€² â‰¤(L(W0,B0) âˆ’Lâˆ— lâ€²â€²)eâˆ’4 âˆ‘lâ€² k=l Î»(k) T Ïƒ2 min( ËœGk)T = (L(W0,B0) âˆ’Lâˆ— lâ€²â€²)eâˆ’4 âˆ‘lâ€² k=l Î»(k) T Ïƒ2 min(XSk)T (62)Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth This completes the proof of Theorem 2 (iii) for the case of I= [n]. Since every step in this proof is valid when we replace f(X,W,B ) by f(X,W,B )âˆ—Iand XSl by X(Sl)âˆ—Iwithout using any assumption on Sor the relation between Slâˆ’1 and S, our proof also yields for the general case of Ithat L(WT,BT) âˆ’Lâˆ— lâ€²â€² â‰¤(L(W0,B0) âˆ’Lâˆ— lâ€²â€²)eâˆ’4 âˆ‘lâ€² k=l Î»(k) T Ïƒ2 min(X(Sk)âˆ—I)T. A.5. Proof of Proposition 3 From Deï¬nition 4, for any lâˆˆ{1,2,...,H }, we have that Ïƒmin( Â¯B(1:l)) = Ïƒmin(B(l)B(lâˆ’1) Â·Â·Â·B(1)) â‰¥Î³for all (W,B) such that L(W,B) â‰¤L(W0,B0). From equation (54) in the proof of Theorem 2, it holds that d dtL(Wt,Bt) â‰¤0 for all t. Thus, we have that L(Wt,Bt) â‰¤L(W0,B0) and hence Ïƒmin( Â¯B(1:l) t ) â‰¥Î³for all t. Under this problem setting (ml â‰¥mx), this implies that Î»min(( Â¯B(1:l) t )âŠ¤Â¯B(1:l) t ) â‰¥Î³2 for all tand thus Î»(1:H) T â‰¥Î³2. A.6. Proof of Theorem 3 The proof of Theorem 3 follows from the intermediate results of the proofs of Theorem 1 and Theorem 2 as we show in the following. For the non-multiscale case, from equation (36) in the proof of Theorem 1, we have that d dtL1(W,B) = âˆ’âˆ¥vec[âˆ‡(H)L(W,B)]âˆ¥2 F(H) âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[âˆ‡(H)L(W,B)] îµ¹îµ¹2 2 where âˆ¥vec[âˆ‡(H)L(W,B)]âˆ¥2 F(H) := vec[âˆ‡(H)L(W,B)]âŠ¤F(H) vec[âˆ‡(H)L(W,B)]. Since equation (36) in the proof of Theorem 2 is derived without the assumption on the square loss, this holds for any differentiable loss â„“. By noticing that âˆ‡(H)L(W,B) = V(X(SH)âˆ—I)âŠ¤, we have that d dtL1(W,B) = âˆ’âˆ¥vec[V(X(SH)âˆ—I)âŠ¤]âˆ¥2 F(H) âˆ’ Hâˆ‘ i=1 îµ¹îµ¹J(i,H) vec[V(X(SH)âˆ—I)âŠ¤] îµ¹îµ¹2 2 . This proves the statement of Theorem 3 (i). For the multiscale case, from equation (53) in the proof of Theorem 2, we have that d dtL2(W,B) = âˆ’ Hâˆ‘ l=0 âˆ¥vec[âˆ‡(l)L(W,B)]âˆ¥2 F(l) âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[âˆ‡(l)L(W,B)] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 (63) where âˆ¥vec[âˆ‡(l)L(W,B)]âˆ¥2 F(l) := vec[âˆ‡(l)L(W,B)]âŠ¤F(l) vec[âˆ‡(l)L(W,B)]. Since equation (53) in the proof of Theorem 2 is derived without the assumption on the square loss, this holds for any differentiable loss â„“. Since every step to derive equation (53) is valid when we replace f(X,W,B ) by f(X,W,B )âˆ—Iand XSl by X(Sl)âˆ—Iwithout using any assumption on Sor the relation between Slâˆ’1 and S, the steps to derive equation (53) also yields this for the general case of I: i.e., âˆ‡(l)L(W,B) = V(X(Sl)âˆ—I)âŠ¤. Thus, we have that d dtL1(W,B) = âˆ’ Hâˆ‘ l=0 âˆ¥vec[V(X(Sl)âˆ—I)âŠ¤]âˆ¥2 F(l) âˆ’ Hâˆ‘ i=1 îµ¹îµ¹îµ¹îµ¹îµ¹ Hâˆ‘ l=i J(i,l) vec[V(X(Sl)âˆ—I)âŠ¤] îµ¹îµ¹îµ¹îµ¹îµ¹ 2 2 This completes the proof of Theorem 3 (ii). B. Additional Experimental Results In this section, we present additional experimental results.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 2000 4000 6000 Iteration 10 1 100 Training Loss multiscale non-multiscale (a) Linear and Cora. 0 2000 4000 6000 Iteration 10 1 100 Training Loss multiscale non-multiscale (b) ReLU and Cora. 0 2000 4000 6000 Iteration 10 1 100 Training Loss multiscale non-multiscale (c) Linear and Citeseer. 0 2000 4000 6000 Iteration 10 1 100 Training Loss multiscale non-multiscale (d) ReLU and Citeseer. Figure 6.Multiscale skip connection accelerates GNN training. We plot the training curves of GNNs with ReLU and linear activation on the Cora and Citeseer dataset. We use the GCN model with learning rate 5eâˆ’ 5, six layers, and hidden dimension 32.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (a) Linear and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (b) ReLU and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (c) Linear and multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (d) ReLU and multiscale. Figure 7.Depth accelerates GNN training . We plot the training curves of GNNs with ReLU and linear activation, multiscale and non-multiscale on the Cora dataset. We use the GCN model with learning rate 5eâˆ’ 5 and hidden dimension 32.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (a) Linear and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (b) ReLU and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (c) Linear and multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss 2 layer 4 layer 6 layer (d) ReLU and multiscale. Figure 8.Depth accelerates GNN training . We plot the training curves of GNNs with ReLU and linear activation, multiscale and non-multiscale on the Citeseer dataset. We use the GCN model with learning rate 5eâˆ’ 5 and hidden dimension 32.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (a) Linear and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (b) ReLU and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (c) Linear and multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (d) ReLU and multiscale. Figure 9.GNNs train faster when the labels have signal instead of random noise . We plot the training curves of multiscale and non-multiscale GNNs with ReLU and linear activation, on the Cora dataset. We use the two-layer GCN model with learning rate 1eâˆ’ 4 and hidden dimension 32.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (a) Linear and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (b) ReLU and non-multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (c) Linear and multiscale. 0 2000 4000 6000 Iteration 10 1 100 Training Loss signal noise (d) ReLU and multiscale. Figure 10.GNNs train faster when the labels have signal instead of random noise . We plot the training curves of multiscale and non-multiscale GNNs with ReLU and linear activation, on the Citeseer dataset. We use the two-layer GCN model with learning rate 1eâˆ’ 4 and hidden dimension 32.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 0 3000 7000 10000 Iteration 10 3 10 2 10 1 100 Training Loss linear GIN ReLU GIN (a) Linear GIN vs. ReLU GIN. 0 3000 7000 10000 Iteration 10 2 10 1 100 Training Loss linear GCN ReLU GCN (b) Linear GCN vs. ReLU GCN. Figure 11.Linear GNNs vs. ReLU GNNs. We plot the training curves of GCN and GIN with ReLU and linear activation on the Cora dataset. The training curves of linear GNNs and ReLU GNNs are similar, both converging to nearly zero training loss with the same linear rate. Moreover, GIN trains faster than GCN, which agrees with our bound in Theorem 1. We use the learning rate 1eâˆ’ 4, two layers, and hidden dimension 32.Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth C. Experimental Setup In this section, we describe the experimental setup for reproducing our experiments. Dataset. We perform all experiments on the Cora and Citeseer datasets (Sen et al., 2008). Cora and Citeer are citation networks and the goal is to classify academic documents into different subjects. The dataset contains bag-of-words features for each document (node) and citation links (edges) between documents. The tasks are semi-supervised node classiï¬cation. Only a subset of nodes have training labels. In our experiments, we use the default dataset split, i.e., which nodes have training labels, and minimize the training loss accordingly. Tabel 1 shows an overview of the dataset statistics. Dataset Nodes Edges Classes Features Citeseer 3,327 4,732 6 3,703 Cora 2,708 5,429 7 1,433 Table 1.Dataset statistics Training details. We describe the training settings for our experiments. Let us ï¬rst describe some common hyperparame- ters and settings, and then for each experiment or ï¬gure we describe the other hyperparameters. For our experiments, to more closely align with the common practice in GNN training, we use the Adam optimizer and keep optimizer-speciï¬c hyperparameters except initial learning rate default. We set weight decay to zero. Next, we describe the settings for each experiment respectively. For the experiment in Figure 1, i.e., the training curves of linear vs. ReLU GNNs, we train the GCN and GIN with two layers on Cora with cross-entropy loss and learning rate 1e-4. We set the hidden dimension to 32. For the experiment in Figure 2a, i.e., computing the graph condition for linear GNNs, we use the linear GCN and GIN model with three layers on Cora and Citeseer. For linear GIN, we set Ïµto zero and MLP layer to one. For the experiment in Figure 2b, i.e., computing and plotting the time-dependent condition for linear GNNs, we train a linear GCN with two layers on Cora with squared loss and learning rate 1e-4. We set the hidden dimension the input dimension for both Cora and for CiteSeer, because the global convergence theorem requires the hidden dimension to be at least the same as input dimension. Note that this requirement is standard in previous works as well, such as Arora et al. (2019a). We use the default random initialization of PyTorch. The formula for computing the time-dependent Î»T is given in the main paper. For the experiment in Figure 2c, i.e., computing and plotting the time-dependent condition for linear GNNs across multiple training settings, we consider the following settings: 1. Dataset: Cora and Citeseer. 2. Model: GCN and GIN. 3. Depth: Two and four layers. 4. Activation: Linear and ReLU. We train the GNN with the settings above with squared loss and learning rate 1e-4. We set the hidden dimension to input dimension for Cora and CiteSeer. We use the default random initialization of PyTorch. The formula for computing the time-dependent Î»T is given in the main paper. For each point, we report the Î»T at last epoch. For the experiment in Figure 3a, i.e., computing the graph condition for multiscale linear GNNs, we use the linear GCN and GIN model with three layers on Cora and Citeseer. For linear GIN, we set Ïµto zero and MLP layer to one. For the experiment in Figure 3b, i.e., computing and plotting the time-dependent condition for multiscale linear GNNs, we train a linear GCN with two layers on Cora with squared loss and learning rate 1e-4. We set the hidden dimension to 2000 for Cora and 4000 for CiteSeer. We use the default random initialization of PyTorch. The formula for computing the time-dependent Î»T is given in the main paper. For the experiment in Figure 3c, i.e., computing and plotting the time-dependent condition for multiscale linear GNNs across multiple training settings, we consider the following settings:Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth 1. Dataset: Cora and Citeseer. 2. Model: Multiscale GCN and GIN. 3. Depth: Two and four layers. 4. Activation: Linear and ReLU. We train the multiscale GNN with the settings above with squared loss and learning rate 1e-4. We set the hidden dimension to 2000 for Cora and 4000 for CiteSeer. We use the default random initialization of PyTorch. The formula for computing the time-dependent Î»T is given in the main paper. For each point, we report the Î»T at last epoch. For the experiment in Figure 4a, i.e., multiscale vs. non-multiscale, we train the GCN with six layers and ReLU activation on Cora with cross-entropy loss and learning rate 5e-5. We set the hidden dimension to 32. We perform more extensive experiments to verify the conclusion for multiscale vs. non-multiscale in Figure 11. There, we train the GCN with six layers with both ReLU and linear activation on both Cora and Citeseer with cross-entropy loss and learning rate 5e-5. We set the hidden dimension to 32. For the experiment in Figure 4b, i.e., acceleration with depth, we train the non-multiscale GCN with two, four, six layers and ReLU activation on Cora with cross-entropy loss and learning rate 5e-5. We set the hidden dimension to 32. We perform more extensive experiments to verify the conclusion for acceleration with depth in Figure 7 and Figure 8. There, we train both multiscale and non-multiscale GCN with 2, 4, 6 layers with both ReLU and linear activation on both Cora and Citeseer with cross-entropy loss and learning rate 5e-5. We set the hidden dimension to 32. For the experiment in Figure 4c, i.e., signal vs. noise, we train the non-multiscale GCN with two layers and ReLU activation on Cora with cross-entropy loss and learning rate 1e-4. We set the hidden dimension to 32. For signal, we use the default labels of Cora. For noise, we randomly choose a class as the label. We perform more extensive experiments to verify the conclusion for signal vs. noise in Figure 9 and Figure 10. There, we train both multiscale and non-multiscale GCN with two layers with both ReLU and linear activation on both Cora and Citeseer with cross-entropy loss and learning rate 1e-4. We set the hidden dimension to 32. For the experiment in Figure 5, i.e., ï¬rst term vs. second term, we use the same setting as in Figure 4c. We use the formula of our Theorem in the main paper. Computing resources. The computing hardware is based on the CPU and the NVIDIA GeForce RTX 1080 Ti GPU. The software implementation is based on PyTorch and PyTorch Geometric (Fey & Lenssen, 2019). For all experiments, we train the GNNs with CPU and compute the eigenvalues with GPU.",
      "meta_data": {
        "arxiv_id": "2105.04550v2",
        "authors": [
          "Keyulu Xu",
          "Mozhi Zhang",
          "Stefanie Jegelka",
          "Kenji Kawaguchi"
        ],
        "published_date": "2021-05-10T17:59:01Z",
        "pdf_url": "https://arxiv.org/pdf/2105.04550v2.pdf"
      }
    },
    {
      "title": "Demystifying Oversmoothing in Attention-Based Graph Neural Networks",
      "abstract": "Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where\nincreasing network depth leads to homogeneous node representations. While\nprevious work has established that Graph Convolutional Networks (GCNs)\nexponentially lose expressive power, it remains controversial whether the graph\nattention mechanism can mitigate oversmoothing. In this work, we provide a\ndefinitive answer to this question through a rigorous mathematical analysis, by\nviewing attention-based GNNs as nonlinear time-varying dynamical systems and\nincorporating tools and techniques from the theory of products of inhomogeneous\nmatrices and the joint spectral radius. We establish that, contrary to popular\nbelief, the graph attention mechanism cannot prevent oversmoothing and loses\nexpressive power exponentially. The proposed framework extends the existing\nresults on oversmoothing for symmetric GCNs to a significantly broader class of\nGNN models, including random walk GCNs, Graph Attention Networks (GATs) and\n(graph) transformers. In particular, our analysis accounts for asymmetric,\nstate-dependent and time-varying aggregation operators and a wide range of\ncommon nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.",
      "full_text": "Demystifying Oversmoothing in Attention-Based Graph Neural Networks Xinyi Wu1,2 Amir Ajorlou2 Zihui Wu3 Ali Jadbabaie1,2 1Institute for Data, Systems and Society (IDSS), MIT 2Laboratory for Information and Decision Systems (LIDS), MIT 3Department of Computing and Mathematical Sciences (CMS), Caltech {xinyiwu,ajorlou,jadbabai}@mit.edu zwu2@caltech.edu Abstract Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) ex- ponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on over- smoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU. 1 Introduction Graph neural networks (GNNs) have emerged as a powerful framework for learning with graph- structured data [4, 8, 9, 13, 20, 33, 39] and have shown great promise in diverse domains such as molecular biology [46], physics [1] and recommender systems [41]. Most GNN models follow the message-passing paradigm [12], where the representation of each node is computed by recursively aggregating and transforming the representations of its neighboring nodes. One notable drawback of repeated message-passing isoversmoothing, which refers to the phenomenon that stacking message-passing GNN layers makes node representations of the same connected component converge to the same vector [5, 19, 20, 25, 27, 32, 42]. As a result, whereas depth has been considered crucial for the success of deep learning in many fields such as computer vision [16], most GNNs used in practice remain relatively shallow and often only have few layers [20, 39, 43]. On the theory side, while previous works have shown that the symmetric Graph Convolution Networks (GCNs) with ReLU and LeakyReLU nonlinearities exponentially lose expressive power, analyzing the oversmoothing phenomenon in other types of GNNs is still an open question [5, 27]. In particular, the question of whether the graph attention mechanism can prevent oversmoothing has not been settled yet. Motivated by the capacity of graph attention to distinguish the importance of different edges in the graph, some works claim that oversmoothing is alleviated in Graph Attention Networks (GATs), heuristically crediting to GATsâ€™ ability to learn adaptive node-wise aggregation operators via the attention mechanism [26]. On the other hand, it has been empirically observed that similar to arXiv:2305.16102v4  [cs.LG]  4 Jun 2024the case of GCNs, oversmoothing seems inevitable for attention-based GNNs such as GATs [32] or (graph) transformers [35]. The latter can be viewed as attention-based GNNs on complete graphs. In this paper, we provide a definitive answer to this question â€” attention-based GNNs also lose expressive power exponentially, albeit potentially at a slower exponential rate compared to GCNs. Given that attention-based GNNs can be viewed as nonlinear time-varying dynamical systems, our analysis is built on the theory of products of inhomogeneous matrices [ 14, 34] and the concept of joint spectral radius [ 31], as these methods have been long proved effective in the analysis of time-inhomogeneous markov chains and ergodicity of dynamical systems [2, 14, 34]. While classical results only apply to generic one-dimensional linear time-varying systems, we address four major challenges arising in analyzing attention-based GNNs: (1) the aggregation operators computed by attention are state-dependent, in contrast to conventional fixed graph convolutions; (2) the systems are multi-dimensional, which involves the coupling across feature dimensions; (3) the dynamics are nonlinear due to the nonlinear activation function in each layer; (4) the learnable weights and aggregation operators across different layers result in time-varying dynamical systems. Below, we highlight our key contributions: â€¢ As our main contribution, we establish that oversmoothing happens exponentially as model depth increases for attention-based GNNs, resolving the long-standing debate about whether attention-based GNNs can prevent oversmoothing. â€¢ We analyze attention-based GNNs through the lens of nonlinear, time-varying dynamical systems. The strength of our analysis stems from its ability to exploit the inherently common connectivity structure among the typically asymmetric state-dependent aggregation operators at different attentional layers. This enables us to derive rigorous theoretical results on the ergodicity of infinite products of matrices associated with the evolution of node representations across layers. Incorporating results from the theory of products of inhomogeneous matrices and their joint spectral radius, we then establish that oversmoothing happens at an exponential rate for attention- based GNNs from our ergodicity results. â€¢ Our analysis generalizes the existing results on oversmoothing for symmetric GCNs to a sig- nificantly broader class of GNN models with asymmetric, state-dependent and time-varying aggregation operators and nonlinear activation functions under general conditions. In partic- ular, our analysis can accommodate a wide range of common nonlinearities such as ReLU, LeakyReLU, and even non-monotone ones like GELU and SiLU. We validate our theoretical results on six real-world datasets with two attention-based GNN architectures and five common nonlinearities. 2 Related Work Oversmoothing problem in GNNs Oversmoothing is a well-known problem in deep GNNs, and many techniques have been proposed in order to mitigate it practically [ 6, 15, 21, 22, 30, 44, 48]. On the theory side, analysis of oversmoothing has only been carried out for the graph convolution case [5, 19, 27, 42]. In particular, by viewing graph convolutions as a form of Laplacian filter, prior works have shown that for GCNs, the node representations within each connected component of a graph will converge to the same value exponentially [ 5, 27]. However, oversmoothing is also empirically observed in attention-based GNNs such as GATs [32] or transformers [35]. Although some people hypothesize based on heuristics that attention can alleviate oversmoothing [ 26], a rigorous analysis of oversmoothing in attention-based GNNs remains open [5]. Theoretical analysis of attention-based GNNs Existing theoretical results on attention-based GNNs are limited to one-layer graph attention. Recent works in this line include Brody et al. [ 3] showing that the ranking of attention scores computed by a GAT layer is unconditioned on the query node, and Fountoulakis et al. [11] studying node classification performance of one-layer GATs on a random graph model. More relevantly, Wang et al. [ 40] made a claim that oversmoothing is asymptotically inevitable in GATs. Aside from excluding nonlinearities in the analysis, there are several flaws in the proof of their main result (Theorem 2). In particular, their analysis assumes the same stationary distribution for all the stochastic matrices output by attention at different layers. This is typically not the case given the state-dependent and time-varying nature of these matrices. In fact, the main challenge in analyzing multi-layer attention lies in the state-dependent and time-varying 2nature of these input-output mappings. Our paper offers novel contributions to the research on attention-based GNNs by developing a rich set of tools and techniques for analyzing multi-layer graph attention. This addresses a notable gap in the existing theory, which has primarily focused on one-layer graph attention, and paves the way for future research to study other aspects of multi-layer graph attention. 3 Problem Setup 3.1 Notations Let R be the set of real numbers and N be the set of natural numbers. We use the shorthands [n] := {1, . . . , n} and Nâ‰¥0 := N âˆª {0}. We denote the zero-vector of length N by 0 âˆˆ RN and the all-one vector of length N by 1 âˆˆ RN . We represent an undirected graph with N nodes by G = (A, X), where A âˆˆ {0, 1}NÃ—N is the adjacency matrix and X âˆˆ RNÃ—d are the node feature vectors of dimension d. Let E(G) be the set of edges of G. For nodes i, jâˆˆ [N], Aij = 1 if and only if i and j are connected with an edge in G, i.e., (i, j) âˆˆ E(G). For each i âˆˆ [N], Xi âˆˆ Rd represents the feature vector for node i. We denote the degree matrix of G by Ddeg = diag(A1) and the set of all neighbors of node i by Ni. Let âˆ¥Â·âˆ¥ 2, âˆ¥Â·âˆ¥ âˆ, âˆ¥Â·âˆ¥ F be the 2-norm, âˆ-norm and Frobenius norm, respectively. We useâˆ¥Â·âˆ¥ max to denote the matrix max norm, i.e., for a matrix M âˆˆ RmÃ—n, âˆ¥Mâˆ¥max := max ij |Mij|. We use â‰¤ew to denote element-wise inequality. Lastly, for a matrix M, we denote its ith row by MiÂ· and jth column by MÂ·j. 3.2 Graph attention mechanism We adopt the following definition of graph attention mechanism. Given node representation vectors Xi and Xj, we first apply a shared learnable linear transformation W âˆˆ RdÃ—dâ€² to each node, and then use an attention function Î¨ : Rdâ€² Ã— Rdâ€² â†’ R to compute a raw attention coefficient eij = Î¨(WâŠ¤Xi, WâŠ¤Xj) that indicates the importance of node jâ€™s features to node i. Then the graph structure is injected into the mechanism by performing masked attention, where for each node i, we only compute its attention to its neighbors. To make coefficients easily comparable across different nodes, we normalize eij among all neighboring nodes j of node i using the softmax function to get the normalized attention coefficients: Pij = softmaxj(eij) = exp(eij)P kâˆˆNi exp(eik) . The matrix P, where the entry in the ith row and the jth column is Pij, is a row stochastic matrix. We refer to P as an aggregation operator in message-passing. 3.3 Attention-based GNNs Having defined the graph attention mechanism, we can now write the update rule of a single graph attentional layer as Xâ€² = Ïƒ(P XW) , where X and Xâ€² are are the input and output node representations, respectively, Ïƒ(Â·) is a pointwise nonlinearity function, and the aggregation operator P is a function of XW . As a result, the output of the tth graph attentional layers can be written as X(t+1) = Ïƒ(P(t)X(t)W(t)) t âˆˆ Nâ‰¥0, (1) where X(0) = X is the input node features, W(t) âˆˆ Rdâ€²Ã—dâ€² for t âˆˆ N and W(0) âˆˆ RdÃ—dâ€² . For the rest of this work, without loss of generality, we assume that d = dâ€². The above definition is based on single-head graph attention. Multi-head graph attention uses K âˆˆ N weight matrices W1, . . . , WK in each layer and averages their individual single-head outputs [11, 39]. Without loss of generality, we consider single graph attention in our analysis in Section 4, but we note that our results automatically apply to the multi-head graph attention setting since K is finite. 33.4 Measure of oversmoothing We use the following notion of oversmoothing, inspired by the definition proposed in Rusch et al. [32]1: Definition 1. For an undirected and connected graphG, Âµ : RNÃ—d â†’ Râ‰¥0 is called a node similarity measure if it satisfies the following axioms: 1. âˆƒc âˆˆ Rd such that Xi = c for all node i if and only if Âµ(X) = 0, for X âˆˆ RNÃ—d; 2. Âµ(X + Y ) â‰¤ Âµ(X) + Âµ(Y ), for all X, Yâˆˆ RNÃ—d. Then oversmoothing with respect to Âµ is defined as the layer-wise convergence of the node-similarity measure Âµ to zero, i.e., lim tâ†’âˆ Âµ(X(t)) = 0. (2) We say oversmoothing happens at an exponential rate if there exists constantsC1, C2 > 0, such that for any t âˆˆ N, Âµ(X(t)) â‰¤ C1eâˆ’C2t. (3) We establish our results on oversmoothing for attention-based GNNs using the following node similarity measure: Âµ(X) := âˆ¥X âˆ’ 1Î³Xâˆ¥F , where Î³X = 1âŠ¤X N . (4) Proposition 1. âˆ¥X âˆ’ 1Î³Xâˆ¥F is a node similarity measure. The proof of the above proposition is provided in Appendix B. Other common node similarity measures include the Dirichlet energy [ 5, 32].2 Our measure is mathematically equivalent to the measure inf Y =1câŠ¤,câˆˆRd {âˆ¥X âˆ’ Y âˆ¥F } defined in Oono and Suzuki [ 27], but our form is more direct to compute. One way to see the equivalence is to consider the orthogonal projection into the space perpendicular to span{1}, denoted by B âˆˆ R(Nâˆ’1)Ã—N . Then our definition of Âµ satisfies âˆ¥X âˆ’ 1Î³xâˆ¥F = âˆ¥BXâˆ¥F , where the latter quantity is exactly the measure defined in [27]. 3.5 Assumptions We make the following assumptions (in fact, quite minimal) in deriving our results: A1 The graph G is connected and non-bipartite. A2 The attention function Î¨(Â·, Â·) is continuous. A3 The sequence {âˆ¥Qk t=0 |W(t)|âˆ¥max}âˆ k=0 is bounded. A4 The point-wise nonlinear activation function Ïƒ(Â·) satisfies 0 â‰¤ Ïƒ(x) x â‰¤ 1 for x Ì¸= 0 and Ïƒ(0) = 0. We note that all of these assumptions are either standard or quite general. Specifically,A1 is a standard assumption for theoretical analysis on graphs. For graphs with more than one connected components, the same results apply to each connected component. A1 can also be replaced with requiring the graph G to be connected and have self-loops at each node. Non-bipartiteness and self-loops both ensure that long products of stochastic matrices corresponding to aggregation operators in different graph attentional layers will eventually become strictly positive. The assumptions on the GNN architecture A2 and A4 can be easily verified for commonly used GNN designs. For example, the attention function LeakyReLU(aâŠ¤[WâŠ¤Xi||WâŠ¤Xj]), aâˆˆ R2dâ€² used in the GAT [39], where [Â·||Â·] denotes concatenation, is a specific case that satisfies A2. Other 1We distinguish the definition of oversmoothing and the rate of oversmoothing. This parallels the notion of stability and its rate in dynamical systems. 2In fact, our results are not specific to our choice of node similarity measure Âµ and directly apply to any Lipschitz node similarity measure, including the Dirichlet energy under our assumptions. See Remark 2 after Theorem 1. 4architectures that satisfy A2 include GATv2 [3] and (graph) transformers [38]. As for A4, one way to satisfy it is to have Ïƒ be 1-Lipschitz and Ïƒ(x) â‰¤ 0 for x <0 and Ïƒ(x) â‰¥ 0 for x >0. Then it is easy to verify that most of the commonly used nonlinear activation functions such as ReLU, LeakyReLU, GELU, SiLU, ELU, tanh all satisfy A4. Lastly, A3 is to ensure boundedness of the node representationsâ€™ trajectories X(t) for all t âˆˆ Nâ‰¥0. Such regularity assumptions are quite common in the asymptotic analysis of dynamical systems, as is also the case for the prior works analyzing oversmoothing in symmetric GCNs [5, 27]. 4 Main Results In this section, we lay out a road-map for deriving our main results, highlighting the key ideas of the proofs. The complete proofs are provided in the Appendices. We start by discussing the dynamical system formulation of attention-based GNNs in Section 4.1. By showing the boundedness of the node representationsâ€™ trajectories, we prove the existence of a common connectivity structure among aggregation operators across different graph attentional layers in Section 4.2. This implies that graph attention cannot fundamentally change the graph connectivity, a crucial property that will eventually lead to oversmoothing. In Section 4.3, we develop a framework for investigating the asymptotic behavior of attention-based GNNs by introducing the notion of ergodicity and its connections to oversmoothing. Then utilizing our result on common connectivity structure among aggregation operators, we establish ergodicity results for the systems associated with attention-based GNNs. In Section 4.4, we introduce the concept of the joint spectral radius for a set of matrices [31] and employ it to deduce exponential convergence of node representations to a common vector from our ergodicity results. Finally, we present our main result on oversmoothing in attention-based GNNs in Section 4.5 and comment on oversmoothing in GCNs in comparison with attention-based GNNs in Section 4.6. 4.1 Attention-based GNNs as nonlinear time-varying dynamical systems The theory of dynamical systems concerns the evolution of some state of interest over time. By view- ing the model depth t as the time variable, the input-output mapping at each graph attentional layer X(t+1) = Ïƒ(P(t)X(t)W(t)) describes a nonlinear time-varying dynamical system. The attention- based aggregation operator P(t) is state-dependent as it is a function of X(t)W(t). Given the notion of oversmoothing defined in Section 3.4, we are interested in characterizing behavior of X(t) as t â†’ âˆ. If the activation function Ïƒ(Â·) is the identity map, then repeated application of (1) gives X(t+1) = P(t) . . . P(0)XW (0) . . . W(t) . The above linear form would enable us to leverage the rich literature on the asymptotic behavior of the products of inhomogeneous row-stochastic matrices (see, e.g., [14, 34]) in analyzing the long-term behavior of attention-based GNNs. Such a neat expansion, however, is not possible when dealing with a nonlinear activation functionÏƒ(Â·). To find a remedy, let us start by observing that element-wise application of Ïƒ to a vector y âˆˆ Rd can be written as Ïƒ(y) = diag \u0012Ïƒ(y) y \u0013 y , (5) where diag \u0010 Ïƒ(y) y \u0011 is a diagonal matrix with Ïƒ(yi)/yi on the ith diagonal entry. Defining Ïƒ(0)/0 := Ïƒâ€²(0) or 1 if the derivative does not exist along with the assumption Ïƒ(0) = 0 in A4, it is easy to check that the above identity still holds for vectors with zero entries. We can use (5) to write the ith column of X(t+1) as X(t+1) Â·i = Ïƒ(P(t)(X(t)W(t))Â·i) = D(t) i P(t)(X(t)W(t))Â·i = D(t) i P(t) dX j=1 W(t) ji X(t) .j , (6) where D(t) i is a diagonal matrix. It follows from the assumption on the nonlinearities A4 that diag(0) â‰¤ew D(t) i â‰¤ew diag(1) . 5We define D to be the set of all possible diagonal matrices D(t) i satisfying the above inequality: D := {diag(d) : d âˆˆ RN , 0 â‰¤ew d â‰¤ew 1}. Using (6) recursively, we arrive at the following formulation forX(t+1) Â·i : X(t+1) Â·i = X jt+1=i, (jt,...,j0)âˆˆ[d]t+1  tY k=0 W(k) jkjk+1 ! D(t) jt+1 P(t)...D(0) j1 P(0)X(0) Â·j0 . (7) 4.2 Common connectivity structure among aggregation operators across different layers We can use the formulation in (7) to show the boundedness of the node representationsâ€™ trajectories X(t) for all t âˆˆ Nâ‰¥0, which in turn implies the boundedness of the input to graph attention in each layer, X(t)W(t). Lemma 1. Under assumptions A3-A4, there exists C >0 such that âˆ¥X(t)âˆ¥max â‰¤ C for all t âˆˆ Nâ‰¥0. For a continuous Î¨(Â·, Â·)3, the following lemma is a direct consequence of Lemma 1, suggesting that the graph attention mechanism cannot fundamentally change the connectivity pattern of the graph. Lemma 2. Under assumptions A2-A4, there exists Ïµ >0 such that for all t âˆˆ Nâ‰¥0 and for any (i, j) âˆˆ E(G), we have P(t) ij â‰¥ Ïµ. One might argue that Lemma 2 is an artifact of the continuity of the softmax function. The softmax function is, however, often favored in attention mechanisms because of its trainability in back propagation compared to discontinuous alternatives such as hard thresholding. Besides trainability issues, it is unclear on a conceptual level whether it is reasonable to absolutely drop an edge from the graph as is the case for hard thresholding. Lemma 2 is an important step towards the main convergence result of this work, which states that all the nodes will converge to the same representation vector at an exponential rate. We define the family of row-stochastic matrices satisfying Lemma 2 below. Definition 2. Let Ïµ > 0. We define PG,Ïµ to be the set of row-stochastic matrices satisfying the following conditions: 1. Ïµ â‰¤ Pij â‰¤ 1, if (i, j) âˆˆ E(G), 2. Pij = 0, if (i, j) /âˆˆ E(G). 4.3 Ergodicity of infinite products of matrices Ergodicity, in its most general form, deals with the long-term behavior of dynamical systems. The oversmoothing phenomenon in GNNs defined in the sense of (2) concerns the convergence of all rows of X(t) to a common vector. To this end, we define ergodicity in our analysis as the convergence of infinite matrix products to a rank-one matrix with identical rows. Definition 3 (Ergodicity). Let B âˆˆ R(Nâˆ’1)Ã—N be the orthogonal projection onto the space orthogo- nal to span{1}. A sequence of matrices {M(n)}âˆ n=0 is ergodic if lim tâ†’âˆ B tY n=0 M(n) = 0 . We will take advantage of the following properties of the projection matrixB already established in Blondel et al. [2]: 1. B1 = 0; 2. âˆ¥Bxâˆ¥2 = âˆ¥xâˆ¥2 for x âˆˆ RN if xâŠ¤1 = 0; 3. Given any row-stochastic matrix P âˆˆ RNÃ—N , there exists a unique matrix ËœP âˆˆ R(Nâˆ’1)Ã—(Nâˆ’1) such that BP = ËœP B . 3More generally, for Î¨(Â·, Â·) that outputs bounded attention scores for bounded inputs. 6We can use the existing results on the ergodicity of infinite products of inhomogeneous stochastic matrices [14, 34] to show that any sequence of matrices in PG,Ïµ is ergodic. Lemma 3. Fix Ïµ >0. Consider a sequence of matrices {P(t)}âˆ t=0 in PG,Ïµ. That is, P(t) âˆˆ PG,Ïµ for all t âˆˆ Nâ‰¥0. Then {P(t)}âˆ t=0 is ergodic. The main proof strategy for Lemma 3 is to make use of the Hilbert projective metric and the Birkhoff contraction coefficient. These are standard mathematical tools to prove that an infinite product of inhomogeneous stochastic matrices is ergodic. We refer interested readers to the textbooks [14, 34] for a comprehensive study of these subjects. Despite the nonlinearity of Ïƒ(Â·), the formulation (7) enables us to express the evolution of the feature vector trajectories as a weighted sum of products of matrices of the form DP where D âˆˆ Dand P âˆˆ PG,Ïµ. We define the set of such matrices as MG,Ïµ := {DP : D âˆˆ D, Pâˆˆ PG,Ïµ}. A key step in proving oversmoothing for attention-based GNNs under our assumptions is to show the ergodicity of the infinite products of matrices in MG,Ïµ. In what follows, we lay out the main ideas of the proof, and refer readers to Appendix F for the details. Consider a sequence {D(t)P(t)}âˆ t=0 in MG,Ïµ, that is, D(t)P(t) âˆˆ MG,Ïµ for all t âˆˆ Nâ‰¥0. For t0 â‰¤ t1, define Qt0,t1 := D(t1)P(t1) . . . D(t0)P(t0), Î´ t = âˆ¥D(t) âˆ’ IN âˆ¥âˆ , where IN denotes the N Ã— N identity matrix. The common connectivity structure among P(t)â€™s established in Section 4.2 allows us to show that long products of matrices DP from MG,Ïµ will eventually become a contraction in âˆ-norm. More precisely, we can show that there exists T âˆˆ N and 0 < c <1 such that for all t âˆˆ Nâ‰¥0, âˆ¥Qt,t+T âˆ¥âˆ â‰¤ 1 âˆ’ cÎ´t. Next, define Î²k := Qk t=0(1 âˆ’ cÎ´t) and let Î² := lim kâ†’âˆ Î²k. Note that Î² is well-defined because the partial product is non-increasing and bounded from below. We can use the above contraction property to show the following key lemma. Lemma 4. Let Î²k := Qk t=0(1 âˆ’ cÎ´t) and Î² := lim kâ†’âˆ Î²k. 1. If Î² = 0, then lim kâ†’âˆ Q0,k = 0 ; 2. If Î² >0, then lim kâ†’âˆ BQ0,k = 0 . The ergodicity of sequences of matrices in MG,Ïµ immediately follows from Lemma 4, which in turn implies oversmoothing as defined in (2). Lemma 5. Any sequence {D(t)P(t)}âˆ t=0 in MG,Ïµ is ergodic. Remark The proof techniques developed in [ 5, 27] are restricted to symmetric matrices hence cannot be extended to more general family of GNNs, as they primarily rely on matrix norms for con- vergence analysis. Analyses solely using matrix norms are often too coarse to get meaningful results when it comes to asymmetric matrices. For instance, while the matrix 2-norm and matrix eigenvalues are directly related for symmetric matrices, the same does not generally hold for asymmetric matrices. Our analysis, on the other hand, exploits the inherently common connectivity structure among these matrices in deriving the ergodicity results in Lemma 3-5. 4.4 Joint spectral radius Using the ergodicity results in the previous section, we can establish that oversmoothing happens in attention-based GNNs. To show that oversmoothing happens at an exponential rate, we introduce the notion of joint spectral radius, which is a generalization of the classical notion of spectral radius of a single matrix to a set of matrices [7, 31]. We refer interested readers to the textbook [18] for a comprehensive study of the subject. 7Definition 4 (Joint Spectral Radius). For a collection of matricesA, the joint spectral radiusJSR(A) is defined to be JSR(A) = lim sup kâ†’âˆ sup A1,A2,...,AkâˆˆM âˆ¥A1A2...Akâˆ¥ 1 k , and it is independent of the norm used. In plain words, the joint spectral radius measures the maximal asymptotic growth rate that can be obtained by forming long products of matrices taken from the set A.To analyze the convergence rate of products of matrices in MG,Ïµ to a rank-one matrix with identical rows, we treat the two cases of linear and nonlinear activation functions, separately. For the linear case, where Ïƒ(Â·) is the identity map, we investigate the dynamics induced by P(t)â€™s on the subspace orthogonal to span{1} and use the third property of the orthogonal projection B established in Section 4.3 to write BP1P2 . . . Pk = ËœP1 ËœP2... ËœPkB, where each ËœPi is the unique matrix in R(Nâˆ’1)Ã—(Nâˆ’1) that satisfies BPi = ËœPiB. Let us define ËœPG,Ïµ := { ËœP : BP = ËœP B, Pâˆˆ PG,Ïµ }. We can use Lemma 3 to show that the joint spectral radius of ËœPG,Ïµ is strictly less than 1. Lemma 6. Let 0 < Ïµ <1. Under assumptions A1-A4, JSR( ËœPG,Ïµ) < 1. For the nonlinear case, let 0 < Î´ <1 and define DÎ´ := {diag(d) : d âˆˆ RN , 0 â‰¤ew d â‰¤ew Î´}, MG,Ïµ,Î´ := {DP : D âˆˆ DÎ´, P âˆˆ PG,Ïµ}. Then again, using the ergodicity result from the previous section, we establish that the joint spectral radius of MG,Ïµ,Î´ is also less than 1. Lemma 7. Let 0 < Ïµ, Î´ <1. Under assumptions A1-A4, JSR(MG,Ïµ,Î´) < 1. The above lemma is specifically useful in establishing exponential rate for oversmoothing when dealing with nonlinearities for which Assumption 4 holds in the strict sense, i.e. 0 â‰¤ Ïƒ(x) x < 1 (e.g., GELU and SiLU nonlinearities). Exponential convergence, however, can still be established under a weaker requirement, making it applicable to ReLU and Leaky ReLU, as we will see in Theorem 1. It follows from the definition of the joint spectral radius that ifJSR(A) < 1, for anyJSR(A) < q <1, there exists a C for which âˆ¥A1A2...Akyâˆ¥ â‰¤Cqkâˆ¥yâˆ¥ (8) for all y âˆˆ RNâˆ’1 and A1, A2, ..., Ak âˆˆ A. 4.5 Main Theorems We have all the ingredients to prove our main results. As a final step, given Lemma 5 and(8), how could we incorporate the weight matrices W(t) into the final analysis? Recall that the formulation of X(t+1) Â·i in (7), then âˆ¥BX(t+2) Â·i âˆ¥2 can be bounded as âˆ¥BX(t+1) Â·i âˆ¥2 = X jt+1=i, (jt,...,j0)âˆˆ[d]t+1  tY k=0 W(k) jkjk+1 ! D(t) jt+1 P(t)...D(0) j1 P(0)X(0) Â·j0 â‰¤ Câˆ¥(|W(0)|...|W(t)|)Â·iâˆ¥1 sup D(n)âˆˆD 0â‰¤nâ‰¤t \r\r\rBD(t)P(t)...D(0)P(0) \r\r\r 2 â‰¤ Câ€² sup D(n)âˆˆD 0â‰¤nâ‰¤t \r\r\rBD(t)P(t)...D(0)P(0) \r\r\r 2 where C = max jâˆˆ[d] âˆ¥X(0) Â·j âˆ¥2, and the last inequality is due to the assumption A3 as it implies that there exists Câ€²â€² âˆˆ R such that for all t â‰¥ 0, i âˆˆ [d], âˆ¥(|W(0)|...|W(t)|)Â·iâˆ¥1 â‰¤ Câ€²â€². It is then evident that the term sup D(n)âˆˆD 0â‰¤nâ‰¤t \r\r\rBD(t)P(t)...D(0)P(0) \r\r\r 2 (9) 8determines both the convergence and its rate, i.e. if (9) converges to zero, so does Âµ(X(t)); if (9) converges to zero exponentially fast, the same applies for Âµ(X(t)). Lemma 6 and Lemma 7 on the joint spectral radius give sufficient conditions to satisfy the above key condition on (9). Specifically, applying (8) to the recursive expansion of X(t+1) Â·i in (7) using the 2-norm, we can prove the exponential convergence of Âµ(X(t)) to zero for the similarity measure Âµ(Â·) defined in (4), which in turn implies the convergence of node representations to a common representation at an exponential rate. This completes the proof of the main result of this paper, which states that oversmoothing defined in (2) is unavoidable for attention-based GNNs, and that an exponential convergence rate can be attained under general conditions. Theorem 1. Under assumptions A1-A4, if in addition, â€¢ (linear) Ïƒ(Â·) is the identity map, or â€¢ (nonlinear) there exists K âˆˆ N and 0 < Î´ <1 for which the following holds: For all m âˆˆ Nâ‰¥0, there is nm âˆˆ {0} âˆª[K âˆ’ 1] such that for any c âˆˆ [d], Ïƒ(X(mK+nm) rcc )/X(mK+nm) rcc â‰¤ Î´ for some rc âˆˆ [N], ( â‹†) then there exists q <1 and C1(q) > 0 such that Âµ(X(t)) â‰¤ C1qt , âˆ€t â‰¥ 0 . As a result, node representations X(t) exponentially converge to the same value as the model depth t â†’ âˆ. Theorem 1 establishes that oversmoothing is asymptotically inevitable for attention-based GNNs with general nonlinearities. Despite similarity-based importance assigned to different nodes via the aggregation operator P(t), such attention-based mechanisms are yet unable to fundamentally change the connectivity structure of P(t), resulting in node representations converging to a common vector. Our results hence indirectly support the emergence of alternative ideas for changing the graph connectivity structure such as edge-dropping [15, 30] or graph-rewiring [22], in an effort to mitigate oversmoothing. Remark 1. For nonlinearities such as SiLU or GELU, the condition (â‹†) is automatically satisfied under A3-A4. For ReLU and LeakyReLU, this is equivalent to requiring that there existsK âˆˆ N such that for all m âˆˆ Nâ‰¥0, there exists nm âˆˆ {0} âˆª[K âˆ’ 1] where for any c âˆˆ [d], X(mK+nm) rcc < 0 for some rc âˆˆ [d]. Remark 2. We note that our results are not specific to the choice of node similarity measure Âµ(X) = âˆ¥X âˆ’ 1Î³X âˆ¥F considered in our analysis. In fact, exponential convergence of any other Lipschitz node similarity measure Âµâ€² to 0 is a direct corollary of Theorem 1. To see this, observe that for a node similarity measure Âµâ€² with a Lipschitz constant L, it holds that Âµâ€²(X) = |Âµâ€²(X) âˆ’ Âµâ€²(1Î³X )| â‰¤Lâˆ¥X âˆ’ 1Î³X âˆ¥F = LÂµ(X). In particular, the Dirichlet energy is Lipschitz given that the input X has a compact domain, es- tablished in Lemma 1. Hence our theory directly implies the exponential convergence of Dirichlet energy. Besides utilizing the properties of specific nonlinearity functions Ïƒ(Â·), another way to derive the convergence of Âµ(X(t)) to zero under general class of nonlinearities D is to restrict the class of weights W(t). To see this, write the update rule of X(t+1) in the vectorized form: vec \u0010 X(t+1) \u0011 = ËœD(t) \u0012\u0010 W(t) \u0011âŠ¤ âŠ— P(t) \u0013 vec \u0010 X(t) \u0011 , (10) where ËœD(t) âˆˆ RNdÃ—Nd represents the effect of Ïƒ(Â·) on each entry of vec \u0000 X(t)\u0001 . We restrict the class of W(t) to satisfy the following stricter condition: Alternative A3 (A3â€™) For any t âˆˆ Nâ‰¥0, W(t) âˆˆ RdÃ—d is a column substochastic matrix4. Further- more, there exists 0 < Î¾ <1 such that for all t âˆˆ Nâ‰¥0, 4Column substochastic means that the sum of entries in each column is no greater than one. 91. W(t) ii â‰¥ Î¾, for all i âˆˆ [d]; 2. If W(t) ij > 0, W(t) ij â‰¥ Î¾ and W(t) ji â‰¥ Î¾ for all i Ì¸= j âˆˆ [d]. Then Lemma 5 directly implies the convergence of Âµ(X(t)) to zero as follows: Theorem 2. Under assumptions A1, A2, A3â€™ and A4, lim tâ†’âˆ Âµ(X(t)) = 0 . Remark 3. We note that the above condition requires W(t) to have a symmetric sparsity pattern (symmetric entries do not need to be equal). Such a symmetric pattern is necessary for the result to hold. To see the necessity, consider the following counterexample: Counterexample Let N = 2, d= 2, and X(t) Â·1 and X(t) Â·2 satisfy the following update rule: ï£± ï£² ï£³ X(t+1) Â·1 = D1 \u0010 W11P X(t) Â·1 + W21P X(t) Â·2 \u0011 X(t+1) Â·2 = D2 \u0010 W12P X(t) Â·1 + W22P X(t) Â·2 \u0011 (11) where D1 = \u0014 1 0 0 1 \u0015 D2 = \u0014 1/2 0 0 1 \u0015 P = \u0014 0.5 0 .5 0.5 0 .5 \u0015 , and W = \u0014 W11 W12 W21 W22 \u0015 = \u0014 2/3 0 1/3 1 \u0015 . Then X = \u0014 1/3 1 2/3 1 \u0015 is a fixed point for the system defined in (11). As a result, if X(0) starts at X, Âµ(X(t)) will not converge to zero. Remark 4. If the sparsity pattern of W(t) can be represented as a connected graphGâ€² with self-loops at each node, then the convergence of Âµ(X(t)) happens in a stronger sense: each entry of X(t) converges to the same value â€” more than just each row ofX(t) converging to the same vector. Such a result, might also be related to the feature overcorrelation phenomenon observed in GNNs [17]. 4.6 Comparison with the GCN Computing or approximating the joint spectral radius for a given set of matrices is known to be hard in general [37], yet it is straightforward to lower bound JSR( ËœPG,Ïµ) as stated in the next proposition. Proposition 2. Let Î» be the second largest eigenvalue of Dâˆ’1/2 deg ADâˆ’1/2 deg . Then under assumptions A1-A4, it holds that Î» â‰¤ JSR( ËœPG,Ïµ). In the linear case, the upper bound q on the convergence rate that we get for graph attention in Theorem 1 is lower bounded by JSR( ËœPG,Ïµ). A direct consequence of the above result is that q is at least as large as Î». On the other hand, previous work has already established that in the graph convolution case, the convergence rate of Âµ(X(t)) is O(Î»t) [5, 27]. It is thus natural to expect attention-based GNNs to potentially have better expressive power at finite depth than GCNs, even though they both inevitably suffer from oversmoothing. This is also evident from the numerical experiments that we present in the next section. 5 Numerical Experiments In this section, we validate our theoretical findings via numerical experiments using the three commonly used homophilic benchmark datasets: Cora, CiteSeer, and PubMed [ 45] and the three commonly used heterophilic benchmark datasets: Cornell, Texas, and Wisconsin [29]. We note that 10100 101 102 10 4 10 3 10 2 10 1 100 101 102 (X) Cora GAT 100 101 102 10 4 10 3 10 2 10 1 100 101 102 CiteSeer GAT 100 101 102 10 4 10 3 10 2 10 1 100 101 102 PubMed GAT 100 101 102 10 4 10 3 10 2 10 1 100 101 102 (X) Cora GCN 100 101 102 10 4 10 3 10 2 10 1 100 101 102 CiteSeer GCN 100 101 102 10 4 10 3 10 2 10 1 100 101 102 PubMed GCN 100 101 102 10 4 10 3 10 2 10 1 100 101 102 (X) Cornell GAT 100 101 102 10 4 10 3 10 2 10 1 100 101 102 T exas GAT 100 101 102 10 4 10 3 10 2 10 1 100 101 102 Wisconsin GAT 100 101 102 10 4 10 3 10 2 10 1 100 101 102 (X) Cornell GCN 100 101 102 number of layers 10 4 10 3 10 2 10 1 100 101 102 T exas GCN 100 101 102 10 4 10 3 10 2 10 1 100 101 102 Wisconsin GCN ReLU LeakyReLU (0.01) LeakyReLU (0.4) LeakyReLU (0.8) GELU Figure 1: Evolution of Âµ(X(t)) (in log-log scale) on the largest connected component of each dataset (top 2 rows: homophilic graphs; bottom 2 rows: heterophilic graphs). Oversmoothing happens exponentially in both GCNs and GATs with the rates varying depending on the choice of activation function. Notably, GCNs demonstrate faster rates of oversmoothing compared to GATs. our theoretical results are developed for generic graphs and thus hold for datasets exhibiting either homophily or heterophily and even those that are not necessarily either of the two. More details about the experiments are provided in Appendix L. For each dataset, we trained a 128-layer single-head GAT and a 128-layer GCN with the random walk graph convolution Dâˆ’1 degA, each having 32 hidden dimensions and trained using the standard features and splits. The GCN with the random walk graph convolution is a special type of attention- based GNNs where the attention function is constant. For each GNN model, we considered various nonlinear activation functions: ReLU, LeakyReLU (with three different negative slope values: 0.01, 0.4 and 0.8) and GELU. Here, we chose GELU as an illustration of the generality of our assumption on nonlinearities, covering even non-monotone activation functions such as GELU. We ran each experiment 10 times. Figure 1 shows the evolution ofÂµ(X(t)) in log-log scale on the largest connected component of each graph as we forward pass the input X into a trained model. The solid curve is the average over 10 runs and the band indicates one standard deviation around the average. We observe that, as predicted by our theory, oversmoothing happens at an exponential rate for both GATs and GCNs, regardless of the choice of nonlinear activation functions in the GNN architectures. Notably, GCNs exhibit a significantly faster rate of oversmoothing compared to GATs. This aligns the observation made in Section 4.6, expecting a potentially better expressive power for GATs than GCNs at finite depth. Furthermore, the exponential convergence rate of oversmoothing varies among GNNs with different nonlinear activation functions. From a theory perspective, as different activation functions constitute different subsets of MG,Ïµ and different sets of matrices have different joint spectral radii, it is not surprising that the choice of nonlinear activation function would affect the convergence rate. In particular, among the nonlinearties we considered, ReLU in fact magnifies oversmoothing the second most. As a result, although ReLU is often the default choice for the standard implementation of many GNN architectures [10, 20], one might wish to consider switching to other nonliearities to better mitigate oversmoothing. 116 Conclusion Oversmoothing is one of the central challenges in developing more powerful GNNs. In this work, we reveal new insights on oversmoothing in attention-based GNNs by rigorously providing a negative answer to the open question of whether graph attention can implicitly prevent oversmoothing. By analyzing the graph attention mechanism within the context of nonlinear time-varying dynamical systems, we establish that attention-based GNNs lose expressive power exponentially as model depth increases. We upper bound the convergence rate for oversmoothing under very general assumptions on the nonlinear activation functions. One may try to tighten the bounds by refining the analysis separately for each of the commonly used activation functions. Future research should also aim to improve the design of graph attention mechanisms based on our theoretical insights and utilize our analysis techniques to study other aspects of multi-layer graph attention. Acknowledgments The authors deeply appreciate Bernard Chazelle for noticing a mistake in the previous version of the paper and suggesting an alternative idea for the proof. XW would like to thank Jennifer Tang and William Wang for helpful discussions throughout the project. The authors are also grateful to Zhijian Zhuo and Yifei Wang for identifying an error in the first draft of the paper, thank the anonymous NeurIPS reviewers for providing valuable feedback, and acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing computing resources that have contributed to the research results reported within this paper. This research has been supported in part by ARO MURI W911NF-19-0217, ONR N00014-20-1-2394, and the MIT-IBM Watson AI Lab. References [1] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and koray kavukcuoglu. Interaction networks for learning about objects, relations and physics. In NeurIPS, 2016. [2] Vincent D. Blondel, Julien M. Hendrickx, Alexander Olshevsky, and John N. Tsitsiklis. Con- vergence in multiagent coordination, consensus, and flocking. Proceedings of the 44th IEEE Conference on Decision and Control, pages 2996â€“3000, 2005. [3] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? InICLR, 2022. [4] Joan Bruna, Wojciech Zaremba, Arthur D. Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In ICLR, 2014. [5] Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. In ICML Graph Representation Learning and Beyond (GRL+) Workshop, 2020. [6] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In ICML, 2020. [7] Ingrid Daubechies and Jeffrey C. Lagarias. Sets of matrices all infinite products of which converge. Linear Algebra and its Applications, 161:227â€“263, 1992. [8] MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In NeurIPS, 2016. [9] David Kristjanson Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael GÃ³mez- Bombarelli, Timothy D. Hirzel, AlÃ¡n Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In NeurIPS, 2015. [10] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. 12[11] Kimon Fountoulakis, Amit Levi, Shenghao Yang, Aseem Baranwal, and Aukosh Jagannath. Graph attention retrospective. ArXiv, abs/2202.13060, 2022. [12] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In ICML, 2017. [13] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In IJCNN, 2005. [14] Darald J. Hartfiel. Nonhomogeneous Matrix Products. 2002. [15] Arman Hasanzadeh, Ehsan Hajiramezanali, Shahin Boluki, Mingyuan Zhou, Nick G. Duffield, Krishna R. Narayanan, and Xiaoning Qian. Bayesian graph neural networks with adaptive connection sampling. In ICML, 2020. [16] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [17] Wei Jin, Xiaorui Liu, Yao Ma, Charu C. Aggarwal, and Jiliang Tang. Feature overcorrelation in deep graph neural networks: A new perspective. In KDD, 2022. [18] RaphaÃ«l M. Jungers. The Joint Spectral Radius: Theory and Applications. 2009. [19] Nicolas Keriven. Not too little, not too much: a theoretical analysis of graph (over)smoothing. In NeurIPS, 2022. [20] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017. [21] Johannes Klicpera, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In ICLR, 2019. [22] Johannes Klicpera, Stefan WeiÃŸenberger, and Stephan GÃ¼nnemann. Diffusion improves graph learning. In Neural Information Processing Systems, 2019. [23] Peter D. Lax. Functional Analysis. 2002. [24] David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov Chains and Mixing Times . 2008. [25] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI, 2018. [26] Yimeng Min, Frederik Wenkel, and Guy Wolf. Scattering gcn: Overcoming oversmoothness in graph convolutional networks. In NeurIPS, 2020. [27] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In ICLR, 2020. [28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas KÃ¶pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. [29] Hongbin Pei, Bingzhen Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. In ICLR, 2020. [30] Yu Rong, Wen bing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In ICLR, 2020. [31] Gian-Carlo Rota and W. Gilbert Strang. A note on the joint spectral radius. 1960. [32] T.Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra. A survey on oversmoothing in graph neural networks. ArXiv, abs/2303.10993, 2023. 13[33] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20:61â€“80, 2009. [34] Eugene Seneta. Non-negative Matrices and Markov Chains. 2008. [35] Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S. Lee, and James Tin-Yau Kwok. Revisiting over-smoothing in bert from the perspective of graph. In ICLR, 2022. [36] Jacques Theys. Joint spectral radius: theory and approximations. Ph. D. dissertation, 2005. [37] John N. Tsitsiklis and Vincent D. Blondel. The Lyapunov exponent and joint spectral radius of pairs of matrices are hardâ€”when not impossibleâ€”to compute and to approximate.Mathematics of Control, Signals and Systems, 10:31â€“40, 1997. [38] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [39] Petar VeliË‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua Bengio. Graph attention networks. In ICLR, 2018. [40] Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec. Improving graph attention networks with large margin-based constraints. ArXiv, abs/1910.11945, 2019. [41] Shiwen Wu, Wentao Zhang, Fei Sun, and Bin Cui. Graph neural networks in recommender systems: A survey. ACM Computing Surveys, 55:1 â€“ 37, 2020. [42] Xinyi Wu, Zhengdao Chen, William Wang, and Ali Jadbabaie. A non-asymptotic analysis of oversmoothing in graph neural networks. In ICLR, 2023. [43] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32:4â€“24, 2019. [44] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In ICML, 2018. [45] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In ICML, 2016. [46] Jiaxuan You, Bowen Liu, Rex Ying, Vijay S. Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In NeurIPS, 2018. [47] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Graphsaint: Graph sampling based inductive learning method. In ICLR, 2020. [48] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In ICLR, 2020. 14A Basic Facts about Matrix Norms In this section, we list some basic facts about matrix norms that will be helpful in comprehending the subsequent proofs. A.1 Matrix norms induced by vector norms Suppose a vector normâˆ¥Â·âˆ¥Î± on Rn and a vector normâˆ¥Â·âˆ¥Î² on Rm are given. Any matrix M âˆˆ RmÃ—n induces a linear operator from Rn to Rm with respect to the standard basis, and one defines the corresponding induced norm or operator norm by âˆ¥Mâˆ¥Î±,Î² = sup \u001aâˆ¥Mvâˆ¥Î² âˆ¥vâˆ¥Î± , vâˆˆ Rn, vÌ¸= 0 \u001b . If the p-norm for vectors (1 â‰¤ p â‰¤ âˆ) is used for both spaces Rn and Rm, then the corresponding operator norm is âˆ¥Mâˆ¥p = sup vÌ¸=0 âˆ¥Mvâˆ¥p âˆ¥vâˆ¥p . The matrix 1-norm and âˆ-norm can be computed by âˆ¥Mâˆ¥1 = max 1â‰¤j mX i=1 |Mij|, that is, the maximum absolute column sum of the matrix M; âˆ¥Mâˆ¥âˆ = max 1â‰¤m nX j=1 |Mij|, that is, the maximum absolute row sum of the matrix M. Remark In the special case of p = 2, the induced matrix norm âˆ¥ Â· âˆ¥2 is called the spectral norm, and is equal to the largest singular value of the matrix. For square matrices, we note that the name â€œspectral norm\" does not imply the quantity is directly related to the spectrum of a matrix, unless the matrix is symmetric. Example We give the following example of a stochastic matrixP, whose spectral radius is 1, but its spectral norm is greater than 1. P = \u0014 0.9 0 .1 0.25 0 .75 \u0015 âˆ¥Pâˆ¥2 â‰ˆ 1.0188 A.2 Matrix (p, q)-norms The Frobenius norm of a matrix M âˆˆ RmÃ—n is defined as âˆ¥Mâˆ¥F = vuut nX j=1 mX i=1 |Mij|2 , and it belongs to a family of entry-wise matrix norms: for 1 â‰¤ p, qâ‰¤ âˆ, the matrix (p, q)-norm is defined as âˆ¥Mâˆ¥p,q = ï£« ï£­ nX j=1  mX i=1 |Mij|p !q/pï£¶ ï£¸ 1/q . The special case p = q = 2 is the Frobenius norm âˆ¥ Â· âˆ¥F , and p = q = âˆ yields the max norm âˆ¥ Â· âˆ¥max. 15A.3 Equivalence of norms For any two matrix norms âˆ¥ Â· âˆ¥Î± and âˆ¥ Â· âˆ¥Î², we have that for all matrices M âˆˆ RmÃ—n, râˆ¥Mâˆ¥Î± â‰¤ âˆ¥Mâˆ¥Î² â‰¤ sâˆ¥Mâˆ¥Î± for some positive numbers r and s. In particular, the following inequality holds for the 2-norm âˆ¥ Â· âˆ¥2 and the âˆ-norm âˆ¥ Â· âˆ¥âˆ: 1âˆšnâˆ¥Mâˆ¥âˆ â‰¤ âˆ¥Mâˆ¥2 â‰¤ âˆšmâˆ¥Mâˆ¥âˆ . B Proof of Proposition 1 It is straightforward to check that âˆ¥X âˆ’1Î³Xâˆ¥F satisfies the two axioms of a node similarity measure: 1. âˆ¥X âˆ’ 1Î³Xâˆ¥F = 0 â‡â‡’X = 1Î³X â‡â‡’Xi = Î³X for all node i. 2. Let Î³X = 1âŠ¤X N and Î³Y = 1âŠ¤Y N , then Î³X + Î³Y = 1âŠ¤(X+Y ) N = Î³X+Y . So Âµ(X + Y ) = âˆ¥(X + Y ) âˆ’ 1(Î³X + Î³Y )âˆ¥F = âˆ¥X âˆ’ 1Î³X + Y âˆ’ 1Î³Y âˆ¥F â‰¤ âˆ¥X âˆ’ 1Î³Xâˆ¥F + âˆ¥Y âˆ’ 1Î³Y âˆ¥F = Âµ(X) + Âµ(Y ) . C Proof of Lemma 1 According to the formulation (7): X(t+1) Â·i = X jt+1=i, (jt,...,j0)âˆˆ[d]t+1  tY k=0 W(k) jkjk+1 ! D(t) jt+1 P(t)...D(0) j1 P(0)X(0) Â·j0 , we thus obtain that âˆ¥X(t+1) Â·i âˆ¥âˆ = \r\r\r\r\r\r X jt+1=i ,(jt,...,j0)âˆˆ[d]t+1  tY k=0 W(k) jkjk+1 ! D(t) jt+1 P(t)...D(0) j1 P(0)X(0) Â·j0 \r\r\r\r\r\r âˆ â‰¤ X jt+1=i ,(jt,...,j0)âˆˆ[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !\r\r\rD(t) jt+1 P(t)...D(0) j1 P(0) \r\r\r âˆ \r\r\rX(0) Â·j0 \r\r\r âˆ â‰¤ X jt+1=i ,(jt,...,j0)âˆˆ[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !\r\r\rX(0) Â·j0 \r\r\r âˆ â‰¤ C0 ï£« ï£­ X jt+1=i ,(jt,...,j0)âˆˆ[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !ï£¶ ï£¸ = C0âˆ¥(|W(0)|...|W(t)|)Â·iâˆ¥1 , where C0 equals the maximal entry in |X(0)|. The assumption A3 implies that there exists Câ€² > 0 such that for all t âˆˆ Nâ‰¥0 and i âˆˆ [d], âˆ¥(|W(0)|...|W(t)|)Â·iâˆ¥1 â‰¤ Câ€²N . Hence there exists Câ€²â€² > 0 such that for all t âˆˆ Nâ‰¥0 and i âˆˆ [d], we have âˆ¥X(t) Â·i âˆ¥âˆ â‰¤ Câ€²â€² , proving the existence of C >0 such that âˆ¥X(t)âˆ¥max â‰¤ C for all t âˆˆ Nâ‰¥0. D Proof of Lemma 2 Lemma 2 is a direct corollary of Lemma 1 and the assumption that Î¨(Â·, Â·) assigns bounded attention scores to bounded inputs. 16E Proof of Lemma 3 E.1 Auxiliary results We make use of the following sufficient condition for the ergodicity of the infinite products of row-stochastic matrices. Lemma 8 (Corollary 5.1 [14]). Consider a sequence of row-stochastic matrices {S(t)}âˆ t=0. Let at and bt be the smallest and largest entries in S(t), respectively. If Pâˆ t=0 at bt = âˆ, then {S(t)}âˆ t=0 is ergodic. In order to make use of the above result, we first show that long products of P(t)â€™s from PG,Ïµ will eventually become strictly positive. For t0 â‰¤ t1, we denote P(t1:t0) = P(t1) . . . P(t0) . Lemma 9. Under the assumption A1, there exist T âˆˆ N and c >0 such that for all t0 â‰¥ 0, c â‰¤ P(t0+T:t0) ij â‰¤ 1 , âˆ€1 â‰¤ i, jâ‰¤ N . Proof. Fix any T âˆˆ Nâ‰¥0. Since âˆ¥P(t)âˆ¥âˆ â‰¤ 1 for any P(t) âˆˆ PG,Ïµ, it follows that âˆ¥P(t0+T:t0)âˆ¥âˆ â‰¤ 1 and hence P(t0+T:t0) ij â‰¤ 1, for all 1 â‰¤ i, jâ‰¤ N. To show the lower bound, without loss of generality, we will show that there existT âˆˆ N and c >0 such that P(T:0) ij â‰¥ c ,âˆ€1 â‰¤ i, jâ‰¤ N . Since each P(t) has the same connectivity pattern as the original graph G, it follows from the assumption A1 that there exists T âˆˆ N such that P(T:0) is a positive matrix, following a similar argument as the one for Proposition 1.7 in [24]: For each pair of nodes i, j, since we assume that the graph G is connected, there exists r(i, j) such that P(r(i,j):0) ij > 0. on the other hand, since we also assume each node has a self-loop, P(t:0) ii > 0 for all t â‰¥ 0 and hence for t â‰¥ r(i, j), P(t:0) ij â‰¥ P(tâˆ’r(i,j)) ii P(r(i,j):0) ij > 0 . For t â‰¥ t(i) := max jâˆˆG r(i, j), we have P(t:0) ij > 0 for all node j in G. Finally, if t â‰¥ T := max iâˆˆG t(i), then P(t:0) ij > 0 for all pairs of nodes i, jin G. Notice that P(T:0) ij is a weighted sum of walks of length T between nodes i and j, and hence P(T:0) ij > 0 if and only if there exists a walk of length T between nodes i and j. Since for all t âˆˆ Nâ‰¥0, P(t) ij â‰¥ Ïµ if (i, j) âˆˆ E(G), we conclude that P(T:0) ij â‰¥ ÏµT := c. E.2 Proof of Lemma 3 Given the sequence {P(t)}âˆ t=0, we use T âˆˆ N from Lemma 9 and define Â¯P(k) := P((k+1)T:kT ) . Then {P(t)}âˆ t=0 is ergodic if and only if { Â¯P(k)}âˆ k=0 is ergodic. Notice that by Lemma 9, for all k âˆˆ Nâ‰¥0, there exists c >0 such that c â‰¤ Â¯P(k) ij â‰¤ 1 , âˆ€1 â‰¤ i, jâ‰¤ N. Then Lemma 3 is a direct consequence of Lemma 8. F Proof of Lemma 5 F.1 Notations and auxiliary results Consider a sequence {D(t)P(t)}âˆ t=0 in MG,Ïµ. For t0 â‰¤ t1, define Qt0,t1 := D(t1)P(t1)...D(t0)P(t0) 17and Î´t = âˆ¥D(t) âˆ’ IN âˆ¥âˆ , where IN denotes the N Ã— N identity matrix. It is also useful to define Ë†Qt0,t1 :=P(t1)Qt0,t1âˆ’1 :=P(t1)D(t1âˆ’1)P(t1âˆ’1)...D(t0)P(t0). We start by proving the following key lemma, which states that long products of matrices inMG,Ïµ eventually become a contraction in âˆ-norm. Lemma 10. There exist 0 < c <1 and T âˆˆ N such that for all t0 â‰¤ t1, âˆ¥ Ë†Qt0,t1+T âˆ¥âˆ â‰¤ (1 âˆ’ cÎ´t1 )âˆ¥ Ë†Qt0,t1 âˆ¥âˆ . Proof. First observe that for every T â‰¥ 0, âˆ¥ Ë†Qt0,t1+T âˆ¥âˆ â‰¤ âˆ¥P(t1+T)D(t1+Tâˆ’1)P(t1+Tâˆ’1)...D(t1+1)P(t1+1)D(t1)âˆ¥âˆâˆ¥ Ë†Qt0,t1 âˆ¥âˆ â‰¤ âˆ¥P(t1+T)P(t1+Tâˆ’1)...P(t1+1)D(t1)âˆ¥âˆâˆ¥ Ë†Qt0,t1 âˆ¥âˆ , where the second inequality is based on the following element-wise inequality: P(t1+T)D(t1+Tâˆ’1)P(t1+Tâˆ’1)...D(t1+1)P(t1+1) â‰¤ew P(t1+T)P(t1+Tâˆ’1)...P(t1+1) . By Lemma 9, there exist T âˆˆ N and 0 < c <1 such that (P(t1+T)...P(t1+1))ij â‰¥ c, âˆ€1 â‰¤ i, jâ‰¤ N . Since the matrix product P(t1+T)P(t1+Tâˆ’1)...P(t1+1) is row-stochastic, multiplying it with the diagonal matrix D(t1) from right decreases the row sums by at least c(1 âˆ’ D(t1) min) = cÎ´t1 , where D(t1) min here denotes the smallest diagonal entry of the diagonal matrix D(t1). Hence, âˆ¥P(t1+T)P(t1+Tâˆ’1)...P(t1+1)D(t1)âˆ¥âˆ â‰¤ 1 âˆ’ cÎ´t1 . F.2 Proof of Lemma 4 Now define Î²k := Qk t=0(1 âˆ’ cÎ´t) and let Î² := lim kâ†’âˆ Î²k. Note that Î² is well-defined because the partial product is non-increasing and bounded from below. Then we present the following result, which is stated as Lemma 4 in the main paper and from which the ergodicity of any sequence in MG,Ïµ is an immediate result. Lemma 4. Let Î²k := Qk t=0(1 âˆ’ cÎ´t) and Î² := lim kâ†’âˆ Î²k. 1. If Î² = 0, then lim kâ†’âˆ Q0,k = 0 ; 2. If Î² >0, then lim kâ†’âˆ BQ0,k = 0 . Proof. We will prove the two cases separately. [Case Î² = 0 ] We will show that Î² = 0 implies lim kâ†’âˆ âˆ¥ Ë†Q0,kâˆ¥âˆ = 0 , and as a result, lim kâ†’âˆ âˆ¥Q0,kâˆ¥âˆ = 0. For 0 â‰¤ j â‰¤ T âˆ’ 1, let us define Î²j := âˆY k=0 (1 âˆ’ Î´j+kT ) . Then by Lemma 10, we get that lim kâ†’âˆ âˆ¥ Ë†Q0,kT âˆ¥âˆ â‰¤ Î²jâˆ¥ Ë†Q0,jâˆ¥âˆ . By construction, Î² = Î Tâˆ’1 j=0 Î²j. Hence, if Î² = 0 then Î²j0 = 0 for some 0 â‰¤ j0 â‰¤ T âˆ’ 1, which yields lim kâ†’âˆ âˆ¥ Ë†Q0,kâˆ¥âˆ = 0. Consequently, lim kâ†’âˆ âˆ¥Q0,kâˆ¥âˆ = 0 implies that lim kâ†’âˆ Q0,k = 0. 18[Case Î² >0] First observe that if Î² >0, then âˆ€0 < Î· <1, there exist m âˆˆ Nâ‰¥0 such that âˆY t=m (1 âˆ’ cÎ´t) > 1 âˆ’ Î· . (12) Using 1 âˆ’ x â‰¤ eâˆ’x for all x âˆˆ R, we deduce âˆY t=m eâˆ’cÎ´t > 1 âˆ’ Î· . It also follows from (12) that 1 âˆ’ cÎ´t > 1 âˆ’ Î·, or equivalently Î´t < Î· c for t â‰¥ m. Choosing Î· <c 2 thus ensures that Î´t < 1 2 for t â‰¥ m. Putting this together with the fact that, there exists5 b >0 such that 1 âˆ’ x â‰¥ eâˆ’bx for all x âˆˆ [0, 1 2 ], we obtain âˆY t=m (1 âˆ’ Î´t) â‰¥ âˆY t=m eâˆ’bÎ´t > (1 âˆ’ Î·) b c := 1 âˆ’ Î·â€² . (13) Define the product of row-stochastic matrices P(M:m) := P(M) . . . P(m). It is easy to verify the following element-wise inequality:  MY t=m (1 âˆ’ Î´t) ! P(M:m) â‰¤ew Qm,M â‰¤ew P(M:m) , which together with (13) leads to (1 âˆ’ Î·â€²)P(M:m) â‰¤ew Qm,M â‰¤ew P(M:m) . (14) Therefore, âˆ¥BQm,M âˆ¥âˆ = âˆ¥B(Qm,M âˆ’ P(M:m)) + BP (M:m)âˆ¥âˆ â‰¤ âˆ¥B(Qm,M âˆ’ P(M:m))âˆ¥âˆ + âˆ¥BP (M:m)âˆ¥âˆ = âˆ¥B(Qm,M âˆ’ P(M:m))âˆ¥âˆ â‰¤ âˆ¥Bâˆ¥âˆâˆ¥Qm,M âˆ’ P(M:m)âˆ¥âˆ â‰¤ Î·â€²âˆ¥Bâˆ¥âˆ â‰¤ Î·â€²âˆš N , where the last inequality is due to the fact that âˆ¥Bâˆ¥2 = 1. By definition, Q0,M = Qm,M Q0,mâˆ’1, and hence âˆ¥BQ0,M âˆ¥âˆ â‰¤ âˆ¥BQm,M âˆ¥âˆâˆ¥Q0,mâˆ’1âˆ¥âˆ â‰¤ âˆ¥BQm,M âˆ¥âˆ â‰¤ Î·â€²âˆš N . (15) The above inequality (15) holds when taking M â†’ âˆ. Then taking Î· â†’ 0 implies Î·â€² â†’ 0 and together with (15), we conclude that lim Mâ†’âˆ âˆ¥BQ0,M âˆ¥âˆ = 0 , and therefore, lim Mâ†’âˆ BQ0,M = 0 . F.3 Proof of Lemma 5 Notice that both cases Î² = 0 and Î² >0 in Lemma 4 imply the ergodicity of {D(t)P(t)}âˆ t=0. Hence the statement is a direct corollary of Lemma 4. 5Choose, e.g., b = 2 log 2. 19G Proof of Lemma 6 In order to show that JSR( ËœPG,Ïµ) < 1, we start by making the following observation. Lemma 11. A sequence {P(n)}âˆ n=0 is ergodic if and only if Qt n=0 ËœP(n) converges to the zero matrix. Proof. For any t âˆˆ Nâ‰¥0, it follows from the third property of the orthogonal projection B (see, Page 6 of the main paper) that B tY n=0 P(n) = tY n=0 ËœM(n)P . Hence {P(n)}âˆ n=0 is ergodic â‡â‡’ lim tâ†’âˆ B tY n=0 P(n) = 0 â‡â‡’ lim tâ†’âˆ tY n=0 ËœP(n)B = 0 â‡â‡’ lim tâ†’âˆ tY n=0 ËœP(n) = 0 . Next, we utilize the following result, as a means to ensure a joint spectral radius strictly less than 1 for a bounded set of matrices. Lemma 12 (Proposition 3.2 in [36]). For any bounded set of matrices M, JSR(M) < 1 if and only if for any sequence {M(n)}âˆ n=0 in M, Qt n=0 M(n) converges to the zero matrix. Here, â€œbounded\" means that there exists an upper bound on the norms of the matrices in the set. Note that PG,Ïµ is bounded because âˆ¥Pâˆ¥âˆ = 1, for all P âˆˆ MG,Ïµ. To show that ËœPG,Ïµ is also bounded, let ËœP âˆˆ ËœPG,Ïµ, then by definition, we have ËœP B= BP, Pâˆˆ MG,Ïµ â‡’ ËœP = BP BT , since BBT = INâˆ’1. As a result, âˆ¥ ËœPâˆ¥2 = âˆ¥BP BT âˆ¥2 â‰¤ âˆ¥Pâˆ¥2 â‰¤ âˆš N , where the first inequality is due to âˆ¥Bâˆ¥2 = âˆ¥BâŠ¤âˆ¥2 = 1, and the second ineuality follows from âˆ¥Pâˆ¥âˆ = 1. Combining Lemma 5, Lemma 11 and Lemma 12, we conclude that JSR( ËœPG,Ïµ) < 1. H Proof of Lemma 7 Note that any sequence {M(n)}âˆ n=0 in MG,Ïµ,Î´ satisfies Î² = 0, where Î² is defined in Lemma 4. This implies that for any sequence {M(n)}âˆ n=0 in MG,Ïµ,Î´, we have lim tâ†’âˆ tY n=0 M(n) = 0 . Since âˆ¥Mâˆ¥âˆ â‰¤ Î´ for all M âˆˆ MG,Ïµ,Î´, again by Lemma 12, we conclude that JSR(MG,Ïµ,Î´) < 1. I Proof of Theorem 1 To derive the exponential convergence rate, consider the linear and nonlinear cases separately: 20I.1 Bounds for the two cases [Case: linear] In the linear case where all D = IN , it follows from Lemma 6 that âˆ¥BX(t+1) Â·i âˆ¥2 = \r\r\r\r\r\r X jt+1=i, (jt,...,j0)âˆˆ[d]t+1  tY k=0 W(k) jkjk+1 ! BP (t)...P(0)X(0) Â·j0 \r\r\r\r\r\r 2 â‰¤ X jt+1=i, (jt,...,j0)âˆˆ[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !\r\r\rBP (t)...P(0)X(0) Â·j0 \r\r\r 2 = X jt+1=i, (jt,...,j0)âˆˆ[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !\r\r\r ËœP(t)... ËœP(0)BX(0) Â·j0 \r\r\r 2 â‰¤ X jt+1=i, (jt,...,j0)âˆˆ[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f ! Cqt+1 \r\r\rBX(0) Â·j0 \r\r\r 2 â‰¤ Câ€²qt+1 ï£« ï£­ X jt+1=i, (jt,...,j0)âˆˆ[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !ï£¶ ï£¸ = Câ€²qt+1âˆ¥(|W(0)|...|W(t)|)Â·iâˆ¥1 , (16) where Câ€² = Cmax jâˆˆ[d] âˆ¥BX(0) Â·j âˆ¥2 and âˆ¥ Â· âˆ¥1 denotes the 1-norm. Specifically, the first inequality follows from the triangle inequality, and the second inequality is due to the property of the joint spectral radius in (8), where JSR( ËœPG,Ïµ) < q <1. [Case: nonlinear] Consider T âˆˆ N defined in Lemma 10, where there exists a1 âˆˆ Nâ‰¥0 and a2 âˆˆ {0} âˆª[K âˆ’ 1] such that T = a1K + a2. Given the condition (â‹†), and Lemma 10, there exists 0 < c <1 such that for all m âˆˆ Nâ‰¥0, âˆ¥ Ë†QmK+nm,mK+nm+T âˆ¥âˆ â‰¤ (1 âˆ’ cÎ´â€²) , where Î´â€² = 1 âˆ’ Î´. Note that since for all nm, mâˆˆ Nâ‰¥0, a2 + nm â‰¤ 2K, we get that nm + T â‰¤ (a1 + 2)K . Since for all m âˆˆ Nâ‰¥0, âˆ¥ Ë†QmK+nm:(m+a1+2)K+nm+a1+2 âˆ¥âˆ â‰¤ (1 âˆ’ cÎ´â€²) , it implies that for all n âˆˆ Nâ‰¥0, âˆ¥Q0:n(a1+2)Kâˆ¥âˆ â‰¤ (1 âˆ’ cÎ´â€²)n = \u0010 (1 âˆ’ cÎ´â€²) n n(a1+2)K \u0011n(a1+2)K = \u0010 (1 âˆ’ cÎ´â€²) 1 (a1+2)K \u0011n(a1+2)K . Denote q := (1 âˆ’ cÎ´â€²) 1 (a1+2)K . By the equivalence of norms, we get that for all n âˆˆ Nâ‰¥0, âˆ¥Q0:n(a1+2)Kâˆ¥2 â‰¤ âˆš Nqn(a1+2)K . Then for any j âˆˆ {0} âˆª[(a1 + 2)K âˆ’ 1], âˆ¥Q0:n(a1+2)K+jâˆ¥2 â‰¤ âˆš Nqâˆ’jqn(a1+2)K+j â‰¤ Cqn(a1+2)K+j where C := âˆš Nqâˆ’(a1+2)K+1. Rewriting the indices, we conclude that âˆ¥Q0:tâˆ¥2 â‰¤ Cqt , âˆ€t â‰¥ 0 . The above bound implies that for any q satisfies q â‰¤ q <1, 21âˆ¥BX(t+1) Â·i âˆ¥2 = \r\r\r\r\r\r X jt+1=i, (jt,...,j0)âˆˆ[d]t+1  tY k=0 W(k) jkjk+1 ! BD(t) jt+1 P(t)...D(0) j1 P(0)X(0) Â·j0 \r\r\r\r\r\r 2 â‰¤ X jt+1=i, (jt,...,j0)âˆˆ[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !\r\r\rBD(t) jt+1 P(t)...D(0) j1 P(0)X(0) Â·j0 \r\r\r 2 â‰¤ X jt+1=i, (jt,...,j0)âˆˆ[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !\r\r\rD(t) jt+1 P(t)...D(0) j1 P(0) \r\r\r \r\r\rX(0) Â·j0 \r\r\r 2 â‰¤ Câ€²qt+1 ï£« ï£­ X jt+1=i, (jt,...,j0)âˆˆ[d]t+1  tY k=0 \f\f\fW(k) jkjk+1 \f\f\f !ï£¶ ï£¸ = Câ€²qt+1âˆ¥(|W(0)|...|W(t)|)Â·iâˆ¥1 , (17) where again, Câ€² = Cmax jâˆˆ[d] âˆ¥BX(0) Â·j âˆ¥2. I.2 Proof of the exponential convergence Based on the inequality extablished for both linear and nonlinear case in (16) and (17), we derive the rest of the proof. Since âˆ¥Bxâˆ¥2 = âˆ¥xâˆ¥2 if xâŠ¤1 = 0 for x âˆˆ RN , we also have that if XâŠ¤1 = 0 for X âˆˆ RNÃ—d, then âˆ¥BXâˆ¥F = âˆ¥Xâˆ¥F , using which we obtain that Âµ(X(t+1)) = âˆ¥X(t+1) âˆ’ 1Î³X(t+1) âˆ¥F = âˆ¥BX(t+1)âˆ¥F = vuut dX i=1 âˆ¥BX(t+1) Â·i âˆ¥2 2 â‰¤ Câ€²qt+1 vuut dX i=1 âˆ¥(|W(0)|...|W(t)|)Â·iâˆ¥2 1 â‰¤ Câ€²qt+1 vuut  dX i=1 âˆ¥|(W(0)|...|W(t)|)Â·iâˆ¥1 !2 = Câ€²qt+1âˆ¥|(W(0)|...|W(t)|âˆ¥1,1 , where âˆ¥ Â· âˆ¥1,1 denotes the matrix (1, 1)-norm (recall from Section A.2 that for a matrix M âˆˆ RmÃ—n, we have âˆ¥Mâˆ¥1,1 = Pm i=1 Pn j=1 |Mij|). The assumption A3 implies that there exists Câ€²â€² such that for all t âˆˆ Nâ‰¥0, âˆ¥(|W(0)|...|W(t)|)âˆ¥1,1 â‰¤ Câ€²â€²d2 . Thus we conclude that there exists C1 such that for all t âˆˆ Nâ‰¥0, Âµ(X(t)) â‰¤ C1qt . Remark 5. Similar to the linear case, one can also use Lemma 7 to establish exponential rate for oversmoothing when dealing with nonlinearities for which Assumption 4 holds in the strict sense, i.e. 0 â‰¤ Ïƒ(x) x < 1 (e.g., GELU and SiLU nonlinearities). Here, we presented an alternative proof requiring weaker conditions, making the result directly applicable to nonlinearities such as ReLU and Leaky ReLU. 22J Proof of Theorem 2 Note that (W(t))âŠ¤ âŠ— P(t) = ï£® ï£¯ï£¯ï£¯ï£¯ï£° W(t) 11 P(t) W(t) 21 P(t) ... W (t) d1 P(t) W(t) 12 P(t) W(t) 22 P(t) ... W (t) d2 P(t) ... ... ... ... W(t) 1d P(t) W(t) 2d P(t) ... W (t) dd P(t) ï£¹ ï£ºï£ºï£ºï£ºï£» âˆˆ RNdÃ—Nd â‰¥0 is a row substochastic matrix such that for all t â‰¥ 0, the nonzero entries of (W(t))âŠ¤ âŠ— P(t) are uniformly bounded below by Î¾Ïµ. Moreover, We can interpret the sparsity pattern of this matrix as connectivities of a meta graphGm with Nd nodes, where the node (f âˆ’1)N + n represents the feature f of node n in the original graph G. We denote such a node as (n, f), where n âˆˆ [N], fâˆˆ [d]. Then: â€¢ When only W(t) ii > 0 for all i âˆˆ [d], together with A1, it ensures that for a fixed feature i, all the nodes (n, i), nâˆˆ [N] fall into the same connected component of Gm, which has the same connectivity pattern as the original graphG. As a result, the connected component is non-bipartite. â€¢ If in addition, W(t) ij , W(t) ji > 0 for i Ì¸= j âˆˆ [d], the two connected components representing the features i and j will merge into one connected component, which is subsequently non-bipartite as well. Then we apply Lemma 5 to each connected component separately and conclude the statement of the theorem. K Proof of Proposition 2 Since Dâˆ’1 degA is similar to Dâˆ’1/2 deg ADâˆ’1/2 deg , they have the same spectrum. For Dâˆ’1 degA, the smallest nonzero entry has value 1/dmax, where dmax is the maximum node degree in G. On the other hand, it follows from the definition of PG,Ïµ that Ïµdmax â‰¤ 1 . Therefore, Ïµ â‰¤ 1/dmax and thus Dâˆ’1 degA âˆˆ PG,Ïµ. We proceed by proving the following result. Lemma 13. For any M in M, the spectral radius of M denoted by Ï(M), satisfies Ï(M) â‰¤ JSR(M) . Proof. Gelfandâ€™s formula states that Ï(M) = lim kâ†’âˆ âˆ¥Mkâˆ¥ 1 k , where the quantity is independent of the norm used [23]. Then comparing with the definition of the joint spectral radius, we can immediately conclude the statement. Let B(Dâˆ’1 degA) = ËœP B. By definition, ËœP âˆˆ ËœPG,Ïµ since Dâˆ’1 degA âˆˆ PG,Ïµ as shown before the lemma. Moreover, the spectrum of ËœP is the spectrum of Dâˆ’1 degA after reducing the multiplicity of eigenvalue 1 by one. Under the assumption A1, the eigenvalue1 of Dâˆ’1 degA has multiplicity 1, and hence Ï( ËœP) = Î», where Î» is the second largest eigenvalue ofDâˆ’1 degA. Putting this together with Lemma 13, we conclude that Î» â‰¤ JSR( ËœPG,Ïµ) as desired. L Numerical Experiments Here we provide more details on the numerical experiments presented in Section 5. All models were implemented with PyTorch [28] and PyTorch Geometric [10]. 23Datasets â€¢ We used torch_geometric.datasets.planetoid provided in PyTorch Geometric for the three homophilic datasets: Cora, CiteSeer, and PubMed with their default training and test splits. â€¢ We used torch_geometric.datasets.WebKB provided in PyTorch Geometric for the three heterophilic datasets: Cornell, Texas, and Wisconsin with their default training and test splits. â€¢ Dataset summary statistics are presented in Table 1. Dataset Type #Nodes %Nodes in LCC Cora 2,708 91.8 % CiteSeer homophilic 3,327 63.7 % PubMed 19,717 100 % Cornell 183 100 % Texas heterophilic 183 100 % Wisconsin 251 100 % Flickr large-scale 89,250 100 % Table 1: Dataset summary statistics. LCC: Largest connected component. Model details â€¢ For GAT, we consider the architecture proposed in VeliË‡ckoviÂ´c et al. [39] with each attentional layer sharing the parameter a in LeakyReLU(aâŠ¤[WâŠ¤Xi||WâŠ¤Xj]), aâˆˆ R2dâ€² to compute the attention scores. â€¢ For GCN, we consider the standard random walk graph convolution Dâˆ’1 degA. That is, the update rule of each graph convolutional layer can be written as Xâ€² = Dâˆ’1 degAXW , where X and Xâ€² are the input and output node representations, respectively, and W is the shared learnable weight matrix in the layer. Compute We trained all of our models on a Tesla V100 GPU. Training details In all experiments, we used the Adam optimizer using a learning rate of 0.00001 and 0.0005 weight decay and trained for 1000 epoch Results on large-scale dataset In addition to the numerical results presented in Sec- tion 5, we also conducted the same experiment on a large-scale dataset Flickr [ 47] using torch_geometric.datasets.Flickr. Figure 2 visualizes the results. 100 101 102 number of layers 10 2 10 1 100 101 102 103 104 105 (X) Flickr GAT 100 101 102 number of layers 10 2 10 1 100 101 102 103 104 105 Flickr GCN ReLU LeakyReLU (0.01) LeakyReLU (0.4) LeakyReLU (0.8) GELU Figure 2: Evolution of Âµ(X(t)) (in log-log scale) on the largest connected component of the large- scale benchmark dataset: Flickr. 24",
      "meta_data": {
        "arxiv_id": "2305.16102v4",
        "authors": [
          "Xinyi Wu",
          "Amir Ajorlou",
          "Zihui Wu",
          "Ali Jadbabaie"
        ],
        "published_date": "2023-05-25T14:31:59Z",
        "pdf_url": "https://arxiv.org/pdf/2305.16102v4.pdf"
      }
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liuâˆ— Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wuâˆ— Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ï¬xed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efï¬ciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiï¬cation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ï¬‚oating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (Lâˆ’1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover âˆ—Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such â€œneighbor explosionâ€ problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above â€œneighbor explosionâ€ problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], qâ‹† ij = Î±ijâˆ¥h(l) j âˆ¥2 âˆ‘ kâˆˆNi Î±ikâˆ¥h(l) k âˆ¥2 for vertex vi, to minimize the variance of the estimator Ë†h(l+1) i involves all its neighborsâ€™ hidden embeddings, i.e.{Ë†h(l) j |vj âˆˆNi}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsÎ±ijâ€™s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulï¬l this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deï¬ne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ï¬rst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi âˆˆV, and edges (vi,vj) âˆˆE. Let the adjacency matrix denote as AâˆˆRNÃ—N. Assuming the feature matrix H(0) âˆˆRNÃ—D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = Ïƒ ( Nâˆ‘ j=1 Î±(vi,vj) h(l) j W(l) ) , l = 0,...,L âˆ’1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, Î±Î±Î±= (Î±(vi,vj)) âˆˆRNÃ—N is a kernel or weight matrix, W(l) âˆˆRD(l)Ã—D(l+1) is the transform parameter on the l-th layer, and Ïƒ(Â·) is the activation function. The weight Î±(vi,vj), or Î±ij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deï¬ne ï¬xed weights as Î±Î±Î±= ËœDâˆ’1 ËœAor Î±Î±Î±= ËœDâˆ’1 2 ËœAËœDâˆ’1 2 respectively, where ËœA= A+I, and ËœDis the diagonal node degree matrix of ËœA. (2) The attentive GNNs [22, 17] deï¬ne a learned weight Î±(vi,vj) by attention functions: Î±(vi,vj) = ËœÎ±(vi,vj;Î¸)âˆ‘ vkâˆˆNi ËœÎ±(vi,vk;Î¸) , where the unnormalized attentions ËœÎ±(vi,vj; Î¸) = exp(ReLU(aT[Whiâˆ¥Whj])), are parameterized by Î¸= {a,W}. Different 2from GCNs, the learned weights Î±ij âˆËœÎ±ij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as Ë†h(l+1) i = Ïƒ ( N(i) Epij [ Ë†h(l) j ] W(l) ) , (2) where pij âˆÎ±ij, and N(i) =âˆ‘ jÎ±ij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = Î±ij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling Ë†h(l+1) i = ÏƒW(l) ( Ë†Âµ(l) i ) = ÏƒW(l) ( Eqij [Î±ij qij Ë†h(l) j ]) , (3) where we use ÏƒW(l) (Â·) to include transform parameter W(l) into the function Ïƒ(Â·) for conciseness. As such, one can ï¬nd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator Ë†Âµ(l) i = 1 k âˆ‘k s=1 Î±ijs qijs Ë†h(l) js , where js âˆ¼qi. Take expectation overqi, we deï¬ne the variance of Ë†Âµ(l) i = Î±ijs qijs Ë†h(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [îµ¹îµ¹îµ¹Ë†Âµ(l) i (t) âˆ’Âµ(l) i (t) îµ¹îµ¹îµ¹ 2] = E [îµ¹îµ¹îµ¹Î±ijs(t) qijs h(l) js (t) âˆ’ âˆ‘ jâˆˆNi Î±ij(t)h(l) j (t) îµ¹îµ¹îµ¹ 2] . (4) Note that Î±ij and h(vj) that are inferred during training may vary over steps tâ€™s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ï¬rst is a function of qi, which we refer to as the effective variance: Ve(qi) = âˆ‘ jâˆˆNi 1 qij Î±2 ijâˆ¥hjâˆ¥2 , (5) while the second does not depend on qi, and we denote it by Vc = îµ¹îµ¹îµ¹âˆ‘ jâˆˆNi Î±ijhj îµ¹îµ¹îµ¹ 2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: qâ‹† ij = Î±ijâˆ¥h(l) j âˆ¥2 âˆ‘ kâˆˆNi Î±ikâˆ¥h(l) k âˆ¥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighborsâ€™ embeddings in the denominator of Eq.(6). Moreover, the Î±ijâ€™s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theÎ±ijâ€™s are ï¬xed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ï¬xed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several â€œlayer samplingâ€ approaches [11, 6, 14, 25] have been proposed to alleviate the â€œneigh- bor explosionâ€ problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efï¬cient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two â€œgraph samplingâ€ approaches [7, 24] ï¬rst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with â€œlayer samplingâ€ approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 â‰¤tâ‰¤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Qâ‹† i = argmin Qi âˆ‘T t=1 Vt e(Qi), such that âˆ‘T t=1 Vt e(Qt i) â‰¤câˆ‘T t=1 Vt e(Qâ‹† i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si âŠ‚Ni where Si âˆ¼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) âˆ’Vt e(Qâ‹† i) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Qâ‹† i we have the upper bound derived on right hand of Eq. (7). We deï¬ne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Qâ‹† i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = âˆ’âˆ‡Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ï¬xed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deï¬ne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs âŠ‚Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =âˆ’âˆ‡qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efï¬cient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si âŠ‚Ni that satisï¬es âˆ‘ Si:jâˆˆSi Qi,Si = qij,âˆ€vj âˆˆNi, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward âˆ’âˆ‡Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ï¬nd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j âˆˆNi else 0, wij(1) = 1if j âˆˆNi else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deï¬ned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect viâ€™sksampled neighbors vj âˆˆSt i, and rewards rt i = {rij(t) :vj âˆˆSt i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ï¬nally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deï¬ned only at the (l+ 1)-th layer, hence we should maintain multiple qiâ€™s at each layer. In practice, we ï¬nd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qiâ€™s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ï¬rst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ï¬rst assume the weights Î±ijâ€™s are ï¬xed, then extend to attentive GNNs that Î±ij(t)â€™s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator Ë†Âµi = 1 k kâˆ‘ s=1 Î±ijs qijs Ë†hjs, js âˆ¼qi. (8) This yields the variance V(qi) = 1 k Eqi [îµ¹îµ¹îµ¹ Î±ijs qijs hjs âˆ’âˆ‘ jâˆˆNi Î±ijhj îµ¹îµ¹îµ¹ 2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =âˆ’âˆ‡qij(t)Vt e(qt i) = Î±2 ij kÂ·qij(t)2 âˆ¥hj(t)âˆ¥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisï¬es âˆ‘ Si:jâˆˆSi Qi,Si = qij,âˆ€vj âˆˆ Ni, where Si âŠ‚Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. Ë†Âµi = âˆ‘ jsâˆˆSi Î±ijs qijs hjs is the unbiased estimator of Âµi = âˆ‘ jâˆˆNi Î±ijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =âˆ‘ SiâŠ‚Ni Qi,Siâˆ¥âˆ‘ jsâˆˆSi Î±ijs qijs hjsâˆ¥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensenâ€™s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) â‰¤âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2. 5Table 1: Dataset summary. â€œsâ€ dontes multi-class task, and â€œmâ€ denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =âˆ‘ jsâˆˆSi Î±ijs qijs(t)2 âˆ¥hjs(t)âˆ¥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = Î±ij qij(t)2 âˆ¥hj(t)âˆ¥2,âˆ€j âˆˆSi. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value Î±ij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ËœÎ±ij. We deï¬ne the adjusted feedback attention values as follows: Î±â€² ij = âˆ‘ jâˆˆSi qij Â· ËœÎ±ijâˆ‘ jâˆˆSi ËœÎ±ij , (10) where ËœÎ±ijâ€™s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use âˆ‘ jâˆˆSi qij as a surrogate of âˆ‘ jâˆˆSi ËœÎ±ij âˆ‘ jâˆˆNi ËœÎ±ij so that we can approximate the truth attention values Î±ij by our adjusted attention values Î±â€² ij. 6 Regret Analysis As we described in section 4, the regret is deï¬ned as âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with Î·= 0.4 and Î´= âˆš (1âˆ’Î·)Î·4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1â‰¤tâ‰¤T, we have Tâˆ‘ t=1 Vt e(Qt i) â‰¤3 Tâˆ‘ t=1 Vt e(Qâ‹† i) + 10 âˆš Tn4 ln(n/k) k3 (11) where T â‰¥ln(n/k)n2(1 âˆ’Î·)/(kÎ·2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability qâ‹† i and using the reward deï¬nition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ï¬xed variance given the speciï¬c estimators, this is the ï¬rst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ï¬x the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(Â±0.014) 0.890(Â±0.002) 0.689(Â±0.005) 0.949(Â±0.001) 0.494(Â±0.001) FastGCN 0.827(Â±0.001) 0.895(Â±0.005) 0.502(Â±0.003) 0.825(Â±0.006) 0.500(Â±0.001) LADIES 0.843(Â±0.003) 0.880(Â±0.006) 0.574(Â±0.003) 0.932(Â±0.001) 0.465(Â±0.007) AS-GCN 0.830(Â±0.001) 0.888(Â±0.006) 0.599(Â±0.004) 0.890(Â±0.013) 0.506(Â±0.012) S-GCN 0.828(Â±0.001) 0.893(Â±0.001) 0.744(Â±0.003) 0.943(Â±0.001) 0.501(Â±0.002) ClusterGCN 0.807(Â±0.006) 0.887(Â±0.001) 0.853(Â±0.001) 0.938(Â±0.002) 0.418(Â±0.002) GraphSAINT 0.815(Â±0.012) 0.899(Â±0.002) 0.787(Â±0.003) 0.965(Â±0.001) 0.507(Â±0.001) GCN-BS 0.855(Â±0.005) 0.903(Â±0.001) 0.905(Â±0.003) 0.957(Â±0.000) 0.513(Â±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(Â±0.001) 0.884(Â±0.003) 0.566(Â±0.002) NA 0.472(Â±0.012) GraphSAINT-GAT0.773(Â±0.036) 0.886(Â±0.016) 0.789(Â±0.001) 0.933(Â±0.012) 0.470(Â±0.002) GAT-BS 0.857(Â±0.003) 0.894(Â±0.001) 0.841(Â±0.001) 0.962(Â±0.001) 0.513(Â±0.001) GAT-BS.M 0.857(Â±0.003) 0.894(Â±0.000) 0.867(Â±0.003) 0.962(Â±0.000) 0.513(Â±0.001) GP-BS 0.811(Â±0.002) 0.890(Â±0.003) 0.958(Â±0.001) 0.964(Â±0.000) 0.507(Â±0.000) GP-BS.M 0.811(Â±0.001) 0.892(Â±0.001) 0.965(Â±0.001) 0.964(Â±0.000) 0.507(Â±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are â€œgraph samplingâ€ techniques that ï¬rst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theâ„“2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ï¬rst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ï¬‚ickr as 200 by doing grid search. We set the architecture of GraphSAINT as â€œ0-1-1â€3 which means MLP layer followed by two graph convolution layers. We use the â€œrwâ€ sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiï¬cantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciï¬ed. As a result, their variances are simply ï¬xed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ï¬nd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighborsâ€™ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48â€“77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efï¬cient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257â€“266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702â€“2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530â€“6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324â€“360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024â€“1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 33, pages 946â€“953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558â€“4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiï¬cation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077â€“2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 33, pages 4424â€“4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593â€“607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiï¬ca- tion in network data. AI magazine, 29(3):93â€“93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375â€“389. Springer, 2010. [22] P. VeliË‡ckoviÂ´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247â€“11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: Î·= 0.4, sample size k, neighbor size n= |Ni|, Î´= âˆš (1 âˆ’Î·)Î·4k5 ln(n/k)/(Tn4). 1: Set Ë†rij(t) =rij(t)/qij(t) if j âˆˆSt i else 0 wij(t+ 1) =wij(t) exp(Î´Ë†rij(t)/n) 2: Set qij(t+ 1)â†(1 âˆ’Î·) wij(t+1)âˆ‘ jâˆˆNi wij(t+1) + Î· n, for j âˆˆNi Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: Î· = 0.4, sample size k, neighbor size n = |Ni|, Î´ = âˆš (1 âˆ’Î·)Î·4k5 ln(n/k)/(Tn4), Ut i = âˆ…. 1: For j âˆˆNi set Ë†rij(t) = {rij(t)/qij(t) if j âˆˆSt i 0 otherwise wij(t+ 1) = {wij(t) exp(Î´Ë†rij(t)/n) if j /âˆˆUt i wij(t) otherwise 2: if maxjâˆˆNi wij(t+ 1)â‰¥( 1 k âˆ’Î· n) âˆ‘ jâˆˆNi wij(t+ 1)/(1 âˆ’Î·) then 3: Decide at so as to satisfy atâˆ‘ wij(t+1)â‰¥at at + âˆ‘ wij(t+1)<at wij(t+ 1)= (1 k âˆ’Î· n)/(1 âˆ’Î·) 4: Set Ut+1 i = {j : wij(t+ 1)â‰¥at} 5: else 6: Set Ut+1 i = âˆ… 7: end if 8: Set wâ€² ij(t+ 1) = {wij(t+ 1) if j âˆˆNi\\Ut+1 i at if j âˆˆUt i 9: Set qij(t+ 1) =k ( (1 âˆ’Î·) wâ€² ij(t+1)âˆ‘ jâˆˆNi wâ€² ij(t+1) + Î· n ) for j âˆˆNi Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with âˆ‘K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set Î² = min{1 âˆ’qi,qj}and Î³ = min{qi,1 âˆ’qj} 6: Update qi and qj as (qi,qj) = { (qi + Î²,qj âˆ’Î²) with probability Î³ Î²+Î³ (qi âˆ’Î³,qj + Î³) with probability Î² Î²+Î³ 7: end while 8: return {i: qi = 1,1 â‰¤iâ‰¤K} 12B Proofs Proposition 1. Ë†Âµi = âˆ‘ jsâˆˆSi Î±ijs qijs hjs is the unbiased estimator of Âµi = âˆ‘ jâˆˆNi Î±ijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si âŠ‚Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy âˆ‘ Si:jâˆˆSi Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[Ë†Âµi] =E ï£® ï£°âˆ‘ jsâˆˆSi Î±ijs qijs hjs ï£¹ ï£» (12) = âˆ‘ SiâŠ‚Ni Qi,Si âˆ‘ jsâˆˆSi Î±ijs qijs hjs (13) = âˆ‘ jâˆˆNi âˆ‘ Si:jâˆˆSi Qi,Si Î±ij qij hj (14) = âˆ‘ jâˆˆNi Î±ij qij hj âˆ‘ Si:jâˆˆSi Qi,Si (15) = âˆ‘ jâˆˆNi Î±ij qij hjqij (16) = âˆ‘ jâˆˆNi Î±ijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) â‰¤âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2. Proof. The variance is V(Qi) =E ï£® ï£¯ï£° îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ jsâˆˆSi Î±ijs qijs hjs âˆ’ âˆ‘ jâˆˆNi Î±ijhj îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2ï£¹ ï£ºï£» = âˆ‘ SiâŠ‚Ni Qi,Si îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ jsâˆˆSi Î±ijs qijs hjs îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2 âˆ’ îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ jâˆˆNi Î±ijhj îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2 . Therefore the effective variance has following upper bound: Ve(Qi) = âˆ‘ SiâŠ‚Ni Qi,Si îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ jsâˆˆSi Î±ijs qijs hjs îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2 â‰¤ âˆ‘ SiâŠ‚Ni Qi,Si âˆ‘ jsâˆˆSi Î±ijs îµ¹îµ¹îµ¹îµ¹ hjs qijs îµ¹îµ¹îµ¹îµ¹ 2 (Jensenâ€²sInequality ) = âˆ‘ jsâˆˆNi âˆ‘ Si:jsâˆˆSi Qi,SiÎ±ijs îµ¹îµ¹îµ¹îµ¹ hjs qijs îµ¹îµ¹îµ¹îµ¹ 2 = âˆ‘ jsâˆˆNi Î±ijs q2 ijs âˆ¥hjsâˆ¥2 âˆ‘ Si:jsâˆˆSi Qi,Si = âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 13Proposition 3. The negative derivative of the approximated effective variance âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =âˆ‘ jsâˆˆSi Î±ijs qijs(t)2 âˆ¥hjs(t)âˆ¥2. Proof. Deï¬ne the upper bound as Ë†Ve(Qi) =âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2, then its derivative is âˆ‡Qi,Si Ë†Ve(Qi) =âˆ‡Qi,Si âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 = âˆ‡Qi,Si âˆ‘ jsâˆˆNi Î±ijsâˆ‘ Sâ€² i:jsâˆˆSâ€² i Qi,Sâ€² i âˆ¥hjsâˆ¥2 = âˆ‡Qi,Si âˆ‘ jsâˆˆSi Î±ijsâˆ‘ Sâ€² i:jsâˆˆSâ€² i Qi,Sâ€² i âˆ¥hjsâˆ¥2 = âˆ’ âˆ‘ jsâˆˆSi Î±js q2 ijs âˆ¥hjsâˆ¥2 (chainrule) Before we give the proof of Theorem 1, we ï¬rst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantÎ·â‰¤1 and any valid distributions Qt i and Qâ‹† i we have (1 âˆ’2Î·)Vt e(Qt i) âˆ’(1 âˆ’Î·)Vt e(Qâ‹† i) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©+ Î·âŸ¨Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ© (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Qâ‹† i we have Vt e(Qt i) âˆ’Vt e(Qâ‹† i) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©. (19) Multiplying both sides of this inequality by 1 âˆ’Î·, we have (1 âˆ’Î·)Vt e(Qt i) âˆ’(1 âˆ’Î·)Vt e(Qâ‹† i) (20) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©âˆ’Î·âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have âŸ¨Qt i,âˆ‡Qt i Vt e(Qt i)âŸ©= âˆ’ âˆ‘ jâˆˆNi qij(t) Î±2 ij kÂ·qij(t)2 âˆ¥hj(t)âˆ¥2 (22) = âˆ’Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective varianceâˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have âŸ¨Qt i,âˆ‡Qt i Vt e(Qt i)âŸ©= âˆ’ âˆ‘ SiâŠ‚Ni Qt i,Si âˆ‘ jsâˆˆSi Î±ijs qijs(t)2 âˆ¥hjsâˆ¥2 (24) = âˆ’ âˆ‘ jsâˆˆNi Î±ijs qijs(t)2 âˆ¥hjsâˆ¥2 âˆ‘ Si:jsâˆˆSi Qt i,Si (25) = âˆ’ âˆ‘ jsâˆˆNi Î±ijs qijs(t)âˆ¥hjsâˆ¥2 (26) = âˆ’Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 âˆ’Î·)Vt e(Qt i) âˆ’(1 âˆ’Î·)Vt e(Qâ‹† i) (28) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©âˆ’Î·âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ© (29) = âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©+ Î·âŸ¨Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©+ Î·Vt e(Qt i). (30) Theorem 1. Using Algorithm 1 with Î·= 0.4 and Î´ = âˆš (1 âˆ’Î·)Î·4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1â‰¤tâ‰¤T, we have Tâˆ‘ t=1 Vt e(Qt i) â‰¤3 Tâˆ‘ t=1 Vt e(Qâ‹† i) + 10 âˆš Tn4 ln(n/k) k3 (31) where T â‰¥ln(n/k)n2(1 âˆ’Î·)/(kÎ·2) and n= |Ni|. Proof. First we explain why condition T â‰¥ln(n/k)n2(1 âˆ’Î·)/(kÎ·2) ensures that Î´Ë†rij(t) â‰¤1, Î´Ë†rij(t) = âˆš (1 âˆ’Î·)Î·4k5 ln(n/k) Tn4 Â·Î±ij(t) q3 ij(t) âˆ¥hj(t)âˆ¥2 (32) â‰¤ âˆš (1 âˆ’Î·)Î·4k5 ln(n/k) Tn4 Â· n3 k3Î·3 (33) â‰¤1 (34) Assuming âˆ¥hj(t)âˆ¥â‰¤ 1, inequality (33) holds because Î±ij(t) â‰¤1 and qij(t) â‰¥kÎ·/n. Then replace T by the condition, we get Î´Ë†rij(t) â‰¤1. Let Wi(t), Wâ€² i(t) denote âˆ‘ jâˆˆNi wij(t), âˆ‘ jâˆˆNi wâ€² ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = âˆ‘ jâˆˆNi\\Ut i wij(t+ 1) Wi(t) + âˆ‘ jâˆˆUt i wij(t+ 1) Wi(t) (35) = âˆ‘ jâˆˆNi\\Ut i wij(t) Wi(t) Â·exp(Î´Ë†rij(t)) + âˆ‘ jâˆˆUt i wij(t) Wi(t) (36) â‰¤ âˆ‘ jâˆˆNi\\Ut i wij(t) Wi(t) [ 1 +Î´Ë†rij(t) + (Î´Ë†rij(t))2] + âˆ‘ jâˆˆUt i wij(t) Wi(t) (37) = 1 +Wâ€² i (t) Wi(t) âˆ‘ jâˆˆNi\\Ut i wij(t) Wâ€² i (t) [ Î´Ë†rij(t) + (Î´Ë†rij(t))2] (38) = 1 +Wâ€² i (t) Wi(t) âˆ‘ jâˆˆNi\\Ut i qij(t)/kâˆ’Î·/n 1 âˆ’Î· [ Î´Ë†rij(t) + (Î´Ë†rij(t))2] (39) â‰¤1 + Î´ k(1 âˆ’Î·) âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†r2 ij(t) (40) Inequality (37) uses ea â‰¤1 +a+ a2 for aâ‰¤1. Equality (39) holds because of update equation of qij(t) deï¬ned in EXP3.M. Inequality (40) holds because Wâ€² i (t) Wi(t) â‰¤1. Since 1 +xâ‰¤ex for xâ‰¥0, we have ln Wi(t+ 1) Wi(t) â‰¤ Î´ k(1 âˆ’Î·) âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†r2 ij(t) (41) 15If we sum, for 1 â‰¤tâ‰¤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = Tâˆ‘ t=1 ln Wi(t+ 1) Wi(t) (42) â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†r2 ij(t) (43) â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)Ë†r2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) â‰¥ln âˆ‘ jâˆˆS wij(T + 1) Wi(1) (45) â‰¥ âˆ‘ jâˆˆS ln wij(T + 1) k âˆ’ln n k (46) â‰¥Î´ k âˆ‘ jâˆˆS âˆ‘ t:j /âˆˆUt i Ë†rij(t) âˆ’ln n k (47) The inequality (46) uses the fact that âˆ‘ jâˆˆS wij(T + 1)â‰¥k( âˆ jâˆˆS wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(Î´ âˆ‘ t:j /âˆˆUt i Ë†rij(t)) From (44) and (47), we get Î´ k âˆ‘ jâˆˆS âˆ‘ t:j /âˆˆUt i Ë†rij(t) âˆ’ln n k â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†r2 ij(t) (48) And we have the following inequality Î´ k âˆ‘ jâˆˆS âˆ‘ t:jâˆˆUt i rij(t) = Î´ k âˆ‘ jâˆˆS âˆ‘ t:jâˆˆUt i qij(t)Ë†rij(t) (49) â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆUt i qij(t)Ë†rij(t) (50) The equality (49) holds beacuse rij(t) =qijË†rij(t) when j âˆˆSt i and Ut i âŠ†St i bacause qt ij = 1for all j âˆˆUt i. Then add inequality (50) in (48) we have Î´ k âˆ‘ jâˆˆS âˆ‘ t:jâˆˆUt i rij(t) +Î´ k âˆ‘ jâˆˆS âˆ‘ t:j /âˆˆUt i Ë†rij(t) âˆ’ln n k (51) â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)Ë†r2 ij(t) (52) Given qij(t) we have E[Ë†r2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that Î´ k Tâˆ‘ t=1 âˆ‘ jâˆˆS rij(t) âˆ’ln n k â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi r2 ij(t) (53) 16By multiplying (53) by Qâ‹† i,S and summing over S, we get Î´ k Tâˆ‘ t=1 âˆ‘ SâŠ‚Ni Qâ‹† i,S âˆ‘ jâˆˆS rij(t) âˆ’ln n k â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi r2 ij(t) (54) As âˆ‘ jâˆˆNi qij(t)rij(t) = âˆ‘ jâˆˆNi âˆ‘ Si:jâˆˆSi Qt i,Sirij(t) (55) = âˆ‘ SiâŠ‚Ni Qt i,Si âˆ‘ jâˆˆSi rij(t) (56) = âˆ’ âˆ‘ SiâŠ‚Ni Qt i,Siâˆ‡Qt i,Si Vt e(Qt i,Si) (57) = âˆ’âŸ¨Qt i,âˆ‡Qt i Vt e(Qt i)âŸ© (58) By plugging (58) in (54) and rearranging it, we ï¬nd Tâˆ‘ t=1 âŸ¨Qt i âˆ’Qâ‹† i ,âˆ‡Qt i Vt e(Qt i)âŸ©+ Î· Tâˆ‘ t=1 âŸ¨Qâ‹† i ,âˆ‡Qt i Vt e(Qt i)âŸ© (59) â‰¤Î´ Tâˆ‘ t=1 âˆ‘ jâˆˆNi r2 ij(t) +(1 âˆ’Î·)k Î´ ln(n/k) Using Lemma 1, we have (1 âˆ’2Î·) Tâˆ‘ t=1 Vt e(Qt i) âˆ’(1 âˆ’Î·) Tâˆ‘ t=1 Vt e(Qâ‹† i ) â‰¤Î´ Tâˆ‘ t=1 âˆ‘ jâˆˆNi r2 ij(t) +(1 âˆ’Î·)k Î´ ln(n/k) (60) Finally, we know that âˆ‘ jâˆˆNi r2 ij(t) = âˆ‘ jâˆˆNi Î±ij(t)2 qij(t)4 (61) â‰¤ âˆ‘ jâˆˆNi Î±ij(t) n4 k4Î·4 (becauseqij(t) â‰¥kÎ·/n) (62) = n4 k4Î·4 (63) By setting Î·= 0.4 and Î´= âˆš (1 âˆ’Î·)Î·4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other â€œlayer samplingâ€ approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between â€œgraph samplingâ€ and â€œlayer samplingâ€ paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with â€œgraph samplingâ€ approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ï¬‚oating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the â€œlayer samplingâ€ paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. â€œGraph samplingâ€ approaches are not applicable to cases where only partial vertices have labels. To summarize, the â€œlayer samplingâ€ approaches are more ï¬‚exible and general compared with â€œgraph samplingâ€ approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ï¬nd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      }
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafaâˆ— nimrah.mustafa@cispa.de Aleksandar Bojchevskiâ€  a.bojchevski@uni-koeln.de Rebekka Burkholzâˆ— burkholz@cispa.de âˆ—CISPA Helmholtz Center for Information Security, 66123 SaarbrÃ¼cken, Germany â€ University of Cologne, 50923 KÃ¶ln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a nodeâ€™s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a nodeâ€™s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different â€˜effectiveâ€™ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. â€¢ We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. â€¢ This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. â€¢ To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E âŠ† V Ã— V , where the neighborhood of a node v is given as N(v) = {u|(u, v) âˆˆ E}, a GNN layer computes each nodeâ€™s representation by aggregating over its neighborsâ€™ representation. In GATs, this aggregation is weighted by parameterized attention coefficients Î±uv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i âˆˆ [nl] Given input representations hlâˆ’1 v for all nodes v âˆˆ V , a GAT 1 layer l transforms those to: hl v = Ï•( X uâˆˆN(v) Î±l uv Â· Wl shlâˆ’1 u ), where (1) Î±l uv = exp((al)âŠ¤ Â· LeakyReLU(Wl shlâˆ’1 u + Wl t hlâˆ’1 v ))P uâ€²âˆˆN(v) exp((al)âŠ¤ Â· LeakyReLU(Wlshlâˆ’1 uâ€² + Wl t hlâˆ’1v ))) . (2) We consider Ï• to be a positively homogeneous activation functions (i.e Ï•(x) =xÏ•â€²(x) and conse- quently, Ï•(ax) =aÏ•(x) for positive scalars a), such as a ReLU Ï•(x) = max{x, 0} or LeakyReLU Ï•(x) = max{x, 0} + âˆ’Î± max{âˆ’x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 âŠ‚ Rd Ã— Rp for M â‰¤ V , let f : Rd â†’ Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l âˆˆ [L] of size nl has associated parameters: a feature weight matrix Wl âˆˆ RnlÃ—nlâˆ’1 and an attention vector al âˆˆ Rnl, where n0 = d and nL = p. Given a differentiable loss function â„“ : Rd Ã— Rp â†’ R, the loss L = 1/M PM i=1 â„“(f(xm), ym) is used to update model parameters w âˆˆ {Wl, al}L l=1 with learning rate Î³ by gradient descent, i.e., wt+1 = wt âˆ’ Î³âˆ‡wL, where âˆ‡wL = [âˆ‚L/âˆ‚w1, . . . ,âˆ‚L/âˆ‚w|w|] and w0 is set by the initialization scheme. For an infinitesimal Î³ â†’ 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = âˆ’âˆ‡wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i âˆˆ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i âˆˆ [dl], jâˆˆ [dlâˆ’1], and k âˆˆ [dl+1], as depicted in Fig. 1. âŸ¨Â·, Â·âŸ© denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l âˆˆ [L âˆ’ 1] in the network is conserved according to the following law: âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© âˆ’ âŸ¨al[i], âˆ‡al[i]LâŸ© = âŸ¨Wl+1[:, i], âˆ‡Wl+1[:,i]LâŸ©. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation Ï• in Eq. (1), gradient flow in the network adheres to the following invariance for l âˆˆ [L âˆ’ 1]: d dt \u0010\r\rWl[i, :] \r\r2 âˆ’ \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) Î´ with GD  (b) Î´ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show Î´ â‰ˆ 0 (can not empirically be exactly zero due to finite Î³ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c Ì¸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc â‰ˆ 0 as the network becomes slightly unbalanced due to the finite Î³ = 0.1. As both Î´ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 âˆ’ \r\ral[i] \r\r2 âˆ’ \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 âˆ’ \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the â€˜degree of balancednessâ€™ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l âˆˆ [L âˆ’ 1] by summing over i âˆˆ [nl] is: d dt \u0010\r\rWl\r\r2 F âˆ’ \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define Î´ as: Î´ = âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© âˆ’ âŸ¨al, âˆ‡alLâŸ© âˆ’ âŸ¨Wl+1[:, i], âˆ‡Wl+1[:,i]LâŸ© = 0. (7) We observe how the value of Î´ varies during training in Fig. 2 for both GD and Adam optimizers with Î³ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (Î´ â‰ˆ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nLâˆ’1) << 1, where the number of outputs is smaller than the layer width nL << nLâˆ’1. In contrast, E \r\rWLâˆ’1[i, :] \r\r2 = 2nLâˆ’1/(nLâˆ’1 + nLâˆ’2) = 1 and EaLâˆ’1[i]2 = 2/(1 +nLâˆ’1). In consequence, the right-hand side of our derived conservation law nLâˆ’2X j=1 (WLâˆ’1 ij )2 âˆ‡WLâˆ’1 ij L WLâˆ’1 ij âˆ’ (aLâˆ’1 i )2 âˆ‡aLâˆ’1 i L aLâˆ’1 i = nLX k=1 (WL ki)2 âˆ‡WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss â†’ 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L âˆ’1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 âˆ‡W(1) jm L W(1) jm âˆ’ Lâˆ’1X l=1 nlX o=1 a(l) o 2 âˆ‡a(l) o L a(l) o = nLâˆ’1X i=1 nLX k=1 W(L) ki 2 âˆ‡W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWLâˆ’1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL âˆ’ 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l âˆˆ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(wâˆ—âˆ’w0)/wâˆ—|. of trained network parameters (wâˆ—) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (wâˆ— â‰¥ 10âˆ’4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with Î³ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to Î±uv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the â€˜chicken and eggâ€™ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l âˆˆ [L]: \r\rWl[i, :] \r\r2 âˆ’ \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l âˆˆ [L]. 2. Set W1[i, :] = W1[i,:] âˆ¥W1[i,:]âˆ¥ âˆšÎ²i, for i âˆˆ [n1] where Î²i is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] âˆ¥Wl+1[:,i]âˆ¥ \r\rWl[i, :] \r\r for i âˆˆ [nl] and l âˆˆ [L âˆ’ 1] In step 2, Î²i determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set Î²i = 2for i âˆˆ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = Î²i for i âˆˆ [nLâˆ’1]. Balanced Orthogonal Initialization The feature weights Wl for l âˆˆ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let âˆ¥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 âˆ¥ âˆ’U1] where U1 âˆˆ R n1 2 Ã—n0 , Wl = [[Ul âˆ¥ âˆ’Ul] |= [âˆ’Ul âˆ¥ Ul]] where Ul âˆˆ R nl 2 Ã— nlâˆ’1 2 for l = {2, . . . , Lâˆ’ 1}, and WL = [UL |= âˆ’UL] where UL âˆˆ RnLÃ— nlâˆ’1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set Î²i = 2for i âˆˆ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss â‰¤ 10âˆ’4) and select the model state with the highest validation accuracy. For each experiment, the mean Â±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) Â± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 Â± 2.73 58 .40 Â± 2.25 24 .70 Â± 8.90 19 .23 Â± 1.54 18 .72 Â± 1.15 BalX 71.62 Â± 0.80 68 .83 Â± 1.62 64 .13 Â± 1.57 54 .88 Â± 7.95 42 .63 Â± 17.47 BalO 72.02 Â± 0.63 70 .63 Â± 0.60 68 .83 Â± 1.68 68 .70 Â± 1.18 63 .40 Â± 1.43 Pubmed Xav 77.26 Â± 1.39 70 .68 Â± 2.16 67 .32 Â± 10.70 36 .52 Â± 11.50 27 .20 Â± 13.99 BalX 78.02 Â± 0.73 75.66 Â± 1.81 77.60 Â± 1.56 76.44 Â± 1.70 75 .74 Â± 2.94 BalO 77.68 Â± 0.45 76.62 Â± 1.59 77.04 Â± 2.14 78.20 Â± 0.61 77 .80 Â± 1.41 Actor Xav 27.32 Â± 0.59 24 .60 Â± 0.93 24 .08 Â± 0.80 22 .29 Â± 3.26 19 .46 Â± 5.75 BalX 26.00 Â± 0.59 23 .93 Â± 1.42 24.21 Â± 0.78 24 .74 Â± 1.14 23.88 Â± 0.97 BalO 26.59 Â± 1.03 24 .61 Â± 0.77 24.17 Â± 0.62 24 .24 Â± 1.05 23.93 Â± 1.53 Chameleon Xav 52.81 Â± 1.37 54.21 Â± 1.05 30 .31 Â± 5.96 22 .19 Â± 2.04 22 .28 Â± 3.15 BalX 51.18 Â± 1.94 54.21 Â± 0.82 52 .11 Â± 3.72 51.89 Â± 1.89 38 .64 Â± 10.31 BalO 50.00 Â± 3.07 53 .95 Â± 1.81 51 .84 Â± 3.21 52.72 Â± 0.13 44 .30 Â± 1.61 Cornell Xav 42.70 Â± 2.51 41.08 Â± 2.51 42.70 Â± 1.34 25.41 Â± 14.64 22 .70 Â± 13.69 BalX 41.08 Â± 6.84 35 .14 Â± 11.82 41 .08 Â± 2.51 40 .00 Â± 4.93 37.84 Â± 5.62 BalO 42.16 Â± 1.64 43.24 Â± 2.12 36.76 Â± 5.02 35.68 Â± 3.29 36.22 Â± 3.42 Squirrel Xav 35.20 Â± 0.44 40 .96 Â± 0.92 21 .65 Â± 1.52 20 .23 Â± 1.69 19 .67 Â± 0.29 BalX 35.95 Â± 1.69 40.98 Â± 0.87 38.98 Â± 1.49 38.35 Â± 1.07 25 .38 Â± 4.62 BalO 35.83 Â± 0.92 42.52 Â± 1.19 38.85 Â± 1.36 39.15 Â± 0.44 26 .57 Â± 1.83 Texas Xav 60.00 Â± 1.34 60 .54 Â± 3.42 58 .92 Â± 1.34 49 .73 Â± 20.97 17 .84 Â± 26.98 BalX 60.54 Â± 1.64 61 .62 Â± 2.51 61 .62 Â± 2.51 58 .92 Â± 2.51 56.22 Â± 1.34 BalO 60.00 Â± 1.34 57 .30 Â± 1.34 56 .76 Â± 0.00 58 .38 Â± 4.55 57.30 Â± 3.91 Wisconsin Xav 51.37 Â± 6.04 51.37 Â± 8.90 51 .76 Â± 3.64 43 .14 Â± 25.07 31 .76 Â± 31.50 BalX 49.80 Â± 8.79 54.51 Â± 4.19 47.84 Â± 7.16 50.59 Â± 10.49 41.18 Â± 1.54 BalO 49.80 Â± 4.24 55 .69 Â± 3.64 51.76 Â± 5.68 49.02 Â± 4.36 48.24 Â± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as Ï‰GAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249â€“256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and ClÃ©ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias MÃ¼ller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias MÃ¼ller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] PÃ¡l AndrÃ¡s Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar VeliË‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727â€“11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted âŸ¨, âŸ© and âŠ™, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noetherâ€™s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(Î¸) is rescale invariant with respect to disjoint subsets of the parameters Î¸1 and Î¸2 if for every Î» >0 we have L(Î¸) =L((Î»Î¸1, Î»âˆ’1Î¸2, Î¸d)), where Î¸ = (Î¸1, Î¸2, Î¸d). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: âŸ¨Î¸1, âˆ‡Î¸1 LâŸ© âˆ’ âŸ¨Î¸2, âˆ‡Î¸2 LâŸ© = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as Î¸1 = {w | w âˆˆ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as Î¸2 = {w | w âˆˆ Wl+1[: , i]} âˆª {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at Î»Î¸1 and Î»âˆ’1Î¸2 and show that it remains invariant for any choice of Î» >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and Î±l uv. We denote the scaled network components with a tilde resulting in Ëœhl u[i], Ëœhl+1 v [k], and ËœÎ±l uv As we show, parameters of upper layers remain unaffected, as Ëœhl+1 v [k] coincides with its original non-scaled variant Ëœhl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that Ëœal[i] =Î»âˆ’1al[i] and ËœWl[i, j] =Î»Wl[i, j]. This implies that ËœÎ±l uv = exp(el uv)P uâ€²âˆˆN(v) exp(eluv) = Î±l uv , because (11) Ëœel uv = (al)âŠ¤ Â· Ï•2(Wl(hlâˆ’1 u + hlâˆ’1 v )) =el uv, (12) which follows from the positive homogeneity of Ï•2 that allows Ëœel uv = Î»âˆ’1al[i]Ï•2( nlâˆ’1X j Î»Wl[i, j](hlâˆ’1 u [j] +hlâˆ’1 v [j]) + nlX iâ€²Ì¸=i al[i]Ï•2( nlâˆ’1X j Wl[i, j](hlâˆ’1 u [j] +hlâˆ’1 v [j]) (13) = Î»âˆ’1Î»al[i]Ï•2( nlâˆ’1X j Wl[i, j](hlâˆ’1 u [j] +hlâˆ’1 v [j]) + nlX iâ€²Ì¸=i al[i]Ï•2( nlâˆ’1X j Wl[i, j](hlâˆ’1 u [j] +hlâˆ’1 v [j]) (14) = el uv. (15) 15Since ËœÎ±l uv = Î±l uv, it follows that Ëœhu l [i] =Ï•1 ï£« ï£­ X zâˆˆN(u) Î±l zu nlâˆ’1X j Î»Wl[i, j]hlâˆ’1 z [j] ï£¶ ï£¸ = Î»Ï•1 ï£« ï£­ X zâˆˆN(u) Î±l zu nlâˆ’1X j Wl[i, j]hlâˆ’1 z [j] ï£¶ ï£¸ = Î»hl u[i] In the next layer, we therefore have Ëœhl+1 v [k] =Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Î»âˆ’1Wl+1[k, i]Ëœhl u[i] ï£¶ ï£¸ = Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Î»âˆ’1Wl+1[k, i]Î»hl u[i] ï£¶ ï£¸ = Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Wl+1[k, i]hl u[i] ï£¶ ï£¸ = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© âˆ’ âŸ¨al[i], âˆ‡al[i]LâŸ© âˆ’ âŸ¨Wl+1[:, i], âˆ‡Wl+1[:,i]LâŸ© = 0. which can be rearranged to Eq.((2.2): âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© âˆ’ âŸ¨al[i], âˆ‡al[i]LâŸ© = âŸ¨Wl+1[:, i], âˆ‡Wl+1[:,i]LâŸ©. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = âˆ’âˆ‡wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2âŸ¨Wl[i, :], d dtWl[i, :]âŸ© = âˆ’2âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 âˆ’ \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . âˆ’2âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© âˆ’(âˆ’2)âŸ¨al[i], âˆ‡al[i]LâŸ© = âˆ’2âŸ¨Wl+1[:, i], âˆ‡Wl+1[:,i]LâŸ©. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i âˆˆ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l âˆˆ [L âˆ’ 1] in the network are governed by the following law: âŸ¨Wl s[i, :], âˆ‡Wls[i,:]LâŸ© + âŸ¨Wl t [i, :], âˆ‡Wl t [i,:]LâŸ© âˆ’ âŸ¨al[i], âˆ‡al[i]LâŸ© = âŸ¨Wl+1 s [:, i], âˆ‡Wl+1 s [:,i]LâŸ© (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets Î¸1 and Î¸2 of the parameter set Î¸, associated with a neuron i in layer l accordingly, as follows: Î¸1 = {w|w âˆˆ Wl s[i, :]} âˆª {w|w âˆˆ Wl t [i, :]} Î¸2 = {w|w âˆˆ Wl+1 s [:, i]} âˆª {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and Î±l uv. We denote the scaled network components with a tilde resulting in Ëœhl u[i], Ëœhl+1 v [k], and ËœÎ±l uv As we show, parameters of upper layers remain unaffected, as Ëœhl+1 v [k] coincides with its original non-scaled variant Ëœhl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that Ëœal[i] =Î»âˆ’1al[i], ËœWs l [i, j] = Î»Wl s[i, j] and ËœWl t [i, j] =Î»Wl t [i, j]. This implies that ËœÎ±l uv = exp(el uv)P uâ€²âˆˆN(v) exp(eluv) = Î±l uv , because (19) Ëœel uv = (al)âŠ¤ Â· Ï•2(Wl shlâˆ’1 u + Wl t hlâˆ’1 v ) =el uv, (20) which follows from the positive homogeneity of Ï•2 that allows Ëœel uv = Î»âˆ’1al[i]Ï•2( nlâˆ’1X j Î»Wl s[i, j]hlâˆ’1 u [j] +Î»Wl t [i, j]hlâˆ’1 v [j] (21) + nlX iâ€²Ì¸=i al[i]Ï•2( nlâˆ’1X j Wl s[i, j]hlâˆ’1 u [j] +Wl t [i, j]hlâˆ’1 v [j]) (22) = Î»âˆ’1Î»al[i]Ï•2( nlâˆ’1X j Wl s[i, j]hlâˆ’1 u [j] +Wl t [i, j]hlâˆ’1 v [j] (23) + nlX iâ€²Ì¸=i al[i]Ï•2( nlâˆ’1X j Wl s[i, j]hlâˆ’1 u [j] +Wl t [i, j]hlâˆ’1 v [j]) (24) = el uv. (25) Since ËœÎ±l uv = Î±l uv, it follows that Ëœhu l [i] =Ï•1 ï£« ï£­ X zâˆˆN(u) Î±l zu nlâˆ’1X j Î»Wl s[i, j]hlâˆ’1 z [j] ï£¶ ï£¸ = Î»Ï•1 ï£« ï£­ X zâˆˆN(u) Î±l zu nlâˆ’1X j Wl s[i, j]hlâˆ’1 z [j] ï£¶ ï£¸ = Î»hl u[i] 17In the next layer, we therefore have Ëœhv l+1 [k] =Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Î»âˆ’1Wl+1 s [k, i]Ëœhl u[i] ï£¶ ï£¸ = Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Î»âˆ’1Wl+1 s [k, i]Î»hl u[i] ï£¶ ï£¸ = Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Wl+1 s [k, i]hl u[i] ï£¶ ï£¸ = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k âˆˆ [K] in a GAT layer of a L layer network. Then the gradients of layer l âˆˆ [L âˆ’ 1] respect the law: KX k âŸ¨Wl k[i, :], âˆ‡Wl k[i,:]LâŸ© âˆ’ KX k âŸ¨al k[i], âˆ‡al k[i]LâŸ© = KX k âŸ¨Wl+1 k [:, i], âˆ‡Wl+1 k [:,i]LâŸ©. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =âˆ¥K k Ï•(P uâˆˆN(v) Î±kuv l Â· Wl khlâˆ’1 u ) , or Average: hl v = 1 K PK k Ï•(P uâˆˆN(v) Î±kuv l Â· Wl khlâˆ’1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to Î³ = 0.1, Î³ = 0.05 and Î³ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, Î³ = 0.005 and Î³ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(wâˆ—âˆ’w0)/wâˆ—| where w0 is the initialized value and wâˆ— is the value for the model with maximum validation accuracy during training. Absolute change |aâˆ— âˆ’ a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as âˆ¥âˆ‡WlWlâˆ¥F/âˆ¥Wlâˆ¥F and âˆ¥âˆ‡alalâˆ¥/âˆ¥alâˆ¥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w âˆˆ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w âˆˆ Wl and all a âˆˆ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) Â± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 Â± 3.02 76 .96 Â± 2.21 79.48 Â± 0.43 10 25 .48 Â± 18.13 77 .72 Â± 1.49 79.46 Â± 1.34 attention heads = 8 5 73 .56 Â± 2.71 77 .44 Â± 1.54 79.58 Â± 0.53 10 25 .50 Â± 18.18 77 .02 Â± 2.76 79.06 Â± 0.73 activation = ELU 5 75 .68 Â± 1.80 79 .20 Â± 1.07 79.64 Â± 0.36 10 73 .02 Â± 2.27 78.64 Â± 1.72 47.76 Â± 7.39 dropout = 0.6 5 42 .14 Â± 15.97 79 .18 Â± 1.17 81.00 Â± 0.62 10 24 .90 Â± 9.50 30 .94 Â± 1.04 44.40 Â± 1.84 weight decay = 0.0005 5 67 .26 Â± 6.30 77 .36 Â± 1.74 79.56 Â± 0.48 10 18 .78 Â± 11.96 76 .56 Â± 2.91 79.40 Â± 1.15 weight sharing = False 5 70 .80 Â± 7.00 77 .28 Â± 1.45 79.82 Â± 0.63 10 19 .54 Â± 14.03 76 .04 Â± 1.77 79.06 Â± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) Â± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 Â± 1.61 79 .38 Â± 2.24 80.20 Â± 0.57 10 70 .86 Â± 3.99 75 .72 Â± 2.35 79.62 Â± 1.27 attention heads = 8 5 75 .62 Â± 1.74 78 .54 Â± 1.06 79.56 Â± 0.85 10 70 .94 Â± 2.76 75 .48 Â± 2.48 79.74 Â± 1.10 activation = ELU 5 76 .56 Â± 1.72 78 .72 Â± 1.02 79.56 Â± 0.64 10 75 .30 Â± 1.42 78.48 Â± 1.79 76.52 Â± 0.97 dropout = 0.6 5 76 .74 Â± 1.44 78 .42 Â± 1.28 79.92 Â± 0.48 10 32 .48 Â± 6.99 31 .76 Â± 1.33 76.34 Â± 0.95 weight decay = 0.0005 5 75 .10 Â± 2.05 78 .52 Â± 1.41 79.80 Â± 0.63 10 28 .74 Â± 12.04 74 .68 Â± 3.06 79.70 Â± 1.14 weight sharing = False 5 76 .56 Â± 1.72 78 .72 Â± 1.02 79.56 Â± 0.64 10 71 .12 Â± 2.23 73 .32 Â± 1.23 79.46 Â± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) Â± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 Â± 0.52 84.58 Â± 0.65 84.55 Â± 0.47 Pubmed 78.38 Â± 0.77 78 .52 Â± 0.54 78.56 Â± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 Â± 0.9 80 .5 Â± 0.5 78 .0 Â± 0.3 80.9 Â± 0.4 5 73 .2 Â± 3.4 78 .3 Â± 0.8 80.3 Â± 0.8 79.6 Â± 1.0 10 24 .1 Â± 4.5 77 .6 Â± 2.0 80 .0 Â± 1.1 80.0 Â± 0.9 20 14 .4 Â± 11.2 62 .8 Â± 3.6 78 .7 Â± 0.5 78.8 Â± 1.5 40 13 .4 Â± 0.9 65 .9 Â± 7.3 28 .3 Â± 16.2 77.1 Â± 0.9 64 9 .8 Â± 5.4 33 .0 Â± 13.4 27 .3 Â± 12.7 76.7 Â± 1.3 80 12 .4 Â± 19.3 33 .8 Â± 12.9 38 .9 Â± 21.3 77.1 Â± 1.3 Citeseer 2 66 .6 Â± 20.0 71 .3 Â± 1.8 66 .0 Â± 3.2 72.3 Â± 0.9 5 60 .9 Â± 12.3 66 .9 Â± 15.0 69 .0 Â± 6.4 70.1 Â± 1.8 10 23 .8 Â± 36.8 66 .0 Â± 5.9 70.6 Â± 0.9 69.8 Â± 10.9 20 16 .4 Â± 18.2 47 .9 Â± 10.0 67 .0 Â± 8.6 69.7 Â± 4.5 40 13 .9 Â± 56.8 37 .3 Â± 92.8 44 .8 Â± 6.8 64.7 Â± 13.6 64 13 .8 Â± 41.4 29 .5 Â± 15.0 37 .3 Â± 79.6 66.3 Â± 0.5 80 12 .4 Â± 42.7 25 .8 Â± 3.6 30 .1 Â± 21.8 64.1 Â± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, Ï‰GAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of Ï‰GAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: Ï‰GAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 Ã— 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l âˆˆ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      }
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors",
      "abstract": "Graph Attention Networks (GATs) are designed to provide flexible neighborhood\naggregation that assigns weights to neighbors according to their importance. In\npractice, however, GATs are often unable to switch off task-irrelevant\nneighborhood aggregation, as we show experimentally and analytically. To\naddress this challenge, we propose GATE, a GAT extension that holds three major\nadvantages: i) It alleviates over-smoothing by addressing its root cause of\nunnecessary neighborhood aggregation. ii) Similarly to perceptrons, it benefits\nfrom higher depth as it can still utilize additional layers for (non-)linear\nfeature transformations in case of (nearly) switched-off neighborhood\naggregation. iii) By down-weighting connections to unrelated neighbors, it\noften outperforms GATs on real-world heterophilic datasets. To further validate\nour claims, we construct a synthetic test bed to analyze a model's ability to\nutilize the appropriate amount of neighborhood aggregation, which could be of\nindependent interest.",
      "full_text": "GATE: How to Keep Out Intrusive Neighbors Nimrah Mustafa 1 Rebekka Burkholz 1 Abstract Graph Attention Networks (GATs) are designed to provide flexible neighborhood aggregation that assigns weights to neighbors according to their importance. In practice, however, GATs are often unable to switch off task-irrelevant neigh- borhood aggregation, as we show experimen- tally and analytically. To address this challenge, we propose GATE, a GAT extension that holds three major advantages: i) It alleviates over- smoothing by addressing its root cause of un- necessary neighborhood aggregation. ii) Simi- larly to perceptrons, it benefits from higher depth as it can still utilize additional layers for (non- )linear feature transformations in case of (nearly) switched-off neighborhood aggregation. iii) By down-weighting connections to unrelated neigh- bors, it often outperforms GATs on real-world heterophilic datasets. To further validate our claims, we construct a synthetic test bed to an- alyze a modelâ€™s ability to utilize the appropri- ate amount of neighborhood aggregation, which could be of independent interest. 1. Introduction Graph neural networks (GNNs) (Gori et al., 2005) are a standard class of models for machine learning on graph- structured data that utilize node feature and graph struc- ture information jointly to achieve strong empirical per- formance, particularly on node classification tasks. In- put graphs to GNNs stem from various domains of real- world systems such as social (Bian et al., 2020), com- mercial (Zhang & Chen, 2020), academic (Hamaguchi et al., 2017), economic (Monken et al., 2021), biochem- ical(Kearnes et al., 2016), physical (Shlomi et al., 2021), and transport (Wu et al., 2019) networks that are diverse in their node feature and graph structure properties. 1CISPA Helmholtz Center for Information Security, 66123 SaarbrÂ¨ucken, Germany. Correspondence to: Nimrah Mustafa <nimrah.mustafa@cispa.de>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). The message-passing mechanism of GNNs (Kipf & Welling, 2017; Xu et al., 2019) involves two key steps: a transformation of the node features, and the aggregation of these transformed features from a nodeâ€™s neighborhood to update the nodeâ€™s representation during training. While this has proven to be largely successful in certain cases, it gen- erally introduces some problems for learning with GNNs, the most notorious of which is over-smoothing (Li et al., 2018). The enforced use of structural information in ad- dition to node features may be detrimental to learning the node classification task, as shown by recent results where state-of-the-art GNNs perform the same as or worse than multi-layer perceptrons (MLPs) (Gomes et al., 2022; Yan et al., 2022; Ma et al., 2022). One such task is where node labels can be easily determined by informative node fea- tures and require no contribution from the neighborhood. Here, standard neighborhood aggregation, as in most GNN architectures, would impair model performance, particu- larly with an increase in model depth. A popular standard GNN architecture that, in principle, tries to resolve this problem is the Graph Attention Net- work (GAT) (VeliË‡ckoviÂ´c et al., 2018; Brody et al., 2022). By design, neighborhood aggregation in GATs is charac- terized by learnable coefficients intended to assign larger weights to more important neighboring nodes (including the node itself) in order to learn better node representations. Therefore, in the above example, GATs should ideally re- sort to assigning near-zero importance to neighbor nodes, effectively switching off neighborhood aggregation. How- ever, we find that, counter-intuitively, GATs are unable to do this in practice and continue to aggregate the uninforma- tive features in the neighborhood which impedes learning. One may ask why one would employ a GAT (or any GNN architecture) if an MLP suffices. In practice, we do not know whether neighborhood aggregation (of raw features or features transformed by a perceptron or MLP), would be beneficial or not beforehand. This raises a pertinent ques- tion for the GNN research community: How much neigh- borhood aggregation is needed for a given task? . Ideally, it is what we would want a model to learn. Otherwise, the right task-specific architecture would need to be identified by time and resource-intensive manual tuning. We address the challenge faced by GAT to effectively de- 1 arXiv:2406.00418v2  [cs.LG]  29 Jul 2024GATE: How to Keep Out Intrusive Neighbors termine how well a node is represented by its own features compared to the features of neighboring nodes, i.e., dis- tinguish between the relative importance of available node features and graph structure information for a given task. Firstly, we provide an intuitive explanation for the problem based on a conservation law of GAT gradient flow dynam- ics derived by Mustafa & Burkholz (2023). Building on this insight, we present GATE, an extension of the GAT architecture that can switch neighborhood aggregation on and off as necessary. This allows our proposed architecture to gain the following advantages over GAT: 1. It alleviates the notorious over-smoothing problem by addressing the root cause of unnecessarily repeated neighborhood aggregation. 2. It allows the model to benefit from more meaningful representations obtained solely by deeper non-linear transformations, similarly to perceptrons, in layers with little to no neighborhood aggregation. 3. It often outperforms GATs on real-world heterophilic datasets by weighing down unrelated neighbors. 4. It offers interpretable learned self-attention coeffi- cients, at the node level, that are indicative of the rel- ative importance of feature and structure information in the locality of the node. In order to validate these claims, we construct a syn- thetic test bed of two opposite types of learning problems for node classification where label-relevant information is completely present only in a nodeâ€™s i) own features and ii) neighboring nodesâ€™ features (see Fig. 2). GATE is able to adapt to both cases as necessary. On real-world datasets, GATE performs competitively on homophilic datasets and is substantially better than GAT on heterophilic datasets. Furthermore, up to our knowledge, it achieves a new state of the art on the relatively large OGB-arxiv dataset (Hu et al., 2021) (i.e., 79.57 Â± 0.84% test accuracy). In sum- mary, our contributions are as follows: â€¢ We identify and experimentally demonstrate a struc- tural limitation of GAT, i.e., its inability to switch off neighborhood aggregation. â€¢ We propose GATE, an extension of GAT, that over- comes this limitation and, in doing so, unlocks several benefits of the architecture. â€¢ We update an existing conservation law relating the structure of gradients in GAT to GATE. â€¢ We construct a synthetic test bed to validate our claims, which could be of independent interest to mea- sure progress in developing adaptive neighborhood aggregation schemes. Figure 1: MLP only performs node feature transforma- tions, whereas GAT also always aggregates over the neigh- borhood. With the ability to switch off neighborhood ag- gregation, GATE can learn to emulate MLP behavior and potentially interleave effective perceptron and standard lay- ers in a flexible manner. This allows for more expressive power that we find to benefit real-world tasks (see Table 3). 2. Related Work To relieve GNNs from the drawbacks of unnecessarily re- peated neighborhood aggregation in deeper models, ini- tial techniques were inspired by classical deep learning of MLPs such as normalization (Cai et al., 2021; Zhao & Akoglu, 2020; Zhou et al., 2020; 2021) and regularization (Papp et al., 2021; Rong et al., 2020; Yang et al., 202; Zou et al., 2019). More recently, the need for deeper models and architectural changes to limit neighborhood aggregation as necessary has been recognized. Some approaches use linear combi- nations of initial features and current layer representation (Gasteiger et al., 2019), add skip connections and identity mapping (Chen et al., 2020; Cong et al., 2021), combine representations of all previous layers at the last layer (Xu et al., 2018), aggregate information from a node-wise de- fined range of k-hop neighbors(Liu et al., 2020), and limit the number of aggregation iterations based on node influ- ence scores (Zhang et al., 2021). However, these architec- tures are not flexible enough to utilize additional network layers to simulate perceptron behavior, which, as we find, benefits heterophilic tasks. (Ma et al., 2023) discuss â€˜goodâ€™ and â€˜badâ€™ heterophily, which are also task-dependent. Other contemporary works for general GNNs propose the use of bilevel optimization to determine a nodeâ€™s strategic discrete action to a received message (Finkelshtein et al., 2023) and a variational inference framework for adaptive message passing (Errica et al., 2023). While these non- attentive architectures improve message passing in GNNs, we focus on identifying and explaining a structural limita- tion of self-attention in GAT, that continues to be used as a strong baseline architecture. An orthogonal line of research uses graph structural learn- ing (Yang et al., 2019; Stretcu et al., 2019; Franceschi et al., 2020) to amend the input graph structure such that neighborhood aggregation benefits the given task. Such 2GATE: How to Keep Out Intrusive Neighbors approaches are difficult to scale, more susceptible to over- smoothing, and potentially destroy any inherent informa- tion in the original graph structure. On the contrary, a standard GNN architecture equipped to selectively per- form neighborhood aggregation avoids these pitfalls. Self- supervision of the attention mechanism has also been pro- posed (Wang et al., 2019; Kim & Oh, 2021). Methods such as graph rewiring (Deac et al., 2022) to overcome problems such as over-squashing (Alon & Yahav, 2021) are comple- mentary and may also be combined with GATE. While we focus our insights on GAT, architectures based on GAT such as Ï‰GAT (Eliasof et al., 2023) also suffer from the same problem (see Fig. 14 in Appendix C). This further confirms that the universal problem with GAT has been correctly identified. In general, recent works direct ef- fort to understand the current limitations of graph attention (Lee et al., 2023; Fountoulakis et al., 2023). 3. Architecture Notation Consider a graph G = ( V, E) with node set V and edge set E âŠ† V Ã— V, where for a node v âˆˆ V the neighborhood is N(v) = {u|(u, v) âˆˆ E} and input features are h0 v. A GNN layer updates each nodeâ€™s repre- sentation by aggregating over its neighborsâ€™ representation and combining it with its own features. The aggregation and combination steps can be performed together by in- troducing self-loops in G such that, âˆ€v âˆˆ V, (v, v) âˆˆ E. We assume the presence of self-loops in G unless spec- ified otherwise. In GATs, this aggregation is weighted by parameterized attention coefficients Î±uv, which indi- cate the importance of node u for v. A network is con- structed by stacking L layers, defined as follows, using a non-linear activation function Ï• that is homogeneous (i.e Ï•(x) = xÏ•â€²(x)) and consequently, Ï•(ax) = aÏ•(x) for positive scalars a) such as ReLU Ï•(x) = max {x, 0} or LeakyReLU Ï•(x) = max{x, 0} + âˆ’Î± max{âˆ’x, 0}. GAT Given input representations hlâˆ’1 v for v âˆˆ V, a GAT 1 layer l âˆˆ [L] transforms those to: hl v = Ï• ï£« ï£­ X uâˆˆN(v) Î±l uv Â· Wl shlâˆ’1 u ï£¶ ï£¸, where (1) Î±l uv = exp \u0000 el uv \u0001 P uâ€²âˆˆN(v) exp \u0000 el uâ€²v \u0001, and (2) el uv = \u0000 al\u0001âŠ¤ Â· Ï• \u0000 Wl shlâˆ’1 u + Wl thlâˆ’1 v \u0001 (3) The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that Ws = Wt. We denote this variant of GAT by GATS. 1Throughout, we refer to GATv2 (Brody et al., 2022) as GAT. GATE In addition, we propose GATE, a GAT variant that flexibly weights the importance of node features and neigh- borhood features. A GATE layer is also defined by Eq. (1) and (2) but modifies euv in Eq. (3) to Eq. (4). el uv = \u0000 1uÌ¸=val s + 1u=val t \u0001âŠ¤ Â·Ï• \u0000 Ulhlâˆ’1 u + Vlhlâˆ’1 v \u0001 . (4) We denote euv in Eq. (3) and (4) as el vv if u = v. For GATE, Wl s in Eq. (1) is denoted as Wl. A weight- sharing variant of GATE, GATE S, is characterized by all feature transformation parameters being shared in a layer (i.e. Wl = Ul = Vl). Note that, then, for a d-dimensional layer, GATE adds onlyd more parameters to GAT. We next present theoretical insights into the reasoning be- hind the inability of GATs to switch off neighborhood ag- gregation, which is rooted in norm constraints imposed by the inherent conservation law for GATs. The gradients of GATE fulfill an updated conservation law (Thoerem 4.3) that enables switching off neighborhood aggregation in a parameter regime with well-trainable attention. 4. Theoretical Insights For simplicity, we limit our discussion here to GATs with weight sharing. We derive similar arguments for GATs without weight sharing in Appendix A.1. The following conservation law was recently derived for GATs to explain trainability issues of standard initialization schemes. Even with improved initializations, we argue that this law lim- its the effective expressiveness of GATs and their ability to switch off neighborhood aggregation when necessary. Theorem 4.1 (Thm. 2.2 by Mustafa & Burkholz (2023)) . The parameters Î¸ of a layer l âˆˆ [L âˆ’ 1] in a GAT network and their gradients âˆ‡Î¸L w.r.t. loss L fulfill: âŸ¨Wl [i,:], âˆ‡Wl [i,:] âŸ© = âŸ¨Wl+1 [:,i] , âˆ‡Wl+1 [:,i] âŸ© + âŸ¨al [i], âˆ‡al [i] âŸ©. (5) Intuitively, this equality limits the budget for the relative change of parameters and imposes indirectly a norm con- straint on the parameters. Under gradient flow that assumes infinitesimally small learning rates, this law implies that the relationship \r\rWl[i, :] \r\r2 âˆ’ \r\ral[i] \r\r2 âˆ’ \r\rWl+1[: i] \r\r2 = c stays constant during training, where c is defined by the initial norms. Other gradient-based optimizers fulfill this norm balance also approximately. Note that the norms\r\rWl[i, :] \r\r generally do not assume arbitrary values but are determined by the required scale of the output. Deeper models are especially less flexible in varying these norms as deviations could lead to exploding or diminishing out- puts and/or gradients. In consequence, the norms of the attention parameters are also bounded. Furthermore, a pa- rameter becomes harder to change during training when its 3GATE: How to Keep Out Intrusive Neighbors magnitude increases. This can be seen by transforming the law with respect to the relative change of a parameter de- fined as âˆ†Î¸ = âˆ‡Î¸L/Î¸ for Î¸ Ì¸= 0 or âˆ†Î¸ = 0 for Î¸ = 0. nl+1X j=1 Wl ij 2 âˆ†Wl ij = nl+2X k=1 Wl+1 ki 2 âˆ†Wl+1 ki + al i 2 âˆ†al i. (6) The higher the magnitude of an attention parameter (al i)2, the smaller will be the relative change âˆ†al i and vice versa. This restricts the attention mechanism in the network to a less-trainable regime without converging to a meaningful model. We next explain why large \r\ral\r\r are required to switch off neighborhood aggregation in a layer. Insight 4.2 (Effective expressiveness of GATs). GATs are challenged to switch off neighborhood aggregation during training, as this requires the model to enter a less trainable regime with large attention parameters âˆ¥aâˆ¥2 >> 1. An intuitive derivation of this insight is presented in the appendix. Here, we outline the main argument based on the observation that to make the contribution of neighbor j insignificant relative to node i, we require Î±ij/Î±ii << 1. We use relativeÎ±ij/Î±ii instead of Î±ij and Î±ii to cancel out normalization constants and simplify the analysis. Our key observation is that, given an insignificant link (i, j), its relative contribution to its two neighborhoods Î±ij/Î±ii << 1 and Î±ji/Î±jj << 1 are affected in oppo- site ways by a feature f of the attention parameters a, i.e. if a[f] contributes to reducing Î±ij/Î±ii, it automatically in- creases Î±ji/Î±jj . However, we require multiple features that contribute to reducing only Î±ij without strengthen- ing Î±ji that may only be possible in a high dimensional space requiring large norms of a. Yet, the norms âˆ¥aâˆ¥2 are constrained by the parameter initialization and cannot in- crease arbitrarily due to the derived conservation law. Note that to switch off all neighborhood aggregation, we require Î±ij/Î±ii << 1, âˆ€ j âˆˆ N(i), further complicating the task. To address this challenge, we modify the GAT architecture by GATE that learns separate attention parameters for the node and the neighborhood contribution. As its conserva- tion law indicates, it can switch off neighborhood aggrega- tion in the well-trainable parameter regime. Theorem 4.3 (Structure of GATE gradients). The param- eters and gradients of a GATE network w.r.t. to loss L for layer l âˆˆ [L âˆ’ 1] are conserved according to the following laws. Given Î˜(Î¸) = âŸ¨Î¸, âˆ‡Î¸LâŸ©, it holds that: Î˜(Wl [i,:]) âˆ’ Î˜(al+1 s [i]) âˆ’ Î˜(al+1 t [i]) = Î˜(Wl+1 [:,i] ). (7) and, if additional independent matrices Ul and Vl are trainable, it also holds that: Î˜(al s[i]) + Î˜(al t[i]) = Î˜(Ul [i,:]) + Î˜(Vl [i,:]). (8) The proof is provided in the appendix. We utilize this the- orem for two purposes. Firstly, it induces an initialization that enables at least the initial trainability of the network. Similarly to GAT (Mustafa & Burkholz, 2023), we initial- ize all a parameters with zero and matrices W with ran- dom orthogonal looks-linear structure in GATE. This also ensures that we have no initial inductive bias or preference for specific neighbor or node features. As an ablation, we also verify that the initialization of the attention parameters in GAT with zero alone can not switch off neighborhood aggregation in GAT (see Fig. 7 in Appendix C). Secondly, the conservation law leads to the insight that a GATE network is more easily capable of switching off neighborhood aggregation or node feature contributions in comparison with GAT. Insight 4.4 (GATE is able to switch off neighborhood ag- gregation.). GATE can flexibly switch off neighborhood aggregation or node features in the well-trainable regime of the attention parameters. This insight follows immediately from the related conser- vation law for GATE that shows that at and as can inter- change the available budget for relative change among each other. Furthermore, the contribution of neighbors and the nodes are controlled separately. We show how the respec- tive switch-off can be achieved with relatively small atten- tion parameter norms that correspond to the well-trainable regime in Appendix A.3. To verify these insights in exper- iments, we next design synthetic data generators that can test the ability of GNNs to take structural infromation into account in a task-appropriate manner. 5. Experiments We validate the ability of GATE to perform the appropri- ate amount of neighborhood aggregation, as relevant for the given task and input graph, on both synthetic and real- world graphs. In order to gauge the amount of neighbor- hood aggregation, we study the distribution of Î±vv values (over the nodes) at various epochs during training and lay- ers in the network. This serves as a fair proxy since âˆ€ v âˆˆ V, Î±vv = 1 âˆ’ P uâˆˆN(v),uÌ¸=v Î±uv. Thus, Î±vv = 1 implies no neighborhood aggregation (i.e. only hv is used) whereas Î±vv = 0 implies only neighborhood aggregation (i.e. hv is not used). Figure 2 shows an exemplary con- struction of both these cases. We defer a discussion of the experimental setup to Appendix B. 5.1. Synthetic Test Bed We construct the synthetic test bed as a node classification task for two types of problems: self-sufficient learning and neighbor-dependent learning. In the self-sufficient learning problem, complete label-relevant information is present in 4GATE: How to Keep Out Intrusive Neighbors (a) No neighborhood contribution required. (b) Only neighborhood contribution required. Figure 2: Examples of synthetic input graphs constructed for learning tasks that are (a) self-sufficient and can be bet- ter solved by switching off neighborhood aggregation, i.e. Î±vv = 1 and (b) neighbor-dependent that benefit from ig- noring the nodeâ€™s own features, i.e.Î±vv = 0. In both cases, âˆ€ v âˆˆ V, P uâˆˆN(v),uÌ¸=v Î±uv + Î±vv = 1. These represent opposite ends of the spectrum whereas real-world tasks of- ten lie in between and require Î±ii âˆˆ [0, 1]. GATEâ€™s atten- tion mechanism is more flexible than GATâ€™s in learning the level of neighborhood aggregation required for a task. a nodeâ€™s own features. On the contrary, in the neighbor- dependent learning problem, label-relevant information is present in the node features of the k-hop neighbors. We discuss both cases in detail, beginning with the simpler one. Learning self-sufficient node labels In order to model this task exactly, we generate an Erd Ëosâ€“RÂ´enyi (ER) graph structure G with N = 1000 nodes and edge probabil- ity p = 0 .01. Node labels yv are assigned uniformly at random from C = [2 , 8] classes. Input node features h0 v are generated as one-hot encoded node labels in both cases, i.e., h0 v = 1yv . Nodes are divided randomly into train/validation/test split with a 2 : 1 : 1 ratio. We also use a real-world graph structure of the Cora dataset. Two cases using this graph structure are tested: i) using the original node labels consisting of7 classes, and ii) randomized labels of 7 classes. Input node features are generated as one-hot encoding of node labels in both cases. The standard splits of Cora are used. Table 1: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of net- work layers, respectively. Original (Orig.) and randomized (Rand.) labels are used for the Cora structure. In all cases, 100% train accuracy is achieved except in ones marked with âˆ— and GATE eventually achieves 100% test accuracy a few epochs later except in one marked with â€¡. GAT S and GAT models marked with â€  also eventually achieve 100% test accuracy. GATE S behaves similarly to GATE and achieves 100% train and test accuracy. . S C L Test Acc.@Epoch of Min. Train Loss GATS GAT GATE Cora O,7 1 99.1@215â€  97.7@166â€  99.0@127 2 93.4@218 94.5@158 99.6@35 5 85.9@92 85.5@72 98.4@36â€¡ R,7 1 99.4@263 â€  99.8@268â€  100@104 2 61.7@2088 âˆ— 52.8@341âˆ— 99.9@36 5 35.1@609 32.1@1299 99.9@23 ER(p = 0.01) R,2 1 100@341 â€  100@182â€  100@1313 2 99.2@100 â€  99.2@119â€  99.6@79 5 64.0@7778 âˆ— 99.6@239 100@45 R,8 1 88.8@9578 âˆ— 98.4@3290 99.2@1755 2 90.4@2459 âˆ— 94.8@2237 99.6@44 5 23.6@8152 26.0@8121 100@28 As evident in Table 1, GAT is unable to perfectly learn this task whereas GATE easily achieves100% train and test ac- curacy, and often in fewer training epochs. In line with the homophilic nature of Cora, GAT achieves reasonably good accuracy when the original labels of the Cora graph structure are used as neighborhood aggregation is relatively less detrimental, particularly in a single-layer model. Nevertheless, in the same case, GATE generalizes better than GAT with an increase in model depth. This in- dicates that over-smoothing, a major cause of performance degradation with model depth in GNNs, is also alleviated due to reduced neighborhood aggregations (see Fig. 3). On the contrary, random labels pose a real challenge to GAT. Since the neighborhood features are fully uninforma- tive about a nodeâ€™s label in the randomized case, aggrega- tion over such a neighborhood distorts the fully informa- tive features of the node itself. This impedes the GAT net- work from learning the task, as it is unable to effectively switch off aggregation (see Fig. 3), whereas GATE is able to adapt to the required level of neighborhood aggregation (i.e. none, in this case). Interestingly, note that a single 5GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 3: Distribution of Î±vv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 1 and 2 layer models. Due to space limitation, we defer the plots of 5 layer networks to Fig. 8 in Appendix C. layer GAT in the case of random labels can almost, though not completely, switch off neighborhood aggregation (see Fig. 3) and achieve (near) perfect accuracy in the simpler cases. This is in line with our theoretical analysis (see In- sight 4.2), as the norms of a single-layer model are not con- strained and thus the attention parameters have more free- dom to change. Overall, the accuracy of GAT worsens drastically along two dimensions simultaneously: i) an increase in the depth of the model (due to increased unnecessary aggregation), and ii) an increase in the complexity of the task (due to an in- crease in the number of classes in an ER graph and conse- quently in node neighborhoods). In the interest of space, we defer results for GATE S to Fig. 9 in Appendix C as aggregation patterns similar to GATE are observed. Having established that GATE excels GAT in avoiding task-irrelevant neighborhood aggregation, it is also impor- tant to verify whether GATE can perform task-relevant neighborhood aggregation when required, and as much as required. We answer this question next by studying the be- havior of GATE, in comparison to GAT, on a synthetically constructed neighbor-dependent learning problem. Learning neighbor-dependent node labels To model this task, we generate an ER graph structure with N = 1000 nodes and edge probability p = 0 .01. Input node features h0 v âˆˆ Rd are sampled from a multivariate normal distribution N(0d, Id). For simplicity, d = 2. This input graph G is fed to a random GAT network Mk with k layers of width d. Note that this input graph G has no self-loops on nodes (i.e. v /âˆˆ N(v)).The parameters of Mk are initialized with the standard Xavier (Glorot & Ben- gio, 2010) initialization. Thus, for each node v, the node embedding output by Mk, hMk v is effectively a function f Table 2: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Mean test accuracy Â±95% confi- dence interval over 5 runs is reported. In all cases, a GATE variant outperforms the GAT variants. We further analyze one experimental run in detail in Table 8 in Appendix C. k L GATS GAT GATE S GATE 1 1 93 .6Â±1.3 92 .3Â±1.3 96.4 Â± 0.7 93.5Â±1.3 2 93 .5Â±0.7 92 .7Â±2.7 97.9 Â± 0.8 94.6Â±2.1 3 88 .2Â±4.9 91 .8Â±3.4 92 .1Â±4.6 94.0 Â± 1.5 2 2 90 .4Â±1.3 87 .7Â±1.6 93.8 Â± 0.5 88.7Â±2.5 3 82 .2Â±4.5 88 .9Â±2.1 85 .8Â±2.5 93.4 Â± 3.3 4 84 .0Â±5.0 83 .0Â±4.8 89.2 Â± 2.3 87.8Â±2.4 3 3 84 .3Â±3.2 83 .8Â±2.7 87 .5Â±1.8 88.6 Â± 2.0 4 71 .4Â±3.9 75 .9Â±7.6 89.2 Â± 1.0 89.0Â±0.5 5 80 .2Â±4.8 83 .9Â±2.2 86 .1Â±0.8 87.8 Â± 1.6 of the k-hop neighboring nodes of node v represented by a random GAT network. Let Nk(v) denote the set of k-hop neighbors of v and v /âˆˆ Nk(v). Finally, we run K-means clustering on the neighborhood aggregated representation of nodes hMk v to divide nodes into C clusters. For simplicity, we set C = 2. This cluster- ing serves as the node labels (i.e. yv = argcâˆˆ[C](v âˆˆ c) for our node classification task. Thus, the label yv of a node v to be learned is highly dependent on the input features of the neighboring nodes h0 u âˆˆ Nk(v) rather than the nodeâ€™s own input features h0 v. The generated input data and the real decision boundary for varying k are shown in Fig. 4. Corresponding results in Table 2 and Fig. 5 exhibit that GATE can better detect the amount of necessary neighborhood aggregation than GAT. However, this task is more challenging than the previous one, and GATE too can not achieve perfect100% test accu- racy. This could be attributed to data points close to the real decision boundary which is not crisply defined (see Fig. 4). 5.2. Real-World Data To demonstrate that the ability of GATE to switch-off neighborhood aggregation has real application relevance, we evaluate GATE on relatively large-scale real-world node classification tasks, namely on five heterophilic benchmark datasets (Platonov et al., 2023) (see Table 3) and three OGB datasets (Hu et al., 2021) (see Table 5). We defer results and discussion on five small-scale datasets with varying homophily levels to Table 7 in Appendix C. To analyze 6GATE: How to Keep Out Intrusive Neighbors (a) k = 1  (b) k = 2  (c) k = 3 Figure 4: (a)-(c): Distribution of node labels of a synthetic dataset, with neighbor-dependent node labels, based on nodesâ€™ own random features (left) and neighborsâ€™ features aggregated k times (right). Figure 5: Distribution of Î±vv against training epoch for the neighbor-dependent learning problem withk = 3. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 3, 4, and 5 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 3 layers of the 4 and 5 layer models perform neighborhood aggregation. the potential benefits of combining MLP and GAT layers in GATE, we compare its behavior with GAT and MLP. We argue that the better performance of GATE, by a large margin in most cases, can be attributed to down-weighting unrelated neighbors, leveraging deeper non-linear feature transformations, and reducing over-smoothing. While we focus our exposition on the neighborhood aggre- gation perspective of GATs, we also consider the FAGCN architecture (Bo et al., 2021), which relies on a similar at- tention mechanism and, in theory, could switch off neigh- borhood aggregation when positive and negative contribu- tions of neighbors cancel out. In contrast to GATE, it re- quires tuning a hyperparameter Ïµ, which controls the con- tribution of raw node features to each layer. Furthermore, on our synthetic tasks, we find that, like GAT, FAGCN is also unable to limit neighborhood contribution. We also provide a detailed qualitative and quantitative discussion comparing GATE and FAGCN using the synthetic testbed in Appendix C. Next, we analyze GATEâ€™s ability to mix MLP and GAT layers. To this end, we evaluate a GNN architecture con- structed by alternately placing GAT and MLP layers in the network that we denote by MLP +GAT on various het- erophilic tasks. The purpose of this experiment is twofold. Firstly, we observe in Table 3 that MLP+GAT outperforms both GAT and MLP in most cases. This highlights the benefit of only performing non-linear transformations on raw or aggregated neighborhood features without immedi- ate further neighborhood aggregation to learn potentially more complex features. Secondly, we find GATE to outper- form MLP+GAT (see Table 3). This illustrates that rigidly embedding MLP layers in a GNN with arbitrary predefined roles is not ideal as the appropriate degree and placement of neighborhood aggregation is unknown a-priori. In con- trast, GATE offers more flexibility to learn intricate com- binations of GNN layers and nonlinear feature transforma- tions that define more adequate models for a given task, as exemplified in Fig. 6. The distributions ofÎ±vv in Fig. 6 across layers in GATE re- veal information about the relative importance of node fea- ture and graph structure at the node level, which allows us to analyze the question to which degree graph information is helpful for a task. For example, in Fig. 6, we observe that the Î±vv values are mostly lower in the minesweeper dataset than the roman-empire dataset. This indicates that aggregation, particularly over the input node features and the final layerâ€™s learned representations, is more beneficial compared to the nodeâ€™s own features for the minesweeper dataset. On the other hand, for roman-empire, the model has a higher preference to utilize features of the node it- self (as most values of Î±vv approach 1) over features of the neighbors. This aligns with the homophily levels, 0.05 and 0.68, of the roman-empire and minesweeper datasets, re- spectively. A similar analysis for datasets Texas and Actor can be found in Fig. 13 in Appendix C. We also observe in Fig. 6 that when neighborhood aggrega- 7GATE: How to Keep Out Intrusive Neighbors Table 3: We report mean test accuracy Â±95% confidence interval for roman-empire and amazon-ratings and AUC-ROC for the other three datasets over the standard 10 splits, following (Platonov et al., 2023). All architectures were run with networks of depth 5 and 10 layers. The better performance for each architecture is shown with the number of network layers used in parentheses. GATE outperforms GAT and other baselines on all datasets, mostly by a significant margin. roman-empire amazon-ratings questions minesweeper tolokers GAT 26.10 Â± 1.25 (5) 45 .58 Â± 0.41 (10) 57 .72 Â± 1.58 (5) 50 .83 Â± 0.41 (5) 63 .57 Â± 1.03 (10) MLP 65.12 Â± 0.25 (5) 43 .26 Â± 0.34 (5) 59 .44 Â± 0.94 (10) 50 .74 Â± 0.56 (5) 62 .67 Â± 1.06 (10) MLP+GAT 70.83 Â± 0.39 (5) 45 .25 Â± 0.17 (10) 59 .12 Â± 1.57 (10) 60 .07 Â± 1.11 (5) 65 .85 Â± 0.64 (10) FAGCN 67.55 Â± 0.81 (5) 42 .85 Â± 0.83 (10) 60 .38 Â± 1.21 (5) 63 .38 Â± 0.91 (10) 60 .89 Â± 1.12 (5) GATE 75.55 Â± 0.30 (5) 45.73 Â± 0.24(10) 62.95 Â± 0.71 (5) 66.14 Â± 1.57 (5) 66.63 Â± 1.15(10) (a) roman-empire, GAT:28.96% test accuracy.  (b) roman-empire, GATE: 75.94% test accuracy. (c) Minesweeper, GAT:50.50% test AUROC.  (d) Minesweeper, GATE:67.57% test AUROC. Figure 6: Distribution of Î±vv against training epoch for a run of 5 layer networks on real-world heterophilic tasks. As ex- pected, GAT is unable to significantly vary neighborhood aggregation whereas GATE exhibits varying aggregation patterns across layers and tasks. These could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods for a given task. We defer the plots of the 10-layer models for tolokers dataset to Fig. 11 in Appendix C. tion takes place, the level of aggregation across all nodes, as indicated by the shape of Î±vv distribution, varies over network layers. This is expected as different nodes need different levels of aggregation depending on where they are situated in the graph topology. For example, peripheral nodes would require more aggregation than central nodes to obtain a similar amount of information. Therefore, as already observed with purposefully constructed synthetic data, GATE offers a more interpretable model than GAT in a real-world setting. While we focus our evaluation in Table 3 on comparison with the most relevant baselines such as attention-based ar- chitectures, we next present a more extensive comparison with 14 other baseline architectures in Table 4. For the re- sults reported in Table 3, we conduct experiments in a sim- ple setting without additional elements that may impact the performance such as skip connections, normalization, etc., to isolate the effect of the architecture and evaluate solely the impact of GATEâ€™s ability to switch off neighborhood aggregation on real-world data. However, for the results in Table 4, we adopt the original codebase of (Platonov et al., 2023), which utilizes such elements to evaluate the performance of baseline GNNs and architectures specifi- cally designed for heterophilic datasets. We evaluate GATE in the same settings optimized for their experiments. For easy comparison, we replicate their results from Table 4 in (Platonov et al., 2023). We observe in Table 4 that while GATE outperforms GAT (and other baselines) significantly, GATE has comparable performance to GAT-sep, a variant of GAT, despite GATE being more parameter efficient by an order of magnitude. More specifically, GAT-sep and GATE introduce d2 and d additional parameters, respectively, in a layer. By corre- spondingly adapting GATE, we find GATE-sep to achieve the best performance in most cases. Therefore, additional techniques generally employed to boost performance are compatible and complementary to GATE. GATEâ€™s ability to benefit from depth in terms of generaliza- tion is demonstrated on OGB datasets (see Table 5). In par- 8GATE: How to Keep Out Intrusive Neighbors Table 4: An extensive comparison of GATE with baseline GNNs using the experimental setup of (Platonov et al., 2023). Accuracy is reported for roman-empire and amazon-ratings, and ROC AUC is reported for the remaining three datasets. roman-empire amazon-ratings minesweeper tolokers questions GATE 89.51 Â± 0.49 52.49 Â± 0.46 92.82 Â± 0.90 84.62 Â± 0.69 78.46 Â± 1.17 GAT 80.87 Â± 0.30 49.09 Â± 0.63 92.01 Â± 0.68 83.70 Â± 0.47 77.43 Â± 1.20 GATE-sep 89.78 Â± 0.54 54.51 Â± 0.38 94.18 Â± 0.43 84.48 Â± 0.57 78.20 Â± 1.00 GAT-sep 88.75 Â± 0.41 52.70 Â± 0.62 93.91 Â± 0.35 83.78 Â± 0.43 76.79 Â± 0.71 GT 86.51 Â± 0.73 51.17 Â± 0.66 91.85 Â± 0.76 83.23 Â± 0.64 77.95 Â± 0.68 GT-sep 87.32 Â± 0.39 52.18 Â± 0.80 92.29 Â± 0.47 82.52 Â± 0.92 78.05 Â± 0.93 GCN 73.69 Â± 0.74 48.70 Â± 0.63 89.75 Â± 0.52 83.64 Â± 0.67 76.09 Â± 1.27 SAGE 85.74 Â± 0.67 53.63 Â± 0.39 93.51 Â± 0.57 82.43 Â± 0.44 76.44 Â± 0.62 H2GCN 60.11 Â± 0.52 36.47 Â± 0.23 89.71 Â± 0.31 73.35 Â± 1.01 63.59 Â± 1.46 CPGNN 63.96 Â± 0.62 39.79 Â± 0.77 52.03 Â± 5.46 73.36 Â± 1.01 65.96 Â± 1.95 GPR-GNN 64.85 Â± 0.27 44.88 Â± 0.34 86.24 Â± 0.61 72.94 Â± 0.97 55.48 Â± 0.91 FSGNN 79.92 Â± 0.56 52.74 Â± 0.83 90.08 Â± 0.70 82.76 Â± 0.61 78.86 Â± 0.92 GloGNN 59.63 Â± 0.69 36.89 Â± 0.14 51.08 Â± 1.23 73.39 Â± 1.17 65.74 Â± 1.19 FAGCN 65.22 Â± 0.56 44.12 Â± 0.30 88.17 Â± 0.73 77.75 Â± 1.05 77.24 Â± 1.26 GBK-GNN 74.57 Â± 0.47 45.98 Â± 0.71 90.85 Â± 0.58 81.01 Â± 0.67 74.47 Â± 0.86 JacobiConv 71.14 Â± 0.42 43.55 Â± 0.48 89.66 Â± 0.40 68.66 Â± 0.65 73.88 Â± 1.16 ResNet 65.88 Â± 0.38 45.90 Â± 0.52 50.89 Â± 1.39 72.95 Â± 1.06 70.34 Â± 0.76 ResNet+SGC 73.90 Â± 0.51 50.66 Â± 0.48 70.88 Â± 0.90 80.70 Â± 0.97 75.81 Â± 0.96 ResNet+adj 52.25 Â± 0.40 51.83 Â± 0.57 50.42 Â± 0.83 78.78 Â± 1.11 75.77 Â± 1.24 Table 5: Mean test accuracy Â±95% confidence interval (and number of network layers). We replicate the results for GAT reported by (Brody et al., 2022). GATE leverages deeper networks to substantially outperform GAT. OGB- GAT GATE arxiv 71.87 Â± 0.16 (3) 79.57 Â± 0.84 (12) products 80.63 Â± 0.46 (3) 86.24 Â± 1.01 (8) mag 32.61 Â± 0.29 (2) 35.29 Â± .36 (5) ticular, GATE improves the SOTA test accuracy (78.03%) on the arxiv dataset achieved by a model using embed- dings learned by a language model instead of raw node features(Duan et al., 2023), as reported on the OGB leader- board. While the better performance of deeper models with limited neighborhood aggregation in certain layers indi- cates reduced over-smoothing, we also verify this insight quantitatively (see Table 9 in Appendix C). Our experimental code is available at https:// github.com/RelationalML/GATE.git. 6. Conclusion We experimentally illustrate a structural limitation of GAT that disables the architecture, in practice, to switch off task- irrelevant neighborhood aggregation. This obstructs GAT from achieving its intended potential. Based on insights from an existing conservation law of gradient flow dynam- ics in GAT, we have explained the source of this problem. To verify that we have identified the correct issue, we re- solve it with a modification of GAT, which we call GATE, and derive the corresponding modified conservation law. GATE holds multiple advantages over GAT, as it can lever- age the benefits of depth as in MLPs, offer interpretable, learned self-attention coefficients, and adapt the model to the necessary degree of neighborhood aggregation for a given task. We verify this on multiple synthetic and real- world tasks, where GATE significantly outperforms GAT and also achieves a new SOTA test accuracy on the OGB- arxiv dataset. Therefore, we argue that GATE is a suitable candidate to answer highly debated questions related to the importance of a given graph structure for standard tasks. 9GATE: How to Keep Out Intrusive Neighbors Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. Bian, T., Xiao, X., Xu, T., Zhao, P., Huang, W., Rong, Y ., and Huang, J. Rumor detection on social media with bi- directional graph convolutional networks. In AAAI Con- ference on Artificial Intelligence, 2020. Bo, D., Wang, X., Shi, C., and Shen, H. Beyond low- frequency information in graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2021. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. Burkholz, R. and Dubatovka, A. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. Cai, C. and Wang, Y . A note on over-smoothing for graph neural networks. In Graph Representation Learning Workshop, International Conference on Machine Learn- ing, 2020. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-Y ., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Simple and deep graph convolutional networks. InInternational Conference on Machine Learning, 2020. Cong, W., Ramezani, M., and Mahdavi, M. On provable benefits of depth in training graph convolutional net- works. In Advances in Neural Information Processing Systems, 2021. Deac, A., Lackenby, M., and VeliË‡ckoviÂ´c, P. Expander graph propagation. In Learning on Graphs Conference, 2022. Duan, K., Liu, Q., Chua, T.-S., Yan, S., Ooi, W. T., Xie, Q., and He, J. Simteg: A frustratingly simple approach improves textual graph learning, 2023. Eliasof, M., Ruthotto, L., and Treister, E. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. Errica, F., Christiansen, H., Zaverkin, V ., Maruyama, T., Niepert, M., and Alesiani, F. Adaptive message passing: A general framework to mitigate oversmoothing, over- squashing, and underreaching, 2023. Finkelshtein, B., Huang, X., Bronstein, M., and Ë™Ismail Ë™Ilkan Ceylan. Cooperative graph neural networks, 2023. Fountoulakis, K., Levi, A., Yang, S., Baranwal, A., and Jagannath, A. Graph attention retrospective. In Journal of Machine Learning Research, 2023. Franceschi, L., Niepert, M., Pontil, M., and He, X. Learn- ing discrete structures for graph neural networks. In In- ternational Conference on Machine Learning, 2020. Gasteiger, J., Bojchevski, A., and G Â¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2019. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Interna- tional Conference on Artificial Intelligence and Statis- tics, volume 9, pp. 249â€“256, May 2010. Gomes, D., Ruelens, F., Efthymiadis, K., Nowe, A., and Vrancx, P. When are graph neural networks better than structure agnostic methods? In Neural Information Pro- cessing Systems Workshop ICBINB, 2022. Gori, M., Monfardini, G., and Scarselli, F. A new model for learnig in graph domains. InIEEE International Joint Conference on Neural Networks, 2005. Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y . Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach. In International Joint Conference on Artificial Intelligence, 2017. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs, 2021. 10GATE: How to Keep Out Intrusive Neighbors Kearnes, S., McCloskey, K., Berndl, M., Pande, V ., and Riley, P. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. Kim, D. and Oh, A. How to find your friendly neighbor- hood: Graph attention design with self-supervision. In International Conference on Learning Representations , 2021. Kipf, T. N. and Welling, M. Semi-supervised classifica- tion with graph convolutional networks. InInternational Conference on Learning Representations, 2017. Lee, S. Y ., Bu, F., Yoo, J., and Shin, K. Towards deep attention in graph neural networks: Problems and reme- dies. In International Conference on Machine Learning, 2023. Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. Liu, M., Gao, H., and Ji, S. Towards deeper graph neu- ral networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2022. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2023. Monken, A., Haberkorn, F., Gopinatha, M., Freeman, L., and Batarseh, F. A. Graph neural networks for modeling causality in international trade. In AAAI Conference on Artificial Intelligence, 2021. Mustafa, N. and Burkholz, R. Are GATS out of balance? In Advances in Neural Information Processing Systems, 2023. Papp, P. A., Martinkus, K., Faber, L., and Wattenhofer, R. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. In Advances in Neural Infor- mation Processing Systems, 2021. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B. Geom-gcn: Geometric graph convolutional networks. In International Conference on Learning Representations , 2020. Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evalua- tion of gnns under heterophily: are we really making progress? In International Conference on Learning Rep- resentations, 2023. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: To- wards deep graph convolutional networks on node clas- sification. In International Conference on Learning Rep- resentations, 2020. Shlomi, J., Battaglia, P., and Vlimant, J.-R. Graph neural networks in particle physics. 2021. Stretcu, O., Viswanathan, K., Movshovitz-Attias, D., Pla- tanios, E., Ravi, S., and Tomkins, A. Graph agreement models for semi-supervised learning. In Advances in Neural Information Processing Systems, 2019. VeliË‡ckoviÂ´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations , 2018. Wang, G., Ying, R., Huang, J., and Leskovec, J. Improving graph attention networks with large margin-based con- straints. In Graph Representation Learning Workshop, Neural Information Processing Systems, 2019. Wu, Z., Pan, S., Long, G., Jiang, J., and Zhang, C. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Confer- ence on Machine Learning, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. Yan, Y ., Hashemi, M., Swersky, K., Yang, Y ., and Koutra, D. Two sides of the same coin: Heterophily and over- smoothing in graph convolutional neural networks. In IEEE International Conference on Data Mining, 2022. Yang, H., Ma, K., and Cheng, J. Rethinking graph regular- ization for graph neural networks. InAdvances in Neural Information Processing Systems, 202. Yang, L., Kang, Z., Cao, X., Jin, D., Yang, B., and Guo, Y . Topology optimization based graph convolutional net- work. In International Joint Conference on Artificial In- telligence, 2019. Zhang, M. and Chen, Y . Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. Zhang, W., Yang, M., Sheng, Z., Li, Y ., Ouyang, W., Tao, Y ., Yang, Z., and Cui, B. Node dependent local smooth- ing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. 11GATE: How to Keep Out Intrusive Neighbors Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmooth- ing in gnns. In International Conference on Learning Representations, 2020. Zhou, K., Huang, X., Li, Y ., Zha, D., Chen, R., and Hu, X. Towards deeper graph neural networks with differen- tiable group normalization. In Advances in Neural Infor- mation Processing Systems, 2020. Zhou, K., Dong, Y ., Wang, K., Lee, W. S., Hooi, B., Xu, H., and Feng, J. Understanding and resolving performance degradation in graph convolutional networks. InConfer- ence on Information and Knowledge Management, 2021. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 12GATE: How to Keep Out Intrusive Neighbors A. Theoretical Derivations A.1. Derivation of Insight 4.2 Statement (Restated Insight 4.2) . GATs are challenged to switch off neighborhood aggregation during training, as this would require the model to enter a less trainable regime with large attention parameters âˆ¥aâˆ¥2 >> 1. We have to distinguish GATs with and without weight sharing in our analysis. GATs with weight sharing: To investigate the ability of a GAT to switch off neighborhood aggregation, let us focus on a link(i, j) that should neither contribute to the feature transformation of i nor j. This implies that we need to find attention parameters a (and potentially feature transformations W) so that Î±ij/Î±ii << 1 with Î±ij/Î±ii = exp ( eij âˆ’ eii). This implies that we require eij âˆ’ eii << 0 and thus aT Ï• (W (hi + hj)) âˆ’ 2aT Ï• (W (hi)) << 0. Since we also require Î±ij/Î±jj << 1, it follows from adding both inequalities that aT [Ï• (W (hi + hj)) âˆ’ (Ï• (Whi) + Ï• (Whj))] << 0. This inequality can only be fulfilled if there exists at least one feature f for which âˆ†fij ; = a[f] [Ï• (W[f, :] (hi + hj)) âˆ’ (Ï• (W[f, :]hi) + Ï• (W[f, :]hi))] fulfills âˆ†fij << 0. Yet, note that if both Ï• (W[f, :]hi) and Ï• (W[f, :]hj) are positive or both are negative, we just get âˆ†fij = 0 because of the definition of a LeakyReLU. Thus, there must exist at least one feature f so that without loss of generality Ï• (W[f, :]hi) < 0 and Ï• (W[f, :]hj) > 0. It follows that if a[f] > 0 that 0 > a[f]Ï• (W[f, :]hi) >> a[f] (Ï• (W[f, :] (hi + hj)) âˆ’ Ï• (W[f, :]hj)) > a[f] (Ï• (W[f, :] (hi + hj)) âˆ’ 2Ï• (W[f, :]hj)) also receives a negative contribution that makes Î±ij/Î±jj smaller. Yet, what happens to Î±ij/Î±ii? By distinguishing two cases, namely W[f, :] (hi + hj) > 0 or W[f, :] (hi + hj) < 0 and computing a[f] [Ï• (W (hi + hj)) âˆ’ 2Ï• (W[f, :]hj)] > 0 we find the feature contribution to be positive. If a[f] < 0, then 0 > a[f]Ï• (W[f, :]hj) >> a[f] (Ï• (W[f, :] (hi + hj)) âˆ’ Ï• (W[f, :]hi)) > a[f] (Ï• (W[f, :] (hi + hj)) âˆ’ 2Ï• (W[f, :]hi)) and Î±ij/Î±jj is reduced. Similarly, we can derive that at the same time Î±ij/Î±ii is increased, however. This implies that any feature that contributes to reducing âˆ†fij automatically increases one feature while it increases another. We therefore need multiple featuresf to contribute to reducing either Î±ij/Î±ii or Î±ij/Î±jj to compensate for other increases. This implies, in order to switch off neighborhood aggregation, we would need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Yet, all these norms are constrained by the derived conservation law and therefore prevent learning a representation that switches off full neighborhoods. GATs without weight sharing: The flow of argumentation without weight sharing is very similar to the one above with weight sharing. Yet, we have to distinguish more cases. 13GATE: How to Keep Out Intrusive Neighbors Similarly to before, we require Î±ij/Î±jj << 1 and Î±ji/Î±ii << 1. It follows from adding both related inequalities that aT [Ï• (Wshi + Wthj) + Ï• (Wshj + Wthi) âˆ’ Ï• ((Ws + Wt) hi) âˆ’ Ï• ((Ws + Wt) hj)] << 0. This implies that for at least one feature f, we require a[f][Ï• \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + Ï• \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 âˆ’ Ï• ((Ws[f, :] + Wt[f, :]) hi) âˆ’ Ï• ((Ws[f, :] + Wt[f, :]) hj)] << 0. (9) Again, our goal is to show that this feature automatically decreases the contribution of one feature while it increases another. As argued above, switching off neighborhood aggregation would therefore need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Our derived norm constraints, however, prevent learning such a model representation. Concretely, without loss of generality, we therefore have to show that if a[f][Ï• \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 âˆ’ Ï• \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 < 0, (10) at the same time, we receive a[f][Ï• \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 âˆ’ Ï• \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 > 0, (11) (or vice versa). In principle, we have to show this for 16 different cases of pre-activation sign configurations for the four terms in Eq. (9). Yet, since the argument is symmetric with respect to exchanging i and j, only 8 different cases remain. Two trivial cases are identical signs for all four terms. These are excluded, as the left hand side (LHS) of Eq. (9) would become zero and thus not contribute to our goal to switch off neighborhood aggregation. In the following, we will discuss the remaining six cases. Please note that for the remainder of this derivation Î± >0 denotes the slope of the leakyReLU and not the attention weights Î±ij. 1. Case (+ âˆ’ ++): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj > 0. From this assumption and the fact that Ï• is a leakyReLU it follows that the LHS of Eq. (9) be- comes: a[f][Ï• \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + Ï• \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 âˆ’ Ï• \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 âˆ’ Ï• \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 ] = a[f](Î± âˆ’1)[Ws[f, :]hj + Wt[f, :]hi]. Since Î± âˆ’1 < 0 and [Ws[f, :]hj + Wt[f, : ]hi] < 0 according to our assumption, Eq. (9) demands a[f] < 0. To switch off neighborhood aggregation, we would need to be able to make the LHS of Eq. (10) and Eq. (11) Eq. (11) negative. Yet, a negative a[f] leads to a positive LHS of Eq. (11). Thus, the assumed sign configuration cannot support switching off neighborhood aggregation. 2. Case (+ âˆ’ âˆ’âˆ’): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 âˆ’Î±)[Ws[f, :]hi + Wt[f, :]hj], which demands a[f] < 0. Accordingly, the LHS of Eq. (10) is clearly negative, while the LHS of Eq. (11) is a[f]Î±Ws[f, :](hj âˆ’ hi) > 0. The last inequality follows from our assumptions that implyWs[f, :]hj < Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj < 0 and Ws[f, :]hi + Wt[f, :]hj > 0. Again, this result implies that the considered sign configuration does not support switching off neighborhood aggregation. 3. Case (+ + + âˆ’): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi > 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 âˆ’Î±)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes positive, which hampers switching-off neighborhood aggregation as discussed. 14GATE: How to Keep Out Intrusive Neighbors 4. Case (âˆ’ âˆ’ âˆ’+): Let us assume that Ws[f, :]hi + Wt[f, :]hj < 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. The LHS of Eq. (9) becomes a[f](Î± âˆ’1)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes clearly negative. However, the LHS of Eq. (11) is positive, asa[f]Î±Ws[f, :](hj âˆ’ hi) > 0. The last inequality follows from our assumptions that imply Ws[f, :]hj > Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj > 0 and Ws[f, :]hi + Wt[f, :]hj < 0. Again, this analysis implies that the considered sign configuration does not support switching off neighborhood aggregation. 5. Case (+ âˆ’ +âˆ’): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. According to our assumptions the LHS of Eq. (10) can only be negative if a[f] < 0. Yet, the LHS of Eq. (11) can only be negative if a[f] > 0. Thus, this case clearly cannot contribute to switching off neighborhood aggregation. 6. Case (+ âˆ’ âˆ’+): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. Eq. (9) becomes a[f](1 âˆ’ Î±)Ws[f, :] (hi âˆ’ hj) < 0. At the same time, the LHS of Eq. (10) simplifies to a[f]Ws[f, : ](hi âˆ’ hj) and the LHS of Eq. (11) is a[f]Î±Ws[f, :](hj âˆ’ hi) > 0. Hence, a negative Eq. (9) leads to a positive Eq. (11). Accordingly, the last possible sign configuration also does not support switching off neighborhood aggregation, which concludes our derivation. A.2. Proof of Theorem 4.3 Statement (Restated Theorem 4.3). The gradients and parameters of GATE for layer l âˆˆ [L âˆ’ 1] are conserved according to the following laws: âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© = âŸ¨Wl+1[:, i], âˆ‡Wl+1[:,i]LâŸ© + âŸ¨al+1 s [i], âˆ‡al+1 s [i]LâŸ© + âŸ¨al+1 t [i], âˆ‡al+1 t [i]LâŸ©. (12) and, if additional independent matrices Ul and Vl are trainable, it also holds âŸ¨al s[i], âˆ‡als[i]LâŸ© + âŸ¨al t[i], âˆ‡al t[i]LâŸ© = âŸ¨Ul[i, :], âˆ‡Ul[i,:]LâŸ© + âŸ¨V l[i, :], âˆ‡V l[i,:]LâŸ©. (13) The proof is analogous to the derivation of Theorem 2.2 by (Mustafa & Burkholz, 2023) that is restated in this work as Theorem 4.1. For ease, we replicate their notation and definitions here. Statement (Rescale invariance: Def 5.1 by Mustafa & Burkholz (2023)). The loss L(Î¸) is rescale-invariant with respect to disjoint subsets of the parametersÎ¸1 and Î¸2 if for everyÎ» >0 we haveL(Î¸) = L((Î»Î¸1, Î»âˆ’1Î¸2, Î¸d)), where Î¸ = (Î¸1, Î¸2, Î¸d). Statement (Gradient structure due to rescale invariance Lemma 5.2 in (Mustafa & Burkholz, 2023)). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: âŸ¨Î¸1, âˆ‡Î¸1LâŸ© âˆ’ âŸ¨Î¸2, âˆ‡Î¸2LâŸ© = 0. (14) We first consider the simpler case of GATES, i.e. W = U = V Theorem A.1 (Structure of GATES gradients). The gradients and parameters of GATES for layer l âˆˆ [Lâˆ’1] are conserved according to the following laws: âŸ¨Wl[i, :], âˆ‡Wl[i,:]LâŸ© = âŸ¨Wl+1[:, i], âˆ‡Wl+1[:,i]LâŸ© + âŸ¨al s[i], âˆ‡als[i]LâŸ© + âŸ¨al t[i], âˆ‡al t[i]LâŸ©. (15) Following a similar strategy to (Mustafa & Burkholz, 2023), we identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Given the following definition of disjoint subsets Î¸1 and Î¸2 of the parameter set Î¸, associated with neuron i in layer l, 15GATE: How to Keep Out Intrusive Neighbors Î¸1 = {x|x âˆˆ Wl[i, :]} Î¸2 = {w|w âˆˆ Wl+1[:, i]} âˆª {al s[i]} âˆª {al t[i]} We show that the loss of GATES remains invariant for any Î» >0. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and Î±l uv. The scaled network parameters are denoted with a tilde as Ëœas l[i] = Î»âˆ’1al s[i], Ëœat l[i] = Î»âˆ’1al t[i], and ËœWl[i, j] = Î»Wl[i, j], and the corresponding networks components scaled as a result are denoted by Ëœhl u[i], Ëœhl+1 v [k], and ËœÎ±l uv. We show that the parameters of upper layers remain unaffected, as Ëœhl+1 v [k] coincides with its original non-scaled variant Ëœhl+1 v [k] = hl+1 v [k]. Also recall Eq. (4) for W = U = V as: el uv = ((1 âˆ’ quv)al s + (quv)al t)âŠ¤ Â· Ï•(Wlhlâˆ’1 u + Wlhlâˆ’1 v ) where quv = 1 if u = v and quv = 0 if u Ì¸= v. For simplicity, we rewrite this as: el uv,uÌ¸=v = (al s)âŠ¤ Â· Ï•(Wlhlâˆ’1 u + Wlhlâˆ’1 v ) (16) el uv,u=v = (al t)âŠ¤ Â· Ï•(Wlhlâˆ’1 u + Wlhlâˆ’1 v ) (17) We show that ËœÎ±l uv = exp(Ëœel uv)P uâ€²âˆˆN(v) exp(Ëœeluv) = Î±l uv , because (18) Ëœel uv,uÌ¸=v = el uv,uÌ¸=v , and Ëœel uv,u=v = el uv,u=v (19) which follows from the positive homogeneity of Ï• that allows Ëœel uv,u=v = Î»âˆ’1al s[i]Ï•( nlâˆ’1X j Î»Wl[i, j](hlâˆ’1 u [j] + hlâˆ’1 v [j]) + nlX iâ€²Ì¸=i al s[iâ€²]Ï•( nlâˆ’1X j Wl[iâ€², j](hlâˆ’1 u [j] + hlâˆ’1 v [j]) (20) = Î»âˆ’1Î»al s[i]Ï•( nlâˆ’1X j Wl[i, j](hlâˆ’1 u [j] + hlâˆ’1 v [j]) + nlX iâ€²Ì¸=i al s[iâ€²]Ï•( nlâˆ’1X j Wl[iâ€², j](hlâˆ’1 u [j] + hlâˆ’1 v [j]) (21) = el uv,uÌ¸=v. (22) and similarly, 16GATE: How to Keep Out Intrusive Neighbors Ëœel uv,u=v = Î»âˆ’1al t[i]Ï•( nlâˆ’1X j Î»Wl[i, j](hlâˆ’1 u [j] + hlâˆ’1 v [j]) + nlX iâ€²Ì¸=i al t[iâ€²]Ï•( nlâˆ’1X j Wl[iâ€², j](hlâˆ’1 u [j] + hlâˆ’1 v [j]) (23) = Î»âˆ’1Î»al t[i]Ï•( nlâˆ’1X j Wl[i, j](hlâˆ’1 u [j] + hlâˆ’1 v [j]) + nlX iâ€²Ì¸=i al t[iâ€²]Ï•( nlâˆ’1X j Wl[iâ€², j](hlâˆ’1 u [j] + hlâˆ’1 v [j]) (24) = el uv,u=v. (25) Since ËœÎ±l uv = Î±l uv, it follows that Ëœhu l [i] = Ï•1 ï£« ï£­ X zâˆˆN(u) Î±l zu nlâˆ’1X j Î»Wl[i, j]hlâˆ’1 z [j] ï£¶ ï£¸ = Î»Ï•1 ï£« ï£­ X zâˆˆN(u) Î±l zu nlâˆ’1X j Wl[i, j]hlâˆ’1 z [j] ï£¶ ï£¸ = Î»hl u[i]. In the next layer, we therefore have Ëœhl+1 v [k] = Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Î»âˆ’1Wl+1[k, i]Ëœhl u[i] ï£¶ ï£¸ = Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Î»âˆ’1Wl+1[k, i]Î»hl u[i] ï£¶ ï£¸ = Ï•1 ï£« ï£­ X uâˆˆN(v) Î±l+1 uv nlX i Wl+1[k, i]hl u[i] ï£¶ ï£¸ = hl+1 v [k]. Thus, the output node representations of the network remain unchanged, and the loss L is rescale-invariant. Next consider the case that Wl, Ul, and V l are independent matrices. Similarly to the previous reasoning, we see that if we scale ËœWl[i, :] = Wl[i, :]Î», then also scaling ËœWl+1[:, i] = Wl+1[:, i]Î»âˆ’1 and Ëœal+1 s [i] = al+1 s [i]Î»âˆ’1 and Ëœal+1 t [i] = al+1 t [i]Î»âˆ’1 will keep the GATE layer unaltered. In this case, we obtain an additional rescaling relationship between al s, al t and Ul, V l. A rescaling of the form Ëœas l[i] = Î»âˆ’1al s[i], Ëœat l[i] = Î»âˆ’1al t[i] could be compensated by ËœUl[i, :] = Ul[i, :]Î» and ËœV l[i, :] = V l[i, :]Î». It follows immediately that Ëœeuv = euv. A.3. Derivation of Insight 4.4 Following the analysis in A.1, in contrast to GAT, Î±ij/Î±ii << 1 can be easily realized in GATE with as[f] < 0 and at[f] > 0 for all or only a subset of the features. Note that for the non-weight-sharing case, U and V in GATE would simply correspond to Ws and Wt, respectively, in GATE and the same line of reasoning holds. Large norms are usually not required to create a notable difference in size between eii and eij. 17GATE: How to Keep Out Intrusive Neighbors B. Experimental Settings Our complete experimental setup is described as follows. Non-linearity For GATS and GAT networks, we substitute Ï• in Eq. (3) with LeakyReLU as defined in the standard architecture. For GATE, we substitute Ï• in Eq. (4) with ReLU in order to be able to interpret the sign of as and at parameters as contributing positively or negatively to neighborhood aggregation. MLP, MLP +GAT , and FAGCN also all use ReLU after every hidden layer. Network Width We vary the depth of GAT and GATE networks in all our experiments as specified. For synthetic datasets, the network width is fixed to 64 in all cases. For OGB datasets, we use the hidden dimensions used by (Brody et al., 2022). For the remaining datasets, the network width is also fixed to 64. Initialization The feature transformation parameter matrices, i.e.,W, U, and V are initialized randomly with an orthog- onal looks-linear structure (Burkholz & Dubatovka, 2019) for MLP, MLP+GAT , GAT(S) and GATE(S). The parameters a in GAT(S) use Xavier initialization (Glorot & Bengio, 2010), as is the standard. In GATE(S), as and at are initialized to 0 to initially give equal weights to the features of a node itself and its neighboring nodes. Optimization Synthetic, OGB, and remaining real-world tasks are run for a maximum of 10000, 2000, 5000 epochs, respectively, using the Adam optimizer. To isolate the effect of the architecture and study the parameter dynamics during training as best as possible, we do not use any additional elements such as weight decay and dropout regularization. We also do not perform any hyperparameter optimization. However, the learning rate is adjusted for different real-world datasets to enable stable training of models as specified in Table 6. Nevertheless, for a fair comparison, the same learning rate is used for a given problem across all architectures. For all synthetic data, a learning rate of 0.005 is used. Real-world datasets use their standard train/test/validation splits, i.e. those provided by Pytorch Geometric for Planetoid datasets Cora and Citeseer, by OGB framework for OGB datasets, and by (Platonov et al., 2023) for all remaining real-world datasets. Code Our experimental code and synthetic data generators are available at https://github.com/ RelationalML/GATE.git. Table 6: Details of real-world datasets used in experiments. dataset # nodes # edges # features # classes learning rate used for L layer networks ogb-arxiv 169, 343 2 , 315, 598 128 40 L = [12] : 0.001 ogb-products 2, 449, 029 123 , 718, 152 100 47 L = [8] : 0.001 ogb-mag 736, 389 10 , 792, 672 128 349 L = [5] : 0.005 roman-empire 22, 662 32 , 927 300 18 L = [5] : 0.001, L= [10] : 0.0005 amazon-ratings 24, 492 93 , 050 300 5 L = [5] : 0.001, L= [10] : 0.0005 questions 48, 921 153 , 540 301 2 L = [5] : 0.001, L= [10] : 0.0005 minesweeper 10, 000 39 , 402 7 2 L = [5] : 0.001, L= [10] : 0.0005 tolokers 11, 758 519 , 000 10 2 L = [5] : 0.001, L= [10] : 0.0005 cora 2, 708 10 , 556 1 , 433 7 L = [2, 5] : 0.005, L= [10, 20] : 0.0005 citeseer 3, 327 9 , 104 3 , 703 6 L = [2, 5] : 0.001, L= [10, 20] : 0.0001 actor 7, 600 26 , 659 932 5 L = [2, 5, 10, 20] : 0.005 texas 183 279 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.0005 wisconsin 251 450 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.005 18GATE: How to Keep Out Intrusive Neighbors C. Additional Results Smaller Real-World Datasets We evaluate GAT and GATE on five small-scale real-world datasets with varying ho- mophily levels Î² as defined in (Pei et al., 2020) and report results in Table 7. Higher values ofÎ² indicate higher homophily, i.e. similar nodes (with the same label) tend to be connected. We note that a 2-layer network of a baseline method for het- erophilic datasets, Geom-GCN (Pei et al., 2020), attains test accuracy (%) of64.1, 67.6, and 31.6 for Wisconsin, Texas, and Actor datasets, respectively, which is in line with that achieved by GATE. Except for Citeseer, the best overall performance for each dataset is achieved on a shallow model. This is not surprising as these datasets are small-scale and potentially prone to over-fitting in large models, particularly since we do not use any skip connections or regularization to retain model performance. Furthermore, the three heterophilic datasets have been recently shown to be problematic (Platonov et al., 2023). Therefore, a better evaluation of GATE is on relatively large-scale OGB datasets (Hu et al., 2021) and more recent heterophilic datasets (Platonov et al., 2023) that can exploit the flexibility of GATE. Although GATE is more pa- rameterized than GAT, it usually requires fewer training epochs and generalizes better, in addition to other advantages over GAT as discussed in the paper. Table 7: Test accuracy (%) of GAT and GATE models for network depthL on small-scale real-world datasets with varying homophily levels Î². Entries marked with * indicate models that achieve 100% training accuracy and stable test accuracy. Otherwise, test accuracy at max. validation accuracy is reported. Data Î² L = 2 L = 5 L = 10 L = 20 GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE Wisc. 0.21 62 .7* 80.4 70.5* 51.0 70.5 60.7* 45.1 62.7 58.8 47 .1 62.7 60.7 Texas 0.11 56 .7* 67.6* 67.6* 51.4 67.6* 67.6* 56.7* 62.2* 62.3* 59.4* 62.1* 64.9 Actor 0.24 27 .1 32.2 31.6 25 .4 27 .5 29.2 25.3 27 .4 27.9 24.5 24 .6 29.4 Cora 0.83 80 .0 81.0* 80.8 79 .8 80.8* 80.4 77 .6 80.0* 79.2 77 .7 77 .2* 79.0 Cite. 0.71 68 .0 67 .6* 68.3 67.2 68.7* 67.8 66 .9 67.6* 67.6 68.2 67 .1* 69.2 Initialization of attention parameters in GAT We show in Fig. 7 that setting the initial value of attention parameters as and at in GATE to zero is, in fact, not what enables neighborhood aggregation but rather the separation ofa into as and at as discussed in Insight 4.4. Figure 7: Distribution of Î±vv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels for GAT with attention parameters a initialized to zero. Left to right:1, 2 and 5 layer models that achieve test accuracy of100%, 52.7%, and 36.2%, respectively, which is similar to the results obtained by standard Xavier initialization of attention parameters in GAT. Further analysis of experiments We present the analysis of Î± coefficients learned for some experiments in the main paper that were deferred to the appendix due to space limitations. 19GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 8: Distribution of Î±vv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 5 layer models. (a) original labels (b) Random labels Figure 9: Distribution of Î±vv against training epoch for the self-sufficient learning problem using Cora graph structure with original (top) and random (bottom) node labels and input node features as a one-hot encoding of labels. Left to right: 1, 2, and 5 layer GATES models that all 100% test accuracy except in the case of 5 layer model using original labels. In this case, although a training accuracy if 100% is achieved at 32 epochs with test accuracy 97.3%, a maximum test accuracy of 98.4% is reached at 7257 epochs. Training the model to run to 15000 epochs only increases it to 98.4%. An increased learning rate did not improve this case. However, we also run the GAT model for 15000 epochs for this case, and it achieves 85.9% test accuracy at epoch 47 where the model achieves 100% accuracy and only achieves a maximum test accuracy of 89.3% briefly at epoch 8. 20GATE: How to Keep Out Intrusive Neighbors Figure 10: Distribution of Î±vv against training epoch for the neighbor-dependent learning problem withk = 1. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 1, 2, and 3 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 1 layer of the 2 and 3 layer models perform neighborhood aggregation. Table 8: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Entries marked with * identify models where 100% train accuracy is not achieved. Underlined entries identify the model with the highest train accuracy at the epoch of max. test accuracy. This provides an insight into how similar the function represented by the trained model is to the function used to generate node labels, i.e. whether the model is simply overfitting to the train data or really learning the task. Higher training and test accuracy simultaneously indicate better learning. In this regard, the difference in train accuracy at max. test accuracy between GATE and GATS or GAT is only 0.4, 1.0 and 0.6 for the settings (k = 1, L= 3), (k = 2, L= 4) and (k = 3, L= 3), respectively. k L Test Acc. @ Epoch of Max. Train Acc. Max Test Acc. @ Epoch GATS GAT GATE GAT S GAT GATE 1 1 92.0@2082* 91.2@6830* 93.2@3712* 93.2@1421 92.0@9564 93.6@3511 2 89.6@8524* 88.0@8935 91.2@942 91.6@5188 92.8@4198 95.6@111 3 86.4@9180* 88.8@997 92.8@618 91.2@6994 92.8@437 97.2@82 2 2 88.8@6736* 89.6@3907 88.8@467 93.2@151 93.2@95 92.0@105 3 82.0@7612 89.2@1950 91.6@370 91.6@1108 93.2@856 95.2@189 4 84.8@4898 82.4@739 87.2@639 88.0@1744 88.4@423 90.4@447 3 3 80.8@8670 80.4@737 85.2@391 86.4@1578 88.8@285 92.0@47 4 78.0@3012 80.4@767 89.6@480 86.8@1762 85.6@469 91.6@139 5 80.0@6611 74.4@1701 86.0@447 85.6@921 83.6@1098 91.2@243 21GATE: How to Keep Out Intrusive Neighbors (a) Tolokers, GAT:61.6% test AUROC.  (b) Tolokers, GATE:69.2% test AUROC. Figure 11: Distribution of Î±vv against training epoch for one run of 10 layer networks on real-world heterophilic task. Figure 12: Distribution of Î±vv against training epoch of 2-layer (left) and 5-layer (right) GAT networks for heterophilic datasets Texas (top) and Actor (bottom) 2-layer modes. Despite having connections to unrelated neighbors, GAT is unable to switch off neighborhood aggregation. Figure 13: Distribution of Î±vv, against training epoch of 2-layer (left) and 5-layer (right) GATE networks for heterophilic datasets Texas (top) and Actor (bottom), across layers could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods. For instance, in the case of Texas, GATE carries out little to no neighborhood aggregation in the first layer over input node features. Instead, aggregation is mainly done over node features transformed in earlier layers that effectuate non-linear feature learning as in perceptrons. However, in the case of Actor, GATE prefers most of the neighborhood aggregation to occur over the input node features, indicating that they are more informative for the task at hand. 22GATE: How to Keep Out Intrusive Neighbors Over-smoothing analysis In the main paper, we have already established the superior performance of GATE, compared to GAT, on several tasks. Intuitively, this can partially be attributed to reduced over-smoothing as its root cause, unnecessary neighborhood aggregation, is alleviated. Here, we verify this insight quantitatively. A widely accepted measure of over-smoothing is the Dirichlet energy (DE) (Cai & Wang, 2020). However, Eliasof et al. (2023) propose a modification of DE to measure GAT energy EGAT , that we use to evaluate over-smoothing in our experiments (see Table 9). We note that the notion of â€˜over-smoothingâ€™ is itself task-dependent. It is difficult to determine the optimal degree of smoothing for a task and the threshold that determines â€˜overâ€™-smoothing. This merits an in-depth analysis and curation of task-dependent smoothness measures that are not our focus. To show that GATE reduced over- smoothing relative to GAT, it suffices that a decrease in smoothing and an increase in accuracy occur simultaneously. Table 9: The measures Einput, EGAT and EGATE denote the smoothness of input node features, node features at the last layer L of the trained GAT and GATE models, respectively. Two cases are considered: All node pairs and only adjacent node pairs to measure smoothing at the global graph and local node level. Higher values indicate less smoothing. Node representations learned by GATE achieve higher test accuracy on all these tasks, as reported in the main paper, and are simultaneously less smooth than GAT in most cases, indicating that GATE potentially alleviates over-smoothing in GATs. (a) Synthetic self-sufficient task: varying graph structure and label distribution, node features as one-hot encoding of labels, L = 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE Cora, Original Labels 6.016 e + 06 1 .281 e + 08 2.037 e + 09 2.006 e + 03 7 .903 e + 03 1.971 e + 05 Cora, Random Labels 6.283 e + 06 3 .472 e + 09 3.747 e + 09 9.080 e + 03 9 .306 e + 05 3.966 e + 06 ER(p = .01), 2 Classes 4.994 e + 05 2 .701 e + 06 4.478 e + 07 5.042 e + 03 2 .272 e + 04 3.229 e + 05 ER(p = .01), 8 Classes 8.745 e + 05 3 .350 e + 07 2.615 e + 08 8.694 e + 03 1 .762 e + 05 1.960 e + 06 (b) Synthetic neighbor-dependent task: graph structure, node features, generator parameter k, and label distribution as in Section 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE k = 1, L= 3 1 .953 e + 06 5.306 e + 07 5.095 e + 07 1 .957 e + 05 4.234 e + 05 3.610 e + 05 k = 2, L= 4 1 .975 e + 06 1 .193 e + 07 2.198 e + 07 2.012 e + 04 1 .016 e + 05 1.939 e + 05 k = 3, L= 5 1 .951 e + 06 1 .645 e + 07 1.053 e + 08 1.966 e + 04 1 .408 e + 05 9.096 e + 05 (c) Real-world tasks. Dataset L All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE roman-empire 5 1 .274 e + 09 1 .002 e + 11 7.491 e + 11 7.878 e + 04 2 .441 e + 06 4.009 e + 07 amazon-ratings 10 3 .844 e + 08 1 .187 e + 10 2.272 e + 10 4.933 e + 04 3 .848 e + 05 7.430 e + 05 minesweeper 5 6 .869 e + 07 1 .386 e + 09 2.531 e + 10 2.628 e + 04 1 .946 e + 05 7.017 e + 06 tolokers 10 1 .391 e + 08 1 .044 e + 11 1.042 e + 11 3.423 e + 05 1 .249 e + 08 1.397 e + 08 cora 10 5 .088 e + 05 1 .437 e + 07 2.783 e + 08 6.490 e + 02 5 .959 e + 02 1.226 e + 04 citeseer 10 3 .463 e + 05 2 .916 e + 05 1.126 e + 07 2.360 e + 02 2 .426 e + 00 1.030 e + 02 texas 10 1 .945 e + 06 3 .280 e + 04 3.877 e + 04 1.758 e + 04 8 .487 e + 01 9.695 e + 01 actor 10 2 .612 e + 08 1.800 e + 07 1.364 e + 07 1 .237 e + 05 2.810 e + 03 2.215 e + 03 wisconsin 10 4 .438 e + 06 1 .057 e + 07 1.008 e + 08 3.299 e + 04 3 .765 e + 04 7.363 e + 08 23GATE: How to Keep Out Intrusive Neighbors Comparison with other GNNs Other GNN architectures could potentially switch off neighborhood aggregation, as we show here. However, they are less flexible in assigning different importance to neighbors, suffer from over-smoothing, or come at the cost of an increased parameter count by increasing the size of the hidden dimensions (e.g. via a concatena- tion operation). We evaluate the performance of three such architectures that, in principle, employ different aggregation methods, which are likely to be capable of switching off neighborhood aggregation, on synthetic datasets empirically and discuss their ability or inability to switch off neighborhood aggregation qualitatively as follows. 1. Ï‰GAT (Eliasof et al., 2023) introduces an additional feature-wise layer parameter Ï‰ that can, in principle, switch off neighborhood aggregation by setting Ï‰ parameters to 0, in addition to the attention mechanism based on GAT. How- ever, in practice, as we verify on our synthetic dataset in Figure 14, it is unable to effectively switch off neighborhood aggregation. Although it outperforms GAT, it is still substantially worse than GATE, especially for the deeper model due to unnecessary neighborhood aggregations. Another architecture based on graph attention, superGAT(Kim & Oh, 2021), falls under the paradigm of structural learning as it uses a self-supervised attention mechanism essentially for link prediction between nodes, and therefore its comparison with GATE is infeasible. 2. GraphSAGE (Hamilton et al., 2018) uses the concatenation operation to combine the nodeâ€™s own representation with the aggregated neighborhood representation. Therefore, it is usually (but not always) able to switch off the neigh- borhood aggregation for the synthetic datasets designed for the self-sufficient learning task (see Table 10). Mostly, GATE performs better on the neighbor-dependent task, in particular for deeper models, where the performance of GraphSAGE drops likely due to over-smoothing (see Table 11). 3. FAGCN (Bo et al., 2021) requires a slightly more detailed analysis. Authors of FAGCN state in the paper that: â€˜When Î±G ij â‰ˆ 0, the contributions of neighbors will be limited, so the raw features will dominate the node representations.â€™ where Î±G ij defined in the paper can be considered analogous to Î±ij in GAT, though they are defined differently. Thus, from an expressivity point of view, FAGCN should be able to assign parameters such that all Î±G ij = 0. However, we empirically observe on synthetic datasets designed for the self-sufficient learning task, values of Î±G ij do not, in fact, approach zero. Despite being unable to switch off neighborhood aggregation, FAGCN, in its default implementation, achieves 100% test accuracy on the task. We discover this is so because FAGCN introduces direct skip connections of non-linearly transformed raw node features to every hidden layer. Given the simplicity of the one-hot encoded features in the datasets and the complete dependence of the label on these features, FAGCN is able to represent the desired function. In order to better judge its ability to switch off neighborhood aggregation by setting Î±G ij = 0, we remove this skip connection. From an expressivity point of view, FAGCN should still be able to achieve 100% test accuracy by using only the (non-)linear transformations of raw features initially and performing no neighborhood aggregation in the hidden layers. However, we find that FAGCN was unable to emulate this behavior in practice. For a fair comparison of the differently designed attention mechanism in FAGCN with GATE, we introduce self-loops in the data so FAGCN may also receive a nodeâ€™s own features in every hidden layer. Even then, FAGCN fails to achieve perfect test accuracy as shown in Table 10. Therefore, we suspect the attention mechanism in FAGCN may also be susceptible to the trainability issues we have identified for the attention mechanism in GAT. Nevertheless, the capacity of FAGCN to learn negative associations with neighboring nodes is complementary to GATE and both could be combined. It would be interesting to derive conservation laws inherent to other architectures such as FAGCN and GraphSAGE and study how they govern the behaviour of parameters. Furthermore, by design, FAGCN does not perform any non-linear transformations of aggregated neighborhood features which may be necessary in some tasks, such as our synthetic dataset for the neighbor-dependent learning task. As Table 11 shows, GATE outperforms FAGCN on such a task. Lastly, we would like to emphasize that our aim is to provide insights into the attention mechanism of GAT and understand its limitations. While it should be able to flexibly assign importance to neighbors and the node itself without the need for concatenated representation or explicit skip connections of the raw features to every layer, it is currently unable to do so in practice. In order to verify our identification of trainability issues, we modify the GAT architecture to enable the trainability of attention parameters which control the trade-off between node features and structural information. 24GATE: How to Keep Out Intrusive Neighbors Figure 14: Distribution of Î±vv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels, for theÏ‰GAT architecture for the1, 2 and 5 layer models that achieve test accuracy of 100%, 98.5%, and 49.3%, respectively. Table 10: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of network layers, respectively. Original (Orig.) and Randomized (Rand.) labels are used for the Cora structure. The FAGCN model is implemented without skip connections from the input layer to every other layer and without any self-loops in input data, whereas FAGCN* denotes the model also without skip connections but with self-loops introduced for all nodes in input data. Structure C L Max. Test Accuracy (%) GAT GATE FAGCN FAGCN* SAGE Cora O,7 1 100 100 90.1 97 .6 100 2 94 .6 100 94.2 94 .9 98 .8 5 88 .5 99.7 87.1 89 .1 92 .4 R,7 1 100 100 61.6 97 .8 100 2 57 .0 100 69.2 70 .5 100 5 36 .7 100 21.2 36 .7 99 .6 ER (p = 0.01) R,2 1 100 100 100 100 100 2 100 100 100 100 100 5 99 .6 100 96.4 99 .2 100 R,8 1 99 .2 100 86.4 98 .8 100 2 97 .6 100 86.0 91 .6 100 5 38 .4 100 31.6 40 .4 100 25GATE: How to Keep Out Intrusive Neighbors Table 11: Neighbor-dependent learning: k and L denote the number of hops aggregated in the neighborhood to generate labels, and the number of layers of the evaluated network, respectively. k L Max Test Accuracy (%) @ Epoch GAT GATE SAGE FAGCN 1 1 92@9564 93.6 @ 3511 93.2@2370 93 .2@1618 2 92 .8@4198 95.6 @ 111 95.6@723 94 .1@1455 3 92 .8@437 97.2 @ 82 96.8@100 81 .2@573 2 2 93.2 @ 95 92.0@105 90 .8@199 90 .4@170 3 93 .2@856 95.2 @ 189 94.4@113 88 .8@283 4 88 .4@423 90 .4@447 92.4 @ 139 87.6@549 3 3 88 .8@285 92.0 @ 47 87.6@45 89 .2@528 4 85 .6@469 91.6 @ 139 88@60 89 .2@3191 5 83 .6@1098 91.2 @ 243 86.0@35 88 .8@205 26",
      "meta_data": {
        "arxiv_id": "2406.00418v2",
        "authors": [
          "Nimrah Mustafa",
          "Rebekka Burkholz"
        ],
        "published_date": "2024-06-01T12:31:15Z",
        "pdf_url": "https://arxiv.org/pdf/2406.00418v2.pdf"
      }
    },
    {
      "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method",
      "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning\nrepresentations of attributed graphs. To scale GCNs to large graphs,\nstate-of-the-art methods use various layer sampling techniques to alleviate the\n\"neighbor explosion\" problem during minibatch training. We propose GraphSAINT,\na graph sampling based inductive learning method that improves training\nefficiency and accuracy in a fundamentally different way. By changing\nperspective, GraphSAINT constructs minibatches by sampling the training graph,\nrather than the nodes or edges across GCN layers. Each iteration, a complete\nGCN is built from the properly sampled subgraph. Thus, we ensure fixed number\nof well-connected nodes in all layers. We further propose normalization\ntechnique to eliminate bias, and sampling algorithms for variance reduction.\nImportantly, we can decouple the sampling from the forward and backward\npropagation, and extend GraphSAINT with many architecture variants (e.g., graph\nattention, jumping connection). GraphSAINT demonstrates superior performance in\nboth accuracy and training time on five large graphs, and achieves new\nstate-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).",
      "full_text": "Published as a conference paper at ICLR 2020 GraphSAINT: G RAPH SAMPLING BASED INDUCTIVE LEARNING METHOD Hanqing Zengâˆ— University of Southern California zengh@usc.edu Hongkuan Zhouâˆ— University of Southern California hongkuaz@usc.edu Ajitesh Srivastava University of Southern California ajiteshs@usc.edu Rajgopal Kannan US Army Research Lab rajgopal.kannan.civ@mail.mil Viktor Prasanna University of Southern California prasanna@usc.edu ABSTRACT Graph Convolutional Networks (GCNs) are powerful models for learning repre- sentations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use variouslayer samplingtechniques to alleviate the â€œneighbor explosionâ€ problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efï¬ciency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure ï¬xed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the for- ward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior performance in both accuracy and training time on ï¬ve large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). 1 I NTRODUCTION Recently, representation learning on graphs has attracted much attention, since it greatly facilitates tasks such as classiï¬cation and clustering (Wu et al., 2019; Cai et al., 2017). Current works on Graph Convolutional Networks (GCNs) (Hamilton et al., 2017; Chen et al., 2018b; Gao et al., 2018; Huang et al., 2018; Chen et al., 2018a) mostly focus on shallow models (2 layers) on relatively small graphs. Scaling GCNs to larger datasets and deeper layers still requires fast alternate training methods. In a GCN, data to be gathered for one output node comes from its neighbors in the previous layer. Each of these neighbors in turn, gathers its output from the previous layer, and so on. Clearly, the deeper we back track, the more multi-hop neighbors to support the computation of the root. The number of support nodes (and thus the training time) potentially grows exponentially with the GCN depth. To mitigate such â€œneighbor explosionâ€, state-of-the-art methods use variouslayer sampling techniques. The works by Hamilton et al. (2017); Ying et al. (2018a); Chen et al. (2018a) ensure that only a small number of neighbors (typically from 2 to 50) are selected by one node in the next layer. Chen et al. (2018b) and Huang et al. (2018) further propose samplers to restrict the neighbor expansion factor to 1, by ensuring a ï¬xed sample size in all layers. While these methods signiï¬cantly speed up training, they face challenges in scalability, accuracy or computation complexity. âˆ—Equal contribution 1 arXiv:1907.04931v4  [cs.LG]  16 Feb 2020Published as a conference paper at ICLR 2020 Present work We present GraphSAINT (Graph SAmpling based INductive learning meThod) to efï¬ciently train deep GCNs. GraphSAINT is developed from a fundamentally different way of minibatch construction. Instead of building a GCN on the full training graph and then sampling across the layers, we sample the training graph ï¬rst and then build a full GCN on the subgraph. Our method is thus graph samplingbased. Naturally, GraphSAINT resolves â€œneighbor explosionâ€, since every GCN of the minibatches is a small yet complete one. On the other hand, graph sampling based method also brings new challenges in training. Intuitively, nodes of higher inï¬‚uence on each other should have higher probability to form a subgraph. This enables the sampled nodes to â€œsupportâ€ each other without going outside the minibatch. Unfortunately, such strategy results in non-identical node sampling probability, and introduces bias in the minibatch estimator. To address this issue, we develop normalization techniques so that the feature learning does not give preference to nodes more frequently sampled. To further improve training quality, we perform variance reduction analysis, and design light-weight sampling algorithms by quantifying â€œinï¬‚uenceâ€ of neighbors. Experiments on GraphSAINT using ï¬ve large datasets show signiï¬cant performance gain in both training accuracy and time. We also demonstrate the ï¬‚exibility of GraphSAINT by integrating our minibatch method with popular GCN architectures such as JK-net (Xu et al., 2018) and GAT (VeliË‡ckoviÂ´c et al., 2017). The resulting deep models achieve new state-of-the-art F1 scores on PPI (0.995) and Reddit (0.970). 2 R ELATED WORK A neural network model that extends convolution operation to the graph domain is ï¬rst proposed by Bruna et al. (2013). Further, Kipf & Welling (2016); Defferrard et al. (2016) speed up graph convolution computation with localized ï¬lters based on Chebyshev expansion. They target relatively small datasets and thus the training proceeds in full batch. In order to scale GCNs to large graphs, layer sampling techniques (Hamilton et al., 2017; Chen et al., 2018b; Ying et al., 2018a; Chen et al., 2018a; Gao et al., 2018; Huang et al., 2018) have been proposed for efï¬cient minibatch training. All of them follow the three meta steps: 1. Construct a complete GCN on the full training graph. 2. Sample nodes or edges of each layer to form minibatches. 3. Propagate forward and backward among the sampled GCN. Steps (2) and (3) proceed iteratively to update the weights via stochastic gradient descent. The layer sampling algorithm of GraphSAGE (Hamilton et al., 2017) performs uniform node sampling on the previous layer neighbors. It enforces a pre-deï¬ned budget on the sample size, so as to bound the minibatch computation complexity. Ying et al. (2018a) enhances the layer sampler of Hamilton et al. (2017) by introducing an importance score to each neighbor. The algorithm presumably leads to less information loss due to weighted aggregation. S-GCN (Chen et al., 2018a) further restricts neighborhood size by requiring only two support nodes in the previous layer. The idea is to use the historical activations in the previous layer to avoid redundant re-evaluation. FastGCN (Chen et al., 2018b) performs sampling from another perspective. Instead of tracking down the inter-layer connections, node sampling is performed independently for each layer. It applies importance sampling to reduce variance, and results in constant sample size in all layers. However, the minibatches potentially become too sparse to achieve high accuracy. Huang et al. (2018) improves FastGCN by an additional sampling neural network. It ensures high accuracy, since sampling is conditioned on the selected nodes in the next layer. Signiï¬cant overhead may be incurred due to the expensive sampling algorithm and the extra sampler parameters to be learned. Instead of sampling layers, the works of Zeng et al. (2018) and Chiang et al. (2019) build mini- batches from subgraphs. Zeng et al. (2018) proposes a speciï¬c graph sampling algorithm to ensure connectivity among minibatch nodes. They further present techniques to scale such training on shared-memory multi-core platforms. More recently, ClusterGCN (Chiang et al., 2019) proposes graph clustering based minibatch training. During pre-processing, the training graph is partitioned into densely connected clusters. During training, clusters are randomly selected to form minibatches, and intra-cluster edge connections remain unchanged. Similar to GraphSAINT, the works of Zeng et al. (2018) and Chiang et al. (2019) do not sample the layers and thus â€œneighbor explosionâ€ is avoided. Unlike GraphSAINT, both works are heuristic based, and do not account for bias due to the unequal probability of each node / edge appearing in a minibatch. Another line of research focuses on improving model capacity. Applying attention on graphs, the architectures of VeliË‡ckoviÂ´c et al. (2017); Zhang et al. (2018); Lu et al. (2019) better capture neighbor features by dynamically adjusting edge weights. Klicpera et al. (2018) combines PageRank with GCNs to enable efï¬cient information propagation from many hops away. To develop deeper models, 2Published as a conference paper at ICLR 2020 â€œskip-connectionâ€ is borrowed from CNNs (He et al., 2015; Huang et al., 2017) into the GCN context. In particular, JK-net Xu et al. (2018) demonstrates signiï¬cant accuracy improvement on GCNs with more than two layers. Note, however, that JK-net (Xu et al., 2018) follows the same sampling strategy as GraphSAGE (Hamilton et al., 2017). Thus, its training cost is high due to neighbor explosion. In addition, high order graph convolutional layers (Zhou, 2017; Lee et al., 2018; Abu-El-Haija et al., 2019) also help propagate long-distance features. With the numerous architectural variants developed, the question of how to train them efï¬ciently via minibatches still remains to be answered. 3 P ROPOSED METHOD : GraphSAINT Graph sampling based method is motivated by the challenges in scalability (in terms of model depth and graph size). We analyze the bias (Section 3.2) and variance (Section 3.3) introduced by graph sampling, and thus, propose feasible sampling algorithms (Section 3.4). We show the applicability of GraphSAINT to other architectures, both conceptually (Section 4) and experimentally (Section 5.2). In the following, we deï¬ne the problem of interest and the corresponding notations. A GCN learns representation of an un-directed, attributed graph G(V,E), where each node v âˆˆV has a length-f attribute xv. Let A be the adjacency matrix and ËœA be the normalized one (i.e., ËœA = Dâˆ’1A, and D is the diagonal degree matrix). Let the dimension of layer-â„“input activation be f(â„“). The activation of node v is x(â„“) v âˆˆRf(â„“) , and the weight matrix is W(â„“) âˆˆRf(â„“)Ã—f(â„“+1) . Note that xv = x(1) v . Propagation rule of a layer is deï¬ned as follows: x(â„“+1) v = Ïƒ (âˆ‘ uâˆˆV ËœAv,u ( W(â„“) )T x(â„“) u ) (1) where ËœAv,u is a scalar, taking an element of ËœA. And Ïƒ(Â·) is the activation function (e.g., ReLU). We use subscript â€œsâ€ to denote parameterd of the sampled graph (e.g.,Gs, Vs, Es). GCNs can be applied under inductive and transductive settings. While GraphSAINT is applicable to both, in this paper, we focus on inductive learning. It has been shown that inductive learning is especially challenging (Hamilton et al., 2017) â€” during training, neither attributes nor connections of the test nodes are present. Thus, an inductive model has to generalize to completely unseen graphs. 3.1 M INIBATCH BY GRAPH SAMPLING 0 1 2 3 4 5 6 8 9 7 0 1 2 3 5 7 0 1 2 3 5 7 0 1 2 3 5 7 Gs = SAMPLE(G) Full GCN on Gs Figure 1: GraphSAINT training algorithm GraphSAINT follows the design philosophy of directly sampling the training graph G, rather than the corresponding GCN. Our goals are to 1. extract appropriately connected subgraphs so that little information is lost when propagating within the subgraphs, and 2. combine information of many subgraphs together so that the training process overall learns good representation of the full graph. Figure 1 and Algorithm 1 illustrate the training algorithm. Before training starts, we perform light-weight pre-processing on Gwith the given sampler SAMPLE. The pre-processing estimates the probability of a node vâˆˆV and an edge eâˆˆE being sampled by SAMPLE. Such probability is later used to normalize the subgraph neighbor aggregation and the minibatch loss (Section 3.2). Afterwards, 3Published as a conference paper at ICLR 2020 Algorithm 1GraphSAINT training algorithm Input: Training graph G(V,E,X); Labels Y ; Sampler SAMPLE; Output: GCN model with trained weights 1: Pre-processing: Setup SAMPLE parameters; Compute normalization coefï¬cients Î±, Î». 2: for each minibatch do 3: Gs (Vs,Es) â†Sampled sub-graph of Gaccording to SAMPLE 4: GCN construction on Gs. 5: {yv |vâˆˆVs}â† Forward propagation of {xv |vâˆˆVs}, normalized by Î± 6: Backward propagation from Î»-normalized loss L(yv,yv). Update weights. 7: end for training proceeds by iterative weight updates via SGD. Each iteration starts with an independently sampled Gs (where |Vs| â‰ª|V|). We then build a full GCN on Gs to generate embedding and calculate loss for every vâˆˆVs. In Algorithm 1, node representation is learned by performing node classiï¬cation in the supervised setting, and each training node vcomes with a ground truth label yv. Intuitively, there are two requirements for SAMPLE: 1. Nodes having high inï¬‚uence on each other should be sampled in the same subgraph. 2. Each edge should have non-negligible probability to be sampled. For requirement (1), an ideal SAMPLE would consider the joint information from node connections as well as attributes. However, the resulting algorithm may have high complexity as it would need to infer the relationships between features. For simplicity, we deï¬ne â€œinï¬‚uenceâ€ from the graph connectivity perspective and design topology based samplers. Requirement (2) leads to better generalization since it enables the neural net to explore the full feature and label space. 3.2 N ORMALIZATION A sampler that preserves connectivity characteristic of Gwill almost inevitably introduce bias into minibatch estimation. In the following, we present normalization techniques to eliminate biases. Analysis of the complete multi-layer GCN is difï¬cult due to non-linear activations. Thus, we analyze the embedding of each layer independently. This is similar to the treatment of layers independently by prior work (Chen et al., 2018b; Huang et al., 2018). Consider a layer-(â„“+ 1)node vand a layer-â„“ node u. If vis sampled (i.e., vâˆˆVs), we can compute the aggregated feature of vas: Î¶(â„“+1) v = âˆ‘ uâˆˆV ËœAv,u Î±u,v ( W(â„“) )T x(â„“) u 1 u|v = âˆ‘ uâˆˆV ËœAv,u Î±u,v Ëœx(â„“) u 1 u|v, (2) where Ëœx(â„“) u = ( W(â„“))T x(â„“) u , and 1 u|v âˆˆ{0,1}is the indicator function given vis in the subgraph (i.e., 1 u|v = 0if vâˆˆVs âˆ§(u,v) Ì¸âˆˆEs; 1 u|v = 1if (u,v) âˆˆEs; 1 u|v not deï¬ned if vÌ¸âˆˆVs). We refer to the constant Î±u,v as aggregator normalization. Deï¬ne pu,v = pv,u as the probability of an edge (u,v) âˆˆE being sampled in a subgraph, and pv as the probability of a node vâˆˆV being sampled. Proposition 3.1. Î¶(â„“+1) v is an unbiased estimator of the aggregation ofvin the full(â„“+ 1)th GCN layer, ifÎ±u,v = pu,v pv . i.e.,E ( Î¶(â„“+1) v ) = âˆ‘ uâˆˆV ËœAv,u Ëœx(â„“) u . Assuming that each layer independently learns an embedding, we use Proposition 3.1 to normalize feature propagation of each layer of the GCN built byGraphSAINT. Further, let Lv be the loss on vin the output layer. The minibatch loss is calculated as Lbatch = âˆ‘ vâˆˆGs Lv/Î»v, where Î»v is a constant that we term loss normalization. We set Î»v = |V|Â· pv so that: E(Lbatch) = 1 |G| âˆ‘ GsâˆˆG âˆ‘ vâˆˆVs Lv Î»v = 1 |V| âˆ‘ vâˆˆV Lv. (3) Feature propagation within subgraphs thus requires normalization factors Î±and Î», which are com- puted based on the edge and node probability pu,v, pv. In the case of random node or random edge samplers, pu,v and pv can be derived analytically. For other samplers in general, closed form expression is hard to obtain. Thus, we perform pre-processing for estimation. Before training starts, 4Published as a conference paper at ICLR 2020 we run the sampler repeatedly to obtain a set of N subgraphs G. We setup a counter Cv and Cu,v for each vâˆˆV and (u,v) âˆˆE, to count the number of times the node or edge appears in the subgraphs of G. Then we set Î±u,v = Cu,v Cv = Cv,u Cv and Î»v = Cv N . The subgraphs Gs âˆˆG can all be reused as minibatches during training. Thus, the overhead of pre-processing is small (see Appendix D.2). 3.3 V ARIANCE We derive samplers for variance reduction. Letebe the edge connectingu, v, and b(â„“) e = ËœAv,u Ëœx(â„“âˆ’1) u + ËœAu,v Ëœx(â„“âˆ’1) v . It is desirable that variance of all estimators Î¶(â„“) v is small. With this objective, we deï¬ne: Î¶ = âˆ‘ â„“ âˆ‘ vâˆˆGs Î¶(â„“) v pv = âˆ‘ â„“ âˆ‘ v,u ËœAv,u pvÎ±u,v Ëœx(â„“) u 1 v1 u|v = âˆ‘ â„“ âˆ‘ e b(â„“) e pe 1 (â„“) e . (4) where 1 e = 1if eâˆˆEs; 1 e = 0if eÌ¸âˆˆEs. And 1 v = 1if vâˆˆVs; 1 v = 0if vÌ¸âˆˆVs. The factor pu in the ï¬rst equality is present so that Î¶is an unbiased estimator of the sum of all node aggregations at all layers: E(Î¶) =âˆ‘ â„“ âˆ‘ vâˆˆVE ( Î¶(â„“) v ) . Note that 1 (â„“) e = 1 e,âˆ€â„“, since once an edge is present in the sampled graph, it is present in all layers of our GCN. We deï¬ne the optimal edge sampler to minimize variance for every dimension of Î¶. We restrict ourselves to independent edge sampling. For each eâˆˆE, we make independent decision on whether it should be in Gs or not. The probability of including eis pe. We further constrain âˆ‘pe = m, so that the expected number of sampled edges equals to m. The budget mis a given sampling parameter. Theorem 3.2. Under independent edge sampling with budgetm, the optimal edge probabilities to minimize the sum of variance of eachÎ¶â€™s dimension is given by:pe = mâˆ‘ eâ€² îµ¹îµ¹îµ¹âˆ‘ â„“ b(â„“) eâ€² îµ¹îµ¹îµ¹ îµ¹îµ¹îµ¹âˆ‘ â„“ b(â„“) e îµ¹îµ¹îµ¹. To prove Theorem 3.2, we make use of the independence among graph edges, and the dependence among layer edges to obtain the covariance of 1 (â„“) e . Then using the fact that sum of pe is a constant, we use the Cauchy-Schwarz inequality to derive the optimal pe. Details are in Appendix A. Note that calculating b(â„“) e requires computing Ëœx(â„“âˆ’1) v , which increases the complexity of sampling. As a reasonable simpliï¬cation, we ignore Ëœx(â„“) v to make the edge probability dependent on the graph topology only. Therefore, we choose pe âˆ ËœAv,u + ËœAu,v = 1 deg(u) + 1 deg(v) . The derived optimal edge sampler agrees with the intuition in Section 3.1. If two nodes u, v are connected and they have few neighbors, then uand vare likely to be inï¬‚uential to each other. In this case, the edge probability pu,v = pv,u should be high. The above analysis on edge samplers also inspires us to design other samplers, which are presented in Section 3.4. Remark We can also apply the above edge sampler to perform layer sampling. Under the indepen- dent layer sampling assumption of Chen et al. (2018b), one would sample a connection ( u(â„“),v(â„“+1)) with probability p(â„“) u,v âˆ 1 deg(u) + 1 deg(v) . For simplicity, assume a uniform degree graph (of degree d). Then p(â„“) e = p. For an already sampled u(â„“) to connect to layer â„“+ 1, at least one of its edges has to be selected by the layer â„“+ 1sampler. Clearly, the probability of an input layer node to â€œsurviveâ€ the Lnumber of independent sampling process is ( 1 âˆ’(1 âˆ’p)d )Lâˆ’1 . Such layer sampler potentially returns an overly sparse minibatch for L> 1. On the other hand, connectivity within a minibatch of GraphSAINT never drops with GCN depth. If an edge is present in layer â„“, it is present in all layers. 3.4 S AMPLERS Based on the above variance analysis, we present several light-weight and efï¬cient samplers that GraphSAINT has integrated. Detailed sampling algorithms are listed in Appendix B. Random node sampler We sample |Vs|nodes from Vrandomly, according to a node probability distribution P(u) âˆ îµ¹îµ¹îµ¹ËœA:,u îµ¹îµ¹îµ¹ 2 . This sampler is inspired by the layer sampler of Chen et al. (2018b). 5Published as a conference paper at ICLR 2020 Random edge sampler We perform edge sampling as described in Section 3.3. Random walk based samplersAnother way to analyze graph sampling based multi-layer GCN is to ignore activations. In such case, Llayers can be represented as a single layer with edge weights given by B = ËœAL. Following a similar approach as Section 3.3, if it were possible to pick pairs of nodes (whether or not they are directly connected in the original ËœA) independently, then we would set pu,v âˆBu,v + Bv,u, where Bu,v can be interpreted as the probability of a random walk to start at uand end at v in Lhops (and Bv,u vice-versa). Even though it is not possible to sample a subgraph where such pairs of nodes are independently selected, we still consider a random walk sampler with walk length Las a good candidate for L-layer GCNs. There are numerous random walk based samplers proposed in the literature (Ribeiro & Towsley, 2010; Leskovec & Faloutsos, 2006; Hu & Lau, 2013; Li et al., 2015). In the experiments, we implement a regular random walk sampler (with rroot nodes selected uniformly at random and each walker goes hhops), and also a multi-dimensional random walk sampler deï¬ned in Ribeiro & Towsley (2010). For all the above samplers, we return the subgraph induced from the sampled nodes. The induction step adds more connections into the subgraph, and empirically helps improve convergence. 4 D ISCUSSION Extensions GraphSAINT admits two orthogonal extensions. First, GraphSAINT can seamlessly integrate other graph samplers. Second, the idea of training by graph sampling is applicable to many GCN architecture variants: 1. Jumping knowledge(Xu et al., 2018): since our GCNs constructed during training are complete, applying skip connections to GraphSAINT is straightforward. On the other hand, for some layer sampling methods (Chen et al., 2018b; Huang et al., 2018), extra modiï¬cation to their samplers is required, since the jumping knowledge architecture requires layer-â„“ samples to be a subset of layer-(â„“âˆ’1) samplesâˆ—. 2. Attention (VeliË‡ckoviÂ´c et al., 2017; Fey, 2019; Zhang et al., 2018): while explicit variance reduction is hard due to the dynamically updated attention values, it is reasonable to apply attention within the subgraphs which are considered as representatives of the full graph. Our loss and aggregator normalizations are also applicableâ€ . 3. Others: To support high order layers (Zhou, 2017; Lee et al., 2018; Abu-El-Haija et al., 2019) or even more complicated networks for the task of graph classiï¬cation (Ying et al., 2018b), we replace the full adjacency matrix A with the (normalized) one for the subgraph As to perform layer propagation. Comparison GraphSAINT enjoys: 1. high scalability and efï¬ciency, 2. high accuracy, and 3. low training complexity. Point (1) is due to the signiï¬cantly reduced neighborhood size compared with Hamilton et al. (2017); Ying et al. (2018a); Chen et al. (2018a). Point (2) is due to the better inter- layer connectivity compared with Chen et al. (2018b), and unbiased minibatch estimator compared with Chiang et al. (2019). Point (3) is due to the simple and trivially parallelizable pre-processing compared with the sampling of Huang et al. (2018) and clustering of Chiang et al. (2019). 5 E XPERIMENTS Setup Experiments are under the inductive, supervised learning setting. We evaluate GraphSAINT on the following tasks: 1. classifying protein functions based on the interactions of human tissue proteins (PPI), 2. categorizing types of images based on the descriptions and common properties of online images (Flickr), 3. predicting communities of online posts based on user comments (Reddit), 4. categorizing types of businesses based on customer reviewers and friendship (Yelp), and 5. classifying product categories based on buyer reviewers and interactions (Amazon). For PPI, we use the small version for the two layer convergence comparison (Table 2 and Figure 2), since Hamilton et al. (2017) and Chen et al. (2018a) report accuracy for this version in their original papers. We use the large version for additional comparison with Chiang et al. (2019) to be consistent with its reported accuracy. All datasets follow â€œï¬xed-partitionâ€ splits. Appendix C.2 includes further details. âˆ—The skip-connection design proposed by Huang et al. (2018) does not have such â€œsubsetâ€ requirement, and thus is compatible with both graph sampling and layer sampling based methods. â€ When applying GraphSAINT to GAT (VeliË‡ckoviÂ´c et al., 2017), we remove the softmax step which normalizes attention values within the same neighborhood, as suggested by Huang et al. (2018). See Appendix C.3. 6Published as a conference paper at ICLR 2020 Table 1: Dataset statistics (â€œmâ€ stands formulti-class classiï¬cation, and â€œsâ€ for single-class.) Dataset Nodes Edges Degree Feature Classes Train / Val / Test PPI 14,755 225,270 15 50 121 (m) 0.66 / 0.12 / 0.22 Flickr 89,250 899,756 10 500 7 (s) 0.50 / 0.25 / 0.25 Reddit 232,965 11,606,919 50 602 41 (s) 0.66 / 0.10 / 0.24 Yelp 716,847 6,977,410 10 300 100 (m) 0.75 / 0.10 / 0.15 Amazon 1,598,960 132,169,734 83 200 107 (m) 0.85 / 0.05 / 0.10 PPI (large version) 56,944 818,716 14 50 121 (m) 0.79 / 0.11 / 0.10 We open source GraphSAINTâ€¡. We compare with six baselines: 1. vanilla GCN (Kipf & Welling, 2016), 2. GraphSAGE (Hamilton et al., 2017), 3. FastGCN (Chen et al., 2018b), 4. S-GCN (Chen et al., 2018a), 5. AS-GCN (Huang et al., 2018), and 6. ClusterGCN (Chiang et al., 2019). All baselines are executed with their ofï¬cially released code (see Appendix C.3 for downloadable URLs and commit numbers). Baselines and GraphSAINT are all implemented in Tensorï¬‚ow with Python3. We run experiments on a NVIDIA Tesla P100 GPU (see Appendix C.1 for hardware speciï¬cation). 5.1 C OMPARISON WITH STATE-OF-THE -ART Table 2 and Figure 2 show the accuracy and convergence comparison of various methods. All results correspond to two-layer GCN models (for GraphSAGE, we use its mean aggregator). For a given dataset, we keep hidden dimension the same across all methods. We describe the detailed architecture and hyperparameter search procedure in Appendix C.3. The mean and conï¬dence interval of the accuracy values in Table 2 are measured by three runs under the same hyperparameters. The training time of Figure 2 excludes the time for data loading, pre-processing, validation set evaluation and model saving. Our pre-processing incurs little overhead in training time. See Appendix D.2 for cost of graph sampling. For GraphSAINT, we implement the graph samplers described in Section 3.4. In Table 2, â€œNodeâ€ stands for random node sampler; â€œEdgeâ€ stands for random edge sampler; â€œRWâ€ stands for random walk sampler; â€œMRWâ€ stands for multi-dimensional random walk sampler. Table 2: Comparison of test set F1-micro score with state-of-the-art methods Method PPI Flickr Reddit Yelp Amazon GCN 0.515 Â±0.006 0.492 Â±0.003 0.933 Â±0.000 0.378 Â±0.001 0.281 Â±0.005 GraphSAGE 0.637 Â±0.006 0.501 Â±0.013 0.953 Â±0.001 0.634 Â±0.006 0.758 Â±0.002 FastGCN 0.513 Â±0.032 0.504 Â±0.001 0.924 Â±0.001 0.265 Â±0.053 0.174 Â±0.021 S-GCN 0.963 Â±0.010 0.482 Â±0.003 0.964 Â±0.001 0.640 Â±0.002 â€” â€¡ AS-GCN 0.687 Â±0.012 0.504 Â±0.002 0.958 Â±0.001 â€” â€¡ â€” â€¡ ClusterGCN 0.875 Â±0.004 0.481 Â±0.005 0.954 Â±0.001 0.609 Â±0.005 0.759 Â±0.008 GraphSAINT-Node 0.960Â±0.001 0.507 Â±0.001 0.962 Â±0.001 0.641 Â±0.000 0.782 Â±0.004 GraphSAINT-Edge 0.981Â±0.007 0.510 Â±0.002 0.966Â±0.001 0.653Â±0.003 0.807 Â±0.001 GraphSAINT-RW 0.981Â±0.004 0.511Â±0.001 0.966Â±0.001 0.653Â±0.003 0.815Â±0.001 GraphSAINT-MRW 0.980Â±0.006 0.510 Â±0.001 0.964 Â±0.000 0.652 Â±0.001 0.809 Â±0.001 Table 3: Additional comparison with ClusterGCN (test set F1-micro score) PPI (large version) Reddit 2 Ã—512 5 Ã—2048 2 Ã—128 4 Ã—128 ClusterGCN 0.903 Â±0.002 0.994 Â±0.000 0.954 Â±0.001 0.966 Â±0.001 GraphSAINT 0.941Â±0.003 0.995Â±0.000 0.966Â±0.001 0.970Â±0.001 â€¡Open sourced code: https://github.com/GraphSAINT/GraphSAINT â€¡The codes throw runtime error on the large datasets (Yelp or Amazon). 7Published as a conference paper at ICLR 2020 0 20 40 600.4 0.6 0.8 1 Validation F1-micro PPI 0 10 20 30 40 0.44 0.46 0.48 0.5 0.52 Flickr 0 50 100 1500.9 0.92 0.94 0.96 0.98 Reddit 0 200 400 600 800 0.25 0.45 0.65 Yelp 0 200 400 0.2 0.4 0.6 0.8 Training time (second) Amazon GCN GraphSAGE FastGCN* S-GCN AS-GCN ClusterGCN GraphSAINT *: CPU execution time -RW Figure 2: Convergence curves of 2-layer models on GraphSAINT and baselines Clearly, with appropriate graph samplers, GraphSAINT achieves signiï¬cantly higher accuracy on all datasets. For GraphSAINT-Node, we use the same node probability as FastGCN. Thus, the accuracy improvement is mainly due to the switching from layer sampling to graph sampling (see â€œRemarkâ€ in Section 3.3). Compared with AS-GCN, GraphSAINT is signiï¬cantly faster. The sampler of AS-GCN is expensive to execute, making its overall training time even longer than vanilla GCN. We provide detailed computation complexity analysis on the sampler in Appendix D.2. For S-GCN on Reddit, it achieves similar accuracy as GraphSAINT, at the cost of over 9Ã—longer training time. The released code of FastGCN only supports CPU execution, so its convergence curve is dashed. Table 3 presents additional comparison with ClusterGCN. We useLÃ—f to specify the architecture, where Land f denote GCN depth and hidden dimension, respectively. The four architectures are the ones used in the original paper (Chiang et al., 2019). Again, GraphSAINT achieves signiï¬cant accuracy improvement. To train models with L> 2 often requires additional architectural tweaks. ClusterGCN uses its diagonal enhancement technique for the 5-layer PPI and 4-layer Reddit models. GraphSAINT uses jumping knowledge connection (Xu et al., 2018) for 4-layer Reddit. Evaluation on graph samplers From Table 2, random edge and random walk based samplers achieve higher accuracy than the random node sampler. Figure 3 presents sensitivity analysis on parameters of â€œRWâ€. We use the same hyperparameters (except the sampling parameters) and network architecture as those of the â€œRWâ€ entries in Table 2. We ï¬x the length of each walker to2 (i.e., GCN depth), and vary the number of roots rfrom 250 to 2250. For PPI, increasing rfrom 250 to 750 signiï¬cantly improves accuracy. Overall, for all datasets, accuracy stabilizes beyond r= 750. 5.2 GraphSAINT ON ARCHITECTURE VARIANTS AND DEEP MODELS In Figure 4, we train a 2-layer and a 4-layer model of GAT (VeliË‡ckoviÂ´c et al., 2017) and JK-net (Xu et al., 2018), by using minibatches of GraphSAGE and GraphSAINT respectively. On the two 4-layer architectures, GraphSAINT achieves two orders of magnitude speedup than GraphSAGE, indicating much better scalability on deep models. From accuracy perspective, 4-layer GAT-SAGE and JK- SAGE do not outperform the corresponding 2-layer versions, potentially due to the smoothening effect caused by the massive neighborhood size. On the other hand, with minibatches returned by our edge sampler, increasing model depth of JK-SAINT leads to noticeable accuracy improvement (from 0.966 of 2-layer to 0.970 of 4-layer). Appendix D.1 contains additional scalability results. 6 C ONCLUSION We have presented GraphSAINT, a graph sampling based training method for deep GCNs on large graphs. We have analyzed bias and variance of the minibatches deï¬ned on subgraphs, and proposed 8Published as a conference paper at ICLR 2020 0 1,000 2,0000.4 0.6 0.8 1 Number of walkers Test F1-micro PPI Flickr Reddit Yelp Amazon Figure 3: Sensitivity analysis 100 102 104 0.93 0.94 0.95 0.96 0.97 Training time (second) Validation F1-micro GAT 100 102 104 JK-net GraphSAINT 2-layer GraphSAINT 4-layer GraphSAGE 2-layer GraphSAGE 4-layer Figure 4: GraphSAINT with JK-net and GAT (Reddit) normalization techniques and sampling algorithms to improve training quality. We have conducted extensive experiments to demonstrate the advantage of GraphSAINT in accuracy and training time. An interesting future direction is to develop distributed training algorithms using graph sampling based minibatches. After partitioning the training graph in distributed memory, sampling can be performed independently on each processor. Afterwards, training on the self-supportive subgraphs can signiï¬cantly reduce the system-level communication cost. To ensure the overall convergence quality, data shufï¬‚ing strategy for the graph nodes and edges can be developed together with each speciï¬c graph sampler. Another direction is to perform algorithm-system co-optimization to accelerate the training of GraphSAINT on heterogeneous computing platforms (Zeng et al., 2018; Zeng & Prasanna, 2019). The resolution of â€œneighbor explosionâ€ by GraphSAINT not only reduces the training computation complexity, but also improves hardware utilization by signiï¬cantly less data trafï¬c to the slow memory. In addition, task-level parallelization is easy since the light-weight graph sampling is completely decoupled from the GCN layer propagation. ACKNOWLEDGEMENT This material is based on work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract Number FA8750-17-C-0086 and National Science Foundation (NSF) under Contract Numbers CCF-1919289 and OAC-1911229. Any opinions, ï¬ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reï¬‚ect the views of DARPA or NSF. REFERENCES Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Hrayr Harutyunyan, Nazanin Alipourfard, Kristina Lerman, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolution architec- tures via sparsiï¬ed neighborhood mixing. arXiv preprint arXiv:1905.00067, 2019. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. CoRR, abs/1312.6203, 2013. URL http://arxiv.org/abs/ 1312.6203. HongYun Cai, Vincent W. Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph embedding: Problems, techniques and applications. CoRR, abs/1709.07604, 2017. URL http://arxiv.org/abs/1709.07604. Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with variance reduction. In ICML, pp. 941â€“949, 2018a. Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations (ICLR), 2018b. Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An ef- ï¬cient algorithm for training deep and large graph convolutional networks. CoRR, abs/1905.07953, 2019. URL http://arxiv.org/abs/1905.07953. 9Published as a conference paper at ICLR 2020 MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ï¬ltering. In Advances in Neural Information Processing Systems, pp. 3844â€“3852, 2016. Matthias Fey. Just jump: Dynamic neighborhood aggregation in graph neural networks. CoRR, abs/1904.04849, 2019. URL http://arxiv.org/abs/1904.04849. Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD â€™18, pp. 1416â€“1424, New York, NY , USA, 2018. ACM. ISBN 978-1-4503-5552-0. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems 30, pp. 1024â€“1034. 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385. Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735â€“1780, 1997. Pili Hu and Wing Cheong Lau. A survey and taxonomy of graph sampling. arXiv preprint arXiv:1308.5865, 2013. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700â€“4708, 2017. Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, pp. 4558â€“4567, 2018. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Thomas N. Kipf and Max Welling. Semi-supervised classiï¬cation with graph convolutional networks. CoRR, abs/1609.02907, 2016. URL http://arxiv.org/abs/1609.02907. Johannes Klicpera, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. Personalized embedding propa- gation: Combining neural networks on graphs with personalized pagerank. CoRR, abs/1810.05997, 2018. URL http://arxiv.org/abs/1810.05997. John Boaz Lee, Ryan A. Rossi, Xiangnan Kong, Sungchul Kim, Eunyee Koh, and Anup Rao. Higher- order graph convolutional networks. CoRR, abs/1809.07697, 2018. URL http://arxiv.org/ abs/1809.07697. Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 631â€“636. ACM, 2006. R. Li, J. X. Yu, L. Qin, R. Mao, and T. Jin. On random walk based graph sampling. In 2015 IEEE 31st International Conference on Data Engineering, pp. 927â€“938, April 2015. doi: 10.1109/ICDE. 2015.7113345. Haonan Lu, Seth H. Huang, Tian Ye, and Xiuyan Guo. Graph star net for generalized multi-task learning. CoRR, abs/1906.12330, 2019. URL http://arxiv.org/abs/1906.12330. Bruno Ribeiro and Don Towsley. Estimating and sampling graphs with multidimensional random walks. In Proceedings of the 10th ACM SIGCOMM conference on Internet measurement, pp. 390â€“403. ACM, 2010. Petar VeliË‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. 10Published as a conference paper at ICLR 2020 Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. CoRR, abs/1901.00596, 2019. URL http: //arxiv.org/abs/1901.00596. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536, 2018. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD â€™18, 2018a. ISBN 978-1-4503-5552-0. Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In Proceedings of the 32Nd International Conference on Neural Information Processing Systems, NIPSâ€™18, pp. 4805â€“4815, USA, 2018b. Curran Associates Inc. URL http://dl.acm.org/citation.cfm?id= 3327345.3327389. Hanqing Zeng and Viktor Prasanna. GraphACT: Accelerating GCN training on CPU-FPGA hetero- geneous platforms. arXiv preprint arXiv:2001.02498, 2019. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Accurate, efï¬cient and scalable graph embedding. CoRR, abs/1810.11899, 2018. URL http: //arxiv.org/abs/1810.11899. Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated atten- tion networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018. Zhenpeng Zhou. Graph convolutional networks for molecules. CoRR, abs/1706.09916, 2017. URL http://arxiv.org/abs/1706.09916. A P ROOFS Proof of Proposition 3.1.Under the condition that vis sampled in a subgraph: E ( Î¶(â„“+1) v ) =E (âˆ‘ uâˆˆV ËœAv,u Î±u,v Ëœx(â„“) u 1 u|v ) = âˆ‘ uâˆˆV ËœAv,u Î±u,v Ëœx(â„“) u E ( 1 u|v ) = âˆ‘ uâˆˆV ËœAv,u Î±u,v Ëœx(â„“) u P((u,v) sampled|vsampled) = âˆ‘ uâˆˆV ËœAv,u Î±u,v Ëœx(â„“) u P((u,v) sampled) P(vsampled) = âˆ‘ uâˆˆV ËœAv,u Î±u,v Ëœx(â„“) u pu,v pv (5) where the second equality is due to linearity of expectation, and the third equality (conditional edge probability) is due to the initial condition that vis sampled in a subgraph. It directly follows that, when Î±u,v = pu,v pv , E ( Î¶(â„“+1) v ) = âˆ‘ uâˆˆV ËœAv,u Ëœx(â„“) u 11Published as a conference paper at ICLR 2020 Proof of Theorem 3.2.Below, we use Cov (Â·) to denote covariance and Var(Â·) to denote variance. For independent edge sampling as deï¬ned in Section 3.3, Cov ( 1 (â„“1) e1 ,1 (â„“2) e2 ) = 0,âˆ€e1 Ì¸= e2. And for a full GCN on the subgraph, Cov ( 1 (â„“1) e ,1 (â„“2) e ) = pe âˆ’p2 e. To start the proof, we ï¬rst assume that the b(â„“) e is one dimensional (i.e., a scalar) and denote it by b(â„“) e . Now, Var(Î¶) = âˆ‘ e,â„“ ( b(â„“) e pe )2 Var ( 1 (â„“) e ) + 2 âˆ‘ e,â„“1<â„“2 b(â„“1) e b(â„“2) e p2e Cov ( 1 (â„“1) e ,1 (â„“2) e ) = âˆ‘ e,â„“ ( b(â„“) e )2 pe âˆ’ âˆ‘ e,â„“ ( b(â„“) e )2 + 2 âˆ‘ e,â„“1<â„“2 b(â„“1) e b(â„“2) e p2e ( pe âˆ’p2 e ) = âˆ‘ e (âˆ‘ â„“ b(â„“) e )2 pe âˆ’ âˆ‘ e (âˆ‘ â„“ b(â„“) e )2 (6) Let a given constant m= âˆ‘ e pe be the expected number of sampled edges. By Cauchy-Schwarz in- equality: âˆ‘ e ( âˆ‘ â„“ b(â„“) e ) 2 pe m= âˆ‘ e (âˆ‘ â„“ b(â„“) eâˆšpe )2 âˆ‘ e (âˆšpe )2 â‰¥ (âˆ‘ e,â„“ b(â„“) e )2 . The equality is achieved when âââ âˆ‘ â„“ b(â„“) eâˆšpe ââââˆâˆšpe. i.e., variance is minimized when pe âˆ ââââˆ‘ â„“ b(â„“) e âââ. It directly follows that: pe = m âˆ‘ eâ€² ââââˆ‘ â„“ b(â„“) eâ€² âââ âââââ âˆ‘ â„“ b(â„“) e âââââ For the multi-dimensional case of b(â„“) e , following similar steps as above, it is easy to show that the optimal edge probability to minimize âˆ‘ i Var(Î¶i) (where iis the index for Î¶â€™s dimensions) is: pe = m âˆ‘ eâ€² îµ¹îµ¹îµ¹âˆ‘ â„“ b(â„“) eâ€² îµ¹îµ¹îµ¹ îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ â„“ b(â„“) e îµ¹îµ¹îµ¹îµ¹îµ¹ B S AMPLING ALGORITHM Algorithm 2 lists the four graph samplers we have integrated into GraphSAINT. The naming of the samplers follows that of Table 2. Note that the sampling parameters nand mspecify a budget rather than the actual number of nodes and edges in the subgraph Gs. Since certain nodes or edges in the training graph Gmay be repeatedly sampled under a single invocation of the sampler, we often have |Vs|<n for node and MRW samplers, |Vs|<2mfor edge sampler, and |Vs|<r Â·hfor RW sampler. Also note that the edge sampler presented in Algorithm 2 is an approximate version of the independent edge sampler deï¬ned in Section 3.4. Complexity (excluding the subgraph induction step) of the original version in Section 3.4 is O(|E|), while complexity of the approximate one is O(m). When mâ‰ª|E|, the approximate version leads to identical accuracy as the original one, for a given m. C D ETAILED EXPERIMENTAL SETUP C.1 H ARDWARE SPECIFICATION AND ENVIRONMENT We run our experiments on a single machine with Dual Intel Xeon CPUs (E5-2698 v4 @ 2.2Ghz), one NVIDIA Tesla P100 GPU (16GB of HBM2 memory) and 512GB DDR4 memory. The code is written in Python 3.6.8 (where the sampling part is written with Cython 0.29.2). We use Tensorï¬‚ow 1.12.0 on CUDA 9.2 with CUDNN 7.2.1 to train the model on GPU. Since the subgraphs are sampled independently, we run the sampler in parallel on 40 CPU cores. 12Published as a conference paper at ICLR 2020 Algorithm 2Graph sampling algorithms by GraphSAINT Input: Training graph G(V,E); Sampling parameters: node budget n; edge budget m; number of roots r; random walk length h Output: Sampled graph Gs (Vs,Es) 1: function NODE (G,n) âŠ¿Node sampler 2: P(v) := îµ¹îµ¹îµ¹ËœA:,v îµ¹îµ¹îµ¹ 2 /âˆ‘ vâ€²âˆˆV îµ¹îµ¹îµ¹ËœA:,vâ€² îµ¹îµ¹îµ¹ 2 3: Vs â†nnodes randomly sampled (with replacement) from Vaccording to P 4: Gs â†Node induced subgraph of Gfrom Vs 5: end function 6: function EDGE (G,m) âŠ¿Edge sampler (approximate version) 7: P((u,v)) := ( 1 deg(u) + 1 deg(v) ) /âˆ‘ (uâ€²,vâ€²)âˆˆE ( 1 deg(uâ€²) + 1 deg(vâ€²) ) 8: Es â†medges randomly sampled (with replacement) from Eaccording to P 9: Vs â†Set of nodes that are end-points of edges in Es 10: Gs â†Node induced subgraph of Gfrom Vs 11: end function 12: function RW(G,r,h) âŠ¿Random walk sampler 13: Vroot â†rroot nodes sampled uniformly at random (with replacement) from V 14: Vs â†Vroot 15: for vâˆˆVroot do 16: uâ†v 17: for d= 1to hdo 18: uâ†Node sampled uniformly at random from uâ€™s neighbor 19: Vs â†Vs âˆª{u} 20: end for 21: end for 22: Gs â†Node induced subgraph of Gfrom Vs 23: end function 24: function MRW(G,n,r) âŠ¿Multi-dimensional random walk sampler 25: VFS â†rroot nodes sampled uniformly at random (with replacement) from V 26: Vs â†VFS 27: for i= r+ 1to ndo 28: Select uâˆˆVFS with probability deg(u)/âˆ‘ vâˆˆVFS deg(v) 29: uâ€²â†Node randomly sampled from uâ€™s neighbor 30: VFS â†(VFS \\{u}) âˆª{uâ€²} 31: Vs â†Vs âˆª{u} 32: end for 33: Gs â†Node induced subgraph of Gfrom Vs 34: end function 13Published as a conference paper at ICLR 2020 100 101 102 103 104 105 10âˆ’6 10âˆ’4 10âˆ’2 100 Degree P(degree â‰¥k) PPI Flickr Reddit Yelp Amazon Figure 5: Degree Distribution C.2 A DDITIONAL DATASET DETAILS Here we present the detailed procedures to prepare the Flickr, Yelp and Amazon datasets. The Flickr dataset originates from NUS-wide Â§. The SNAP website Â¶collected Flickr data from four different sources including NUS-wide, and generated an un-directed graph. One node in the graph represents one image uploaded to Flickr. If two images share some common properties (e.g., same geographic location, same gallery, comments by the same user, etc.), there is an edge between the nodes of these two images. We use as the node features the 500-dimensional bag-of-word representation of the images provided by NUS-wide. For labels, we scan over the 81 tags of each image and manually merged them to 7 classes. Each image belongs to one of the 7 classes. The Yelp dataset is prepared from the rawjson data of businesses, users and reviews provided in the open challenge websiteâˆ¥. For nodes and edges, we scan the friend list of each user in the raw json ï¬le of users. If two users are friends, we create an edge between them. We then ï¬lter out all the reviews by each user and separate the reviews into words. Each review word is converted to a 300-dimensional vector using the Word2Vec model pre-trained on GoogleNewsâˆ—âˆ—. The word vectors of each node are added and normalized to serve as the node feature (i.e., xv). As for the node labels, we scan the raw json ï¬le of businesses, and use the categories of the businesses reviewed by a user vas the multi-class label of v. For the Amazon dataset, a node is a product on the Amazon website and an edge (u,v) is created if products uand vare bought by the same customer. Each product contains text reviews (converted to 4-gram) from the buyer. We use SVD to reduce the dimensionality of the 4-gram representation to 200, and use the obtained vectors as the node feature. The labels represent the product categories (e.g., books, movies, shoes). Figure 5 shows the degree distribution of the ï¬ve graphs. A point (k,p) in the plot means the probability of a node having degree at least kis p. C.3 A DDITIONAL DETAILS IN EXPERIMENTAL CONFIGURATION Table 4 summarizes the URLs to download the baseline codes. The optimizer for GraphSAINT and all baselines is Adam (Kingma & Ba, 2014). For all baselines and datasets, we perform grid search on the hyperparameter space deï¬ned by: â€¢Hidden dimension: {128,256,512} Â§http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm Â¶https://snap.stanford.edu/data/web-flickr.html âˆ¥https://www.yelp.com/dataset âˆ—âˆ—https://code.google.com/archive/p/word2vec/ 14Published as a conference paper at ICLR 2020 Table 4: URLs and commit number to run baseline codes Baseline URL Commit Vanilla GCN github.com/williamleif/GraphSAGE a0fdef GraphSAGE github.com/williamleif/GraphSAGE a0fdef FastGCN github.com/matenure/FastGCN b8e6e6 S-GCN github.com/thu-ml/stochastic_gcn da7b78 AS-GCN github.com/huangwb/AS-GCN 5436ec ClusterGCNgithub.com/google-research/google-research/tree/master/cluster_gcn99021e Table 5: Training conï¬guration of GraphSAINT for Table 2 Sampler Dataset Training Sampling Learning rate Dropout Node budget Edge budget Roots Walk length Node PPI 0.01 0.0 6000 â€” â€” â€” Flickr 0.01 0.2 8000 â€” â€” â€” Reddit 0.01 0.1 8000 â€” â€” â€” Yelp 0.01 0.1 5000 â€” â€” â€” Amazon 0.01 0.1 4500 â€” â€” â€” Edge PPI 0.01 0.1 â€” 4000 â€” â€” Flickr 0.01 0.2 â€” 6000 â€” â€” Reddit 0.01 0.1 â€” 6000 â€” â€” Yelp 0.01 0.1 â€” 2500 â€” â€” Amazon 0.01 0.1 â€” 2000 â€” â€” RW PPI 0.01 0.1 â€” â€” 3000 2 Flickr 0.01 0.2 â€” â€” 6000 2 Reddit 0.01 0.1 â€” â€” 2000 4 Yelp 0.01 0.1 â€” â€” 1250 2 Amazon 0.01 0.1 â€” â€” 1500 2 MRW PPI 0.01 0.1 8000 â€” 2500 â€” Flickr 0.01 0.2 12000 â€” 3000 â€” Reddit 0.01 0.1 8000 â€” 1000 â€” Yelp 0.01 0.1 2500 â€” 1000 â€” Amazon 0.01 0.1 4500 â€” 1500 â€” â€¢Dropout: {0.0,0.1,0.2,0.3} â€¢ Learning rate: {0.1,0.01,0.001,0.0001} The hidden dimensions used for Table 2, Figure 2, Figure 3 and Figure 4 are: 512 for PPI, 256 for Flickr, 128 for Reddit, 512 for Yelp and 512 for Amazon. All methods terminate after a ï¬xed number of epochs based on convergence. We save the model producing the highest validation set F1-micro score, and reload it to evaluate the test set accuracy. For vanilla GCN and AS-GCN, we set the batch size to their default value 512. For GraphSAGE, we use the mean aggregator with the default batch size 512. For S-GCN, we set the ï¬‚ag -cv -cvd (which stand for â€œcontrol variateâ€ and â€œcontrol variate dropoutâ€) with pre-computation of the ï¬rst layer aggregation. According to the paper (Chen et al., 2018a), such pre-computation signiï¬cantly reduces training time without affecting accuracy. For S-GCN, we use the default batch size 1000, and for FastGCN, we use the default value 400. For ClusterGCN, its batch size is determined by two parameters: the cluster size and the number of clusters per batch. We sweep the cluster size from 500 to 10000 with step 500, and the number of clusters per batch from {1,10,20,40}to determine the optimal conï¬guration for each dataset / architecture. Considering that for ClusterGCN, the cluster structure may be sensitive to the cluster size, and for FastGCN, the minibatch connectivity may increase with the sample size, we present additional experimental results to reveal the relation between accuracy and batch size in Appendix D.3. 15Published as a conference paper at ICLR 2020 Table 6: Training conï¬guration of GraphSAINT for Table 3 Arch. Sampler Dataset Training Sampling Learning rate Dropout Node budget Edge budget Roots Walk length 2Ã—512 MRW PPI (large) 0.01 0.1 1500 â€” 300 â€” 5Ã—2048 RW PPI (large) 0.01 0.1 â€” â€” 3000 2 2Ã—128 Edge Reddit 0.01 0.1 â€” 6000 â€” â€” 4Ã—128 Edge Reddit 0.01 0.2 â€” 11000 â€” â€” Table 7: Training conï¬guration of GraphSAINT for Figure 4 (Reddit) 2-layer GAT-SAINT 4-layer GAT-SAINT 2-layer JK-SAINT 4-layer JK-SAINT Hidden dimension 128 128 128 128 AttentionK 8 8 â€” â€” Aggregationâ¨ â€” â€” Concat. Concat. Sampler RW RW Edge Edge (root: 3000; length: 2) (root: 2000; length: 4) (budget: 6000) (budget: 11000) Learning rate 0.01 0.01 0.01 0.01 Dropout 0.2 0.2 0.1 0.2 Conï¬guration of GraphSAINT to reproduce Table 2 results is shown in Table 5. Conï¬guration of GraphSAINT to reproduce Table 3 results is shown in Table 6. Below we describe the conï¬guration for Figure 4. The major difference between a normal GCN and a JK-net (Xu et al., 2018) is that JK-net has an additional ï¬nal layer that aggregates all the output hidden features of graph convolutional layers 1 to L. Mathematically, the additional aggregation layer outputs the ï¬nal embedding xJK as follows: xJK = Ïƒ ( WT JK Â· Lâ¨ â„“=1 x(â„“) v ) (7) where based on Xu et al. (2018), â¨is the vector aggregation operator: max-pooling, concatenation or LSTM (Hochreiter & Schmidhuber, 1997) based aggregation. The graph attention of GAT (Veli Ë‡ckoviÂ´c et al., 2017) calculates the edge weights for neighbor aggregation by an additional neural network. With multi-head ( K) attention, the layer- (â„“âˆ’1) features propagate to layer-(â„“) as follows: x(â„“) v = îµ¹îµ¹îµ¹îµ¹îµ¹ K k=1 Ïƒ ï£« ï£­ âˆ‘ uâˆˆneighbor(v) Î±k u,vWkx(â„“âˆ’1) v ï£¶ ï£¸ (8) where âˆ¥is the vector concatenation operation, and the coefï¬cient Î±is calculated with the attention weights ak by: Î±k u,v = LeakyReLU (( ak)T [ Wkxuâˆ¥Wkxv ]) (9) Note that the Î±calculation is slightly different from the original equation in VeliË‡ckoviÂ´c et al. (2017). Namely, GAT-SAINT does not normalize Î±by softmax across all neighbors of v. We make such modiï¬cation since under the minibatch setting, node vdoes not see all its neighbors in the training graph. The removal of softmax is also seen in the attention design of Huang et al. (2018). Note that during the minibatch training, GAT-SAINT further applies another edge coefï¬cient on top of attention for aggregator normalization. Table 7 shows the conï¬guration of the GAT-SAINT and JK-SAINT curves in Figure 4. 16Published as a conference paper at ICLR 2020 2 3 4 5 60 2 4 6 8 GCN depth Normalized training time GraphSAINT: Reddit S-GCN: Reddit GraphSAINT: Yelp S-GCN: Yelp Figure 6: Comparison of training efï¬ciency PPI Flickr Reddit Yelp Amazon 0 0.5 1 1.5 Fraction of training time Node Edge RW MRW Figure 7: Fraction of training time on sampling D A DDITIONAL EXPERIMENTS D.1 T RAINING EFFICIENCY ON DEEP MODELS We evaluate the training efï¬ciency for deeper GCNs. We only compare with S-GCN, since implemen- tations for other layer sampling based methods have not yet supported arbitrary model depth. The batch size and hidden dimension are the same as Table 2. On the two large graphs (Reddit and Yelp), we increase the number of layers and measure the average time per minibatch execution. In Figure 6, training cost of GraphSAINT is approximately linear with GCN depth. Training cost of S-GCN grows dramatically when increasing the depth. This reï¬‚ects the â€œneighbor explosionâ€ phenomenon (even though the expansion factor of S-GCN is just 2). On Yelp, S-GCN gives â€œout-of-memoryâ€ error for models beyond 5 layers. D.2 C OST OF SAMPLING AND PRE-PROCESSING Cost of graph samplers ofGraphSAINT Graph sampling introduces little training overhead. Let ts be the average time to sample one subgraph on a multi-core machine. Let tt be the average time to perform the forward and backward propagation on one minibatch on GPU. Figure 7 shows the ratio ts/tt for various datasets. The parameters of the samplers are the same as Table 2. For Node, Edge and RW samplers, we observe that time to sample one subgraph is in most cases less than 25% of the training time. The MRW sampler is more expensive to execute. Regarding the complete pre-processing procedure, we repeatedly run the sampler for N = 50Â·|V| /|Vs|times before training, to estimate the node and edge probability as discussed in Section 3.2 (where |Vs|is the average subgraph size). These sampled subgraphs are reused as training minibatches. Thus, if training runs for more than N iterations, the pre-processing is nearly zero-cost. Under the setting of Table 2, pre-processing on PPI and Yelp and Amazon does not incur any overhead in training time. Pre-processing on Flickr and Reddit (with RW sampler) takes less than 40% and 15% of their corresponding total training time. Cost of layers sampler of AS-GCNAS-GCN uses an additional neural network to estimate the conditional sampling probability for the previous layer. For a node v already sampled in layer â„“, features of layer-(â„“âˆ’1) corresponding to all vâ€™s neighbors need to be fed to the sampling neural network to obtain the node probability. For sake of analysis, assume the sampling network is a single layer MLP, whose weight WMLP has the same shape as the GCN weights W(â„“). Then we can show, for a L-layer GCN on a degree-dgraph, per epoch training complexity of AS-GCN is approximately Î³ = (dÂ·L) /âˆ‘Lâˆ’1 â„“=0 dâ„“ times that of vanilla GCN. For L = 2, we have Î³ â‰ˆ2. This explains the observation that AS-GCN is slower than vanilla GCN in Figure 2. Additional, Table 8 shows the training time breakdown for AS-GCN. Clearly, its sampler is much more expensive than the graph sampler of GraphSAINT. 17Published as a conference paper at ICLR 2020 Table 8: Per epoch training time breakdown for AS-GCN Dataset Sampling time (sec) Forward / Backward propagation time (sec) PPI 1.1 0.2 Flickr 5.3 1.1 Reddit 20.7 3.5 Cost of clustering of ClusterGCNClusterGCN uses the highly optimized METIS softwareâ€ â€ to perform clustering. Table 9 summarizes the time to obtain the clusters for the ï¬ve graphs. On the large and dense Amazon graph, the cost of clustering increase dramatically. The pre-processing time of ClusterGCN on Amazon is more than 4Ã—of the total training time. On the other hand, the sampling cost of GraphSAINT does not increase signiï¬cantly for large graphs (see Figure 7). Table 9: Clustering time of ClusterGCN PPI Flickr Reddit Yelp Amazon Time (sec) 2.2 11.6 40.0 106.7 2254.2 Taking into account the pre-processing time, sampling time and training time altogether, we sum- marize the total convergence time of GraphSAINT and ClusterGCN in Table 10 (corresponding to Table 2 conï¬guration). On graphs that are large and dense (e.g., Amazon), GraphSAINT achieves signiï¬cantly faster convergence. Note that both the sampling of GraphSAINT and clustering of ClusterGCN can be performed ofï¬‚ine. Table 10: Comparison of total convergence time (pre-processing + sampling + training, unit: second) PPI Flickr Reddit Yelp Amazon GraphSAINT-Edge 91.0 7.0 16.6 273.9 401.0 GraphSAINT-RW 103.6 7.5 17.2 310.1 425.6 ClusterGCN 163.2 12.9 55.3 256.0 2804.8 D.3 E FFECT OF BATCH SIZE Table 11 shows the change of test set accuracy with batch sizes. For each row of Table 11, we ï¬x the batch size, tune the other hyperparameters according to Appendix C.3, and report the highest test set accuracy achieved. For GraphSAGE, S-GCN and AS-GCN, their default batch sizes (512,1000 and 512, respectively) lead to the highest accuracy on all datasets. For FastGCN, increasing the default batch size (from 400 to 4000) leads to noticeable accuracy improvement. For ClusterGCN, different datasets correspond to different optimal batch sizes. Note that the accuracy in Section 5.1 is already tuned by identifying the optimal batch size on a per graph basis. For FastGCN, intuitively, increasing batch size may help with accuracy improvement since the minibatches may become better connected. Such intuition is veriï¬ed by the rows of 400 and 2000. However, increasing the batch size from 2000 to 4000 does not further improve accuracy signiï¬cantly. For ClusterGCN, the optimal batch size depends on the cluster structure of the training graph. For PPI, small batches are better, while for Amazon, batch size does not have signiï¬cant impact on accuracy. For GraphSAGE, overly large batches may have negative impact on accuracy due to neighbor explosion. Approximately, GraphSAGE expand 10Ã—more neighbors per layer. For a 2-layer GCN, a size 2 Ã—103 minibatch would then require the support of 2 Ã—105 nodes from the â€ â€ http://glaros.dtc.umn.edu/gkhome/metis/metis/download âˆ—Default batch size Â¶The training does not converge. â€¡The codes throw runtime error on the large datasets (Yelp or Amazon). 18Published as a conference paper at ICLR 2020 Table 11: Test set F1-micro for the baselines under various batch sizes Method Batch size PPI Flickr Reddit Yelp Amazon GraphSAGE 256 0.600 0.474 0.934 0.563 0.428 512âˆ— 0.637 0.501 0.953 0.634 0.758 1024 0.610 0.482 0.935 0.632 0.705 2048 0.625 0.374 0.936 0.563 0.447 FastGCN 400âˆ— 0.513 0.504 0.924 0.265 0.174 2000 0.561 0.506 0.934 0.255 0.196 4000 0.564 0.507 0.934 0.260 0.195 S-GCN 500 0.519 0.462 â€” Â¶ â€” Â¶ â€” â€¡ 1000âˆ— 0.963 0.482 0.964 0.640 â€” â€¡ 2000 0.646 0.482 0.949 0.614 â€” â€¡ 4000 0.804 0.482 0.949 0.594 â€” â€¡ 8000 0.694 0.481 0.950 0.613 â€” â€¡ AS-GCN 256 0.682 0.504 0.950 â€” â€¡ â€” â€¡ 512âˆ— 0.687 0.504 0.958 â€” â€¡ â€” â€¡ 1024 0.687 0.502 0.951 â€” â€¡ â€” â€¡ 2048 0.670 0.502 0.952 â€” â€¡ â€” â€¡ ClusterGCN 500 0.875 0.481 0.942 0.604 0.752 1000 0.831 0.478 0.947 0.602 0.756 1500 0.865 0.480 0.954 0.602 0.752 2000 0.828 0.469 0.954 0.609 0.759 2500 0.849 0.476 0.954 0.598 0.745 3000 0.840 0.473 0.954 0.607 0.754 3500 0.846 0.473 0.952 0.602 0.754 4000 0.853 0.472 0.949 0.605 0.756 input layer. Note that the full training graph size of Reddit is just around 1.5 Ã—105. Thus, no matter which nodes are sampled in the output layer, GraphSAGE would almost always propagate features within the full training graph for initial layers. We suspect this would lead to difï¬culties in learning. For S-GCN, with batch size of 500, it fails to learn properly on Reddit and Yelp. The accuracy ï¬‚uctuates in a region of very low value, even after appropriate hyperparameter tuning. For AS-GCN, its accuracy is not sensitive to the batch size, since AS-GCN addresses neighbor explosion and also ensures good inter-layer connectivity within the minibatch. 19",
      "meta_data": {
        "arxiv_id": "1907.04931v4",
        "authors": [
          "Hanqing Zeng",
          "Hongkuan Zhou",
          "Ajitesh Srivastava",
          "Rajgopal Kannan",
          "Viktor Prasanna"
        ],
        "published_date": "2019-07-10T21:11:13Z",
        "pdf_url": "https://arxiv.org/pdf/1907.04931v4.pdf"
      }
    },
    {
      "title": "GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings",
      "abstract": "We present GNNAutoScale (GAS), a framework for scaling arbitrary\nmessage-passing GNNs to large graphs. GAS prunes entire sub-trees of the\ncomputation graph by utilizing historical embeddings from prior training\niterations, leading to constant GPU memory consumption in respect to input node\nsize without dropping any data. While existing solutions weaken the expressive\npower of message passing due to sub-sampling of edges or non-trainable\npropagations, our approach is provably able to maintain the expressive power of\nthe original GNN. We achieve this by providing approximation error bounds of\nhistorical embeddings and show how to tighten them in practice. Empirically, we\nshow that the practical realization of our framework, PyGAS, an easy-to-use\nextension for PyTorch Geometric, is both fast and memory-efficient, learns\nexpressive node representations, closely resembles the performance of their\nnon-scaling counterparts, and reaches state-of-the-art performance on\nlarge-scale graphs.",
      "meta_data": {
        "arxiv_id": "2106.05609v1",
        "authors": [
          "Matthias Fey",
          "Jan E. Lenssen",
          "Frank Weichert",
          "Jure Leskovec"
        ],
        "published_date": "2021-06-10T09:26:56Z",
        "pdf_url": "https://arxiv.org/pdf/2106.05609v1.pdf"
      }
    },
    {
      "title": "GOAT: A Global Transformer on Large-scale Graphs"
    },
    {
      "title": "Scalable Graph Neural Networks via Bidirectional Propagation",
      "abstract": "Graph Neural Networks (GNN) is an emerging field for learning on\nnon-Euclidean data. Recently, there has been increased interest in designing\nGNN that scales to large graphs. Most existing methods use \"graph sampling\" or\n\"layer-wise sampling\" techniques to reduce training time. However, these\nmethods still suffer from degrading performance and scalability problems when\napplying to graphs with billions of edges. This paper presents GBP, a scalable\nGNN that utilizes a localized bidirectional propagation process from both the\nfeature vectors and the training/testing nodes. Theoretical analysis shows that\nGBP is the first method that achieves sub-linear time complexity for both the\nprecomputation and the training phases. An extensive empirical study\ndemonstrates that GBP achieves state-of-the-art performance with significantly\nless training/testing time. Most notably, GBP can deliver superior performance\non a graph with over 60 million nodes and 1.8 billion edges in less than half\nan hour on a single machine. The codes of GBP can be found at\nhttps://github.com/chennnM/GBP .",
      "full_text": "Scalable Graph Neural Networks via Bidirectional Propagation Ming Chen Renmin University of China chennnming@ruc.edu.cn Zhewei Weiâˆ— Renmin University of China zhewei@ruc.edu.cn Bolin Ding Alibaba Group bolin.ding@alibaba-inc.com Yaliang Li Alibaba Group yaliang.li@alibaba-inc.com Ye Yuan Beijing Institute of Technology yuan-ye@bit.edu.cn Xiaoyong Du Renmin University of China duyong@ruc.edu.cn Ji-Rong Wen Renmin University of China jrwen@ruc.edu.cn Abstract Graph Neural Networks (GNN) is an emerging ï¬eld for learning on non-Euclidean data. Recently, there has been increased interest in designing GNN that scales to large graphs. Most existing methods use \"graph sampling\" or \"layer-wise sampling\" techniques to reduce training time. However, these methods still suffer from degrading performance and scalability problems when applying to graphs with billions of edges. This paper presents GBP, a scalable GNN that utilizes a localized bidirectional propagation process from both the feature vectors and the training/testing nodes. Theoretical analysis shows that GBP is the ï¬rst method that achieves sub-linear time complexity for both the precomputation and the training phases. An extensive empirical study demonstrates that GBP achieves state-of-the- art performance with signiï¬cantly less training/testing time. Most notably, GBP can deliver superior performance on a graph with over 60 million nodes and 1.8 billion edges in less than half an hour on a single machine. 1 Introduction Recently, the ï¬eld of Graph Neural Networks (GNNs) has drawn increasing attention due to its wide range of applications such as social analysis [23, 20, 28], biology [10, 26], recommendation systems [36], and computer vision [39, 7, 13]. Graph Convolutional Network (GCN) [ 15] adopts a message-passing approach and gathers information from the neighbors of each node from the previous layer to form the new representation. The vanilla GCN uses a full-batch training process and stores each nodeâ€™s representation in the GPU memory, which leads to limited scalability. On the other hand, training GCN with mini-batches is difï¬cult, as the neighborhood size could grow exponentially with the number of layers. These techniques can be broadly divided into three categories: 1) Layer-wise sampling methods: GraphSAGE [11] proposes a neighbor-sampling method to sample a ï¬xed number of neighbors for each node. VRGCN [ 6] leverages historical activations to restrict the number of sampled nodes âˆ—Zhewei Wei is the corresponding author. Work partially done at Gaoling School of Artiï¬cial Intelligence, Beijing Key Laboratory of Big Data Management and Analysis Methods, MOE Key Lab DEKE, Renmin University of China, and Pazhou Lab, Guangzhou, 510330, China. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2010.15421v3  [cs.LG]  2 Sep 2021and reduce the variance of sampling. FastGCN [ 5] samples nodes of each layer independently based on each nodeâ€™s degree and keeps a constant sample size in all layers to achieve scales linearly. LADIES [40] further proposes a layer-dependent sampler to constrain neighbor dependencies, which guarantees the connectivity of the sampled adjacency matrix. 2) Graph Sampling methods: Cluster- GCN [8] builds a complete GCN from clusters in each mini-batch. GraphSAINT [ 37] proposes several light-weight graph samplers and introduces a normalization technique to eliminate biases of mini-batch estimation. 3) Linear Models: SGC [30] computes the product of the feature matrix and the k-th power of the normalized adjacency matrix during the preprocessing step and performs standard logistic regression to remove redundant computation. PPRGo [ 4] uses Personalized PageRank to capture multi-hop neighborhood information and uses a forward push algorithm [ 2] to accelerate computation. While the above methods signiï¬cantly speed up the training time of GNNs, they still suffer from three major drawbacks. First of all, the time complexity is linear to m, the number of edges in the graph. In theory, this complexity is undesirable for scalable GNNs. Secondly, as we shall see in Section 4, the existing scalable GNNs, such as GraphSAINT, LADIES, and SGC, fail to achieve satisfying results in the semi-supervised learning tasks. Finally and most importantly, none of the existing methods can offer reliable performance on billion-scale graphs. Our contributions. In this paper, we ï¬rst carefully analyze the theoretical complexity of existing scalable GNNs and explain why they cannot scale to graphs with billions of edges. Then, we present GBP (Graph neural network via Bidirectional Propagation), a scalable Graph Neural Network with sub-linear time complexity in theory and superior performance in practice. GBP performs propagation from both the feature vector and the training/testing nodes, yielding an unbiased estimator for each representation. Each propagation is executed in a localized fashion, leading to sub-linear time complexity. After the bidirectional propagation, each nodeâ€™s representation is ï¬xed and can be trivially trained with mini-batches. The empirical study demonstrates that GBP consistently improves the performance and scalability across a wide variety of datasets on both semi-supervised and fully-supervised tasks. Finally, we present the ï¬rst empirical study on a graph with over 1.8 billion edges. The result shows that GBP achieves superior results in less than 2,000 seconds on a moderate machine. 2 Theoretical analysis of existing methods In this section, we review some of the recent scalable GNNs and analyze their theoretical time complexity. We consider an undirected graph G=(V,E), where V and Erepresent the set of vertices and edges, respectively. For ease of presentation, we assume that Gis a self-looped graph [15], with a self-loop attached to each node in V. Let n= |V|and m= |E|denote the number of vertices and edges in G, respectively. Each node is associated with an F-dimensional feature vector and we use X âˆˆRnÃ—F to denote the feature matrix. We use A and D to represent the adjacency matrix and the diagonal degree matrix of G, respectively. For each node uâˆˆV, N(u) is the set of neighbors of u, and d(u) = |N(u)|is the degree of u. We use d = m n to denote the average degree of G. Following [15], we deï¬ne the normalized adjacency matrix of G as ËœA = Dâˆ’1/2ADâˆ’1/2. The (â„“+ 1)-th layer H(â„“+1) of the vanilla GCN is deï¬ned as H(â„“+1) = Ïƒ ( ËœAH(â„“)W(â„“) ) , (1) where W(â„“) is the learnable weight matrix and Ïƒ(Â·) is the activation function. The training and inference time complexity of a GCN with Llayers can be bounded by O ( LmF + LnF2) , where O(LmF) is the total cost of the sparse-dense matrix multiplication ËœAH(â„“), and O(LnF2) is the total cost of the feature transformation by applying W(â„“). At ï¬rst glance, O(LnF2) seems to be the dominating term, as the average degree don scale-free networks is usually much smaller than the feature dimension F and hence LnF2 > LndF= LmF. However, in reality, feature transformation can be performed with signiï¬cantly less cost due to better parallelism of dense-dense matrix multiplications. Consequently, O(LmF) is the dominating complexity term of GCN and performing full neighbor propagation ËœAH(â„“) is the main bottleneck for achieving scalability. 2In order to speed up GCN training, a few recent methods use various techniques to approximate the full neighbor propagation ËœAH(â„“) and enable mini-batch training. We divide these methods into three categories and summarize their time complexity in Table 1. Layer-wise samplingmethods sample a subset of the neighbors at each layer to reduce the neighbor- hood size. GraphSAGE [11] samples sn neighbors for each node and only aggregates the embeddings from the sampled nodes. With a batch-size b, the cost of feature propagation is bounded by O(bsL nF), and thus the total per-epoch cost of GraphSAGE is O(nsL nF + nsLâˆ’1 n F2). This complexity grows exponentially with the number of layers Land is not scalable on large graphs. Another work [ 6] based on node-wise sampling further reduces sampling variance to achieve a better convergence rate. However, it suffers from worse time and space complexity. FastGCN [5] and LADIES [40] restrict the same sample size across all layers to limit the exponential expansion. If we use sl to denote the number of nodes sampled per layer, the per-batch feature propagation time is bounded by O(Ls2 l F). Since n sl batches are needed in an epoch, it follows that the per-epoch forward propagation time is bounded by O(LnslF + LnF2). Mini-batch training signiï¬cantly accelerates the training process of the layer-wise sampling method. However, the training time complexity is still linear to mas the number of samples sl is usually much larger than the average degree d. Furthermore, it has been observed in [8] that the overlapping nodes in different batches will lead to high computational redundancy, especially in fully-supervised learning. Graph samplingmethods sample a sub-graph at the beginning of each batch and perform forward propagation on the same subgraph across all layers. Cluster-GCN [ 8] uses graph clustering tech- niques [14] to partition the original graph into several sub-graphs, and samples one sub-graph to perform feature propagation in each mini-batch. In the worst case, the number of clusters in the graph is 1, and Cluster-GCN essentially becomes vanilla GCN in terms of time complexity. Graph- SAINT [37] samples a certain amount of nodes and uses the induced sub-graph to perform feature propagation in each mini-batch. Let bdenote the number of sampled node per-batch, and n b denote the number of batches. Given a sampled nodeu, the probability that a neighbor ofuis also sampled is b/n. Therefore, the expected number of edges in the sub-graph is bounded by O(b2d/n). Summing over n b batches follows that the per-epoch feature propagation time of GraphSAINT is bounded by O(LbdF), which is sub-linear to the number of edges in the graph. However, GraphSaint requires a full forward propagation in the inference phase, leading to the O(LmF + LnF2) time complexity. Linear modelremoves the non-linearity between each layer in the forward propagation, which allows precomputation of the ï¬nal feature propagation matrix and result in an optimal training time complexity of O(nF2). SGC [30] repeatedly perform multiplication of normalized adjacency matrix ËœA and feature matrix X in the precomputation phase, which requires O(LmF) time. PPRGo [ 4] calculates approximate the Personalized PageRank (PPR) matrix âˆ‘âˆ â„“=0 Î±(1 âˆ’Î±)â„“ ËœAâ„“ by forward push algorithm [2] and then applies the PPR matrix to the feature matrix X to derive the propagation matrix. Let Îµdenote the error threshold of the forward push algorithm, the precomputation cost is bounded by O(m Îµ ). A major drawback of PPRGo is that it takes O(n Îµ ) space to store the PPR matrix, rendering it infeasible on billion-scale graphs. Table 1: Summary of time complexity for GNN training and inference. Method Precomputation Training Inference GCN - O ( LmF + LnF2) O ( LmF + LnF2) GraphSAGE - O ( nsL nF + nsLâˆ’1 n F2) O ( nsL nF + nsLâˆ’1 n F2) FastGCN - O ( LnslF + LnF2) O ( LnslF + LnF2) LADIES - O ( LnslF + LnF2) O ( LnslF + LnF2) SGC O(LmF) O ( nF2) O ( nF2) PPRGo O ( m Îµ ) O ( nKF + LnF2) O ( nKF + LnF2) Cluster-GCN O(m) O ( LmF + LnF2) O ( LmF + LnF2) GraphSAINT - O ( LbdF + LnF2) O ( LmF + LnF2) GBP (This paper) O ( LnF + L âˆšm lg n Îµ F ) O ( LnF2) O ( LnF2) Other related work. Another line of research devotes to attention model [ 29, 27, 22], where the adjacency matrix of each layer is replaced by a learnable attention matrix. GIN [ 31] studies the expressiveness of GNNs, and shows that GNNs are not better than the Weisfeiler-Lehman test in 3distinguishing graph structures. GDC [17] proposes to replace the graph convolutional matrix ËœA with a graph diffusion matrix, such as the Heat Kernel PageRank or the Personalized PageRank matrix. Mixhop [1] mixes higher-order information to learn a wider class of representations. JKNet [ 32] explores the relationship between node inï¬‚uence and random walk in GNNs. DropEdge [ 24] and PairNorm [38] focus on over-smoothing problem and improve performance on GCNs by increasing the number of layers. These works focus on the effectiveness of GNNs; Thus, they are orthogonal to this paperâ€™s contributions. 3 Bidirectional Propagation Method Generalized PageRank. To achieve high scalability, we borrow the idea of decoupling predic- tion and propagation from SGC [ 30] and APPNP [ 16]. In particular, we precompute the feature propagation with the following Generalized PageRank matrix [21]: P = Lâˆ‘ â„“=0 wâ„“T(â„“) = Lâˆ‘ â„“=0 wâ„“ ( Drâˆ’1ADâˆ’r)â„“ Â·X, (2) where r âˆˆ[0,1] is the convolution coefï¬cient, wâ„“â€™s are the weights of different order convolution matrices that satisfy âˆ‘âˆ â„“=0 wâ„“ â‰¤1, and T(â„“) = ( Drâˆ’1ADâˆ’r)â„“ Â·X denotes the â„“-th step propagation matrix. After the feature propagation matrix P is derived, we can apply a multi-layer neural network with mini-batch training to make the prediction. For example, for multi-class classiï¬cation tasks, a two-layer GBP model makes prediction with Y = SoftMax (Ïƒ(PW1) W2) where W1 and W2 are the learnable weight matrices, and Ïƒis the activation function. We note that equation (2) can be easily generalized to various existing models. By setting r= 0.5,0 and 1, the convolution matrix Drâˆ’1ADâˆ’r represents the symmetric normalization adjacency matrix Dâˆ’1/2ADâˆ’1/2 [15, 30, 16], the transition probability matrix ADâˆ’1 [11, 8, 37], and the reverse transition probability matrix Dâˆ’1A [32], respectively. We can also manipulate the weights wâ„“ to simulate various diffusion processes as in [17]. However, we will mainly focus on two setups in this paper: 1) wâ„“ = Î±(1 âˆ’Î±)â„“ for some constant decay factor Î±âˆˆ(0,1), in which case P becomes the Personalized PageRank used in APPNP and PPRGo [ 16, 17, 4]; 2) wâ„“ = 0for â„“= 0,...,L âˆ’1 and wL = 1, in which case P degenerates to the L-th transition probability matrix in SGC [30]. Algorithm 1:Bidirectional Propagation Algorithm Input: Graph G, level L, training set Vt, weight coefï¬cients wâ„“, convolutional coefï¬cient r, threshold rmax, number of walks per node nr, feature matrix XnÃ—F Output: Embedding matrix PnÃ—F 1 S(â„“) â†Sparse(0nÃ—n) for â„“= 0,...,L ; 2 for each node sâˆˆVt do 3 Generate nr random walks from s, each of length L; 4 if The j-th random walk visits node uat the â„“-th step, â„“= 0,...,L , j = 1,...,n r then 5 S(â„“)(s,u) += 1 nr ; 6 Q(â„“),R(â„“) â†Sparse(0nÃ—F ) for â„“= 1,...,L ; 7 Q(0) â†0nÃ—F and R(0) â†ColumnNormalized (Dâˆ’rX); 8 for â„“from 0 to Lâˆ’1 do 9 for each uâˆˆV and kâˆˆ{0,...,F âˆ’1}with ââR(â„“)(u,k) ââ>rmax do 10 for each vâˆˆN(u) do 11 R(â„“+1)(v,k) += R(â„“)(u,k) d(v) ; 12 Q(â„“)(u,k) â†R(â„“)(u,k) and R(â„“)(u,k) â†0; 13 Q(L) â†R(L) and RL â†Sparse(0nÃ—F ); 14 Ë†P â†âˆ‘L â„“=0 wâ„“ Â·Dr Â· ( Q(â„“) + âˆ‘â„“ t=0 S(â„“âˆ’t)R(t) ) ; 15 return Embedding matrix Ë†PnÃ—F ; 43.1 The Bidirectional Propagation Algorithm To reduce the time complexity, we propose approximating the Generalized PageRank matrix P with a localized bidirectional propagation algorithm from both the training/testing nodes and the feature vectors. Similar techniques have been used for computing probabilities in Markov Process [3]. Algorithm 1 illustrates the pseudo-code of the Bidirectional Propagation algorithm. The algorithm proceeds in three phases: Monte-Carlo Propagation (lines 1-5), Reverse Push Propagation (lines 6-13), and the combining phase (line 14). Monte Carlo Propagation from the training/testing nodes.We start with a simple Monte-Carlo propagation (Lines 1-5 in Algorithm 1) from the training/testing nodes to estimate the transition probabilities. Given a graph Gand a training/testing node set Vt, we generate a number nr of random walks from each node sâˆˆVt, and record S(â„“)(s,u), the fraction of random walks that visit node uat the â„“-th steps. Note that S(â„“) âˆˆRnÃ—n is a sparse matrix with at most |Vt|Â·nr non-zero entries. Since each random walk is independently sampled, we have that S(â„“) is an unbiased estimator for the â„“-th transition probability matrix ( Dâˆ’1A )â„“ . We also note that ( Drâˆ’1ADâˆ’r)â„“ = Dr ( Dâˆ’1A )â„“ Dâˆ’r, which means we can use DrS(â„“)Dâˆ’rX as an unbiased estimator for the â„“-th propagation matrix T(â„“) = ( Drâˆ’1ADâˆ’r)â„“ X. Consequently, âˆ‘L â„“=0 wâ„“DrS(â„“)Dâˆ’rX serves as an unbiased estimator for the Generalized PageRank Matrix P. However, this estimation requires a large number of random walks from each training node and thus is infeasible for fully-supervised training on large graphs. Reverse Push Propagation from the feature vectors.To reduce the variance of the Monte-Carlo estimator, we employ a deterministic Reverse Push Propagation (lines 6-13 in Algorithm 1) from the feature vectors. Given a feature matrix X, the algorithm outputs two sparse matrices for each level â„“= 0,...,L : the reserve matrix Q(â„“) that represents the probability mass to stay at level â„“, and the residue matrix R(â„“) that represents the probability mass to be distributed beyond level â„“. We begin by setting the initial residue R(0) as the degree normalized feature matrix Dâˆ’rX. We also perform column-normalization on R(0) such that each dimension of R(0) has the same L1-norm. Starting from level â„“= 0, if the absolute value of the residue entry R(â„“)(u,k) exceeds a threshold rmax, we increase the residue of each neighbor vat level â„“+ 1by R(â„“)(u,k) d(v) and transfer the residue of uto its reserve Q(â„“)(u,k). Note that by maintaining a list of residue entries R(â„“)(u,k) that exceed the threshold rmax, the above push operation can be done without going through every entry in R(â„“). Finally, we transfer the non-zero residue R(L)(u,k) of each node uto its reserve at level L. We will show that when Reverse Push Propagation terminates, the reserve matrixQ(â„“) satisï¬es T(â„“)(s,k) âˆ’d(s)r Â·(â„“+ 1)Â·rmax â‰¤ ( DrQ(â„“) ) (s,k) â‰¤T(â„“)(s,k) (3) for each training/testing node sâˆˆVt and feature dimension k. Recall that T(â„“) = ( Drâˆ’1ADâˆ’r)â„“ X is the â„“-th propagation matrix. This property implies that we can useâˆ‘L â„“=0 wâ„“DrQ(â„“) to approximate the Generalized PageRank matrix P = âˆ‘L â„“=0 wâ„“T(â„“). However, there are two drawbacks to this approximation. First, this estimator is biased, which could potentially hurt the performance of the prediction. Secondly, the Reverse Push Propagation does not take advantage of the semi-supervised learning setting where the number of training nodes may be signiï¬cantly less than the total number of nodes n. Combining Monte-Carlo and Reverse Push Propagation.Finally, we combine the results from the Monte-Carlo and Reverse Push Propagation to form a more accurate unbiased estimator. In particular, we use the following equation to derive an approximation of the â„“-th propagation matrix T(â„“) = ( Drâˆ’1ADâˆ’r)â„“ X: Ë†T(â„“) = Dr Â· ( Q(â„“) + â„“âˆ‘ t=0 S(â„“âˆ’t)R(t) ) . (4) As we shall prove in Section 3.2, Ë†T(â„“) is an unbiased estimator for T(â„“) = ( Drâˆ’1ADâˆ’r)â„“ X. Consequently, we can use Ë†P = âˆ‘L â„“=0 wâ„“ Ë†T(â„“) = âˆ‘L â„“=0 wâ„“Dr Â· ( Q(â„“) + âˆ‘â„“ t=0 S(â„“âˆ’t)R(t) ) as an 5unbiased estimator for the Generalized PageRank matrix P deï¬ned in equation (2). To see why equation (4) is a better approximation than the naive estimator DrS(â„“)Dâˆ’rX, note that each entry in the residue matrix R(t) is bounded by a small real number rmax, which means the variance of the Monte-Carlo estimator is reduced by a factor of rmax. It is also worth mentioning that the two matrices S(â„“âˆ’t) and R(t) are sparse, so the time complexity of the matrix multiplication only depends on their numbers of non-zero entries. 3.2 Analysis We now analyze the time complexity and the approximation quality of the Bidirectional Propagation algorithm. Due to the space limit, we defer all proofs in this section to the appendix. Recall that |Vt| is the number of training/testing nodes, nr is the number of random walks per node , and rmax is the push threshold. We assume Dâˆ’rX is column-normalized, as described in Algorithm 1. We ï¬rst present a Lemma that bounds the time complexity of the Bidirectional Propagation algorithm. Lemma 1. The time complexity of Algorithm 1 is bounded by O ( L2|Vt|nrF + LdF rmax ) . Intuitively, the L|Vt|nrF term represents the time for the Monte-Carlo propagation, and the LdF rmax term is the time for the Reverse Push propagation. Next, we will show how to set the number of random walks nr and the push threshold rmax to obtain a satisfying approximation quality. In particular, the following technical Lemma states that the Reverse Push Propagation maintains an invariant during the push process. Lemma 2. For any residue and reserve matricesQ(â„“),R(â„“),â„“ = 0,...,L , we have T(â„“) = ( Drâˆ’1ADâˆ’r)â„“ X = Dr Â· ( Q(â„“) + â„“âˆ‘ t=0 ( Dâˆ’1A )â„“âˆ’t R(t) ) . (5) We note that the only difference between equations (4) and (5) is that we replace Dâˆ’1A with S(â„“) in equation (4). Recall that S(â„“) is an unbiased estimator for the â„“-th transition probability matrix ( Dâˆ’1A )â„“ . Therefore, Lemma 2 ensures that Ë†T(â„“) is also an unbiased estimator for T(â„“). Consequently, Ë†P = âˆ‘L â„“=0 wâ„“ Ë†T(â„“) is an unbiased estimator for the propagation matrix P. Finally, to minimize the overall time complexity of Algorithm 1 in Lemma 1, the general principle is to balance the costs of the Monte-Carlo and the Reverse Push propagations. In particular, we have the following Theorem. Theorem 1. By setting nr = O ( 1 Îµ âˆš d log n |Vt| ) and rmax = O ( Îµ âˆš d |Vt|log n ) , Algorithm 1 produces an estimator Ë†P of the propagation matrix P, such that for any s âˆˆVt and k = 0,...,F âˆ’1, the probability that âââË†P(s,k) âˆ’P(s,k) ââââ‰¤d(s)rÎµis at least 1 âˆ’1 n.The time complexity is bounded by O ( LÂ· âˆš L|Vt|d log (nL) Îµ Â·F ) . For fully-supervised learning, we have |Vt|= nand thus the time complexity of GBP becomes O ( LÂ· âˆš Lm log (nL) Îµ Â·F ) . In practice, we can also make a trade-off between efï¬ciency and accuracy by manipulating the push threshold rmax and the number of walks nr . Parallelism of GBP. The Bidirectional Propagation algorithm is embarrassingly parallelizable: We can generate the random walks on multiple nodes and perform Reverse Push on multiple features in parallel. After we obtain the Generalized PageRank matrix P, it is trivially to construct mini-batches for training the neural networks. 4 Experiments Datasets. We use seven open graph datasets with different size: three citation networks Cora, Citeser and Pubmed [ 25], a Protein-Protein interaction network PPI [ 11], a customer interaction 6Table 2: Dataset statistics. Dataset Task Nodes Edges Features Classes Label rate Cora multi-class 2,708 5,429 1,433 7 0.052 Citeseer multi-class 3,327 4,732 3,703 6 0.036 Pubmed multi-class 19,717 44,338 500 3 0.003 PPI multi-label 56,944 818,716 50 121 0.79 Yelp multi-label 716,847 6,977,410 300 100 0.75 Amazon multi-class 2,449,029 61,859,140 100 47 0.70 Friendster multi-class 65,608,366 1,806,067,135 100 (random) 500 0.001 network Yelp [37], a co-purchasing networks Amazon [8] and a large social network Friendster [34]. Table 2 summarizes the statistics of the datasets. We ï¬rst evaluate GBPâ€™s performance for transductive semi-supervised learning on the three popular citation networks (Cora, Citeseer, and Pubmed). Then we compare GBP with scalable GNN methods three medium to large graphs PPI, Yelp, Amazon in terms of inductive learning ability. Finally, we present the ï¬rst empirical study of transductive semi-supervised on billion-scale network Friendster. Baselines and detailed setup.We adopt three state-of-the-art GNN methods GCN [15], GAT [29], GDC [17] and APPNP [16] as the baselines for evaluation on small graphs. We also use one state-of- the-art scalable GNN from each of the three categories: LADIES (layer sampling) [40], GraphSAINT (graph sampling) [37], SGC and PPRGo (linear model) [30, 4]. We implement GBP in PyTorch and C++, and employ initial residual connection [ 12] across the hidden layers to facilitate training. For simplicity, we use the Personalized PageRank weights (wâ„“ = Î±(1 âˆ’Î±)â„“ for some hyperparameter Î± âˆˆ(0,1)). As we shall see, this weight sequence generally achieves satisfying results on graphs with real-world features. On the Friendster dataset, where the features are random noises, we use both Personalized PageRank and transition probability (wL = 1,w0 =,..., = wLâˆ’1 = 0) for GBP. We set L= 4across all datasets. Table 3 summaries other hyper-parameters of GBP on different datasets. We use Adam optimizer to train our model, with a maximum of 1000 epochs and a learning rate of 0.01. For a fair comparison, we use the ofï¬cially released code of each baseline (see the supplementary materials for URL and commit numbers) and perform a grid search to tune hyperparameters for models. All the experiments are conducted on a machine with an NVIDIA TITAN V GPU (12GB memory), Intel Xeon CPU (2.20 GHz), and 256GB of RAM. Table 3: Hyper-parameters of GBP.rmax is the Reverse Push Threshold, wis the number of random walks from the training nodes, wâ„“ is the weight sequence, r is the Laplacian parameter in the convolutional matrix Drâˆ’1ADâˆ’r. Data Dropout Hidden dimension L2 Batch size rmax w w â„“ r Cora 0.5 64 5e-4 16 1e-4 0 Î±(1 âˆ’Î±)â„“,Î± = 0.1 0.5 Citeseer 0.5 64 2e-1 64 1e-5 0 Î±(1 âˆ’Î±)â„“,Î± = 0.15 0.4 Pubmed 0.5 128 5e-4 16 1e-5 0 Î±(1 âˆ’Î±)â„“,Î± = 0.05 0.5 PPI 0.1 2048 - 2048 5e-7 0 Î±(1 âˆ’Î±)â„“,Î± = 0.3 0 Yelp 0.1 2048 - 30000 5e-7 0 Î±(1 âˆ’Î±)â„“,Î± = 0.9 0.3 Amazon 0.1 1024 - 100000 1e-7 0 Î±(1 âˆ’Î±)â„“,Î± = 0.2 0.2 Friendster (PPR) 0.1 128 - 2048 4e-8 10000 Î±(1 âˆ’Î±)â„“,Î± = 0.1 0.5 Friendster 0.1 128 - 2048 4e-8 10000 {0,0,0,1} 0.5 Transductive learning on small graphs.Table 4 shows the results for the semi-supervised trans- ductive node classiï¬cation task on the three small standard graphs Cora, Citeseer, and Pubmed. Following [15], we apply the standard ï¬xed training/validation/testing split with 20 nodes per class for training, 500 nodes for validation and 1,000 nodes for testing. For each method, we set the number of hidden layers to 2 and take the mean accuracy with the standard deviation after ten runs. We observe that GBP outperforms APPNP (and consequently all other baselines) across all datasets. For the scalable GNNs, SGC is outperformed by the vanilla GCN due to the simpliï¬cation [30]. On the other hand, the results of LADIES and GraphSAINT are also not at par with the non-scalable GNNs 7Table 4: Results on Cora, Citeseer and Pubmed. Method Cora Citeseer Pubmed GCN 81.5 Â±0.6 71.3 Â±0.4 79.1 Â±0.4 GAT 83.3 Â±0.8 71.9 Â±0.7 78.0 Â±0.4 GDC 83.3 Â±0.2 72.2 Â±0.3 78.6 Â±0.4 APPNP 83.3 Â±0.3 71.4 Â±0.6 80.1 Â±0.2 SGC 81.0 Â±0.1 71.8 Â±0.1 79.0 Â±0.1 LADIES 79.6 Â±0.5 68.6 Â±0.3 77.0 Â±0.5 PPRGo 82.4 Â±0.2 71.3 Â±0.3 80.0 Â±0.4 GraphSAINT 81.3 Â±0.4 70.5 Â±0.4 78.2 Â±0.8 GBP 83.9 Â±0.7 72.9 Â±0.5 80.6 Â±0.4 such as GAT or APPNP, which suggests that the sampling technique alone might not be sufï¬cient to achieve satisfying performance on small graphs. Table 5: Results of inductive learning with scalable GNNs. 4-layer 6-layer 8-layer F1-score Time (s) F1-score Time (s) F1-score Time (s) PPI SGC 65.7 Â±0.01 76 62.4 Â±0.01 173 57.8 Â±0.01 295 LADIES 57.9 Â±0.30 187 59.4 Â±0.25 206 57.4 Â±0.24 315 PPRGo 61.5 Â±0.13 866 61.1 Â±0.02 1976 55.1 Â±0.19 1080 GraphSAINT 99.2 Â±0.05 1291 99.4 Â±0.03 1961 99.3 Â±0.01 2615 GBP 99.3 Â±0.02 117 99.3 Â±0.03 167 99.3 Â±0.01 220 Yelp SGC 41.5 Â±0.21 43 36.8 Â±0.33 70 34.8 Â±0.52 92 LADIES 27.3 Â±0.56 34 28.5 Â±0.97 39 30.0 Â±0.32 51 PPRGo 64.0 Â±0.16 550 63.7 Â±0.71 1215 63.4 Â±0.49 1665 GraphSAINT 64.7 Â±0.08 712 62.0 Â±0.10 996 59.1 Â±0.35 1298 GBP 65.4 Â±0.03 19 65.5 Â±0.03 30 65.4 Â±0.05 37 Amazon SGC 90.4 Â±0.01 233 89.9 Â±0.03 284 89.7 Â±0.03 342 LADIES 85.4 Â±0.14 734 85.2 Â±0.20 784 84.6 Â±0.09 1421 PPRGo 83.3 Â±0.51 2775 83.3 Â±0.09 5206 81.6 Â±0.22 9300 GraphSAINT 91.5 Â±0.01 957 91.3 Â±0.05 1228 91.4 Â±0.05 2618 GBP 91.5 Â±0.01 225 91.5 Â±0.01 243 91.6 Â±0.01 300 Inductive learning on medium to large graphs.Table 5 reports the F1-score and running time (precomputation + training) of each method with various depths on three large datasets PPI, Yelp, and Amazon. For each dataset, we set the hidden dimension to be the same across all methods: 2048(PPI), 2048(Yelp), and 1024(Amazon). We use â€œï¬xed-partitionâ€ splits for each dataset, following [37, 8] (see the supplementary materials for further details). The critical observation is that GBP can achieve comparable performance as GraphSAINT does, with 5-10x less running time. This demonstrates the superiority of GBPâ€™s sub-linear time complexity. For PPRGo, it has a longer running time than other methods because of its expensive feature propagation per epoch. On the other hand, SGC and LADIES are also able to run faster than GraphSAINT; However, these two modelsâ€™ accuracy is not comparable to that of GraphSAINT and GBP. Figure 1 shows the convergence rate of each method, in which the time for data loading, pre- processing, validation set evaluation, and model saving are excluded. We observe that the convergence rate of GBP and SGC is much faster than that of LADIES and GraphSAINT, which is a beneï¬t from decoupling the feature propagation and the neural networks. Transductive semi-supervised learning on billion-scale graph Friendster.Finally, we perform the ï¬rst empirical evaluation of scalable GNNs on a billion-scale graph Friendster. We extracted the top-500 ground-truth communities from [34] and use the community ids as the labels of each node. Note that one node may belong to multiple communities, in which case we pick the largest community as its label. The goal is to perform multi-class classiï¬cation with only the graph structural information. This setting has been adapted in various works on community detection [19, 18, 35]. For each node, we generate a sparse random feature by randomly set one entry to be1 in an d-dimensional all-zero vector. Note that even with a random feature matrix, GNNs are still able to extract structural 80 100 200 300 400 Training time (second) 40 60 80 100Validation F1 PPI SGC LADIES PPRGo GraphSAINT GBP 0 100 200 300 400 Training time (second) 20 30 40 50 60Validation F1 Yelp SGC LADIES PPRGo GraphSAINT GBP 0 100 200 300 400 Training time (second) 70 75 80 85 90Validation F1 Amazon SGC LADIES PPRGo GraphSAINT GBP Figure 1: Convergence curves of 4-layer models. information to perform the prediction [33]. Among the labeled nodes, we use 50,000 nodes (100 from each class) for training, 15,982 for validation, and 25,000 for testing. Table 6 report the running time and F1-score of each method with feature dimension F = 10,40,70 and 100. We omit GraphSAINT and LADIES as they run out of the 256 GB memory even with the dimension d set to 10. We ï¬rst observe that both GBP and SGC can capture the structural information with random features, while PPRGo and GBP(PPR) fail to converge. This is because Personalized PageRank emphasizes each nodeâ€™s original feature (withw0 being the maximum weight among w0,...,w L) and, yet, the original features of Friendster are random noises. We also point out that PPRGo starts to converge and achieves an F1-score of 0.15 in 4500 seconds when the feature dimension is increased to 10000. On the other hand, we observe that GBP can achieve a signiï¬cantly higher F1-score with less running time. Notably, on this 500-class classiï¬cation task, GBP is able to achieve an F1-score of 0.79 with less than half an hour. Table 6: Results for semi-supervised learning on Friendster. Dimension 10 40 70 100 F1-score / Time F1-score Time F1-score Time F1-score Time F1-score Time SGC 2.0 Â±0.27 1130 12.9 Â±0.01 2930 27.1 Â±0.01 4549 40.2 Â±0.01 6379 PPRGo 1.6 Â±0.01 - 1.6 Â±0.01 - 1.6 Â±0.01 - 1.6 Â±0.01 - GBP(PPR) 1.6 Â±0.01 - 1.6 Â±0.01 - 7.3 Â±0.20 - 7.3 Â±0.12 - GBP 7.5 Â±0.10 757 26.6 Â±0.04 863 50.3 Â±0.44 1392 79.7 Â±0.32 1849 5 Conclusion This paper presents GBP, a scalable GNN based on localized bidirectional propagation. Theoretically, GBP is the ï¬rst method that achieves sub-linear time complexity for precomputation, training, and inference. The bidirectional propagation process computes a Generalized PageRank matrix that can express various existing graph convolutions. Extensive experiments on real-world graphs show that GBP obtains signiï¬cant improvement over the state-of-the-art methods in terms of efï¬ciency and performance. Furthermore, GBP is the ï¬rst method that can scale to billion-edge networks on a single machine. For future work, an interesting direction is to extend GBP to heterogeneous networks. Broader Impact The proposed GBP algorithm addresses the challenge of scaling GNNs on large graphs. We consider this algorithm a general technical and theoretical contribution, without any foreseeable speciï¬c impacts. For applications in bioinformatics, computer vision, and natural language processing, applying the GBP algorithm may improve the scalability of existing GNN models. We leave the exploration of other potential impacts to future work. Acknowledgments and Disclosure of Funding Ji-Rong Wen was supported by National Natural Science Foundation of China (NSFC) No.61832017, and by Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098. Zhewei Wei was supported by NSFC No. 61972401 and No. 61932001, by the Fundamental Research Funds for the Central Universities and the Research Funds of Renmin University of China under Grant 18XNLG21, and by Alibaba Group through Alibaba Innovative Research Program. Ye Yuan was supported by NSFC No. 61932004 and No. 61622202, and by FRFCU No. N181605012. Xiaoyong Du was supported by NSFC No. U1711261. 9References [1] ABU-EL-HAIJA , S., P EROZZI , B., K APOOR , A., A LIPOURFARD , N., L ERMAN , K., H ARU - TYUNYAN , H., S TEEG , G. V., AND GALSTYAN , A. Mixhop: Higher-order graph convolutional architectures via sparsiï¬ed neighborhood mixing. In ICML (2019). [2] ANDERSEN , R., C HUNG , F. R. K., AND LANG , K. J. Local graph partitioning using pagerank vectors. In FOCS (2006), IEEE Computer Society, pp. 475â€“486. [3] BANERJEE , S., AND LOFGREN , P. Fast bidirectional probability estimation in markov models. In NIPS (2015), pp. 1423â€“1431. [4] BOJCHEVSKI , A., K LICPERA , J., P EROZZI , B., K APOOR , A., B LAIS , M., R Ã“ZEMBERCZKI , B., L UKASIK , M., AND GÃœNNEMANN , S. Scaling graph neural networks with approximate pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (New York, NY , USA, 2020), ACM. [5] CHEN , J., M A, T., AND XIAO , C. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In ICLR (2018). [6] CHEN , J., Z HU, J., AND SONG , L. Stochastic training of graph convolutional networks with variance reduction. In ICML (2018). [7] CHEN , Z., W EI, X., W ANG , P., AND GUO, Y. Multi-label image recognition with graph convolutional networks. In CVPR (2019). [8] CHIANG , W., L IU, X., S I, S., L I, Y., BENGIO , S., AND HSIEH , C. Cluster-gcn: An efï¬cient algorithm for training deep and large graph convolutional networks. In KDD (2019), ACM, pp. 257â€“266. [9] CHUNG , F. R. K., AND LU, L. Survey: Concentration inequalities and martingale inequalities: A survey. Internet Math. 3, 1 (2006), 79â€“127. [10] FOUT, A., B YRD , J., S HARIAT , B., AND BEN-HUR, A. Protein interface prediction using graph convolutional networks. In NeurIPS (2017), pp. 6530â€“6539. [11] HAMILTON , W. L., Y ING , R., AND LESKOVEC , J. Inductive representation learning on large graphs. In NeurIPS (2017). [12] HE, K., Z HANG , X., R EN, S., AND SUN, J. Deep residual learning for image recognition. In CVPR (2016), pp. 770â€“778. [13] HOSSEINI , B., M ONTAGNE , R., AND HAMMER , B. Deep-aligned convolutional neural network for skeleton-based action recognition and segmentation. In Data Science and Engineering (2020). [14] KARYPIS , G., AND KUMAR , V. A fast and high quality multilevel scheme for partitioning irregular graphs. SIAM J. Scientiï¬c Computing 20, 1 (1998), 359â€“392. [15] KIPF, T. N., AND WELLING , M. Semi-supervised classiï¬cation with graph convolutional networks. In ICLR (2017). [16] KLICPERA , J., B OJCHEVSKI , A., AND GÃœNNEMANN , S. Predict then propagate: Graph neural networks meet personalized pagerank. In ICLR (2019). [17] KLICPERA , J., W EISSENBERGER , S., AND GÃœNNEMANN , S. Diffusion improves graph learning. In NeurIPS (2019), pp. 13333â€“13345. [18] KLOSTER , K., AND GLEICH , D. F. Heat kernel based community detection. In KDD (2014), ACM, pp. 1386â€“1395. [19] LESKOVEC , J., L ANG , K. J., AND MAHONEY , M. W. Empirical comparison of algorithms for network community detection. In WWW (2010), ACM, pp. 631â€“640. [20] LI, C., AND GOLDWASSER , D. Encoding social information with graph convolutional networks forpolitical perspective detection in news media. In ACL (2019). [21] LI, P., C HIEN , I. E., AND MILENKOVIC , O. Optimizing generalized pagerank methods for seed-expansion community detection. In NeurIPS (2019), H. M. Wallach, H. Larochelle, A. Beygelzimer, F. dâ€™AlchÃ©-Buc, E. B. Fox, and R. Garnett, Eds., pp. 11705â€“11716. [22] LU, H., H UANG , S. H., Y E, T., AND GUO, X. Graph star net for generalized multi-task learning. CoRR abs/1906.12330 (2019). 10[23] QIU, J., T ANG , J., M A, H., D ONG , Y., WANG , K., AND TANG , J. Deepinf: Social inï¬‚uence prediction with deep learning. In KDD (2018), ACM, pp. 2110â€“2119. [24] RONG , Y., HUANG , W., XU, T., AND HUANG , J. Dropedge: Towards deep graph convolutional networks on node classiï¬cation. In ICLR (2020). [25] SEN, P., NAMATA, G., B ILGIC , M., G ETOOR , L., G ALLAGHER , B., AND ELIASSI -RAD, T. Collective classiï¬cation in network data. AI Magazine 29, 3 (2008), 93â€“106. [26] SHANG , J., X IAO , C., M A, T., L I, H., AND SUN, J. Gamenet: Graph augmented memory networks for recommending medication combination. In AAAI (2019). [27] THEKUMPARAMPIL , K. K., W ANG , C., O H, S., AND LI, L. Attention-based graph neural network for semi-supervised learning. CoRR abs/1803.03735 (2018). [28] TONG , P., Z HANG , Q., AND YAO, J. Leveraging domain context for question answering over knowledge graph. In Data Science and Engineering (2019). [29] VELI Ë‡CKOVI Â´C, P., C UCURULL , G., C ASANOVA , A., R OMERO , A., L IÃ’, P., AND BENGIO , Y. Graph Attention Networks. ICLR (2018). [30] WU, F., S OUZA , A., Z HANG , T., F IFTY, C., Y U, T., AND WEINBERGER , K. Simplifying graph convolutional networks. In ICML (2019), pp. 6861â€“6871. [31] XU, K., H U, W., L ESKOVEC , J., AND JEGELKA , S. How powerful are graph neural networks? In ICLR (2019). [32] XU, K., L I, C., T IAN , Y., S ONOBE , T., K AWARABAYASHI , K., AND JEGELKA , S. Represen- tation learning on graphs with jumping knowledge networks. In ICML (2018). [33] YANG , C., X IAO , Y., Z HANG , Y., S UN, Y., AND HAN, J. Heterogeneous network representa- tion learning: Survey, benchmark, evaluation, and beyond. arXiv preprint arXiv:2004.00216 (2020). [34] YANG , J., AND LESKOVEC , J. Deï¬ning and evaluating network communities based on ground-truth. In ICDM (2012), IEEE Computer Society, pp. 745â€“754. [35] YANG , R., X IAO , X., W EI, Z., B HOWMICK , S. S., Z HAO , J., AND LI, R. Efï¬cient estimation of heat kernel pagerank for local clustering. In SIGMOD Conference (2019), ACM, pp. 1339â€“ 1356. [36] YING , R., H E, R., C HEN , K., E KSOMBATCHAI , P., H AMILTON , W. L., AND LESKOVEC , J. Graph convolutional neural networks for web-scale recommender systems. In KDD (2018), ACM, pp. 974â€“983. [37] ZENG , H., Z HOU , H., S RIVASTAVA, A., K ANNAN , R., AND PRASANNA , V. GraphSAINT: Graph sampling based inductive learning method. In ICLR (2020). [38] Z HAO , L., AND AKOGLU , L. Pairnorm: Tackling oversmoothing in gnns. In ICLR (2020). [39] ZHAO , L., P ENG , X., T IAN , Y., K APADIA , M., AND METAXAS , D. N. Semantic graph convolutional networks for 3d human pose regression. In CVPR (2019), pp. 3425â€“3435. [40] ZOU, D., H U, Z., W ANG , Y., J IANG , S., S UN, Y., AND GU, Q. Layer-dependent impor- tance sampling for training deep and large graph convolutional networks. In NeurIPS (2019), pp. 11247â€“11256. 11A Proofs We need the following Chernoff Bound for bounded i.i.d. random variables. Lemma 3(Chernoff Bound [9]). Consider a set {xi}(iâˆˆ[1,nr]) of i.i.d. random variables with mean Âµand xi âˆˆ[0,r], we have Pr [âââââ 1 nr nrâˆ‘ i=1 xi âˆ’Âµ ââââââ‰¥Îµ ] â‰¤exp ( âˆ’ nr Â·Îµ2 r (2 3 Îµ+ 2Âµ ) ) . (6) A.1 Proof of Lemma 1 We note that the Bidirectional Propagation process shown in Algorithm 1 can be divided into three phases: the Monte Carlo Propagation phase (Line 1-5 in Algorithm 1), the Reverse Push Propagation phase (Line 6-13) and the combination phase (Line 14). In the combination phase, there are at most O(|Vt|Â· nr) non-zero entries in each derived S(â„“). Thus, the time cost to compute âˆ‘L â„“=0 wâ„“ âˆ‘â„“ t=0 S(â„“âˆ’t)R(t) can be bounded by O(L2|Vt|nrF). Additionally, addingâˆ‘L â„“=0 wâ„“ âˆ‘â„“ t=0 S(â„“âˆ’t)R(t) to âˆ‘L â„“=0 Q(â„“) costs O(L|Vt|F). Hence, the time cost of the combi- nation phase is bounded by O(L2|Vt|nrF). In the Monte-Carlo Propagation phase of Algorithm 1, we ï¬rst generate nr random walks of length L for each training/testing node s âˆˆ Vt to estimate the â„“-th transition probability matrix S(â„“), â„“ = 0,...,L . Since the number of training/testing nodes is |Vt|, the total cost is bounded by O(L|Vt|nr). Furthermore, the time cost of the Reverse Push Propagation can be bounded byLdF rmax , where ddenotes the average degree of the given graph. Before proving this cost, we ï¬rst introduce Lemma 4 to bound the sum of each column in Q(â„“). Lemma 4. After the termination of Algorithm 1, the sum of the k-th column (âˆ€kâˆˆ{0,...,F âˆ’1}) in Q(â„“) (âˆ€â„“âˆˆ{0,...,L }) can be bounded as E [âˆ‘ uâˆˆV ââQ(â„“)(u,k) ââ] â‰¤1. Proof. As we shall prove in Lemma 2, any reserve matrix Q(â„“) (âˆ€â„“= 0,...,L ) can be bounded as: Dr Â·Q(â„“) â‰¤T(â„“) = ( Drâˆ’1ADâˆ’r)â„“ Â·X = Dr Â· ( Dâˆ’1A )â„“ Â· ( Dâˆ’rX ) , following Q(â„“) â‰¤ ( Dâˆ’1A )â„“ Â·(Dâˆ’rX). Hence, for âˆ€kâˆˆ{0,...,F âˆ’1}and âˆ€uâˆˆV, we have: Q(â„“)(u,k) â‰¤ [( Dâˆ’1A )â„“ Â· ( Dâˆ’rX )] (u,k) = âˆ‘ vâˆˆV ( Dâˆ’1A )â„“ (u,v) Â· ( Dâˆ’rX ) (v,k). Consequently, the sum of each column in Q(â„“) can be bounded by âˆ‘ uâˆˆV âââQ(â„“)(u,k) ââââ‰¤ âˆ‘ uâˆˆV âˆ‘ vâˆˆV âââ ( Dâˆ’1A )â„“ (u,v) Â· ( Dâˆ’rX ) (v,k) âââ = âˆ‘ vâˆˆV ââ( Dâˆ’rX ) (v,k) ââÂ· âˆ‘ uâˆˆV ( Dâˆ’1A )â„“ (u,v) = âˆ‘ vâˆˆV ââ( Dâˆ’rX ) (v,k) ââ= 1. (7) In the ï¬rst equality, we apply the fact that each entry of ( Dâˆ’1A )â„“ is nonzero. In the second equality, we apply the property that the sum of each column in the â„“-hop reverse transition probability matrix( Dâˆ’1A )â„“ equals 1, where â„“ âˆˆ{0,1,...}. And in the last equality, we use the fact that Dâˆ’rX is column normalized before the Reverse Push Propagation phase (line 7 in Algorithm 1). Thus, Lemma 4 follows. Based on Lemma 4, we can further bound the time cost of the Reverse Propagation process. Recall that in the Reverse Propagation phase of Algorithm 1, we push the residue R(â„“)(u,k) of node uto its 12neighbors whenever ââR(â„“)(u,k) ââ>rmax, k= 0,...,F âˆ’1. By Lemma 4, for a given level â„“and a given feature dimension k, the sum âˆ‘ uâˆˆV ââQ(â„“)(u,k) âââ‰¤1. Because Qâ„“ = Râ„“ before we empty Râ„“ (Line 13 in Algorithm 1), we have âˆ‘ uâˆˆV ââR(â„“)(u,k) âââ‰¤1. Thus, there are at most 1/rmax nodes with residues larger than rmax. For random features, the average cost for this push operation is d, the average degree of the graph. Consequently, the cost of Reverse Push for a given levelâ„“and a given feature dimension kis d rmax . Summing up â„“= 0,...,L âˆ’1 and k = 0,...,F âˆ’1, we can derive the total time cost of the Reverse Push Propagation phase as LdF rmax . By summing up the time cost of the three phases mentioned above, the time complexity of Algorithm 1 is bounded by O ( L2|Vt|nrF + LdF rmax ) , and Lemma 1 follows. A.2 Proof of Lemma 2 Let RHSdenote the right hand side of equation (5); We prove the Lemma by induction. Recall that in Algorithm 1, we initialize Q(t) = 0 and R(t) = 0 for t = 0,...,â„“ , and R(0) = Dâˆ’rX . Consequently, we have RHS= Dr ( Dâˆ’1A )â„“ R(0) = Dr ( Dâˆ’1A )â„“ Dâˆ’rX = ( Drâˆ’1ADâˆ’r)â„“ X = T(â„“), which is true by deï¬nition. Assuming Equation (5) holds at some stage, we will show that the invariant still holds after a push operation on node u. More speciï¬cally, let Iuk âˆˆRnÃ—F denote the matrix with entry at (u,k) setting to 1 and the rest setting to zero. Consider a push operation on uâˆˆV and kâˆˆ0,...,F âˆ’1 with |R(t)(u,k)|>rmax. We have two cases: (1) If t â‰¤â„“âˆ’1 , we have R(t) is decremented by R(t)(u,k) Â·Iuk and R(t+1) is incremented by R(t)(u,k) d(v) Â·Ivk for each vâˆˆN(u). Consequently, we have RHS= T(â„“) + Dr Â· ( Dâˆ’1A )â„“âˆ’t (âˆ’R(t)(u,k) Â·Iuk) +Dr(Dâˆ’1A)â„“âˆ’tâˆ’1 Â· âˆ‘ vâˆˆN(u) R(t)(u,k) d(v) Â·Ivk = T(â„“) + R(t)(u,k) Â·Dr(Dâˆ’1A)â„“âˆ’tâˆ’1 Â· ï£« ï£­ âˆ‘ vâˆˆN(u) 1 d(v) Â·Ivk âˆ’Dâˆ’1AIuk ï£¶ ï£¸ = T(â„“) + R(t)(u,k) Â·Dr(Dâˆ’1A)â„“âˆ’tâˆ’10 = T(â„“). For the second last equation, we use the fact that âˆ‘ vâˆˆN(u) 1 d(v) Â·Ivk = Dâˆ’1AIuk. (2) If t= â„“, we have R(â„“) is decremented by R(â„“)(u,k) Â·Iuk and Q(â„“) is incremented by R(â„“)(u,k) Â· Iuk. Consequently, we have RHS= T(â„“) + Dr Â· ( âˆ’R(â„“)(u,k) Â·Iuk ) + Dr Â· ( R(â„“)(u,k) Â·Iuk ) = T(â„“) + Dr Â·0 = T(â„“). Therefore, the induciton holds, and the Lemma follows. A.3 Proof of Theorem 1 Recall that P is the Generalized PageRank matrix that P = âˆ‘L â„“=0 wâ„“T(â„“). The weights wâ„“ satisfyâˆ‘âˆ â„“=0 wâ„“ â‰¤1. This suggests that for any sâˆˆVt and k= 0,...,F âˆ’1, by showing âââË†T(â„“)(s,k) âˆ’T(â„“)(s,k) âââ>d(s)rÎµ (8) holds with probability at most 1 nL, Algorithm 1 also achieves the desired accuracy. Note that here we also apply the union bound: Pr [âââË†P(s,k) âˆ’P(s,k) ââââ‰¤d(s)rÎµ ] â‰¥Pr [ Lâ‹‚ â„“=0 âââË†T(â„“)(s,k) âˆ’T(â„“)(s,k) ââââ‰¤d(s)rÎµ ] = 1âˆ’Pr [ Lâ‹ƒ â„“=0 âââË†T(â„“)(s,k)âˆ’T(â„“)(s,k) âââ>d(s)rÎµ ] â‰¥1âˆ’ Lâˆ‘ â„“=0 Pr [âââË†T(â„“)(s,k) âˆ’T(â„“)(s,k) âââ>d(s)rÎµ ] . 13Recall that we deï¬ne T(â„“) = Dr Â· ( Q(â„“) + âˆ‘â„“ t=0 ( Dâˆ’1A )â„“âˆ’t R(t) ) . And Lemma 2 ensures Ë†T(â„“) = Dr Â· ( Q(â„“) + âˆ‘â„“ t=0 S(â„“âˆ’t)R(t) ) is an unbiased estimator for T(â„“). Hence, we have: âââË†T(â„“)(s,k) âˆ’T(â„“)(s,k) âââ= d(s)r Â· âââââ â„“âˆ‘ t=0 ( S(â„“âˆ’t)R(t) ) (s,k) âˆ’ â„“âˆ‘ t=0 (( Dâˆ’1A )â„“âˆ’t Â·R(t) ) (s,k) âââââ = d(s)r Â· âââââ â„“âˆ‘ t=0 âˆ‘ uâˆˆV S(â„“âˆ’t)(s,u)R(t)(u,k) âˆ’ â„“âˆ‘ t=0 âˆ‘ uâˆˆV ( Dâˆ’1A )â„“âˆ’t (s,u) Â·R(t)(u,k) âââââ. According to Algorithm 1, each entry in residue matrix R(â„“) is bounded by rmax after the Reverse Push Propagation phase. It follows: âââË†T(â„“)(s,k)âˆ’T(â„“)(s,k) ââââ‰¤d(s)r Â· âââââ â„“âˆ‘ t=0 âˆ‘ uâˆˆV S(â„“âˆ’t)(s,u) Â·rmax âˆ’ â„“âˆ‘ t=0 âˆ‘ uâˆˆV ( Dâˆ’1A )â„“âˆ’t (s,u) Â·rmax âââââ, where S(â„“)(s,u) records the fraction of nr random walks starting from sâˆˆVt visit node uat the â„“-th steps. We can use the Chernoff Bound given in Lemma 3 to show âââË†T(â„“)(s,k) âˆ’T(â„“)(s,k) âââholds with high probability. To make use of the Chernoff Bound, we ï¬rst need to boundr, the maximum value of the random variables âˆ‘â„“ t=0 âˆ‘ uâˆˆV S(â„“âˆ’t)(s,u) Â·rmax. We note that âˆ‘ uâˆˆV S(â„“âˆ’t)(s,u) = âˆ‘ uâˆˆV âˆ‘nr w=1 x(â„“âˆ’t) w (s,u) nr â‰¤1. Here x(â„“âˆ’t) w (s,u) is an indicator variable. x(â„“âˆ’t) w (s,u) = 1when the w-th random walks starting from s walk at node u at the (â„“âˆ’t)-th step. Thus, we haveâˆ‘â„“ t=0 âˆ‘ uâˆˆV S(â„“âˆ’t)(s,u) Â·rmax â‰¤(â„“+ 1)Â·rmax. Plugging r= (â„“+ 1)Â·rmax into Lemma 3, we can derive: Pr [âââË†T(â„“)(s,k) âˆ’T(â„“)(s,k) âââ>d(s)rÎµ ] â‰¤Pr [âââââ â„“âˆ‘ t=0 âˆ‘ uâˆˆV S(â„“âˆ’t)(s,u)Â·rmaxâˆ’ â„“âˆ‘ t=0 âˆ‘ uâˆˆV ( Dâˆ’1A )â„“âˆ’t (s,u)Â·rmax âââââ>Îµ ] â‰¤exp ( âˆ’nr Â·Îµ2 (â„“+ 1)rmax (2Îµ 3 + 2Âµ ) ) , where Âµ= âˆ‘â„“ t=0 âˆ‘ uâˆˆV ( Dâˆ’1A )â„“âˆ’t (s,u)Â·rmax. Applying the fact that for any (â„“âˆ’t) âˆˆ{0,1,...},âˆ‘ uâˆˆV ( Dâˆ’1A )â„“âˆ’t (s,u) = 1, we have Âµ=âˆ‘â„“ t=0 rmax Â·âˆ‘ uâˆˆV ( Dâˆ’1A )â„“âˆ’t (s,u) = (â„“+ 1)Â·rmax. By setting nr = rmax(2ÎµL 3 +2rmaxÂ·L2)Â·log (nL) Îµ2 , we can ensure: Pr [âââË†T(â„“)(s,k) âˆ’T(â„“)(s,k) âââ>d(s)rÎµ ] â‰¤exp ( âˆ’nr Â·Îµ2 (â„“+ 1)rmax (2Îµ 3 + 2Âµ ) ) â‰¤ 1 nL, In the experimental part, we set L = 4, which is much smaller than Îµ and rmax. Hence, nr = O ( rmaxÂ·log (nL) Îµ2 ) . According to Lemma 1, the time complexity of Algorithm 1 can be expressed as: O ( LÂ·L|Vt|rmax Â·log (nL) Îµ2 Â·F + LÂ· d rmax Â·F ) . We observe that the above complexity is minimized whenLÂ·L|Vt|rmaxÂ·log (nL) Îµ2 Â·F = LÂ· d rmax Â·F, which implies the optimal rmax = âˆš Îµ2d L|Vt|log (nL) . Hence, the total time complexity of Algorithm 1 can be further bounded as O ( LÂ· âˆš L|Vt|d log (nL) Îµ Â·F ) , which follows Theorem 1. A.4 Proof of inequality(3) According to Lemma 2, the following Equation holds for any residue and reserve matricesQ(â„“),R(â„“), â„“= 0,...,L : T(â„“) = Dr Â· ( Q(â„“) + â„“âˆ‘ t=0 ( Dâˆ’1A )â„“âˆ’t R(t) ) . (9) 14Thus, for âˆ€sâˆˆV and âˆ€kâˆˆ{0,...,F âˆ’1}, we have Tâ„“(s,k) â‰¥ ( Dr Â·Qâ„“) (s,k). (10) On the other hand, when the Reverse Propagation process terminates, each residue entry satisï¬es R(â„“)(s,k) â‰¤rmax for âˆ€uâˆˆV and âˆ€kâˆˆ{0,...,F âˆ’1}. Hence, we can derive: ( â„“âˆ‘ t=0 ( Dâˆ’1A )â„“âˆ’t R(t) ) (s,k) = â„“âˆ‘ t=0 âˆ‘ uâˆˆV ( Dâˆ’1A )â„“âˆ’t (s,u) Â·R(t)(u,k) â‰¤(â„“+ 1)Â·rmax. In the last inequality, we apply the fact that R(t)(u,k) â‰¤rmax and âˆ‘ uâˆˆV ( Dâˆ’1A )â„“âˆ’t (s,u) = 1. Plugging into Equation (9), we can further derive the upper bound of Tâ„“(s,k) as below. T(â„“)(s,k) â‰¤ ( Dr Â·Qâ„“) (s,k) +d(s)r Â·(â„“+ 1)Â·rmax. (11) By combining the inequality (10) and inequality (11), we have: T(â„“)(s,k) âˆ’d(s)r Â·(â„“+ 1)Â·rmax â‰¤ ( Dr Â·Qâ„“) (s,k) â‰¤T(â„“)(s,k), which follows inequality (3). B Additional experimental results B.1 Comparison of inference time Figure 2 shows the inference time of each method. We observe that in terms of the inference time, the three linear models, SGC, PPRGo and GBP, have a signiï¬cant advantage over the two sampling-based models, LADIES and GraphSAINT. PPI Yelp Amazon 100 101 102 103 Time(second) SGC LADIES PPRGo GraphSAINT GBP Figure 2: Inference time of 6-layers models on the entire test graph. B.2 Additional details in experimental setup Table 7 summarizes URLs and commit numbers of baseline codes. Table 7: URLs of baseline codes. Methods URL Commit GCN https://github.com/rusty1s/pytorch_geometric 5692a8 GAT https://github.com/rusty1s/pytorch_geometric 5692a8 APPNP https://github.com/rusty1s/pytorch_geometric 5692a8 GDC https://github.com/klicperajo/gdc 14333f SGC https://github.com/Tiiiger/SGC 6c450f LADIES https://github.com/acbull/LADIES c7f987 PPRGo https://github.com/TUM-DAML/pprgo_pytorch d9f991 GraphSAINT https://github.com/GraphSAINT/GraphSAINT cd31c3 15",
      "meta_data": {
        "arxiv_id": "2010.15421v3",
        "authors": [
          "Ming Chen",
          "Zhewei Wei",
          "Bolin Ding",
          "Yaliang Li",
          "Ye Yuan",
          "Xiaoyong Du",
          "Ji-Rong Wen"
        ],
        "published_date": "2020-10-29T08:55:33Z",
        "pdf_url": "https://arxiv.org/pdf/2010.15421v3.pdf"
      }
    },
    {
      "title": "GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding",
      "abstract": "Graph embedding techniques have been increasingly deployed in a multitude of\ndifferent applications that involve learning on non-Euclidean data. However,\nexisting graph embedding models either fail to incorporate node attribute\ninformation during training or suffer from node attribute noise, which\ncompromises the accuracy. Moreover, very few of them scale to large graphs due\nto their high computational complexity and memory usage. In this paper we\npropose GraphZoom, a multi-level framework for improving both accuracy and\nscalability of unsupervised graph embedding algorithms. GraphZoom first\nperforms graph fusion to generate a new graph that effectively encodes the\ntopology of the original graph and the node attribute information. This fused\ngraph is then repeatedly coarsened into much smaller graphs by merging nodes\nwith high spectral similarities. GraphZoom allows any existing embedding\nmethods to be applied to the coarsened graph, before it progressively refine\nthe embeddings obtained at the coarsest level to increasingly finer graphs. We\nhave evaluated our approach on a number of popular graph datasets for both\ntransductive and inductive tasks. Our experiments show that GraphZoom can\nsubstantially increase the classification accuracy and significantly accelerate\nthe entire graph embedding process by up to 40.8x, when compared to the\nstate-of-the-art unsupervised embedding methods.",
      "full_text": "Published as a conference paper at ICLR 2020 GRAPH ZOOM : A MULTI -LEVEL SPECTRAL APPROACH FOR ACCURATE AND SCALABLE GRAPH EMBEDDING Chenhui Dengâˆ— Cornell University, Ithaca, USA cd574@cornell.edu Zhiqiang Zhaoâˆ— Michigan Technological University, Houghton, USA qzzhao@mtu.edu Yongyu Wang Michigan Technological University, Houghton, USA yongyuw@mtu.edu Zhiru Zhang Cornell University, Ithaca, USA zhiruz@cornell.edu Zhuo Feng Stevens Institute of Technology, Hoboken, USA zfeng12@stevens.edu ABSTRACT Graph embedding techniques have been increasingly deployed in a multitude of different applications that involve learning on non-Euclidean data. However, ex- isting graph embedding models either fail to incorporate node attribute informa- tion during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high com- putational complexity and memory usage. In this paper we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsuper- vised graph embedding algorithms. 1 GraphZoom ï¬rst performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coars- ened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding methods to be applied to the coars- ened graph, before it progressively reï¬ne the embeddings obtained at the coarsest level to increasingly ï¬ner graphs. We have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase the classiï¬cation accuracy and signiï¬cantly accelerate the entire graph embedding process by up to 40.8Ã—, when compared to the state-of-the-art unsupervised embedding methods. 1 I NTRODUCTION Recent years have seen a surge of interest in graph embedding, which aims to encode nodes, edges, or (sub)graphs into low dimensional vectors that maximally preserve graph structural information. Graph embedding techniques have shown promising results for various applications such as ver- tex classiï¬cation, link prediction, and community detection (Zhou et al., 2018); (Cai et al., 2018); (Goyal & Ferrara, 2018). However, current graph embedding methods have several drawbacks in terms of either accuracy or scalability. On the one hand, random-walk-based embedding algorithms, such as DeepWalk (Perozzi et al., 2014) and node2vec (Grover & Leskovec, 2016), attempt to embed a graph based on its topology without incorporating node attribute information, which limits their embedding power. Later, graph convolutional networks (GCN) are developed with the basic notion that node embeddings should be smoothed over the entire graph (Kipf & Welling, 2016). While GCN leverages both topology and node attribute information for simpliï¬ed graph convolution in each layer, it may suffer from high-frequency noise in the initial node features, which compromises âˆ—Equal contributions 1Source code of GraphZoom is freely available at: github.com/cornell-zhang/GraphZoom. 1 arXiv:1910.02370v2  [cs.LG]  17 Feb 2020Published as a conference paper at ICLR 2020 the embedding quality (Maehara, 2019). On the other hand, few embedding algorithms can scale well to large graphs with millions of nodes due to their high computation and storage cost (Zhang et al., 2018a). For example, graph neural networks (GNNs) such as GraphSAGE (Hamilton et al., 2017) collectively aggregate feature information from the neighborhood. When stacking multiple GNN layers, the ï¬nal embedding vector of a node involves the computation of a large number of in- termediate embeddings from its neighbors. This will not only cause drastic increase in the amount of computation among nodes, but also lead to high memory usage for storing the intermediate results. In literature, increasing the accuracy and improving the scalability of graph embedding methods are largely viewed as two orthogonal problems. Hence most research efforts are devoted to addressing only one of the problems. For instance, Chen et al. (2018) and Fu et al. (2019) proposed multi- level methods to obtain high-quality embeddings by training unsupervised models at every level; but their techniques do not improve scalability due to the additional training overhead. Liang et al. (2018) developed a heuristic algorithm to coarsen the graph by merging nodes with similar local structures. They use GCN to reï¬ne the embedding results on the coarsened graphs, which is not only time consuming to train but also potentially degrading accuracy when the number of GCN layers increases. More recently, Akbas & Aktas (2019) proposed a similar strategy to coarsen the graph, where certain useful structural properties of the graph are preserved (e.g., local neighborhood proximity). However, this work lacks proper reï¬nement methods to improve the embedding quality. In this paper we propose GraphZoom, a multi-level spectral approach to enhancing both the quality and scalability of unsupervised graph embedding methods. More concretely, GraphZoom consists of four major kernels: (1) graph fusion, (2) spectral graph coarsening, (3) graph embedding, and (4) embedding reï¬nement. The graph fusion kernel ï¬rst converts the node feature matrix into a feature graph and then fuses it with the original topology graph. The fused graph provides richer information to the ensuing graph embedding step to achieve a higher accuracy. Spectral graph coarsening produces a series of successively coarsened graphs by merging nodes based on their spectral similarities. We show that our coarsening algorithm can effectively and efï¬ciently retain the ï¬rst few eigenvectors of the graph Laplacian matrix, which is critical for preserving the key graph structures. During the graph embedding step, any of the existing unsupervised graph embedding techniques can be applied to obtain node embeddings for the graph at the coarsest level.2 Embedding reï¬nement is then employed to reï¬ne the embeddings back to the original graph by applying a proper graph ï¬lter to ensure embeddings are smoothed over the graph. We evaluate the proposed GraphZoom framework on three transductive benchmarks: Cora, Citeseer and Pubmed citation networks as well as two inductive dataset: PPI and Reddit for vertex classiï¬- cation task. We further test the scalability of our approach on friendster dataset, which contains 8 million nodes and 400 million edges. Our experiments show that GraphZoom can improve the clas- siï¬cation accuracy over all baseline embedding methods for both transductive and inductive tasks. Our main technical contributions are summarized as follows: â€¢GraphZoom generates high-quality embeddings. We propose novel algorithms to encode graph structures and node attribute information in a fused graph and exploit graph ï¬ltering during re- ï¬nement to remove high-frequency noise. This results in a relative increase of the embedding accuracy over the prior arts by up to 19.4% while reducing the execution time by at least 2Ã—. â€¢GraphZoom improves scalability. Our approach can signiï¬cantly reduce the embedding run time by effectively coarsening the graph without losing the key spectral properties. Experiments show that GraphZoom can accelerate the entire embedding process by up to 40.8Ã—while produc- ing a similar or better accuracy than state-of-the-art techniques. â€¢GraphZoom is highly composable. Our framework is agnostic to underlying graph embedding techniques. Any of the existing unsupervised embedding methods, either transductive or inductive, can be incorporated by GraphZoom in a plug-and-play manner. 2 R ELATED WORK There is a large and active body of research on multi-level graph embedding and graph ï¬ltering, from which GraphZoom draws inspiration to boost the performance and speed of unsupervised 2In this work, we do not attempt to preserve node label information in the coarsened graph. Nonetheless, we believe that our approach can be extended to support supervised embedding models such as GAT (Gulcehre et al., 2019) and PPNP (Klicpera et al., 2019), as brieï¬‚y discussed in Section 5. 2Published as a conference paper at ICLR 2020 embedding methods. Due to the space limitation, we only summarize some of the recent efforts in these two areas. Multi-level graph embedding attempts to coarsen the original graph into a series of smaller graphs with decreasing size where existing or new graph embedding techniques can be applied at different coarsening levels. For example, Chen et al. (2018); Lin et al. (2019) generate a hierarchy of coars- ened graphs and perform embedding from the coarsest level to the original one. Fu et al. (2019) construct and embed multi-level graphs through hierarchical, and the resulting embedding vectors are concatenated to obtain the ï¬nal node embeddings for the original graph. These methods, how- ever, only focus on improving embedding quality but not the scalability. Later, Zhang et al. (2018b); Akbas & Aktas (2019) attempt to make graph embedding more scalable by only embedding on the coarsest graph. However, their methods lack proper reï¬nement methods to generate high-quality embeddings for the original graph. Liang et al. (2018) propose MILE, which only trains the coarsest graph to obtain coarse embeddings, and leverages GCN as reï¬nement method to improve embed- ding quality. However, MILE requires training a GCN model which is very time consuming for large graphs and leading to poor performance when multiple GCN layers are stacked together (Li et al., 2018). In contrast to the prior arts, GraphZoom is motivated by theoretical results in spectral graph embedding (Tang & Liu, 2011) and practically efï¬cient so that it can improve both accuracy and scalability of the unsupervised graph embedding tasks. Graph ï¬ltering are direct analogs of classical ï¬lters in signal processing ï¬eld, but intended for signals deï¬ned on graphs. Shuman et al. (2013) deï¬ne graph ï¬lters in both vertex and spectral domains, and apply graph ï¬lter on image denoising and reconstruction tasks. Wu et al. (2019) leverage graph ï¬lter to simplify GCN model by removing redundant computation. More recently, Maehara (2019) show the fundamental link between graph embedding and ï¬ltering by proving that GCN model implicitly exploits graph ï¬lter to remove high-frequency noise from the node feature matrix; a ï¬lter neural network (gfNN) is then proposed to derive a stronger graph ï¬lter to improve the embedding results. Li et al. (2019) further derive two generalized graph ï¬lters and apply them on graph embedding models to improve their embedding quality for various classiï¬cation tasks. In GraphZoom we adopt graph ï¬lter to properly smooth the intermediate embedding results during the iterative reï¬nement step, which is crucial for improving the quality of the ï¬nal embedding. 3 G RAPH ZOOM FRAMEWORK GraphZoomFramework Original Graph Topology 1. Graph Fusion 2. Spectral Coarsening3. Graph Embedding 4. Embedding Refinement !\" Node Attributes â€¦!# !$ !%â€¦ &\"&#&% &' Graph w/ Node Embeddings Figure 1: Overview of the GraphZoom framework. Figure 1 shows the proposed GraphZoom framework, which consists of four key phases: Phase (1) is graph fusion, which combines the node attributes and topology information of the original graph to construct a fused weighted graph; In Phase (2), a spectral graph coarsening process is applied to form a hierarchy of coarsened fused graphs with decreasing size; In Phase (3), any of the existing graph embedding methods can be applied to the fused graph at the coarsest level; In Phase (4), the embedding vectors obtained at the coarsest level are mapped onto a ï¬ner graph using the mapping 3Published as a conference paper at ICLR 2020 operators determined during the coarsening phase. This is followed by a reï¬nement (or smoothing) procedure, where the procedure in Phase (4) is applied in an iterative manner to increasingly ï¬ner graphs; Eventually the embedding vectors for the original graph are obtained. In the rest of this section, we describe each of these four phases in more detail. 3.1 P HASE 1: G RAPH FUSION Graph fusion aims to construct a weighted graph that has the same number of nodes as the original graph but potentially different set of edges (weights) that encapsulate the original graph topology as well as node attribute information. Speciï¬cally, given an undirected graph G= (V,E) with N = |V|nodes, its adjacency matrix Atopo âˆˆRNÃ—N and its node attribute (feature) matrix X âˆˆRNÃ—K, where K corresponds to the dimension of node attribute vector, graph fusion can be interpreted as a function f(Â·) that outputs a weighted graph Gfusion = (V,Efusion ) represented by its adjacency matrix Afusion âˆˆRNÃ—N , namely, Afusion = f(Atopo,X). We ï¬rst convert the initial attribute matrixXinto a weighted node attribute graphGfeat = (V,Efeat ) by generating a k-nearest-neighbor (kNN) graph based on the l2-norm distance between the at- tribute vectors of each node pair. A straightforward implementation requires comparing all possible node pairs and then selecting top-knearest neighbors. However, such a naÂ¨Ä±ve approach has a worst- case time complexity of O(N2), which certainly does not scale to large graphs. Our goal is to construct the attribute graph in nearly linear time by leveraging an efï¬cient spectral graph coarsen- ing scheme that is described in more detail in Section 3.2. More speciï¬cally, our approach starts with coarsening the original graph Gto obtain a substantially reduced graph that has much fewer nodes with an O(|E|) time complexity. Note that this procedure bears similarity to spectral graph clustering, which aims to group nodes into clusters of high conductance (Peng et al., 2015). Once the node clusters are formed through spectral coarsening, we can select the top-knearest neighbors within each cluster with O(M2) comparisons, where M is the average node count per cluster. Since we have roughly N/M clusters, the time complexity for constructing the approximate kNN graph is O(MN). When a proper coarsening ratio is chosen where M â‰ªN (e.g., M = 50), the overall time complexity will become almost linear. After the attribute graph is formed, we assign a weight to each edge based on the cosine similarity between the attribute vectors of the two incident nodes, namelywi,j = (Xi,: Â·Xj,:)/(âˆ¥Xi,:âˆ¥âˆ¥Xj,:âˆ¥), where Xi,: and Xj,: are the attribute vectors of nodes iand j. Finally, we can construct the fused graph by combining the topology graph and the attribute graph using a weighted sum: Afusion = Atopo + Î²Afeat , where Î²allows us to balance the topological and node attribute information during the fusion process. This fused graph can be fed into any downstream graph embedding procedures. 3.2 P HASE 2: S PECTRAL COARSENING Graph coarsening via global spectral embedding. To reduce the size of the original graph while preserving important spectral properties (e.g., the ï¬rst few eigenvalues and eigenvectors of the graph Laplacian matrix 3), a straightforward way is to ï¬rst embed the graph into a k-dimensional space using the ï¬rst k eigenvectors of the graph Laplacian matrix, which is also known as the spectral graph embedding technique (Belkin & Niyogi, 2003; Peng et al., 2015). Next, the graph nodes that are close to each other in the low-dimensional embedding space can be aggregated to form the coarse-level nodes and subsequently the reduced graph. However, it is very costly to calculate the eigenvectors of the original graph Laplacian, especially for very large graphs. Graph coarsening via local spectral embedding. In this work, we leverage an efï¬cient yet ef- fective local spectral embedding scheme to identify node clusters based on emerging graph signal processing techniques (Shuman et al., 2013). There are obvious analogies between the traditional signal processing (Fourier analysis) and graph signal processing: (1) The signals at different time points in classical Fourier analysis correspond to the signals at different nodes in an undirected graph; (2) The more slowly oscillating functions in time domain correspond to the graph Laplacian eigenvectors associated with lower eigenvalues or the more slowly varying (smoother) components across the graph. Instead of directly using the ï¬rst few eigenvectors of the original graph Laplacian, 3Laplacian matrix L is deï¬ned as L = D âˆ’ A, where D is degree matrix and A is adjacency matrix. 4Published as a conference paper at ICLR 2020 we apply the simple smoothing (low-pass graph ï¬ltering) function to k random vectors to obtain smoothed vectors for k-dimensional graph embedding, which can be achieved in linear time. Consider a random vector (graph signal) xthat is expressed with a linear combination of eigenvec- tors u of the graph Laplacian. We adopt low-pass graph ï¬lters to quickly ï¬lter out the high-frequency components of the random graph signal or the eigenvectors corresponding to high eigenvalues of the graph Laplacian. By applying the smoothing function on x, we obtain a smoothed vector Ëœx, which is basically the linear combination of the ï¬rst few eigenvectors: x = Î£N i=1Î±iui smoothing = = = = =â‡’ Ëœx = Î£n i=1 ËœÎ±iui , nâ‰ªN (1) More speciï¬cally, we apply a few (typically ï¬ve to ten) Gauss-Seidel iterations for solving the linear system of equations LGx(i) = 0 to a set of tinitial random vectors T = (x(1),...,x (t)) that are orthogonal to the all-one vector 1 satisfying 1âŠ¤x(i) = 0, and LGis the Laplacian matrix of graph G or Gfusion . Based on the smoothed vectors inT, we embed each node into at-dimensional space such that nodes pand qare considered spectrally similar if their low-dimensional embedding vectors xp âˆˆRt and xq âˆˆRt are highly correlated. Here the node distance is measured by the spectral node afï¬nity ap,q for neighboring nodes pand q(Livne & Brandt, 2012; Chen & Safro, 2011): ap,q = |(Tp,:,Tq,:)|2 âˆ¥Tp,:âˆ¥2âˆ¥Tq,:âˆ¥2 , (Tp,:,Tq,:) = Î£t k=1(x(k) p Â·x(k) q ) (2) Once the node aggregation schemes are determined, we can easily obtain the graph mapping operator Hi+1 i between two coarsening levels iand i+ 1. More precisely, Hi+1 i is a matrix of size |VGi+1 |Ã— |VGi |. (Hi+1 i )p,q = 1 if node q in graph Gi is aggregated to (clustered) node p in graph Gi+1; otherwise, it is set to 0. Additional discussions on the properties of Hi+1 i are available in Appendix F, which shows this operator a surjective mapping and locality preserving. We leverage H1 0 ,H2 1 ,Â·Â·Â· ,Hl lâˆ’1 for constructing a series of spectrally-reduced graphs G1,G2,Â·Â·Â· ,Gl. Speciï¬cally, The coarser graph Laplacian LGi+1 can be computed by Eq. (3). It is worth noting that G0 (i.e., Gfusion ) is the original fused graph and |V0|= N >|V1|>Â·Â·Â· >|Vl|. LGi+1 = Hi+1 i LGi Hi i+1, Hi i+1 = (Hi+1 i )T (3) We emphasize that the aggregation scheme based on the above spectral node afï¬nity calculations will have a (linear) complexity of O(|Efusion |) and thus allow preserving the spectral (global or structural) properties of the original graph in a highly efï¬cient and effective way. As suggested in (Zhao & Feng, 2019; Loukas, 2019), a spectral sparsiï¬cation procedure can be applied to effectively control densities of coarse level graphs. In this work, a similarity-aware spectral sparsiï¬cation tool â€œGRASSâ€ (Feng, 2018) has been adopted for achieving a desired graph sparsity at the coarsest level. 3.3 P HASE 3: G RAPH EMBEDDING Embedding the Coarsest Graph. Once the coarsest graph Gl is constructed, node embeddings El on Gl can be obtained by El = g(Gl), where g(Â·) can be any unsupervised embedding methods. 3.4 P HASE 4: E MBEDDING REFINEMENT Given Ei+1, the node embeddings of graph Gi+1 at level i+ 1, we can use the corresponding projection operator Hi i+1 to project Ei+1 to Gi, which is the ï¬ner graph at level i: Ë†Ei = Hi i+1Ei+1 (4) Due to the property of the projection operator, embedding of the node in the coarser graph will be directly copied to the nodes of the same aggregation set in the ï¬ner graph at the preceding level. In this case, spectrally-similar nodes in the ï¬ner graph will have the same embedding results if they are aggregated into a single node during the coarsening phase. 5Published as a conference paper at ICLR 2020 To further improve the quality of the mapped embeddings, we apply a local reï¬nement process motivated by Tikhonov regularization to smooth the node embeddings over the graph by minimizing the following objective: min Ei { îµ¹îµ¹îµ¹Ei âˆ’ Ë†Ei îµ¹îµ¹îµ¹ 2 2 + tr(Ei TLiEi)} (5) where Li and Ei are the normalized Laplacian matrix and mapped embedding matrix of the graph at the i-th coarsening level, respectively. We obtain the reï¬ned embedding matrix ËœEi by solving Eq. (5). Here the ï¬rst term enforces the reï¬ned embeddings to agree with mapped embeddings, while the second term employs Laplacian smoothing to smooth ËœEi over the graph. By taking the derivative of the objective function in Eq. (5) and setting it to zero, we have: Ei = (I + Li)âˆ’1 Ë†Ei (6) where I is the identity matrix. However, obtaining reï¬ned embeddings in this manner can be very inefï¬cient since it involves matrix inversion whose time complexity is O(N3). Instead, we exploit a more efï¬cient spectral graph ï¬lter to smooth the embeddings. By transforming h(L) = (I+ L)âˆ’1 into spectral domain, we obtain the graph ï¬lter: h(Î») = (1 + Î»)âˆ’1. To avoid the inversion term, we approximateh(Î») by its ï¬rst-order Taylor expansion, namely, Ëœh(Î») = 1 âˆ’Î». We then generalize Ëœh(Î») to Ëœhk(Î») = (1 âˆ’Î»)k, where k controls the power of graph ï¬lter. After transforming Ëœhk(Î») into the spatial domain, we have: Ëœhk(L) = ( I âˆ’L)k = (Dâˆ’1 2 ADâˆ’1 2 )k, where Ais the adjacency matrix and D is the degree matrix. It is not difï¬cult to show that adding a proper self-loop for every node in the graph will allowËœhk(L) to more effectively ï¬lter out high-frequency noise components (Maehara, 2019) (see Appendix H). Thus, we modify the adjacency matrix as ËœA = A+ ÏƒI, where Ïƒ is a small value to ensure every node has its own self-loop. Finally, the low-pass graph ï¬lter can be utilized to smooth the mapped embedding matrix, as shown in (7): Ei = ( ËœDi âˆ’1 2 ËœAi ËœDi âˆ’1 2 )k Ë†Ei = ( ËœDi âˆ’1 2 ËœAi ËœDi âˆ’1 2 )kHi i+1Ei+1 (7) We iteratively apply Eq. (7) to obtain the embeddings of the original graph (i.e., E0). Note that our reï¬nement stage does not involve training and can be simply considered as several (sparse) matrix multiplications, which can be efï¬ciently computed. 4 E XPERIMENTS We have performed comparative evaluation of GraphZoom framework against several state-of-the- art unsupervised graph embedding techniques and multi-level embedding frameworks on ï¬ve stan- dard graph-based datasets (transductive as well as inductive). In addition, we evaluate the scalability of GraphZoom on Friendster dataset, which contains8 million nodes and 400 million edges. Finally, we conduct ablation study to understand the effectiveness of the major GraphZoom kernels. 4.1 E XPERIMENTAL SETUP Table 1: Statistics of datasets used in our experiments. Dataset Type Task Nodes Edges Classes Features Cora Citation network Transductive 2,708 5,429 7 1,433 Citeseer Citation network Transductive 3,327 4,732 6 3,703 Pubmed Citation network Transductive 19,717 44,338 3 500 PPI Biology network Inductive 14,755 222,055 121 50 Reddit Social network Inductive 232,965 57,307,946 210 5,414 Friendster Social network Transductive 7,944,949 446,673,688 5,000 N/A Datasets. Table 1 reports the statistics of the datasets used in our experiments. We include Cora, Citeseer, Pubmed, and Friendster for evaluation on transductive learning tasks, and PPI as well as 6Published as a conference paper at ICLR 2020 Reddit for inductive learning. We split the training and testing data in the same way as suggested in Kipf & Welling (2016); Hamilton et al. (2017). Transductive baseline models. A number of popular graph embedding techniques are transductive learning methods, which require all nodes in the graph be present during training. Hence such embedding models must be retrained whenever a new node is added. Here we compare GraphZoom with three transductive models: DeepWalk (Perozzi et al., 2014), node2vec (Grover & Leskovec, 2016), and Deep Graph Infomax (DGI) (Velikovi et al., 2019) 4. These methods have shown state- of-the-art unsupervised embedding results on the datasets used in our experiments. In addition, we compare GraphZoom with two multi-level graph embedding frameworks: HARP (Chen et al., 2018) and MILE (Liang et al., 2018), which have reported improvement over DeepWalk and node2vec in either embedding quality or scalability. Inductive baseline models. In contrast to transductive tasks, training an inductive graph embedding model does not require seeing the whole graph structure. Hence the resulting trained model can still be applied when new nodes are added to graph. To show GraphZoom can also enhance inductive learning, we compare it against GraphSAGE (Hamilton et al., 2017) using four different aggregation functions including GCN, mean, LSTM, and pooling. More details of datasets and baselines are available in Appendix A and B. We optimize hyperpa- rameters of DeepWalk, node2vec, DGI, and GraphSAGE to achieve highest possible accuracy on original datasets; we then choose the same hyperparameters to embed the coarsened graph in HARP, MILE, and GraphZoom. We run all the experiments on a Linux machine with an Intel Xeon Gold 6242 CPU (32 cores @ 2.40GHz) and 384 GB of RAM. 4.2 P ERFORMANCE AND SCALABILITY OF GRAPH ZOOM Since HARP and MILE only support transductive learning, we compare them with GraphZoom using DeepWalk, node2vec, and DGI (Velikovi et al., 2019) as embedding kernels. For inductive tasks, we compare GraphZoom with GraphSAGE using four different aggregation functions. Tables 2 and 3 report the mean classiï¬cation accuracy for the transductive task and micro-averaged F1 score for the inductive task, respectively, as well as the execution time for all baselines and Graph- Zoom. Speciï¬cally, we use the CPU time for graph embedding as the execution time of DeepWalk, node2vec, DGI, and GraphSAGE. We further measure the execution time of HARP and MILE by summing up CPU time for graph coarsening, graph embedding, and embedding reï¬nement. Simi- larly, we add up the CPU time for graph fusion, graph coarsening, graph embedding, and embedding reï¬nement as the execution time of GraphZoom. In regard to hyperparameters, we use10 walks with a walk length of 80, a window size of 10, and an embedding dimension of 128 for both DeepWalk and node2vec; we further set the return parameter pand the in-out parameter qin node2vec as 1.0 and 0.5, respectively. Moreover, we choose early stopping strategy for DGI with a learning rate of 0.001 and an embedding dimension of 512. Apropos the conï¬guration of GraphSAGE, we train a two-layer model for one epoch, with a learning rate of 0.00001, an embedding dimension of 128, and a batch size of 256. Comparing GraphZoom with baseline embedding methods. We show the results of GraphZoom with three coarsening levels for transductive learning and two levels for inductive learning. The size of coarsened graphs and results with larger coarsening level are available in Appendix D, Figure 3 (blue curve), and the Appendix J. Our results demonstrate that GraphZoom is agnostic to underlying embedding methods and capable of boosting the accuracy and speed of state-of-the-art unsupervised embedding methods on various datasets. More speciï¬cally, for transductive learning tasks, GraphZoom improves classiï¬cation accuracy upon both DeepWalk and node2vec by a relative gain of8.3%, 10.4%, and 19.4% 5 on Cora, Pubmed, and Citeseer, respectively, while achieving up to40.8Ã—run-time reduction. In regard to comparing with DGI, GraphZoom achieves comparable or better accuracy with speedup up to 11.2Ã—. Similarly, GraphZoom outperforms all the baselines by a margin of 3.4% and 3.3% on PPI and Reddit for inductive learning tasks, respectively, with speedup up to 7.6Ã—. These results indicate that our 4DGI also supports inductive tasks, although the authors have only released the source code for transductive learning at github.com/PetarV-/DGI, as of the date of this experiment. 5GZoom(N2V ,l=1) improves over node2vec on Citeseer by 19.4% = (54.7 âˆ’ 45.8)/45.8. 7Published as a conference paper at ICLR 2020 Table 2: Summary of results in terms of mean classiï¬cation accuracy and CPU time for transduc- tive tasks, on Cora, Citeseer, and Pubmed datasets â€” DW, N2V , and GZoom denote DeepWalk, node2vec, and GraphZoom, respectively;lmeans the graph coarsening level; GZoom F+MILE rep- resents the best performance achieved when adding GraphZoom fusion kernel into MILE. Cora Citeseer Pubmed Method Accuracy(%) Time(secs) Accuracy(%) Time(secs) Accuracy(%) Time(mins) DeepWalk 71.4 97.8 47.0 120.0 69.9 14.1 HARP(DW) 71.3 296.7 (0.3 Ã—) 43.2 272.4 (0.4 Ã—) 70.6 33.9 (0.4 Ã—) MILE(DW,l=1) 71.9 68.7 (1.4 Ã—) 46.5 53.7 (2.2 Ã—) 69.6 7.0 (2.0 Ã—) MILE(DW,l=2) 71.3 30.9 (3.2 Ã—) 47.3 22.5 (5.3 Ã—) 66.7 4.4 (2.3 Ã—) MILE(DW,l=3) 70.6 15.9 (6.1 Ã—) 47.1 9.9 (12.1 Ã—) 64.5 2.5 (5.8 Ã—) GZoomF+MILE(DW) 73.8 70.6 (1.4 Ã—) 48.9 24.7 (4.9 Ã—) 72.1 7.0 (2.0 Ã—) GZoom(DW,l=1) 76.9 39.6 (2.5 Ã—) 49.7 19.6 (2.1 Ã—) 75.3 4.0 (3.6 Ã—) GZoom(DW,l=2) 77.3 15.6 (6.3Ã—) 50.8 6.7 (6.0Ã—) 75.9 1.7 (8.3 Ã—) GZoom(DW,l=3) 75.1 2.4 (40.8Ã—) 49.5 1.3 (30.8Ã—) 77.2 0.6 (23.5 Ã—) node2vec 71.5 119.7 45.8 126.9 71.3 15.6 HARP(N2V) 72.3 171.0 (0.7 Ã—) 44.8 174.3 (0.7 Ã—) 70.1 46.1 (0.3 Ã—) MILE(N2V ,l=1) 72.1 57.3 (2.1 Ã—) 46.1 60.9 (2.1 Ã—) 70.8 7.3 (2.1 Ã—) MILE(N2V ,l=2) 71.8 30.0 (4.0 Ã—) 45.7 28.8 (4.4 Ã—) 67.3 4.3 (3.6 Ã—) MILE(N2V ,l=3) 68.5 16.5 (7.2 Ã—) 45.2 15.6 (8.1 Ã—) 61.8 1.8 (8.0 Ã—) GZoomF+MILE(N2V) 74.3 59.2 (2.0 Ã—) 48.3 62.3 (2.0 Ã—) 72.9 7.3 (2.1 Ã—) GZoom(N2V ,l=1) 77.3 43.5 (2.8Ã—) 54.7 38.1 (3.3Ã—) 77.0 3.0 (5.2 Ã—) GZoom(N2V ,l=2) 77.0 13.5 (8.9 Ã—) 51.7 15.3 (8.3 Ã—) 77.8 1.5 (10.4Ã—) GZoom(N2V ,l=3) 75.3 3.0 (39.9Ã—) 50.7 4.5 (28.2Ã—) 77.4 0.4 (39.0Ã—) DGI 82.3 89.7 71.8 94.6 76.8 23.7 MILE(DGI,l=1) 80.9 45.9 (1.9 Ã—) 69.9 53.2 (1.8 Ã—) 76.1 10.4 (2.3 Ã—) MILE(DGI,l=2) 80.3 27.5 (3.3 Ã—) 69.2 31.1 (3.0 Ã—) 74.3 4.3 (5.5 Ã—) MILE(DGI,l=3) 79.2 15.6 (4.4 Ã—) 67.9 18.5 (5.1 Ã—) 74.4 2.7 (8.8 Ã—) GZoomF+MILE(DGI) 81.3 45.9 (1.9 Ã—) 70.4 52.3 (1.8 Ã—) 75.9 10.4 (2.3 Ã—) GZoom(DGI,l=1) 83.9 27.3 (3.3Ã—) 71.1 29.3 (3.2 Ã—) 77.1 9.6 (2.5 Ã—) GZoom(DGI,l=2) 83.8 15.2 (5.9 Ã—) 70.8 17.9 (5.3 Ã—) 77.6 4.4 (5.4Ã—) GZoom(DGI,l=3) 83.5 8.0 (11.2Ã—) 70.7 9.6 (9.8Ã—) 76.9 2.1 (11.2Ã—) multi-level spectral approach improves both embedding speed and quality â€” GraphZoom runs much faster since we only train the embedding model on the smallest fused graph at the coarsest level. In addition to reducing the graph size, our coarsening method further ï¬lters out redundant information from the original graph while preserving key spectral properties for the embedding. Hence we also observe improved embedding quality in terms of classiï¬cation accuracy. Comparing GraphZoom with multi-level frameworks. As shown in Table 2, HARP slightly improves the accuracy in several cases but increases the CPU time. Although MILE improves both accuracy and speed over a few baseline embedding methods, the performance of MILE be- comes worse with increasing coarsening levels. For instance, the classiï¬cation accuracy of MILE drops from 0.708 to 0.618 on Pubmed dataset with node2vec as the embedding kernel. GraphZoom achieves a better accuracy and speedup compared to MILE with the same coarsening level across all datasets. Moreover, when increasing the coarsening levels, namely, decreasing number of nodes on the coarsened graph, GraphZoom still produces comparable or even a better embedding accuracy with much shorter CPU times. This further conï¬rms GraphZoom can retain the key graph structure information to be utilized by underlying embedding models to boost embedding quality. More re- sults of GraphZoom on non-attributed graph for both node classiï¬cation and link prediction tasks are available in Appendix K. GraphZoom for large graph embedding. To show GraphZoom can signiï¬cantly improve per- formance and scalability of underlying embedding model on large graph, we test GraphZoom and MILE on Friendster dataset, which contains 8 million nodes and 400 million edges. For both cases, 8Published as a conference paper at ICLR 2020 Table 3: Summary of results in terms of micro-averaged F1 score and CPU time for inductive tasks, on PPI and Reddit datasets â€” The baselines are GraphSAGE with four different aggregation functions. GZoom and GSAGE denote GraphZoom and GraphSAGE, respectively; l means the graph coarsening level. PPI Reddit Method Micro-F1 Time(mins) Micro-F1 Time(hours) GraphSAGE-GCN 0.601 9.6 0.908 10.1 GZoom(GSAGE-GCN, l=1) 0.621 4.8 (2.0Ã—) 0.923 3.4 (3.0Ã—) GZoom(GSAGE-GCN, l=2) 0.612 1.8 (5.2Ã—) 0.917 1.6 (6.3Ã—) GraphSAGE-mean 0.598 11.1 0.897 8.1 GZoom(GSAGE-mean, l=1) 0.614 5.2 (2.2 Ã—) 0.925 2.6 (3.1Ã—) GZoom(GSAGE-mean, l=2) 0.617 1.8 (6.2 Ã—) 0.919 1.2 (6.8Ã—) GraphSAGE-LSTM 0.596 387.3 0.907 92.2 GZoom(GSAGE-LSTM, l=1) 0.614 151.8 (2.6 Ã—) 0.920 39.8 (2.3Ã—) GZoom(GSAGE-LSTM, l=2) 0.615 52.5 (7.4 Ã—) 0.917 14.5 (6.4Ã—) GraphSAGE-pool 0.602 144.9 0.892 84.3 GZoom(GSAGE-pool, l=1) 0.611 66.0 (2.2 Ã—) 0.921 27.0 (3.1Ã—) GZoom(GSAGE-pool, l=2) 0.614 23.4 (6.2 Ã—) 0.912 12.4 (6.8Ã—) 1 2 3 4 5 Coarsening Level 0.2 0.4 0.6 0.8Micro-F1 Friendster (Micro-F1) 1 2 3 4 5 Coarsening Level 0 50 100CPU time Speedup Friendster (Speedup) GraphZoom(DW) MILE(DW) DeepWalk Figure 2: Comparisons of GraphZoom and MILE on Friendster dataset. we use DeepWalk as the embedding kernel. As shown in Figure 2, GraphZoom drastically boosts the Micro-F1 score up to 47.6% compared to MILE and 49.9% compared to DeepWalk with a speedup up to 119.8Ã—. When increasing the coarsening level, GraphZoom achieves a higher speedup while the embedding accuracy decreases gracefully. This shows the key strength of GraphZoom: it can effectively coarsen a large graph by merging many redundant nodes that are spectrally similar, thus preserving the graph spectral (structural) properties that are important to the underlying embedding model. When applying basic embedding model on coarsest graph, it can learn more global infor- mation from spectral domain, leading to high-quality node embeddings. On the contrary, heuristic graph coarsening algorithm used in MILE fails to preserve a meaningful coarsest graph, especially when coarsening graph by a large reduction ratio. 4.3 A BLATION ANALYSIS ON GRAPH ZOOM KERNELS To study the effectiveness of our proposed GraphZoom kernels separately, we compare each of them against the corresponding kernel in MILE while ï¬xing other kernels. As shown in Figure 3, when ï¬xing coarsening kernel and comparing reï¬nement kernel of GraphZoom with that of MILE, Our re- 9Published as a conference paper at ICLR 2020 1 2 3 4 Coarsening Level 70 72 74 76 78Accuracy Cora (Accuracy) 1 2 3 4 5 Coarsening Level 46 47 48 49 50 51Accuracy Citeseer (Accuracy) 1 2 3 4 5 Coarsening Level 50 60 70 80Accuracy Pubmed (Accuracy) GZoom_F + GZoom_C + GZoom_R (DW) GZoom_C + GZoom_R (DW) GZoom_C + MILE_R (DW) MILE_C + GZoom_R (DW) MILE_C + MILE_R (DW) DeepWalk Figure 3: Comparisons of different kernel combinations in GraphZoom and MILE in classiï¬cation accuracy on Cora, Citeseer, and Pubmed datasets â€” We choose DeepWalk (DW) as the embed- ding kernel. GZoom F, GZoom C, GZoom R denote the fusion, coarsening, and reï¬nement kernels proposed in GraphZoom, respectively; MILE C and MILE R denote the coarsening and reï¬nement kernels in MILE, respectively; The blue curve is basically GraphZoom and the yellow one is MILE. ï¬nement kernel can improve embedding results upon MILE reï¬nement kernel, especially when the coarsening level is large. This indicates that our proposed graph ï¬lter in reï¬nement kernel can suc- cessfully ï¬lter out high-frequency noise from the graph to improve embedding quality. Similarly, when comparing coarsening kernels in GraphZoom and MILE with the reï¬nement kernel ï¬xed, GraphZoom coarsening kernel can also improve embedding quality upon MILE coarsening kernel, which shows that our spectral graph coarsening algorithm can indeed retain key graph structure for underlying graph embedding models to exploit. When combining the GraphZoom coarsening and reï¬nement kernels, we can achieve a better classiï¬cation accuracy compared to the ones using any other kernels in MILE. This suggests that the GraphZoom coarsening and reï¬nement kernels play useful yet distinct roles to boost embedding performance and their combination can further im- prove embedding result. Moreover, adding graph fusion improves classiï¬cation accuracy by a large margin, which indicates that graph fusion can properly incorporate both graph topology and node attribute information that are crucial for lifting the embedding quality of downstream embedding models. Results of each kernel CPU time and speedup comparison are available in Appendix G and Appendix I. 5 C ONCLUSIONS This work introduces GraphZoom, a multi-level framework to improve the accuracy and scalability of unsupervised graph embedding tasks. GraphZoom ï¬rst fuses the node attributes and topology of the original graph to construct a new weighted graph. It then employs spectral coarsening to generate a hierarchy of coarsened graphs, where embedding is performed on the smallest graph at the coarsest level. Afterwards, proper graph ï¬lters are used to iteratively reï¬ne the graph embeddings to obtain the ï¬nal result. Experiments show that GraphZoom improves both classiï¬cation accuracy and embedding speed on a number of popular datasets. An interesting direction for future work is to derive a proper way to propagate node labels to the coarsest graph, which would allow GraphZoom to support supervised graph embedding. ACKNOWLEDGMENTS This work is supported in part by Semiconductor Research Corporation and DARPA, Intel Corpo- ration under the ISRA Program, and NSF Grants #1350206, #1909105, and #1618364. We would like to thank Prof. Zhengfu Xu of Michigan Technological University for his helpful discussion on the formulation of the embedding reï¬nement problem. 10Published as a conference paper at ICLR 2020 REFERENCES Esra Akbas and Mehmet Aktas. Network Embedding: on compression and learning. arXiv preprint:1907.02811, 2019. Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation (NC), 15(6):1373â€“1396, 2003. Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph embedding: Problems, techniques, and applications. Transactions on Knowledge and Data Engi- neering (TKDE), 30(9):1616â€“1637, 2018. Haochen Chen, Bryan Perozzi, Yifan Hu, and Steven Skiena. HARP: Hierarchical representation learning for networks. The AAAI Conference on Artiï¬cial Intelligence (AAAI), 2018. Jie Chen and Ilya Safro. Algebraic distance on graphs. SIAM Journal on Scientiï¬c Computing (SISC), 33(6):3468â€“3490, 2011. Zhuo Feng. Similarity-aware spectral sparsiï¬cation by edge ï¬ltering. Design Automation Confer- ence (DAC), pp. 1â€“6, 2018. Guoji Fu, Chengbin Hou, and Xin Yao. Learning topological representation for networks via hier- archical sampling. arXiv preprint:1902.06684, 2019. Hongyang Gao and Shuiwang Ji. Graph U-Nets. International Conference on Machine Learning (ICML), pp. 2083â€“2092, 2019. Palash Goyal and Emilio Ferrara. Graph embedding techniques, applications, and performance: A survey. Knowledge-Based Systems (KBS), 151:78â€“94, 2018. Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. International Conference on Knowledge Discovery and Data mining (KDD), pp. 855â€“864, 2016. Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, and Nando de Freitas. Hyperbolic Attention Networks. International Conference on Learning Representations (ICLR), 2019. URL https://openreview.net/forum?id=rJxHsjRqFQ. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Neural Information Processing Systems (NeurIPS), pp. 1024â€“1034, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. The Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770â€“778, 2016. Thomas N Kipf and Max Welling. Semi-supervised classiï¬cation with graph convolutional net- works. International Conference on Learning Representations (ICLR) , 2016. URL https: //openreview.net/forum?id=SJU4ayYgl. Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gnnemann. Combining neural net- works with personalized pagerank for classiï¬cation on graphs. International Conference on Learning Representations (ICLR) , 2019. URL https://openreview.net/forum?id= H1gL-2A9Ym. Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. The AAAI Conference on Artiï¬cial Intelligence (AAAI), 2018. Qimai Li, Xiao-Ming Wu, Han Liu, Xiaotong Zhang, and Zhichao Guan. Label efï¬cient semi- supervised learning via graph ï¬ltering. The Conference on Computer Vision and Pattern Recog- nition (CVPR), pp. 9582â€“9591, 2019. Jiongqian Liang, Saket Gurukar, and Srinivasan Parthasarathy. MILE: A multi-level framework for scalable graph embedding. arXiv preprint:1802.09612, 2018. Wenqing Lin, Feng He, Faqiang Zhang, Xu Cheng, and Hongyun Cai. Effective and efï¬cient net- work embedding initialization via graph partitioning. arXiv preprint:1908.10697, 2019. 11Published as a conference paper at ICLR 2020 Oren E Livne and Achi Brandt. Lean Algebraic Multigrid (LAMG): Fast graph laplacian linear solver. SIAM Journal on Scientiï¬c Computing (SISC), 34(4):B499â€“B522, 2012. Andreas Loukas. Graph reduction with spectral and cut guarantees. Journal of Machine Learning Research (JMLR), 20(116):1â€“42, 2019. Takanori Maehara. Revisiting graph neural networks: All we have is low-pass ï¬lters. arXiv preprint:1905.09550, 2019. Richard Peng, He Sun, and Luca Zanetti. Partitioning well-clustered graphs: Spectral clustering works! The Conference on Learning Theory (COLT), pp. 1423â€“1455, 2015. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. DeepWalk: Online learning of social representa- tions. International Conference on Knowledge Discovery and Data mining (KDD), pp. 701â€“710, 2014. David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging ï¬eld of signal processing on graphs: Extending high-dimensional data analysis to net- works and other irregular domains. Signal Processing Magazine (SPM), 30(3):83â€“98, 2013. Lei Tang and Huan Liu. Leveraging social media networks for classiï¬cation. Data Mining and Knowledge Discovery (DMKD), 23(3):447â€“478, 2011. Petar Velikovi, William Fedus, William L. Hamilton, Pietro Li, Yoshua Bengio, and R Devon Hjelm. Deep Graph Infomax. International Conference on Learning Representations (ICLR), 2019. URL https://openreview.net/forum?id=rklz9iAcKQ. Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q Weinberger. Simplifying graph convolutional networks. International Conference on Machine Learning (ICML), pp. 6861â€“6871, 2019. Jaewon Yang and Jure Leskovec. Deï¬ning and evaluating network communities based on ground- truth. Knowledge and Information Systems (KAIS), 42(1):181â€“213, 2015. Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. arXiv preprint:1603.08861, 2016. Daokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang. Network representation learning: A survey. Transactions on Big Data (TBD), 2018a. Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Maosong Sun, Zhichong Fang, Bo Zhang, and Leyu Lin. COSINE: Compressive network embedding on large-scale information networks. arXiv preprint:1812.08972, 2018b. Zhiqiang Zhao and Zhuo Feng. Effective-resistance preserving spectral reduction of graphs. Design Automation Conference (DAC), pp. 1â€“6, 2019. Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint:1812.08434, 2018. 12Published as a conference paper at ICLR 2020 APPENDIX A D ETAILS OF DATASETS Transductive task. We follow the experiments setup in Yang et al. (2016) for three standard cita- tion network benchmark datasets: Cora, Citeseer, and Pubmed. In all these three citation networks, nodes represent documents and edges correspond to citations. Each node has a sparse bag-of-word feature vector and a class label. We allow only 20 labels per class for training and 1,000 labeled nodes for testing. In addition, we further evaluate on Friendster dataset (Yang & Leskovec, 2015), which contains 8 million nodes and 400 million edges, with 2.5% of the nodes used for training and 0.3% nodes for testing. In Friendster, nodes represent users and a pair of nodes are linked if they are friends; each node has a class label but is not associated with a feature vector. Inductive task. We follow Hamilton et al. (2017) for setting up experiments on both protein- protein interaction (PPI) and Reddit dataset. PPI dataset consists of graphs corresponding to human tissues, where nodes are proteins and edges represent interaction effects between proteins. Reddit dataset contains nodes corresponding to usersâ€™ posts: two nodes are connected through an edge if the same users comment on both posts. We use 60% nodes for training, 40% for testing on PPI and 65% for training and 35% for testing on Reddit. APPENDIX B D ETAILS OF BASELINES DeepWalk ï¬rst generates random walks based on graph structure. Then, walks are treated as sen- tences in a language model and Skip-Gram model is exploited to obtain node embeddings. node2vec is different from DeepWalk in terms of generating random walks by introducing the return parameter pand the in-out parameter q, which can combine DFS-like and BFS-like neighborhood exploration. Deep Graph Infomax (DGI) is an unsupervised approach that generates node embeddings by max- imizing mutual information between patch representations (local information) and corresponding high-level summaries (global information) of graphs. GraphSAGE embeds nodes in an inductive way by learning an aggregation function that aggregates node features to obtain embeddings. GraphSAGE supports four different aggregation functions: GraphSAGE-GCN, GraphSAGE-mean, GraphSAGE-LSTM and GraphSAGE-pool. HARP coarsens the original graph into several levels and apply underlying embedding model to train the coarsened graph at each level sequentially to obtain the ï¬nal embeddings on original graph. Since the coarsening level is ï¬xed in their implementation, we run HARP in our experiments without changing the coarsening level. MILE is the state-of-the-art multi-level unsupervised graph embedding framework and similar to our GraphZoom framework since it also contains graph coarsening and embedding reï¬nement ker- nels. More speciï¬cally, MILE ï¬rst uses its heuristic-based coarsening kernel to reduce the graph size and trains underlying unsupervised graph embedding model on coarsest graph. Then, its reï¬nement kernel employs Graph Convolutional Network (GCN) to reï¬ne embeddings on the original graph. We compare GraphZoom with MILE on various datasets, including Friendster that contains 8 mil- lion nodes and 400 million edges (shown in Table 2 and Figure 2). Moreover, we further compare each kernel in GraphZoom and MILE in Figure 3. APPENDIX C I NTEGRATE GRAPH ZOOM WITH GRAPH SAGE AND DGI As both GraphSAGE and DGI require node features for embedding on coarsest graph, we map the initial node feature matrix X0 to the coarsened graph by iteratively performing Xi+1 = Ë†Hi+1 i Xi, where Ë†Hi+1 i is the mapping operatorHi+1 i with l1-normalization per row. When applying DGI to the coarsest graph (i.e., the l-th coarsening level), the node embeddings El may lose local information due to the property of DGI. Inspired by skip connection (He et al., 2016) and Graph U-nets (Gao & Ji, 2019), we use Ë†El = El||Xl as the node embeddings on the coarsest graph, where ||denotes featurewise concatenation. 13Published as a conference paper at ICLR 2020 APPENDIX D G RAPH SIZE AT DIFFERENT COARSENING LEVEL Table 4: Number of nodes at different GraphZoom coarsening levels. GZoom-0 means GraphZoom with 0 coarsening level (i.e., without coarsening), GZoom-1 means GraphZoom with 1 coarsening level and so forth. â€â€”â€ means number of nodes is less than 50. Dataset GZoom-0 GZoom-1 GZoom-2 GZoom-3 GZoom-4 GZoom-5 Cora 2,708 1,169 519 218 100 â€” Citeseer 3,327 1,488 606 282 131 58 Pubmed 19,717 7,903 3,562 1,651 726 327 PPI 14,755 5,061 1,815 685 281 120 Reddit 232,965 84,562 30,738 11,598 4,757 2,117 Friendster 7,944,949 2,734,483 1,048,288 409,613 134,956 44,670 APPENDIX E G RAPH ZOOM ALGORITHM Algorithm 1: GraphZoom algorithm Input: Adjacency matrix Atopo âˆˆRNÃ—N ; node feature matrix X âˆˆRNÃ—K; base embedding function g(Â·); coarsening level l Output: Node embedding matrix E âˆˆRNÃ—D 1 A0 = graph fusion(Atopo,X); 2 for i= 1...ldo 3 Ai,Hi iâˆ’1 = spectral coarsening(Aiâˆ’1); 4 end 5 EL = g(Al); 6 for i= l...1 do 7 Ë†Eiâˆ’1 = (Hi iâˆ’1) T Ei; 8 Eiâˆ’1 = refinement( Ë†Eiâˆ’1); 9 end 10 E = E0; APPENDIX F S PECTRAL COARSENING Coarsening is one type of graph reduction whose objective is to achieve computational acceleration by reducing the size (i.e., number of nodes) of the original graphs while maintaining the similar graph structure between the original graph and the reduced ones. For each coarsen level i+ 1, a surjection is deï¬ned between the original node set Vi and the reduced node set Vi+1, where each node v âˆˆVi+1 corresponds to a small set of adjacent vertices in the original node set Vi. Mapping operator Hi+1 i can be deï¬ned based on the mapping relationship between Vi and Vi+1, Note that the operator Hi+1 i âˆˆ {0,1}|Vi+1|Ã—|Vi|is a matrix containing only 0s and 1s. It has following properties: â€¢ The row (column) index of Hi+1 i corresponds to the node index in graph Gi+1 (Gi). â€¢ It is a surjective mapping of the node set, where(Hi+1 i )p,q = 1 if node qin graph Gi is aggregated to super-node pin graph Gi+1, and (Hi+1 i )pâ€²,q = 0 for all nodes pâ€²âˆˆ{vâˆˆVi+1 : vÌ¸= p}. â€¢ It is a locality-preserving operator, where the coarsened version of Gi induced by the non-zero entries of (Hi+1 i )p,: is connected for each pâˆˆVi+1. 14Published as a conference paper at ICLR 2020 Algorithm 2: spectral coarsening algorithm Input: Adjacency matrix Ai âˆˆR|Vi|Ã—|Vi| Output: Adjacency matrix Ai+1 âˆˆR|Vi+1|Ã—|Vi+1|of the reduced graph Gi+1, mapping operator Hi+1 i âˆˆR|Vi+1|Ã—|Vi| 1 n= |Vi|, nc = n; 2 [graph reduction ratio] Î³max = 1.8 , Î´= 0.9; 3 for each edge (p,q) âˆˆEi do 4 [spectral node afï¬nity set] C â†ap,q deï¬ned in Eq. 2 ; 5 end 6 for each node pâˆˆVi do 7 d(p) = âââ(Ai)p,: âââ, dm(p) =median (âââ(Ai)q,: âââ for all qâˆˆ{q|(p,q) âˆˆEi} ) ; 8 if d(p) â‰¥8 Â·dm(p) then 9 [node aggregation ï¬‚ag] z(p) = 0; 10 else 11 [node aggregation ï¬‚ag] z(p) =âˆ’1; 12 end 13 end 14 Î³ = 1; 15 while Î³ <Î³max do 16 S = âˆ…, U = âˆ…; 17 [unaggregated node set] U â†pâˆˆ{p|z(p) ==âˆ’1 âˆ€pâˆˆVi}; 18 S â†pâˆˆ{p|ap,q â‰¥Î´Â·max sÌ¸=p,q ( max (p,s)âˆˆEi ( ap,s ) , max (q,s)âˆˆEi ( aq,s )) âˆ€pâˆˆVi}; 19 for each node pin S âˆ©U do 20 if z(p) ==âˆ’1 then 21 q= arg max (p,q)âˆˆEi ( ap,q ) ; 22 23 if z(q) ==âˆ’1 then 24 z(q) = 0,z(p) =q, Ë†q= q; 25 else if z(q) == 0then 26 z(p) =q, Ë†q= q; 27 else 28 z(p) =z(q), Ë†q= z(q); 29 end 30 update smoothed vectors in T with x(k) p = x(k) Ë†q for k= 1,Â·Â·Â· ,t nc = nc âˆ’1; 31 end 32 end 33 Î³ = n/nc, Î´= 0.7 Â·Î´; 34 z(p) =pfor node pâˆˆ{p|z(p) == 0} 35 end 36 form Hi+1 i and Ai+1 based on z 15Published as a conference paper at ICLR 2020 APPENDIX G CPU TIME OF EACH GRAPH ZOOM KERNEL m=1 m=2 m=3 m=4 m=5 m-th coarsening level 10 2 10 1 100 101 102 103 Single CPU time (s) 1.86 1.87 1.91 1.92 1.91 0.95 0.97 1.01 1.08 1.12 791.74 309.36 45.26 6.42 1.45 0.01 0.02 0.02 0.03 0.03 CPU time of GraphZoom kernels on Cora Fusion Coarsening Embedding Refinement (a) GraphZoom with DeepWalk as embedding kernel m=1 m=2 m=3 m=4 m=5 m-th coarsening level 10 2 10 1 100 101 102 103 Single CPU time (s) 2.19 2.25 2.22 2.21 2.26 1.21 1.26 1.28 1.38 1.42 1162.88 390.36 59.31 7.66 3.23 0.01 0.02 0.02 0.02 0.03 CPU time of GraphZoom kernels on Citeseer Fusion Coarsening Embedding Refinement (b) GraphZoom with DeepWalk as embedding kernel m=1 m=2 m=3 m=4 m=5 m-th coarsening level 10 1 100 101 102 103 Single CPU time (s) 6.18 6.68 6.32 6.33 6.21 1.33 1.47 1.53 1.59 1.81 4842.51 2027.82 814.81 261.38 12.59 0.08 0.11 0.14 0.14 0.18 CPU time of GraphZoom kernels on Pubmed Fusion Coarsening Embedding Refinement (c) GraphZoom with DeepWalk as embedding kernel m=1 m=2 m=3 m=4 m=5 m-th coarsening level 102 103 104 105 Single CPU time (s) 827.78 836.36 798.23 802.87 793.79 41.26 64.51 65.86 71.92 70.5 193605.53 88968.64 23881.05 11784.75 5821.77 51.81 70.25 76.03 82.81 78.83 CPU time of GraphZoom kernels on Reddit Fusion Coarsening Embedding Refinement (d) GraphZoom with GSAGE as embedding kernel Figure 4: CPU time of GraphZoom kernels As shown in Figure 4 (note that the y axis is in logarithmic scale), the GraphZoom embedding kernel dominates the total CPU time, which can be more effectively reduced with a greater coarsening level L. All other kernels in GraphZoom are very efï¬cient, which enable the GraphZoom framework to drastically reduce the total graph embedding time. APPENDIX H G RAPH FILTERS AND LAPLACIAN EIGENVALUES Figure 5a shows the original distribution of graph Laplacian eigenvalues which also can be inter- preted as frequencies in graph spectral domain (smaller eigenvalue means lower frequency). The proposed graph ï¬lter for embedding reï¬nement (as shown in Figure 5e) can be considered as a band- stop ï¬lter that passes all frequencies with the exception of those within the middle stop band that is greatly attenuated. Therefore, the band-stop ï¬lter may not be very effective for removing high- frequency noises from the graph signals. Fortunately, it has been shown that by adding self-loops to each node in the graph as follows ËœA= A+ÏƒI (shown in Figure 5b, 5c, 5d, whereÏƒ= 0.5,1.0,2.0), the distribution of Laplacian eigenvalues can be squeezed to the left (towards zero) (Maehara, 2019). By properly choosingÏƒsuch that large eigenvalues will mostly lie in the stop band (e.g.,Ïƒ= 1.0,2.0 shown in Figure 5c and 5d), the graph ï¬lter will be able to effectively ï¬ltered out high-frequency components (corresponding to high eigenvalues) while retaining low-frequency components, which is similar to a low-pass graph ï¬lter as shown in Figure 5f. It is worth noting that ifÏƒis too large, then most eigenvalues will be very close to zero, which makes the graph ï¬lter less effective for removing noises. In this work, we choose Ïƒ= 2.0 for all our experiments. APPENDIX I S PEEDUP OF GRAPH ZOOM KERNELS COMPARED TO MILE As shown in Figure 6, the combination of GraphZoom coarsening and reï¬nement kernels can always achieve the greatest speedups (green curves); adding GraphZoom fusion kernel (blue curves) will lower the speedups by a small margin but further boost the embedding quality, showing a clear trade- 16Published as a conference paper at ICLR 2020 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Eigenvalues( ) 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14Probability (a) Original eigenvalues with ËœA = A 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Eigenvalues( ) 0.00 0.02 0.04 0.06 0.08 0.10Probability (b) Squeezed eigenvalues with ËœA = A + 0.5I 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Eigenvalues( ) 0.00 0.02 0.04 0.06 0.08 0.10 0.12Probability (c) Squeezed eigenvalues with ËœA = A + 1.0I 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Eigenvalues( ) 0.00 0.02 0.04 0.06 0.08 0.10Probability (d) Squeezed eigenvalues with ËœA = A + 2.0I 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Eigenvalue( ) 0.0 0.2 0.4 0.6 0.8 1.0hk( ) = |(1 )k| k = 1 k = 2 k = 3 k = 4 (e) Filters for original laplacian eigenvalues 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Eigenvalue( ) 0.0 0.2 0.4 0.6 0.8 1.0hk( ) = |(1 )k| k = 1 k = 2 k = 3 k = 4 (f) Filters for squeezed laplacian eigenvalues Figure 5: Distribution of graph laplacian eigenvalues with different self-loops on Cora off between embedding quality and runtime efï¬ciency: to achieve the highest graph embedding quality, the graph fusion kernel should be included. APPENDIX J M ORE RESULTS ON PPI DATASETS As shown in Figure 7, the embedding results by GraphZoom with increasing number of coarsening levels are still better than the ones by GraphSAGE with different aggregation functions. It is worth noting that although the level-5 (coarsened) graph contains only 120 nodes (as shown in D), Graph- 17Published as a conference paper at ICLR 2020 1 2 3 4 Coarsening Level 0 50 100 150 200 250CPU time speedup Cora (Speedup) 1 2 3 4 5 Coarsening Level 0 100 200 300 400CPU time speedup Citeseer (Speedup) 1 2 3 4 5 Coarsening Level 0 50 100 150 200CPU time speedup Pubmed (Speedup) GZoom_F + GZoom_C + GZoom_R (DW) GZoom_C + GZoom_R (DW) GZoom_C + MILE_R (DW) MILE_C + GZoom_R (DW) MILE_C + MILE_R (DW) Figure 6: Comparisons of different combinations of kernels in GraphZoom and MILE in terms of CPU time speedup on Cora, Citeseer, and Pubmed datasets â€” We choose DeepWalk (DW) as basic embedding method. GZoom F, GZoom C, GZoom R, MILE C and MILE R represent GraphZoom fusion kernel, GraphZoom coarsening kernel, GraphZoom reï¬nement kernel, MILE coarsening ker- nel and MILE reï¬nement kernel, respectively. 010002000300040005000 #Nodes in Coarsened Graph 0.50 0.52 0.54 0.56 0.58 0.60 0.62 0.64Micro-F1 Comparison of GraphZoom and GraphSAGE-GCN on PPI GraphZoom GraphSAGE-GCN (a) GraphSAGE with GCN as aggregation function 010002000300040005000 #Nodes in Coarsened Graph 0.50 0.52 0.54 0.56 0.58 0.60 0.62 0.64Micro-F1 Comparison of GraphZoom and GraphSAGE-mean on PPI GraphZoom GraphSAGE-mean (b) GraphSAGE with mean as aggregation function 010002000300040005000 #Nodes in Coarsened Graph 0.50 0.52 0.54 0.56 0.58 0.60 0.62 0.64Micro-F1 Comparison of GraphZoom and GraphSAGE-lstm on PPI GraphZoom GraphSAGE-lstm (c) GraphSAGE with LSTM as aggregation function 010002000300040005000 #Nodes in Coarsened Graph 0.50 0.52 0.54 0.56 0.58 0.60 0.62 0.64Micro-F1 Comparison of GraphZoom and GraphSAGE-pool on PPI GraphZoom GraphSAGE-pool (d) GraphSAGE with pool as aggregation function Figure 7: Comparisons of GraphZoom and GraphSAGE on PPI Zoom embedding results can still beat the GraphSAGE baseline obtained with the original graph containing 14,755 nodes. 18Published as a conference paper at ICLR 2020 APPENDIX K M ORE RESULTS ON NON -ATTRIBUTED DATASETS Table 5: Node classiï¬cation results on PPI and Wiki datasets. PPI(Homo Sapiens) Wiki Method Mircro-F1 Time(mins) Mircro-F1 Time(s) DeepWalk 0.231 2.3 0.705 94.4 MILE(DW, l=1) 0.256 1.1 (2.1 Ã—) 0.703 42.5 (2.2 Ã—) MILE(DW, l=2) 0.253 0.7 (3.3 Ã—) 0.639 20.9 (4.5 Ã—) GZoom(DW, l=1) 0.261 0.6 (3.8Ã—) 0.726 29.4 (3.2Ã—) GZoom(DW, l=2) 0.255 0.2 (11.5Ã—) 0.678 6.7 (14.1Ã—) Table 6: Link prediction results on PPI and Wiki datasets. PPI(Homo Sapiens) Wiki Method AUC Time(mins) AUC Time(mins) DeepWalk 0.721 5.2 0.774 3.0 MILE(DW, l=1) 0.772 3.7 (1.4 Ã—) 0.877 1.6 (1.8 Ã—) MILE(DW, l=2) 0.701 2.3 (2.3 Ã—) 0.868 0.9 (3.3 Ã—) MILE(DW, l=3) 0.723 1.2 (4.3 Ã—) 0.842 0.5 (6.0 Ã—) GZoom(DW, l=1) 0.818 2.1 (2.5 Ã—) 0.901 1.2 (2.5 Ã—) GZoom(DW, l=2) 0.834 0.7 (7.4 Ã—) 0.902 0.4 (7.5 Ã—) GZoom(DW, l=3) 0.855 0.1 (52.0 Ã—) 0.908 0.1 (30.0 Ã—) To further show that GraphZoom can work on non-attributed datasets, we evaluate it on PPI(Homo Sapiens) and Wiki datasets, following the same dataset conï¬guration as used in Grover & Leskovec (2016); Liang et al. (2018). As shown in Table 5, GraphZoom (without fusion kernel) improves the performance of basic embedding model (i.e., DeepWalk) at the ï¬rst coarsening level. When further increasing coarsening level, GraphZoom achieves much larger speedup while its performance may drop a little bit. For link prediction task, we follow Grover & Leskovec (2016) by choosing Hadamard operator to transform node embeddings into edge embeddings. Our link prediction results show that GraphZoom signiï¬cantly improve performance by a margin of 18.5% with speedup up to 52.0Ã—, showing GraphZoom can generate high-quality embeddings for various tasks. APPENDIX L S ENSITIVITY ANALYSIS OF GRAPH FUSION KERNEL 0 1 2 3 Î² 74.074.575.075.576.076.577.077.5Accuracy Cora (Accuracy) 0 1 2 3 Î² 48 49 50 51 52Accuracy Citeseer (Accuracy) 0 1 2 3 Î² 73 74 75 76 77Accuracy Pubmed (Accuracy) GZoom(DW, l =1)  GZoom(DW, l =2) GZoom(DW, l =3) Figure 8: Comparisons of classiï¬cation accuracy using various Î² values in the graph fusion kernel for the Cora, Citeseer, and Pubmed datasets. lrepresents the l-th coarsening level. As graph fusion kernel fuses node attribute with graph topology viaAfusion = Atopo + Î²Afeat , the value of Î²plays a critical role in balancing attribute and topological information. On the one hand, if Î² is too small, we may lose too much node attribute information. On the other hand, if Î² is too 19Published as a conference paper at ICLR 2020 large, the node attribute information will dominate the fused graph and therefore graph topological information is undermined. As shown in Figure 8, we vary Î² from 0.1 to 3 in graph fusion kernel and evaluate it on Cora, Citeseer, and Pubmed datasets. The results indicate that Î² = 1 achieves the best performance on Cora and Pubmed, while Î² = 0.5 is the best conï¬guration on Citeseer. We choose Î² = 1 in all our experiments. 20",
      "meta_data": {
        "arxiv_id": "1910.02370v2",
        "authors": [
          "Chenhui Deng",
          "Zhiqiang Zhao",
          "Yongyu Wang",
          "Zhiru Zhang",
          "Zhuo Feng"
        ],
        "published_date": "2019-10-06T04:43:46Z",
        "venue": "International Conference on Learning Representations, ICLR 2020",
        "pdf_url": "https://arxiv.org/pdf/1910.02370v2.pdf"
      }
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "Sketch-GNN: Scalable Graph Neural Networks with Sublinear Training Complexity",
      "abstract": "Graph Neural Networks (GNNs) are widely applied to graph learning problems\nsuch as node classification. When scaling up the underlying graphs of GNNs to a\nlarger size, we are forced to either train on the complete graph and keep the\nfull graph adjacency and node embeddings in memory (which is often infeasible)\nor mini-batch sample the graph (which results in exponentially growing\ncomputational complexities with respect to the number of GNN layers). Various\nsampling-based and historical-embedding-based methods are proposed to avoid\nthis exponential growth of complexities. However, none of these solutions\neliminates the linear dependence on graph size. This paper proposes a\nsketch-based algorithm whose training time and memory grow sublinearly with\nrespect to graph size by training GNNs atop a few compact sketches of graph\nadjacency and node embeddings. Based on polynomial tensor-sketch (PTS) theory,\nour framework provides a novel protocol for sketching non-linear activations\nand graph convolution matrices in GNNs, as opposed to existing methods that\nsketch linear weights or gradients in neural networks. In addition, we develop\na locality-sensitive hashing (LSH) technique that can be trained to improve the\nquality of sketches. Experiments on large-graph benchmarks demonstrate the\nscalability and competitive performance of our Sketch-GNNs versus their\nfull-size GNN counterparts.",
      "full_text": "arXiv:2406.15575v1  [cs.LG]  21 Jun 2024 Sketch-GNN: Scalable Graph Neural Networks with Sublinear T raining Complexity Mucong Ding , T ahseen Rabbani , Bang An , Evan Z W ang , Furong Hu ang Department of Computer Science, University of Maryland {mcding, trabbani, bangan, furongh}@cs.umd.edu Abstract Graph Neural Networks (GNNs) are widely applied to graph lea rning problems such as node classiï¬cation. When scaling up the underlying g raphs of GNNs to a larger size, we are forced to either train on the complete gr aph and keep the full graph adjacency and node embeddings in memory (which is often infeasible) or mini-batch sample the graph (which results in exponentia lly growing computa- tional complexities with respect to the number of GNN layers ). V arious sampling- based and historical-embedding-based methods are propose d to avoid this expo- nential growth of complexities. However, none of these solu tions eliminates the linear dependence on graph size. This paper proposes a sketc h-based algorithm whose training time and memory grow sublinearly with respec t to graph size by training GNNs atop a few compact sketches of graph adjacency and node embed- dings. Based on polynomial tensor-sketch (PTS) theory, our framework provides a novel protocol for sketching non-linear activations and gr aph convolution matrices in GNNs, as opposed to existing methods that sketch linear we ights or gradients in neural networks. In addition, we develop a locality sensi tive hashing (LSH) technique that can be trained to improve the quality of sketc hes. Experiments on large-graph benchmarks demonstrate the scalability and co mpetitive performance of our Sketch-GNNs versus their full-size GNN counterparts . 1 Introduction Graph Neural Networks (GNNs) have achieved state-of-the-art graph learning in numerous ap- plications, including classiï¬cation [ 27], clustering [ 3], recommendation systems [ 43], social net- works [ 16] and more, through representation learning of target nodes using information aggregated from neighborhoods in the graph. The manner in which GNNs uti lize graph topology, however, makes it challenging to scale learning to larger graphs or de eper models with desirable computa- tional and memory efï¬ciency. Full-batch training that stor es the Laplacian of the complete graph suffers from a memory complexity of O(m + ndL + d2L) on an n-node, m-edge graph with node features of dimension d when employing an L-layer graph convolutional network (GCN). This lin- ear memory complexity dependence on n and the limited memory capacity of GPUs make it difï¬- cult to train on large graphs with millions of nodes or more. A s an example, the MAG240M-LSC dataset [ 21] is a node classiï¬cation benchmark with over 240 million nod es that takes over 202 GB of GPU memory when fully loaded. T o address the memory constraints, two major lines of resear ch are proposed: (1) Sampling-based approaches [ 18, 11, 12, 14, 45] based on the idea of implementing message passing only betw een the neighbors within a sampled mini-batch; (2) Historical- embedding based techniques, such as GNNAutoScale [ 17] and VQ-GNN [ 15]), which maintain the expressive power of GNNs on sampled subgraphs using historical embeddings. However, all of the se methods require the number of mini- 36th Conference on Neural Information Processing Systems ( NeurIPS 2022).batches to be proportional to the size of the graph for ï¬xed me mory consumption. In other words, they signiï¬cantly increase computational time complexity in exchange for memory efï¬ciency when scaling up to large graphs. For example, training a 4-layer G CN with just 333K parameters (1.3 MB) for 500 epochs on ogbn-papers100M can take more than 2 days on a powerful A WS p4d.24x large instance [ 21]. W e seek to achieve efï¬cient training of GNNs with time and mem ory complexities sublinear in graph size without signiï¬cant accuracy degradation. Despi te the difï¬culty of this goal, it should be achievable given that (1) the number of learnable parameter s in GNNs is independent of the graph size, and (2) training may not require a traversal of all loca l neighborhoods on a graph but rather only the most representative ones (thus sublinear in graph s ize) as some neighborhoods may be very similar. In addition, commonly-used GNNs are typicall y small and shallow with limited model capacity and expressive power, indicating that a modest pro portion of data may sufï¬ce. This paper presents Sketch-GNN, a framework for training GNNs with sublinear time and memor y complexity with respect to graph size. Using the idea of sket ching, which maps high-dimensional data structures to a lower dimension through entry hashing, we sketch the nÃ—n adjacency matrix and the n Ã—d node feature matrix to a few c Ã—c and c Ã—d sketches respectively before training, where c is the sketch dimension. While most existing literature foc uses on sketching linear weights or gradients, we introduce a method for sketching non-linear a ctivation units using polynomial tensor sketch theory [ 19]. This preserves prediction accuracy while avoiding the ne ed to â€œunsketchâ€ back to the original high dimensional graph-node space n, thereby eliminating the dependence of training complexity on the underlying graph size n. Moreover, we propose to learn and update the sketches in an online manner using learnable locality-sensitive hashi ng (LSH) [ 9]. This reduces the performance loss by adaptively enhancing the sketch quality while incur ring minor overhead sublinear in the graph size. In practice, we ï¬nd that the sketch-ratio c/n required to maintain â€œfull-graphâ€ model performance drops as n increases; as a result, our Sketch-GNN enjoys sublinear tra ining scalability. Sketch-GNN applies sketching techniques to GNNs to achieve training co mplexity sublinear to the data size. This is fundamentally different from the few existing works which sketch the weights or gradients [ 30, 13, 26, 29, 37] to reduce the memory footprint of the model and speed up opti mization. T o the best of our knowledge, Sketch-GNN is the ï¬rst sub-linear complexity training algorithm for GNNs, based on LSH and tensor sketching. The sublinear efï¬ci ency obtained applies to various types of GNNs, including GCN [ 27] and GraphSAGE [ 18]. Compared to the data compression approach [ 22, 23], which compresses the input graph to a smaller one with fewe r nodes and edges before training, our Sketch-GNN is advantageous since it do es not suffer from an extremely long preprocessing time (which renders the training speedups me aningless) and performs much better across GNN types/architectures. The remainder of this paper is organized as follows. Section 2 summarizes the notions and prelimi- naries of GNNs and sketching. Section 3 describes how to approximate the GNN operations on the full graph topology with sketches. Section 3.3 introduces potential drawbacks of using ï¬xed sketches and develops algorithms for updating sketches using learna ble LSHs. In Section 4, we compare our approach to the graph compression approach and other GNN sca lability methods. In Section 5, we report the performance and efï¬ciency of Sketch-GNNs as well as several proof-of-concept and ab- lation experiments. Finally, Section 6 concludes this paper with a summary of limitations, future directions, and broader impacts. 2 Preliminaries Basic Notations.Consider a graph with n nodes and m edges. Connectivity is given by the ad- jacency matrix A âˆˆ{0, 1}nÃ—n and features on nodes are represented by the matrix X âˆˆRnÃ—d, where d is the number of features. Given a matrix C, let Ci,j , Ci,:, and C:,j denote its (i, j)-th entry, i-th row , and j-th column, respectively. âŠ™denotes the element-wise (Hadamard) product, whereas CâŠ™k represents the k-th order element-wise power. âˆ¥Â·âˆ¥ F is the symbol for the Frobenius norm. In âˆˆRnÃ—n denotes the identity matrix, whereas 1n âˆˆRn is the vector whose elements are all ones. Med{Â·}represents the element-wise median over a set of matrices. S uperscripts are used to indicate multiple instances of the same kind of variable; fo r instance, X(l) âˆˆRnÃ—dl are the node representations on layer l. 2Uniï¬ed Framework of GNNs. A Graph Neural Network (GNN) layer receives the node representa- tion of the preceding layer X(l) âˆˆRnÃ—d as input and outputs a new representation X(l+1) âˆˆRnÃ—d, where X = X(0) âˆˆRnÃ—d are the input features. Although GNNs are designed followin g different guiding principles, such as neighborhood aggregation (Gra phSAGE), spatial convolution (GCN), self-attention (GA T), and W eisfeiler-Lehman (WL) alignme nt (GIN [ 44]), the great majority of GNNs can be interpreted as performing message passing on nod e features, followed by feature trans- formation and an activation function. The update rule of the se GNNs can be summarized as [ 15] X(l+1) = Ïƒ (âˆ‘ q C(q)X(l)W (l,q) ) . (1) Where C(q) âˆˆRnÃ—n denotes the q-th convolution matrix that deï¬nes the message passing oper ator, q âˆˆZ+ is index of convolution, Ïƒ(Â·) is some choice of nonlinear activation function, and W (l,q) âˆˆ RdlÃ—dl+1 denotes the learnable linear weight matrix for the l-th layer and q-th ï¬lter. GNNs under this paradigm differ from each other by their choice of convoluti on matrices C(q), which can be either ï¬xed (GCN and GraphSAGE) or learnable (GA T). In Appendix A.1, we re-formulate a number of well-known GNNs under this framework. Unless otherwise spe ciï¬ed, we assume q = 1 and d = dl for every layer l âˆˆ[L] for notational convenience. Count Sketch and T ensor Sketch. (1) Count sketch [7, 41] is an efï¬cient dimensionality re- duction method that projects an n-dimensional vector u into a smaller c-dimensional space us- ing a random hash table h : [ n] â†’ [c] and a binary Rademacher variable s : [ n] â†’ {Â±1}, where [n] = {1, . . . , n }. Count sketch is deï¬ned as CS(u)i = âˆ‘ h(j)=i s(j)uj , which is a linear transformation of u, i.e., CS(u) = Ru. Here, R âˆˆ RcÃ—n denotes the so-called count sketch matrix , which has exactly one non-zero element per column. (2) T ensor sketch [32] is proposed as a generalization of count sketch to the tensor pr oduct of vectors. Given z âˆˆ Rn and an order k, consider a k number of i.i.d. hash tables h(1), . . . , h (k) : [ n] â†’ [c] and i.i.d. binary Rademacher variables s(1), . . . , s (k) : [ n] â†’ {Â±1}. T ensor sketch also projects vector z âˆˆRn into Rc, and is deï¬ned as TSk(z)i = âˆ‘ h(j1,Â·Â·Â· ,jk )=i s(1)(j1) Â·Â·Â· s(k)(jk)zj1 Â·Â·Â· zjk , where h(j1, Â·Â·Â· , jk) = ( h(1)(j1) + Â·Â·Â· + h(k)(jk)) mod c. By deï¬nition, a tensor sketch of order k = 1 degenerates to count sketch; TS1(Â·) = CS(Â·). (3) W e deï¬ne count sketch of a matrix U âˆˆRdÃ—n as the count sketch of each row vector individually, i.e., CS(U) âˆˆRdÃ—c where [CS(U)]i,: = CS(Ui,:). The tensor sketch of a matrix is deï¬ned in the same way. Pham and Pagh [ 32] devise a fast compu- tation of tensor sketch of U âˆˆRdÃ—n (sketch dimension c and order k) using count sketches and the Fast Fourier Transform (FFT): TSk(U) = FFTâˆ’1 (â¨€ k p=1 FFT ( CS(p)(U) ) ) , (2) where CS(p)(Â·) is the count sketch with hash function h(p) and Rademacher variable s(p). FFT(Â·) and FFTâˆ’1(Â·) are the FFT and its inverse applied to each row of a matrix. Locality Sensitive Hashing. The deï¬nition of count sketch and tensor sketch is based on ha sh table(s) that only requires a data independent uniformity, i.e., with high probability the hash-buckets are of similar size. In contrast, locality sensitive hashin g (LSH) is a hashing scheme that uses locality-sensitive hash function H : Rd â†’[c] to ensure that nearby vectors are hashed into the same bucket (out of c buckets in total) with high probability while distant ones a re not. SimHash achieves the locality-sensitive property by employing random proje ctions [ 8]. Given a random matrix P âˆˆ Rc/2Ã—d, SimHash deï¬nes a locality-sensitive hash function H(u) = arg max ([ P u âˆ¥âˆ’P u]) , (3) where [Â· âˆ¥ Â·] denotes concatenation of two vectors and arg max returns the index of the largest element. SimHash is efï¬cient for large batches of vectors [ 1]. In this paper, we apply a learnable version of SimHash that is proposed by Chen et al. [ 9], in which the projection matrix P is updated using gradient descent; see Section 3.3 for details. 33 Sketch-GNN Framework via Polynomial T ensor Sketch Problem and Insights.W e intend to develop a â€œsketched counterpartâ€ of GNNs, where training is based solely on (dimensionality-reduced) compact sketc hes of the convolution and node feature matrices, the sizes of which can be set independently of the g raph size n. In each layer, Sketch-GNN receives some sketches of the convolution matrix C and node representation matrix X(l) and outputs some sketches of the node representations X(l+1). As a result, the memory and time complexities are inherently independent of n. The bottleneck of this problem is estimating the nonlinear activated product Ïƒ(CX (l)W (l)), where W (l) is the learnable weight of the l-th layer. Before considering the nonlinear activation, as a ï¬rst step , we approximate the linear product CX (l)W (l), using dimensionality reduction techniques such as random projections and low-rank de- compositions. As a direct corollary of the (distributional ) Johnsonâ€“Lindenstrauss (JL) lemma [ 25], there exists a projection matrix R âˆˆ RcÃ—n such that CX (l)W (l) â‰ˆ ( CRT)( RX(l)W (l)) [15]. T ensor sketch is one of the techniques that can achieve the JL bound [ 2]; for an error bound, see Lemma 1 in Appendix B. Count sketch offers a good estimation of a matrix product, CX (l)W (l) â‰ˆCS(C)CS((X(l)W (l))T)T. While tensor sketch can be used to approximate the power of ma trix product, i.e., (CX (l)W (l))âŠ™k â‰ˆ TSk(C)TSk((X(l)W (l))T)T, where (Â·)âŠ™k is the k-th order element-wise power. If we combine the estimators of element-wise powers of CX (l)W (l), we can approximate the (element-wise) activation Ïƒ(Â·) on CX (l)W (l). This technique is known as a polynomial tensor sketch (PTS) and is discussed in [ 19]. In this paper, we apply PTS to sketch the message passing of GNNs, including the nonlinear activations. 3.1 Sketch-GNN: Approximated Update Rules Polynomial T ensor Sketch.Our goal is to approximate the update rule of GNNs (Eq. ( 1)) in each layer. W e ï¬rst expand the element-wise non-linearity Ïƒ as a power series, and then approximate the powers using count/tensor sketch, i.e., X(l+1) = Ïƒ(CX (l)W (l)) â‰ˆ âˆ‘ r k=1 ck ( CX (l)W (l)) âŠ™k â‰ˆ âˆ‘ r k=1 ck TSk(C) TSk ( (X(l)W (l))T) T , (4) where the k = 0 term always evaluates to zero as Ïƒ(0) = 0 . In Eq. ( 4), coefï¬cients ck are in- troduced to enable learning or data-driven selection of the weights when combing the terms of different order k. This allows for the approximation of a variety of nonlinear activation functions, such as sigmoid and ReLU. The error of this approximation rel ies on the precise estimation of the coefï¬cients {ck}r k=1. T o identify the coefï¬cients, Han et al. [ 19] design a coreset-based regres- sion algorithm, which requires at least O(n) additional time and memory. Since the coefï¬cients {ck}r k=1 that achieve the best performance for the classiï¬cation tas ks do not necessarily approxi- mate a known activation, we propose learning the coefï¬cient s {ck}r k=1 to optimize the classiï¬cation loss directly using gradient descent with simple L2 regularization. Experiments indicate that the learned coefï¬cients can approximate the sigmoid activatio n with relative errors comparable to those of the coreset-based method; see Fig. 1a in Section 5. Approximated Update Rules. The remaining step is to approximate the operations of GNNs u sing PTS (Eq. ( 4)) on sketches of convolution matrix C and node representation matrix X(l). Consider r pairwise-independent count sketches {CS(k)(Â·)}r k=1 with sketch dimension c, associated with hash tables h(1), . . . , h (r) and binary Rademacher variables s(1), . . . , s (r), deï¬ned prior to training an L- layer Sketch-GNN. Using these hash tables and Rademacher variables, we may al so construct tensor sketches {TSk(Â·)}r k=2 up to the maximum order r. 4In Sketch-GNN, sketches of node representations (instead of the O(n) standard representation) are propagated between layers. T o get rid of the dependence on n, we count sketch both sides of Eq. ( 4) S(l+1,kâ€²) X := CS(kâ€²)( (X(l+1))T) â‰ˆCS(kâ€²)(âˆ‘ r k=1 c(l) k TSk ( (X(l)W (l))T) TSk(C)T) = âˆ‘ r k=1 c(l) k TSk ( (X(l)W (l))T) CS(kâ€² )( TSk(C)T) = âˆ‘ r k=1 c(l) k FFTâˆ’1 (â¨€ k p=1 FFT ( (W (l))TS(l,p) X ) ) S(l,k,kâ€²) C , (5) where S(l+1,kâ€²) X = CS(kâ€²)((X(l+1))T) âˆˆ RdÃ—c is the transpose of column-wise count sketch of X(l+1), and the superscripts of S(l+1,kâ€²) X indicate that it is the kâ€²-th count sketch of X(l+1) (i.e., sketched by CS (k)(Â·)). In the second line of Eq. ( 5), we can move the matrix, c(l) k TSk((X(l)W (l))T), multiplied on the left to TSk(C)T out of the count sketch function CS(kâ€²)(Â·), since the operation of row-wise count sketch CS(kâ€²)(Â·) is equivalent to multiplying the associated count sketch ma trix R(kâ€²) on the right, i.e., for any U âˆˆRnÃ—n, CS(kâ€²)(U) = UR(kâ€²). In the third line of Eq. ( 5), we denote the â€œtwo-sided sketchâ€ of the convolution matrix as S(l,k,kâ€²) C := CS(kâ€²)(TSk(C)T) âˆˆRcÃ—c and expand the tensor sketch TSk((X(l)W (l))T) using the FFT -based formula (Eq. ( 2)). Eq. ( 5) is the (recursive) update rule of Sketch-GNN, which approximates the operation of the original GNN and learns the sketches of representations. Lo oking at the both ends of Eq. ( 5), we obtain a formula that approximates the sketches of X(l+1) using the sketches of X(l) and C, with learnable weights W (l) âˆˆRdÃ—d and coefï¬cients {c(l) k âˆˆR}r k=1. In practice, to mitigate the error accumulation when propagating through multiple layers, we employ skip-connections across layers in Sketch-GNNs (Eq. ( 5) and their full-size GNN counterparts. The forward-pass an d backward- propagation between the input sketches {S(0,k) X }r k=1 and the sketches of the ï¬nal layer representa- tions {S(L,k)}r k=1 take O(c) time and memory (see Section 3.3 for complexity details). 3.2 Error Bound on Estimated Representation Based on Lemma 1 and the results in [ 19], we establish an error bound on the estimated ï¬nal layer representation ËœX(L) for GCN; see Appendix B for the proof and discussions. Theorem 1. F or a Sketch-GNN with L layers, the estimated ï¬nal layer representation is ËœX(L) = Med{R(k)S(L,k) X | k = 1 , Â·Â·Â· , r}, where the sketches are recursively computed using Eq. (5). F or Î“ (l) = max {5âˆ¥X(l)W (l)âˆ¥2 F, (2 + 3 r) âˆ‘ i(âˆ‘ j [X(l)W (l)]i,j )r}, it holds that E(âˆ¥X(L) âˆ’ ËœX(L)âˆ¥2 F)/âˆ¥X(L)âˆ¥2 F â‰¤ âˆ L l=1(1 + 2 /(1 + cÎ»(l)2 /nrÎ“ (l))) âˆ’1, where Î»(l) â‰¥ 0 is the smallest singular value of the matrix Z âˆˆRndÃ—r and Z:,k is the vectorization of (CX (l)W (l))âŠ™k. More- over , if (c(Î»(l))2/nrÎ“ (l)) â‰«1 holds true for every layer , the relative error is O(L(n/c)), which is proportional to the depth of the model, and inversely propor tional to the sketch ratio (c/n). Remarks. Despite the fact that in Theorem 1 the error bound grows for smaller sketch ratios c/n, we observe in experiments that the sketch-ratio required fo r competitive performance decreases as n increases; see Section 5. As for the number of independent sketches r, we know from Lemma 1 that the dependence of r on n is r = â„¦(3 logc n) which is negligible when n is not too small; thus, in practice r = 3 is used. The theoretical framework may not completely correspond to reality. Experimentally, the coefï¬- cients {{c(l) k }r k=1}L l=1 with the highest performance do not necessarily approximat e a known activa- tion. W e defer the challenging problem of bounding the error of sketches and coefï¬cients learned by gradients to future studies. Although the error bound is in e xpectation, we do not train over different sketches per iteration due to the instability caused by rand omness. Instead, we introduce learnable lo- cality sensitive hashing (LSH) in the next section to counte ract the approximation limitations caused by the ï¬xed number of sketches. 53.3 A Practical Implementation: Learning Sketches using LS H Motivations of Learnable Sketches. In Section 3, we apply polynomial tensor sketch (PTS) to ap- proximate the operations of GNNs on sketches of the convolut ion and feature matrices. Nonetheless, the pre-computed sketches are ï¬xed during training, result ing in two major drawbacks : (1) The performance is limited by the quality of the initial sketche s. For example, if the randomly-generated hash tables {h(k)}r k=1 have unevenly distributed buckets, there will be more hash c ollisions and consequently worse sketch representations. The performan ce will suffer because only sketches are used in training. (2) More importantly, when multiple Sketch-GNN layers are stac ked, the input representation X(l) changes during training (starting from the second layer). F ixed hash tables are not tailored to the â€œchangingâ€ hidden representations. W e seek a method for efï¬ciently constructing high-quality h ash tables tailored for each hidden em- bedding. Locality sensitive hashing (LSH) is a suitable too l since it is data-dependent and preserves data similarity by hashing similar vectors into the same buc ket. This can signiï¬cantly improve the quality of sketches by reducing the errors due to hash collis ions. Combining LSH with Sketching. At the time of sketching, the hash table h(k) : [ n] â†’[c] is replaced with an LSH function H(k) : Rd â†’[c], for any k âˆˆ[r]. Speciï¬cally, in the l-th layer of a Sketch-GNN, we hash the i-th node to the H(k)(X(l) i,: )-th bucket for every i âˆˆ[n], where X(l) i,: is the embedding vector of node i. As a result, we deï¬ne a data-dependent hash table h(l,k)(i) = H(k)(X(l) i,: ) (6) that can be used for computing the sketches of S(l,k) X and S(l,k,kâ€²) C . This LSH-based sketching can be directly applied to sketch the ï¬xed convolution matrix an d the input feature matrix. If SimHash is used, i.e., H(k)(u) = arg max ([ P (k)u âˆ¥âˆ’P (k)u ]) (Eq. ( 3)), an additional O(ncr(log c + d)) computational overhead is introduced to hash the n nodes for the r hash tables during preprocessing; see Appendix F more information. SimHash(es) are implemented as simple ma trix multiplications that are practically very fast. In order to employ LSH-based hash functions customized to each layer to sketch the hidden repre- sentations of a Sketch-GNN (i.e., l = 2 , . . . , L âˆ’1), we face two major challenges : (1) Unless we explicitly unsketch in each layer, the estimated hidden rep resentations ËœX(l)(l = 2 , . . . , L âˆ’1) cannot be accessed and used to compute the hash tables. However, uns ketching any hidden representation, i.e., ËœX(l) = Med{R(k)S(l,k) X |k = 1 , Â·Â·Â· , r}, requires O(n) memory and time. W e need to come up with an efï¬cient algorithm that updates the hash tables wi thout having to unsketch the complete representation. (2) Itâ€™s unclear how to change the underlying hash table of a sket ch across layers without unsketching to the n-dimensional space, even if we know the most up-to-date hash tables suited to each layer. The challenge (2) , i.e., changing the underlying hash table of across layers, can be solved by main- taining a sparse cÃ—c matrix T (l,k) := R(l,k)(R(l+1,k))T for each k âˆˆ[r], which only requires O(cr) memory and time overhead; see Appendix C for more information and detailed discussions. W e focus on challenge (1) for the remainder of this section. Online Learning of Sketches. T o learn a hash table tailored for a hidden layer using LSH wit hout unsketching, we develop an efï¬cient algorithm to update the LSH function using only a size- |B| subset of the length- n unsketched representations, where B denotes a subset of nodes we select. This algorithm, which we term online learning of sketches , is made up of two key parts: (P art 1) select a subset of nodes B âŠ†[n] to effectively update the hash table, and (P art 2) update the LSH function H(Â·) with a triplet loss computed using this subset. (1) Selection of subset B: Because model parameters are updated slowly during neural n etwork training, the data-dependent LSH hash tables also changes s lowly (this behavior was detailed in [ 9]). The number of updates to the hash table drops very fast along w ith training, empirically veriï¬ed in Fig. 1b (left) in Section 5. Based on this insight, we only need to update a small fractio n of the hash table during training. T o identify this subset B âˆˆ[n] of nodes, gradient signals can be used. Intuitively, a node representation vector hashed into the w rong bucket will be aggregated with distant vectors and lead to larger errors and subsequently larger gr adient signals. Speciï¬cally, we propose ï¬nding the candidate set B of nodes by taking the union of the several buckets with the la rgest 6gradients, i.e., B = {i |h(l,k)(i) = arg max j [S(l,k) X ]j,: for some k}. The memory and overhead required to update the entries in B in the hash table is O(|B|). (2) Update of LSH function: In order to update the projection matrix P that deï¬nes a SimHash H(k) : Rd â†’[c] (Eq. ( 3)), instead of the O(n) full triplet loss introduced by [ 9], we consider a sampled version of the triplet loss on the candidate set B with O(|B|) complexity, namely L(H, P+, Pâˆ’ ) = max { 0, âˆ‘ (u,v)âˆˆP âˆ’ cos(H(u), H (v)) âˆ’ âˆ‘ (u,v)âˆˆP + cos(H(u), H (v)) +Î± } , (7) where P+ = {( ËœXi,:, ËœXj,:) | i, j âˆˆ B, âŸ¨ËœXi,:, ËœXj,:âŸ©> t +}and Pâˆ’ = {( ËœX:,i, ËœX:,j) | i, j âˆˆ B, âŸ¨ËœX:,i, ËœX:,jâŸ©< t âˆ’}are the similar and dissimilar node-pairs in the subset B; t+ > t âˆ’ and Î± > 0 are hyper-parameters. This triplet loss L(H, P+, Pâˆ’) is used to update P using gradient descent, as described in [ 9], with a O(c|B|d + |B|2) overhead. Experimental validation of this LSH update mechanism can be found in Fig. 1b in Section 5. A voiding O(n) in Loss Evaluation. W e can estimate the ï¬nal layer representation using the r sketches {S(L,k)}r k=1, i.e., ËœX(L) = Med{R(k)S(L,k) X |k = 1 , Â·Â·Â· , r}and compute the losses of all nodes for node classiï¬cation (or some node pairs for link prediction). However, the complexity of loss evaluation is O(n), proportional to the number of ground-truth labels. In orde r to avoid O(n) complexity completely, rather than un-sketching the node r epresentation for all labeled nodes, we employ the locality sensitive hashing (LSH) technique ag ain for loss calculation so that only a subset of node losses are evaluated based on a set of hash tabl es. Speciï¬cally, we construct an LSH hash table for each class in a node classiï¬cation problem, wh ich indexes all of the labeled nodes of this class and can be utilized to choose the nodes with poor pr edictions by leveraging the locality property. This technique, introduced in [ 10], is known as sparse forward-pass and back-propagation, and we defer the descriptions to Appendix C. One-time Preprocessing. If the convolution matrix C is ï¬xed (GCN, GraphSAGE), the â€œtwo-sided sketchâ€ S(l,k,kâ€²) C = CS(kâ€² )(TSk(C)T) âˆˆRcÃ—c is the same in each layer and may be denoted as S(k,kâ€²) C . In addition, all of the r2 sketches of C, i.e., {{S(k,kâ€²) C âˆˆRcÃ—c}r k=1}r kâ€²=1 can be computed during the preprocessing phase. If the convolution matrix C is sparse (which is true for most GNNs following Eq. ( 1) on a sparse graph), we can use the sparse matrix representat ions for the sketches {{S(k,kâ€²) C âˆˆRcÃ—c}r k=1}r kâ€²=1, and the total memory taken by the r2 sketches is O(r2c(m/n)) where (2m/n) is the average node degree (see Appendix F for details). W e also need to compute the r count sketches of the input node feature matrix X = X(0), i.e., {S(0,k) X }r k=1 during preprocessing, which requires O(rcd) memory in total. In this regard, we have substituted the inpu t data with compact graph-size independent sketches (i.e., O(c) memory). Although the preprocessing time required to compute these sketches is O(n), it is a one-time cost prior to training, and it is widely known that sketching is practically very fast. Complexities of Sketch-GCN.The theoretical complexities of Sketch-GNN is summarized a s fol- lows, where for simplicity we assume bounded maximum node de gree, i.e., m = O(n). (1) T rain- ing Complexity : (1a) F orward and backward propagation : O(Lcrd(log(c) + d + m/n)) = O(c) time and O(Lr(cd + rm/n)) = O(c) memory. (1b) Hash and sketch update : O(Lr(c + |B|d)) = O(c) time and memory. (2) Preprocessing : O(r(rm+n+c)) = O(n) time and O(rc(d+rm/n)) = O(c) memory. (3) Inference : O(Ld(m + nd)) = O(n) time and O(m + Ld(n + d)) = O(n) mem- ory (the same as a standard GCN). W e defer a detailed summary o f the theoretical complexities of Sketch-GNN to Appendix F. W e generalize Sketch-GNN to more GNN models in Appendix D and the pseudo-code which out- lines the complete workï¬‚ow of Sketch-GNN can be ï¬nd in Append ix E. 4 Related W ork T owards sublinear GNNs. Nearly all existing scalable methods focus on mini-batchin g the large graph and resolving the memory bottleneck of GNNs, without r educing the epoch training time. Few recent work focus on graph compression [ 22, 24] can also achieve sublinear training time by coarsen- ing (e.g., using [ 31]) the graph during preprocessing or condensing the graph with dataset condensa- tion techniques like gradient-matching [ 46], so that we can train GNNs on the coarsened/condensed 7graph with fewer nodes and edges. Nevertheless, these strat egies suffer from two issues : (1) Al- though graph coarsening/condensation is a one-time cost, t he memory and time overheads are often worse than O(n) and can be prohibitively large on graphs with over 100K nodes . Even the fastest graph coarsening algorithm used by [ 22] takes more than 68 minutes to process the 233K-node Red- dit graph [ 45]. The long preprocessing time renders any training speedup s meaningless. (2) The test performance of a model trained on the coarsened graph highly depends on the GNN type. For graph condensation, if we do not carefully choose the GNN architec ture used during condensation, the test performance of downstream GNNs can suffer from a 9.5% accura cy drop on the Cora graph [ 23]. For graph coarsening, although the performance of [ 22] on GCN is good, signiï¬cant performance degradations are observed on GraphSAGE and GA T; see Section 5. Other scalable methods for GNNs can be categorized into four classes, all of them still requi re linear training complexities. (A) On a large sparse graph with n nodes and m edges, the â€œfull-graphâ€ training of a L-layer GCN with d-dimensional (hidden) features per layer requires O(m+ndL+d2L) memory and O(mdL + nd2L) epoch time. (B) Sampling-based methods sample mini-batches from the complete graph following three schemes: (1) node-wisel y sample a subset of neighbors in each layer to reduce the neighborhood size; (2) layer-wisely sam ple a set of nodes independently in each layer; (3) subgraph-wisely sample a subgraph directly and s imply forward-pass and back-propagate on that subgraph. (B.1) GraphSAGE [ 18] samples r neighbors for each node while ignoring mes- sages from other neighbors. O(brL) nodes are sampled in a mini-batch (where b is the mini-batch size), and the epoch time is O(ndrL); therefore, GraphSAGE is impractical for deep GNNs on a large graph. FastGCN [ 12] and LADIES [ 48] are layer-sampling methods that apply importance sampling to reduce variance. (B.2) The subgraph-wise scheme has the best performance and is mos t prevalent. Cluster-GCN [ 14] partitions the graph into many densely connected subgraph s and sam- ples a subset of subgraphs (with edges between subgraphs add ed back) for training per iteration. GraphSAINT [ 45] samples a set of nodes and uses the induced subgraph for mini -batch training. Both Cluster-GCN and GraphSAINT require O(mdL + nd2L) epoch time, which is the same as â€œfull-graphâ€ training, although Cluster-GCN also needs O(m) pre-processing time. (C) Apart from sampling strategies, historical-embedding-based method s propose mitigating sampling errors and improving performance using some stored embeddings. GNNAu toScale [ 17] keeps a snapshot of all embeddings in CPU memory, leading to a large O(ndL) memory overhead. VQ-GNN [ 15] main- tains a vector quantized data structure for the historical e mbeddings, whose size is independent of n. (D) Linearized GNNs [ 42, 4, 33] replace the message passing operation in each layer with a one-time message passing during preprocessing. They are pr actically efï¬cient, but the theoretical complexities remain O(n). Linearized models usually over-simplify the correspondi ng GNN and limit its expressive power. W e defer discussion of more scalable GNN papers and the broad literature of sketching and LHS for neural networks to Appendix G. 5 Experiments T able 3: Performance of Sketch-GNN in comparison to Graph Co arsening [ 22] on ogbn-arxiv. Benchmark ogbn-arxiv GNN Model GCN GraphSAGE GA T â€œFull-Graphâ€ (oracle) .7174 Â±.0029 .7149 Â±.0027 .7233 Â±.0045 Sketch Ratio ( c/n) 0.1 0.2 0.4 0.1 0.2 0.4 0.1 0.2 0.4 Coarsening .6508 Â±.0091 .6665 Â±.0010 .6892 Â±.0035 .5264 Â±.0251 .5996 Â±.0134 .6609 Â±.0061 .5177 Â±.0028 .5946 Â±.0027 .6307 Â±.0041 Sketch-GNN (ours) .6913 Â±.0154 .7004 Â±.0096 .7028 Â±.0087 .6929 Â±.0194 .6963 Â±.0056 .7048 Â±.0080 .6967 Â±.0067 .6910 Â±.0135 .7053 Â±.0034 In this section, we evaluate the proposed Sketch-GNN algorithm and compare it with the (oracle) â€œfull-graphâ€ training baseline, a graph-coarsening based approach ( Coarsening [22]) and a dataset condensation based approach ( GCond [23]) which enjoy sublinear training time, and other scalable methods including: a sampling-based method ( GraphSAINT [45]), a historical-embedding based method ( VQ-GNN [15]), and a linearized GNN ( SGC [42]). W e test on two small graph benchmarks including Cora, Citeseer and several large graph benchmarks including ogbn-arxiv (169K nodes, 1.2M edges), Reddit (233K nodes, 11.6M edges), and ogbn-products (2.4M nodes, 61.9M edges) from [ 20, 45]. See Appendix H for the implementation details. 80.02 0 .04 0 .06 0 .08 0 .10 10âˆ’3.0 10âˆ’2.5 10âˆ’2.0 10âˆ’1.5 10âˆ’1.0 Sketch Ratio (c/n) Relative Error Coreset T aylor Learned via GD (Ours) (a) Error of PTS 2 4 6 8 10 0.05 0.10 0.15 0.20 Epoch âˆ† Hamming 0 5 10 15 20 25 30 0.05 0.10 0.15 0.20 Epoch Hamming(hlearned, hgroundâˆ’ truth) (b) Learnable LSH Figure 1: Figure 1a Relative errors when applying polynomial tensor sketch (PT S) to the nonlinear unit Ïƒ (CXW ) following Eq. ( 4). The dataset used is Cora [ 34]. Ïƒ is the sigmoid activation. W e set r = 5 and test on a GCN with ï¬xed W = Id âˆˆ RdÃ— d. The coefï¬cients {ck}r k=1 can be computed by a coreset regression [ 19] ( blue), by a T aylor expansion of Ïƒ (Â·) (orange), or learned from gradient descent proposed by us (green). Figure 1b The left plot shows the Hamming distance changes of the hash t able in the 2nd layer during the training of a 2-layer Sketch-GCN, where the hash table is constructed from the unsketched rep resentation ËœX(1) using SimHash. The right plot shows the Hamming distances be tween the hash table learned using our algorithm and the hash table constructed directly from ËœX(1). T able 1: Performance of Sketch-GCN in comparison to Graph Condensation [ 23] and Graph Coarsening [ 22] on Cora and Citeseer with 2-layer GCNs. Benchmark Cora Citeseer GNN Model GCN â€œFull-Graphâ€ (oracle) .8119 Â±.0023 .7191 Â±.0018 Sketch-Ratio ( c/n). 0.013 0.026 0.009 0.018 Coarsening .3121 Â±.0024 .6518 Â±.0051 .5218 Â±.0049 .5908 Â±.0045 GCond .7971 Â±.0113 .8002 Â±.0075 .7052 Â±.0129 .7059 Â±.0087 Sketch-GNN (ours) .8012 Â±.0104 .8035 Â±.0071 .7091 Â±.0093 .7114 Â±.0059 T able 2: Performance across GNN architectures in comparison to Graph Condensation [ 23] on Cora with sketch ratio c/n = 0. 026. Preprocessing Architecture Downstream Architecture GCN GraphSAGE â€œFull-Graphâ€ (oracle) N/A .8119 Â±.0023 .7981 Â±.0053 GCond GCN .7065 Â±.0367 .6024 Â±.0203 GraphSAGE .7694 Â±.0051 .7618 Â±.0087 Sketch-GNN (ours) N/A .8035 Â±.0071 .7914 Â±.0121 T able 4: Performance of Sketch-GNN versus SGC [ 42], GraphSAINT [ 45], and VQ-GNN [ 15]. Benchmark ogbn-arxiv Reddit ogbn-product SGC .6944 Â±.0005 .9464 Â±.0011 .6683 Â±.0029 GNN Model GCN GraphSAGE GA T GCN GraphSAGE GA T GCN GraphSAGE G A T â€œFull-Graphâ€ (oracle) .7174 Â±.0029 .7149 Â±.0027 .7233 Â±.0045 OOM OOM OOM OOM OOM OOM GraphSAINT .7079 Â±.0057 .6987 Â±.0039 .7117 Â±.0032 .9225 Â±.0057 .9581 Â±.0074 .9431 Â±.0067 .7602 Â±.0021 .7908 Â±.0024 .7971 Â±.0042 VQ-GNN .7055 Â±.0033 .7028 Â±.0047 .7043 Â±.0034 .9399 Â±.0021 .9449 Â±.0024 .9438 Â±.0059 .7524 Â±.0032 .7809 Â±.0019 .7823 Â±.0049 Sketch Ratio ( c/n) 0.4 0.3 0.2 Sketch-GNN (ours) .7028 Â±.0087 .7048 Â±.0080 .7053 Â±.0034 .9280 Â±.0034 0 .9485 Â±.0061 .9326 Â±.0063 .7553 Â±.0105 .7762 Â±.0093 .7748 Â±.0071 Proof-of-Concept Experiments: (1) Errors of gradient-lea rned PTS coefï¬cients : In Fig. 1a, we train the PTS coefï¬cients to approximate the sigmoid-act ivated Ïƒ(CXW ) to evaluate its ap- proximation power to the ground-truth activation. The rela tive errors are comparable to those of the coreset-based method. (2) Slow-change phenomenon of LSH hash tables : In Fig. 1b (left), we count the changes of the hash table constructed from an uns ketched hidden representation for each epoch, characterized by the Hamming distances between consecutive updates. The changes drop rapidly as training progresses, indicating that apart from the beginning of training, the hash codes of most nodes do not change at each update. (3) Sampled triplet loss for learnable LSH : In Fig. 1b (right), we verify the effectiveness of our update mechanis m for LSH hash functions as the learned hash table gradually approaches the â€œground tru thâ€, i.e., the hash table constructed from the unsketched hidden representation. Performance of Sketch-GNNs.W e ï¬rst compare the performance of Sketch-GNN with the other sublinear training methods, i.e., graph coarsening [ 22] and graph condensation [ 23] under various sketch ratios to understand how their performance is affect ed by the memory bottleneck. Since graph condensation (GCond) requires learning the condensed grap h from scratch and cannot be scaled to large graphs with a large sketch ratio [ 23], we ï¬rst compare with GCond and Coarsening on the two small graphs using a 2-layer GCN in T able 1. W e see GCond and Sketch-GNN can outperform graph coarsening by a large margin and can roughly match the f ull-graph training performance. However, GCond suffers from a processing time that is longer than the training time (see below) and generalizes poorly across GNN architectures. In T able 2, we compare the performance of Sketch- 9GNN and GCond across two GNN architectures (GCN and GraphSAG E). While graph condensation (GCond) relies on a â€œreference architectureâ€ during conden sation, Sketch-GNN does not require preprocessing, and the sublinear complexity is granted by s ketching â€œon the ï¬‚yâ€. In T able 2, we see the performance of GCond is signiï¬cantly degraded when gene ralized across architectures, while Sketch-GNNsâ€™ performance is always close to that of full-gr aph training. In T able 3, we report the test accuracy of both approaches on ogbn-arxiv, with a 3-layer GCN, GraphSAGE, or GA T as the backbone and a sketch ratio of 0.1, 0.2, or 0.4. W e see there are signiï¬cant performance degradations when applying Coarse ning to GraphSAGE and GA T , even under sketch ratio 0.4, indicating that Coarsening may be compatible only with spe ciï¬c GNNs (GCN and APPNP as explained in [ 22]). In contrast, the performance drops of Sketch-GNN are always small across all architectures, even when the sketch ratio is 0.1. Therefore, our approach generalizes to more GNN architectures and consistently out performs the Coarsening method. W e move on to compare Sketch-GNN with linearized GNNs (SGC), sampling-based (GraphSAINT), and historical-embedding-based (VQ-GNN) methods. In T abl e 4, we report the performance of SGC, the â€œfull-graphâ€ training (oracle), GraphSAINT and VQ -GNN with mini-batch size 50K (their performance is not affected by the choice of mini-batch size if it is not too small), and Sketch- GNN with appropriate sketch ratios ( 0.4 on ogbn-arxiv, 0.3 on Reddit, and 0.2 on ogbn-product). From T able 4, we conï¬rm that, with an appropriate sketch ratio, the perfo rmance of Sketch-GNN is usually close to the â€œfull-graphâ€ oracle and competitive with the other scalable approaches. The needed sketch ratio c/n for Sketch-GNN to achieve competitive performance reduces as graph size grows. This further illustrates that, as previously indica ted, the required training complexities (to get acceptable performance) are sublinear to the graph size . Efï¬ciency of Sketch-GNNs. For efï¬ciency measures, we are interested in the comparison to Coars- ening and GCond, since these two approaches achieve subline ar training time at the cost of some preprocessing overheads. Firstly, we want to address that b oth Coarsening and GCond suffer from an extremely long preprocessing time. On ogbn-arxiv, Coarsening and GCond require 358 and 494 seconds on average, respectively, to compress the origi nal graph. In contrast, our Sketch-GNN sketch the input graph â€œon the ï¬‚yâ€ and does not suffer from a pr eprocessing overhead. On ogbn- arxiv with a learning rate of 0.001, full-graph training of GCN for 300 epochs is more than enoug h for convergence, which only takes 96 seconds on average. The preprocessing time of Coarsening and GCond is much longer than the convergence time of full-gr aph training, which renders their training speedups meaningless. However, Sketch-GNN often requires more training memory than Coarsening and GCond to maintain the copies of sketches and a dditional data structures, although these memory overheads are small, e.g., only 16.6 MB more tha n Coarsening on ogbn-arxiv with sketch ratio c/n = 0 .1. All three sublinear methods (Corasening, GCond, Sketch-G NN) lead to a denser adjacency/convolution matrix and thus increased me mory per node. However, this overhead is small for Sketch-GNN because although we sketched the adj acency, its sparsity is still preserved to some extent, as sketching is a linear/multi-linear opera tion. Ablation Studies: (1) Dependence of sketch dimension c on graph size n. Although the the- oretical approximation error increases under smaller sket ch ratio c/n, we observe competitive ex- perimental results with smaller c/n, especially on large graphs. In practice, the sketch ratio r e- quired to maintain â€œfull-graphâ€ model performance decreas es with n. (2) Learned Sketches versus Fixed Sketches. W e ï¬nd that learned sketches can improve the performance of a ll models and on all datasets. Under sketch-ratio c/n = 0 .2, the Sketch-GCN with learned sketches achieves 0.7004 Â±0.0096 accuracy on ogbn-arxiv while ï¬xed randomized sketches degrade performance to 0.6649 Â±0.0106. 6 Conclusion W e present Sketch-GNN, a sketch-based GNN training framework with sublinear trai ning time and memory complexities. Our main contributions are (1) approx imating nonlinear operations in GNNs using polynomial tensor sketch (PTS) and (2) updating sketc hes using learnable locality-sensitive hashing (LSH). Our novel framework has the potential to be ap plied to other architectures and ap- plications where the amount of data makes training even simp le models impractical. The major limitation of Sketch-GNN is that the sketched nonlinear act ivations are less expressive than the original activation functions, and the accumulated error o f sketching makes it challenging to sketch 10much deeper GNNs. W e expect future research to tackle the abo ve-mentioned issues and apply the proposed neural network sketching techniques to other type s of data and neural networks. Consid- ering broader impacts, we view our work mainly as a methodolo gical and theoretical contribution, and there is no obviously foreseeable negative social impac t. Acknowledgments and Disclosure of Funding This work is supported by National Science Foundation NSF-I IS-F AI program, DOD-ONR-Ofï¬ce of Naval Research, DOD-DARP A-Defense Advanced Research Pr ojects Agency Guaranteeing AI Robustness against Deception (GARD), Adobe, Capital One, J P Morgan faculty fellowships, and NSF DGE-1632976. References [1] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Raz enshteyn, and Ludwig Schmidt. Prac- tical and optimal lsh for angular distance. Advances in neural information processing systems , 28, 2015. [2] Haim A vron, Huy Nguyen, and David W oodruff. Subspace emb eddings for the polynomial kernel. Advances in neural information processing systems , 27, 2014. [3] Filippo Maria Bianchi, Daniele Grattarola, and Cesare A lippi. Spectral clustering with graph neural networks for graph pooling. In International Conference on Machine Learning , pages 874â€“883. PMLR, 2020. [4] Aleksandar Bojchevski, Johannes Klicpera, Bryan Peroz zi, Amol Kapoor, Martin Blais, Benedek RÃ³zemberczki, Michal Lukasik, and Stephan GÃ¼nnema nn. Scaling graph neural net- works with approximate pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 2464â€“2473, 2020. [5] Daniele Calandriello, Alessandro Lazaric, Ioannis Kou tis, and Michal V alko. Improved large- scale graph learning through ridge spectral sparsiï¬cation . In International Conference on Ma- chine Learning , pages 688â€“697. PMLR, 2018. [6] Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal V i ncent, Gerald T esauro, and Y oshua Bengio. Hierarchical memory networks. arXiv preprint arXiv:1605.07427 , 2016. [7] Moses Charikar, Kevin Chen, and Martin Farach-Colton. F inding frequent items in data streams. In International Colloquium on Automata, Languages, and Prog ramming, pages 693â€“703. Springer, 2002. [8] Moses S Charikar. Similarity estimation techniques fro m rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of comput ing, pages 380â€“388, 2002. [9] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jona than Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. Mongoose : A learnable lsh framework for efï¬cient neural network training. In International Conference on Learning Representations , 2020. [10] Beidi Chen, Tharun Medini, James Farwell, Charlie T ai, Anshumali Shrivastava, et al. Slide: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems. Proceedings of Machine Learning and Systems , 2:291â€“306, 2020. [11] Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with variance reduction. In International Conference on Machine Learning , pages 942â€“950. PMLR, 2018. [12] Jie Chen, T engfei Ma, and Cao Xiao. Fastgcn: Fast learni ng with graph convolutional networks via importance sampling. In International Conference on Learning Representations , 2018. [13] W enlin Chen, James Wilson, Stephen T yree, Kilian W einb erger, and Y ixin Chen. Compressing neural networks with the hashing trick. In International conference on machine learning , pages 2285â€“2294. PMLR, 2015. 11[14] W ei-Lin Chiang, Xuanqing Liu, Si Si, Y ang Li, Samy Bengi o, and Cho-Jui Hsieh. Cluster- gcn: An efï¬cient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conferenc e on Knowledge Discovery & Data Mining , pages 257â€“266, 2019. [15] Mucong Ding, Kezhi Kong, Jingling Li, Chen Zhu, John Dic kerson, Furong Huang, and T om Goldstein. Vq-gnn: A universal framework to scale up graph n eural networks using vector quantization. Advances in Neural Information Processing Systems , 34, 2021. [16] W enqi Fan, Y ao Ma, Qing Li, Y uan He, Eric Zhao, Jiliang T a ng, and Dawei Y in. Graph neural networks for social recommendation. In The W orld W ide W eb Conference , pages 417â€“426, 2019. [17] Matthias Fey, Jan E Lenssen, Frank W eichert, and Jure Le skovec. Gnnautoscale: Scalable and expressive graph neural networks via historical embedding s. In International Conference on Machine Learning , pages 3294â€“3304. PMLR, 2021. [18] William L Hamilton, Rex Y ing, and Jure Leskovec. Induct ive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 1025â€“1035, 2017. [19] Insu Han, Haim A vron, and Jinwoo Shin. Polynomial tenso r sketch for element-wise function of low-rank matrix. In International Conference on Machine Learning , pages 3984â€“3993. PMLR, 2020. [20] W eihua Hu, Matthias Fey, Marinka Zitnik, Y uxiao Dong, H ongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems , 33:22118â€“22133, 2020. [21] W eihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Y uxia o Dong, and Jure Leskovec. Ogb- lsc: A large-scale challenge for machine learning on graphs . In Thirty-ï¬fth Conference on Neural Information Processing Systems Datasets and Benchm arks T rack (Round 2) , 2021. [22] Zengfeng Huang, Shengzhong Zhang, Chong Xi, T ang Liu, a nd Min Zhou. Scaling up graph neural networks via graph coarsening. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining , pages 675â€“684, 2021. [23] W ei Jin, Lingxiao Zhao, Shichang Zhang, Y ozen Liu, Jili ang T ang, and Neil Shah. Graph con- densation for graph neural networks. In International Conference on Learning Representations , 2021. [24] W ei Jin, Lingxiao Zhao, Shichang Zhang, Y ozen Liu, Jili ang T ang, and Neil Shah. Graph con- densation for graph neural networks. In International Conference on Learning Representations , 2022. [25] William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Contemporary Mathematics , 26(189-206):1, 1984. [26] Shiva Prasad Kasiviswanathan, Nina Narodytska, and Ho ngxia Jin. Network approximation using tensor sketching. In International Joint Conference on Artiï¬cial Intelligence , pages 2319â€“2325, 2018. [27] Thomas N Kipf and Max W elling. Semi-supervised classiï¬ cation with graph convolutional networks. In International Conference on Learning Representations , 2017. [28] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Ref ormer: The efï¬cient transformer. In International Conference on Learning Representations , 2019. [29] Y ibo Lin, Zhao Song, and Lin F Y ang. T owards a theoretica l understanding of hashing-based neural nets. In The 22nd International Conference on Artiï¬cial Intelligen ce and Statistics , pages 127â€“137. PMLR, 2019. [30] Zirui Liu, Kaixiong Zhou, Fan Y ang, Li Li, Rui Chen, and X ia Hu. Exact: Scalable graph neural networks training via extreme activation compressi on. In International Conference on Learning Representations , 2021. 12[31] Andreas Loukas. Graph reduction with spectral and cut g uarantees. Journal of Machine Learn- ing Research , 20:1â€“42, 2019. [32] Ninh Pham and Rasmus Pagh. Fast and scalable polynomial kernels via explicit feature maps. In Proceedings of the 19th ACM SIGKDD international conferenc e on Knowledge discovery and data mining , pages 239â€“247, 2013. [33] Emanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Dav ide Eynard, Michael Bronstein, and Federico Monti. Sign: Scalable inception graph neural n etworks. arXiv preprint arXiv:2004.11198 , 2020. [34] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise G etoor, Brian Galligher, and Tina Eliassi- Rad. Collective classiï¬cation in network data. AI magazine , 29(3):93â€“93, 2008. [35] Y ang Shi and Animashree Anandkumar. Higher-order coun t sketch: Dimensionality reduction that retains efï¬cient tensor operations. In 2020 Data Compression Conference (DCC) , pages 394â€“394. IEEE, 2020. [36] Ryan Spring and Anshumali Shrivastava. Scalable and su stainable deep learning via random- ized hashing. In Proceedings of the 23rd ACM SIGKDD International Conferenc e on Knowl- edge Discovery and Data Mining , pages 445â€“454, 2017. [37] Ryan Spring, Anastasios Kyrillidis, V ijai Mohan, and A nshumali Shrivastava. Compressing gradient optimizers via count-sketches. In International Conference on Machine Learning , pages 5946â€“5955. PMLR, 2019. [38] Rakshith S Srinivasa, Cao Xiao, Lucas Glass, Justin Rom berg, and Jimeng Sun. Fast graph attention networks using effective resistance based graph sparsiï¬cation. arXiv preprint arXiv:2006.08796 , 2020. [39] Petar V eli Ë‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pie tro Lio, and Y oshua Bengio. Graph attention networks. In International Conference on Learning Rep- resentations, 2018. [40] Y ining W ang, Hsiao-Y u Tung, Alexander J Smola, and Anim a Anandkumar. Fast and guaran- teed tensor decomposition via sketching. Advances in neural information processing systems , 28, 2015. [41] Kilian W einberger, Anirban Dasgupta, John Langford, A lex Smola, and Josh Attenberg. Fea- ture hashing for large scale multitask learning. In Proceedings of the 26th annual international conference on machine learning , pages 1113â€“1120, 2009. [42] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fift y, T ao Y u, and Kilian W einberger. Simplifying graph convolutional networks. In International conference on machine learning , pages 6861â€“6871. PMLR, 2019. [43] Shiwen Wu, Fei Sun, W entao Zhang, Xu Xie, and Bin Cui. Gra ph neural networks in recom- mender systems: a survey. ACM Computing Surveys (CSUR) , 2020. [44] Keyulu Xu, W eihua Hu, Jure Leskovec, and Stefanie Jegel ka. How powerful are graph neural networks? In International Conference on Learning Representations , 2018. [45] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajg opal Kannan, and V iktor Prasanna. Graphsaint: Graph sampling based inductive learning metho d. In International Conference on Learning Representations , 2019. [46] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset co ndensation with gradient match- ing. In International Conference on Learning Representations , 2020. [47] Cheng Zheng, Bo Zong, W ei Cheng, Dongjin Song, Jingchao Ni, W enchao Y u, Haifeng Chen, and W ei W ang. Robust graph representation learning via neur al sparsiï¬cation. In International Conference on Machine Learning , pages 11458â€“11468. PMLR, 2020. [48] Difan Zou, Ziniu Hu, Y ewen W ang, Song Jiang, Y izhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large g raph convolutional networks. Ad- vances in Neural Information Processing Systems , 32:11249â€“11259, 2019. 13Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction a ccurately reï¬‚ect the paperâ€™s contributions and scope? [Y es] See Section 1. (b) Did you describe the limitations of your work? [Y es] Currently, our work has two major limitations: (1) our theoretical assumptions and res ults may not perfectly cor- respond to the reality; see the theoretical remarks in Secti on 3, and (2) our imple- mentation is not fully-optimized with the more advanced lib raries; see the efï¬ciency discussions in Section 5. (c) Did you discuss any potential negative societal impacts of your work? [Y es] W e see our work as a theoretical and methodological contributi on toward more resource- efï¬cient graph representation learning. Our methodologic al advances may enable larger-scale network analysis for societal good. However, progress in graph embed- ding learning may potentially inspire other hostile social network studies, such as monitoring ï¬ne-grained user interactions. (d) Have you read the ethics review guidelines and ensured th at your paper conforms to them? [Y es] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretic al results? [Y es] See Lemma 1 and Theorem 1. (b) Did you include complete proofs of all theoretical resul ts? [Y es] See Appendix B. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed t o reproduce the main ex- perimental results (either in the supplemental material or as a URL)? [Y es] See Ap- pendix H. (b) Did you specify all the training details (e.g., data spli ts, hyperparameters, how they were chosen)? [Y es] See Appendix H. (c) Did you report error bars (e.g., with respect to the rando m seed after running experi- ments multiple times)? [Y es] See Section 5. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Y es] See Appendix H. 4. If you are using existing assets (e.g., code, data, models ) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creato rs? [Y es] See Appendix H. (b) Did you mention the license of the assets? [Y es] See Appendix H. (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data youâ€™re using/curating? [No] (e) Did you discuss whether the data you are using/curating c ontains personally identiï¬- able information or offensive content? [No] 5. If you used crowdsourcing or conducted research with huma n subjects... (a) Did you include the full text of instructions given to par ticipants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with l inks to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to partici pants and the total amount spent on participant compensation? [N/A] 14Supplementary Material for Sketch-GNN: Scalable Graph Neural Networks with Sublinear T raining Complexity A More Preliminaries In this appendix, further preliminary information and relevant discussions are provided. A.1 Common GNNs in the Uniï¬ed Framework Here we list the common GNNs that can be re-formulated into the uniï¬ed framework, which is introduced in Section 2. The majority of GNNs can be interpreted as performing messa ge passing on node features, followed by feature transformation and an activation function, a process known as â€œgeneralized graph convolutionâ€ (Eq. ( 1)). Within this common framework, different types of GNNs differ from each other by their choice of convolution matric es C(q), which can be either ï¬xed or learnable. A learnable convolution matrix depends on the in puts and learnable parameters and can be different in each layer (thus denoted as C(l,q)), C(l,q) i,j = C(q) i,jî´™î´˜î´—î´š ï¬xed Â·h(q) Î¸(l,q) (X(l) i,: , X (l) j,: ) î´™ î´˜î´— î´š learnable , (8) where C(q) denotes the ï¬xed mask of the q-th learnable convolution, which may depend on the adja- cency matrix A and input edge features Ei,j . While h(q)(Â·, Â·) : Rfl Ã—Rfl â†’R can be any learnable model parametrized by Î¸(l,q). Sometimes a learnable convolution matrix may be further ro w-wise normalized as C(l,q) i,j â†C(l,q) i,j / âˆ‘ j C(l,q) i,j , for example Graph Attention Network (GA T [ 39]). Ac- cording to [ 15], we list some well-known GNN models that fall inside this fr amework in T able 5. T able 5: Summary of GNNs re-formulated as generalized graph convolution [ 15]. Model Name Design Idea Conv. Matrix T ype # of Conv. Convoluti on Matrix GCN1 [27] Spatial Conv. Fixed 1 C = ËœDâˆ’1/2 ËœA ËœDâˆ’1/2 GIN1 [44] WL-T est Fixed + Learnable 2 { C(1) = A C(2) = In and h(2) Ç«(l) = 1 + Ç«(l) SAGE2 [ 18] Message Passing Fixed 2 { C(1) = In C(2) = Dâˆ’1A GA T3 [39] Self-Attention Learnable # of heads ï£± ï£´ ï£² ï£´ ï£³ C(q) = A + In and h(q) a(l,q) (X(l) i,: , X (l) j,: ) = exp ( LeakyReLU( (X(l) i,: W (l,q) âˆ¥X(l) j,: W (l,q)) Â·a(l,q)) ) 1 Where ËœA = A + In, ËœD = D + In. 2 C(2) represents mean aggregator. W eight matrix in [ 18] is W (l) = W (l,1) âˆ¥ W (l,2). 3 Need row-wise normalization. C(l,q) i,j is non-zero if and only if Ai,j = 1, thus GA T follows direct-neighbor aggregation. A.2 Deï¬nition of Locality Sensitivity Hashing The deï¬nitions of count sketch and tensor sketch are based on the hash table(s) that merely require data-independent uniformity, i.e., a high likelihood that the hash-buckets are of comparable size. In contrast, locality sensitive hashing (LSH) is a hashing sch eme with a locality-sensitive hash function H : Rd â†’[c] that assures close vectors are hashed into the same bucket wi th a high probability while distant ones are not. Consider a locality-sensitive hash fu nction H : Rd â†’[c] that maps vectors in Rd to the buckets {1, . . . , c }. A family of LSH functions His (D, tD, p 1, p2)-sensitive if and only if for any u, v âˆˆRd and any H selected uniformly at random from H, it satisï¬es if Sim (u, v) â‰¥D then P [H(u) = H(v)] â‰¥p1, (9) if Sim (u, v) â‰¤tD then P [H(u) = H(v)] â‰¤p2, where Sim (Â·, Â·) is a similarity metric deï¬ned on Rd. 15B Polynomial T ensor Sketch and Error Bounds In this appendix, we provide additional theoretical details regarding the concentration guarantees of sketching the linear part in each GNN layer (Lemma 1), and the proof of our multi-layer error bound (Theorem 1). B.1 Error Bound for Sketching Linear Products Here, we discuss the problem of approximating the linear product CX (l)W (l) using count/tensor sketch. Since we rely on count/tensor sketch to compress the individual components C and X(l)W (l) of the intermediate product CX (l)W (l) before we sketch the nonlinear activation, it is useful to kn ow how closely sketching approximates the product. W e have the following result: Lemma 1. Given matrices C âˆˆRnÃ—n and (X(l)W (l))T âˆˆRdÃ—n, consider a randomly selected cuont sketch matrix R âˆˆRcÃ—n (deï¬ned in Section 2), where c is the sketch dimension, and it is formed using r = jâˆšn underlying hash functions drawn from a 3-wise independent h ash family H for some j â‰¥1. If c â‰¥(2 + 3 j )/(Îµ2Î´), we have Pr (îµ¹ îµ¹(CRT k)(RkX(l)W (l)) âˆ’CX (l)W (l)îµ¹ îµ¹2 F> Îµ 2âˆ¥Câˆ¥2 Fâˆ¥X(l)W (l)âˆ¥2 F ) â‰¤Î´. (10) Proof. The proof follows immediately from the Theorem 1 of [ 2]. For j â‰¥1 fulï¬lling c â‰¥(2 + 3 j)/(Îµ2Î´), we have j = O(log3 c), and consequently r = ( n)1/j = â„¦(3 logc n). In practice, when n is not too small, logc n â‰ˆ1 since c grows sublinearly with respect to n. In this sense, the dependence of r on n is negligible. B.2 Proof of Error Bound for Final-Layer Representation (Th eorem 1). Proof. For ï¬xed degree r of a polynomial tensor sketch (PTS), by the Theorem 5 of [ 19], for Î“ (1) = max { 5âˆ¥X(l)W (l)âˆ¥2 F, (2 + 3 r) âˆ‘ i(âˆ‘ j [X(l)W (l)]i,j )r} , it holds that E(âˆ¥Ïƒ(CX (l)W (l)) âˆ’ËœX(l+1)âˆ¥2 F) â‰¤ ( 2 1 + cÎ»(l)2 nrÎ“ (l) ) âˆ¥Ïƒ(CX (l)W (l))âˆ¥2 F, (11) where Î»(1) â‰¥0 is the smallest singular value of the matrix Z âˆˆRndÃ—r, each column, Z:,k, being the vectorization of (CX (1)W (1))âŠ™k. This is the error bound for sketching a single layer, includ ing the non-linear activation units. Consider starting from the ï¬rst layer (l = 1 ), for simplicity, let us denote the upper bound when l = 1 as E1. The error in the second layer ( l âˆ’2), including the propagated error from the ï¬rst layer E1, is expressible as âˆ¥Ïƒ(CX (2)W (2)) âˆ’ËœX(3) + E1âˆ¥2 F, which by sub-multiplicativity and the inequality (a + b)2 â‰¤2a2 + 2b2 gives âˆ¥Ïƒ(CX (2)W (2)) âˆ’ËœX(3) + E1âˆ¥2 Fâ‰¤2âˆ¥Ïƒ(CX (2)W (2)) âˆ’ËœX(3)âˆ¥2 F+ 2||E1||. (12) By repeatedly invoking the update rule/recurrence in Eq. ( 1) and the Theorem 5 in [ 19] up to the ï¬nal layer l = L, we obtain the overall upper bound on the total error as claim ed. C Learnable Sketches and LSH C.1 Learning of the Polynomial T ensor Sketch Coefï¬cients. W e propose to learn the coefï¬cients {ck}r k=1 using gradient descent with an L2 regularization, Î» âˆ‘ r k=1 c2 k. For a node classiï¬cation task, the coefï¬cients in all layer s are directly optimized to minimize the classiï¬cation loss. Experimentally, the coef ï¬cients that obtain the best classiï¬cation accuracy do not necessarily correspond to a known activatio n. For the proof of concept experiment (Fig. 1a in Section 5), the coefï¬cients {ck}r k=1 in the ï¬rst layer are learned to approximate the sigmoid activated hidden emb eddings Ïƒ(CX (1)W (1)). The relative 16errors are evaluated relative to the â€œsigmoid activated gro und-truthâ€. W e ï¬nd in our experiments that the relative errors are comparable to the coreset-based app roach. C.2 Change the Hash T able of Count Sketches Here we provide more information regarding the solution to the challenge (2) in Section 3.3. Since the hash tables utilized by each layer is different, we have t o change the underlying hash table of the sketched representations when propagating through Sketch -GNN. Consider the Sketch-GNN forward-pass described by Eq. ( 5), while the count sketch functions are now different in each layer. W e denote the kâ€²-th count sketch function in the l-th layer by CS(l,kâ€²)(Â·) (adding the superscript (l)), and denote its underlying hash table by h(l,k). Since the hash table used to count sketch S(l,k,kâ€²) C is h(l,kâ€²), what we obtain using Eq. ( 5) is CS(l,kâ€² )((X(l+1))T). However, we actually need S(l+1,kâ€²) X = CS(l+1,kâ€²)((X(l+1))T) as the input to the subsequent layer. By deï¬nition, we can change the underlying hash table like S(l+1,kâ€²) X = CS(l+1,kâ€²)((X(l+1))T) = CS(l,kâ€²)((X(l+1))T)R(l,kâ€²)(R(l+1,kâ€²))T, where R(l,kâ€²) is the count sketch matrix of CS(l,kâ€²)(Â·). In fact, we only need to right multiply a cÃ—c matrix T (l,kâ€²) := R(l,kâ€²)(R(l+1,kâ€²))T, which is O(c2) and can be efï¬ciently computed by [T (l,kâ€²)]i,j = nâˆ‘ a=1 s(l+1,kâ€²) a s(l,kâ€²) a /BD {h(l,kâ€²)(a) = i}/BD {h(l+1,kâ€²)(a) = j}. (13) W e can maintain this c Ã—c matrix T (l,kâ€²) as a signature of both hash tables h(l,kâ€²) and h(l+1,kâ€²). W e are able to update T (l,kâ€²) efï¬ciently when we update the hash tables on a subset B of entries (see Section 3.3). W e can also compute the sizes of buckets for both hash funct ions from T (l,kâ€²), which is useful to sketch the attention units in GA T; see Appe ndix D. C.3 Sparse Forward-Pass and Back-Propagation for Loss Eval uation Here we provide more details on using the sparse forward-pas s and back-propagation technique in [ 10] to avoid O(n) complexity in loss evaluation. For a node classiï¬cation tas k, we construct an LSH hash table for each class, which indexes all the labele d nodes in the training split that belong to this class. These LSH hash tables can be used to sele ct the nodes with bad predictions in constant time, i.e., nodes whose predicted class scores hav e a small inner product with respect to the ground truth (one-hot encoded) label. Consequently, we onl y evaluate the loss on the selected nodes, avoiding the O(n) complexity. The LSH hash tables are updated using the same me thod described in challenge (1) in Section 3.3. D Generalize to More GNNs This appendix brieï¬‚y describes how to generalizing Sketch- GNN from GCN to some other GNN architectures, including GraphSAGE [ 18] and GA T [ 39]. D.1 Sketch-GraphSA GE: Sketching Multiple Fixed Convoluti ons The update rule (Eq. ( 5)) of Sketch-GNN can be directly applied to GNNs with only one ï¬xed convolution matrix, such as GCN by setting C = ËœDâˆ’1/2 ËœA ËœDâˆ’1/2. Here we seek to generalize Sketch-GNN to GNNs with multiple ï¬xed convolutions, for exa mple, GraphSAGE with C(1) = In and C(2) = Dâˆ’1A. This can be accomplished by rewriting the update rule of Gra phSAGE X(l+1) = Ïƒ(X(l)W (l,1) + Dâˆ’1AX(l)W (l,2)) as a form resembling Ïƒ(UV T), so that the polynomial tensor sketch technique may still be used. 17Therefore, we replace the update rule (Eq. ( 5)) with the following for GraphSAGE, Ïƒ(X(l)W (l,1) + Dâˆ’1AX(l)W (l,2)) = Ïƒ ([ In âˆ¥(Dâˆ’1A)T]T[ X(l)W (l,1) âˆ¥X(l)W (l,2) ]) â‰ˆ râˆ‘ k=1 ckTSk ([ In âˆ¥(Dâˆ’1A)T ]T ) TSk ([ X(l)W (l,1) âˆ¥X(l)W (l,2) ]T) T . (14) D.2 Sketch-GA T : Sketching Self-Attention Units GA T employs self-attention to learn the convolution matrixC(l) (superscript (l) denotes the convolu- tion matrices learned are different in each layer). For the s ake of simplicity, we assume single-headed attention while we can generalize to multiple heads using th e same method as for GraphSAGE. The convolution matrix of GA T is deï¬ned as C(l) = ( A + In) âŠ™((expâŠ™(Z(l))1n)T)âˆ’1 expâŠ™(Z(l)), where 1n âˆˆRn is a vector of ones, Z(l) âˆˆRnÃ—n is the raw attention scores in the l-th layer, de- ï¬ned as Z(l) i,j = LeakyReLU([ X(l) i,: W (l) âˆ¥X(l) j,: W (l)] Â·a(l)), with a(l) âˆˆR2n being the learnable parameter vector. Our goal is to approximate the sketches of the convolution matrix S(l,k,kâ€² ) C using the sketches of node representations S(l,k) X and the learnable weights W (l), a(l). W e accomplish this by utilizing the locality-sensitive property of the sketches and by assu ming that the random Rademacher vari- ables s(l,1), Â·Â·Â· , s(l,k) are ï¬xed to +1. W e ï¬nd that setting all Rademacher variables to +1 has no discernible effect on the performance of Sketch-GA T . With this additional assumption, each vector of node repres entation can be approximated by the average of vectors hashed into the same bucket, i.e., X(l) i,: â‰ˆ âˆ‘ j /BD {h(l,k)(i) = h(l,k)(j)}X(l) j,: / âˆ‘ j /BD {h(l,k)(i) = h(l,k)(j)}for any k âˆˆ[r]. More speciï¬cally, the numerator is exactly the h(l,k)(i)-th column vector of the sketch S(l,k) X , i.e., âˆ‘ j /BD {h(l,k)(i) = h(l,k)(j)}X(l) j,: = [S(l,k) X ]:,h(l,k)(i). Using only the sketch S(l,k) X and the bucket sizes in the hash table h(l,k), we can approximate any X(l) i,: as a function of h(l,k)(i) (instead of i), and thus approximate the entries of this n Ã—n matrix Z(l) with c2 distinct values only. Even after the element-wise exponent ial and row-wise normalization, any attention score [((expâŠ™(Z(l))1n)T)âˆ’1 expâŠ™(Z(l))]i,j can still be es- timated as a function of the tuple (h(l,k)(i), h(l,k)(j)), where Z(l) i,j = âŸ¨X(l) i,: , X (l) j,: âŸ©. This means we can approximate the attention scores [((expâŠ™(Z(l))1n)T)âˆ’1 expâŠ™(Z(l))] using the sketched representation S(l,k) X , using the fact that Z(l) i,j = âŸ¨X(l) i,: , X (l) j,: âŸ© â‰ˆ âŸ¨[S(l) X ]:,h(l)(i)/|{a|h(l)(a) = h(l)(i)}|[S(l) X ]:,h(l)(j)/|{a|h(l)(a) = h(l)(j)}|âŸ©, where {a|h(l)(a) = h(l)(i)}|is the bucket size of h(l)(i)-th hash bucket. W e can see that computing the sketches of C(l) (the sketch functions are deï¬ned by the same hash table h(l,k)(Â·)) only requires (1) the c2 distinct estimations of the entries in ((expâŠ™(Z(l))1n)T)âˆ’1 expâŠ™(Z(l)), and (2) an â€œaveraged c Ã—c versionâ€ of the mask (A + In), which is exactly the two-sided count sketch of (A + In) deï¬ned by the hash table h(i,j). In conclusion, we ï¬nd a O(c2) algorithm to estimate the sketches of the convolution matri x S(l,k,kâ€²) C using the sketches of node representations S(l,k) X and a pre-computed two-sided count sketch of the mask matrix (A + In). E The Complete Pseudo-Code The following is the pseudo-code outlining the workï¬‚ow of Sk etch-GNN (assuming GCN back- bone). 18Algorithm 1 Sketch-GNN: sketch-based approximate training of GNNs with sublinear complexities Require: GNNâ€™s convolution matrix C, input node features X, ground-truth labels Y 1 procedure PR E P RO C E S S (C,X ) 2 Sketch X = X(0) into {S(0,k) X }r k=1 and sketch C into {{S(k,kâ€²) C }r k=1}r kâ€²=1 3 procedure TR A I N({{S(k,kâ€²) C }r k=1}r kâ€²=1, {S(0,k) X }r k=1, Y ) 4 Initialize weights {W (l)}L l=1, coefï¬cients {{c(l) k }r k=1}L l=1, and LSH projections {{P (l) k }r k=1}L l=1. 5 for epoch t = 1, . . . , T do 6 for layer l = 1, . . . , L âˆ’ 1 do 7 Forward-pass and compute S(l+1,kâ€²) X via Eq. ( 5). 8 Evaluate losses on a subset B of nodes in buckets with the largest gradients for each class . 9 Back-propagate and update weights {W (l)}L l=1 and coefï¬cients {{c(l) k }r k=1}L l=1. 10 Update the LSH projections {{P (l) k }r k=1}L l=1 with the triplet loss Eq. ( 7) for every TLSH epoch. 11 return Learned weights {W (l)}L l=1 12 procedure IN F E R E N C E ({W (l)}L l=1) 13 Predict via the corresponding standard GNN update rule, usi ng the learned weights {W (l)}L l=1 F Summary of Theoretical Complexities In this appendix, we provide more details on the theoreticalcomplexities of Sketch-GNN with a GCN backbone. For simplicity, we assume bounded maximum nod e degree, i.e., m = Î¸(n). Preprocessing. The r sketches of the node feature matrix take O(r(n+c)d) time and occupy O(rdc) memory. And the r2 sketches of the convolution matrix require O(r(m + c) + r2m) time (the LSH hash tables are determined by the node feature vectors alrea dy) and O(r2cm/n) memory. The total preprocessing time is O(r2m + rm + r(n + c)d) = O(n) and the memory taken by the sketches is O(rc(d + rm/n)) = O(c). Forward and backward passes. For each sketch in each layer, matrix multiplications take O(cd(d+ m/n)) time, FFT and its inverse take O(dc log(c)) time, thus the total forward/backward pass time is O(Lcrd(log(c) + d + m/n)) = O(c). The memory taken by sketches in a Sketch-GNN is just L times the memory of input sketches, i.e., O(Lrc(d + rm/n)) = O(c). LSH hash updates and loss evaluation. Computing the triplet loss and updating the corresponding part of the hash table requires O(Lrb(n/c)) where b = |B|is the number of nodes selected based on the gradients (for each sketch). Updates of the sketches a re only performed every TLSH epochs. Inference is conducted on the standard GCN model with parameters {W (l)}L l=1 learned via Sketch- GNN, which takes O(Ld(m/n + d)) time on average for a node sample. Remarks. (1) Sparsity of sketched convolution matrix. The two-sided sketch CS(CS(C)T) âˆˆ RcÃ—c maintains sparsity for sparse convolution C, as CS(CS(C)T) = RCRT (a product of 3 sparse matrices) is still sparse, where count-sketch matrix R âˆˆRcÃ—n has one non-zero entry per column (by its deï¬nition see Section 2). If C has at most s non-zeros per column, there are â‰¤s non-zeros per column in RC when c â‰«s (holds for sparse graphs that real-world data exhibits). Th us, we avoid the O(c2) memory cost and are strictly O(c). (2) Overhead of computing the LSH hash tables. Following Eq. ( 3) and Eq. ( 6), we need O(cd) overhead to obtain the LSH hash index of each node, and since we have n nodes in total and we maintain r independent hash tables, the total overhead for computing the LSH hash tables is O(ncrd) during preprocessing. In conclusion, we achieve sublinear training complexity ex cept for the one-time preprocessing step. G More Related W ork Discussions G.1 Sketch-GNN v .s. GraphSAINT GraphSAINT[ 45] is a graph sampling method that enables training on a mini-b atch of subgraphs instead of on the large input graph. GraphSAINT is easily app licable to any graph neural network (GNN), introduces minor overheads, and usually works well i n practice. However, GraphSAINT is 19not a sub-linear training algorithm, it saves memory at the c ost of time overhead. W e have to iterate through the full batch of subgraphs in an epoch, and the train ing time complexity is still linear in the graph size. In contrast, our proposed Sketch-GNN is an appro ximated training algorithm of some GNNs with sub-linear time and memory complexities. Sketch- GNN has the potential to scale better than GraphSAINT on larger graphs. Besides, as a sketching al gorithm, Sketch-GNN is suitable for some scenarios, for example, sketching big graphs in an onli ne/streaming fashion. Sketch-GNN can also be combined with subgraph sampling to scale up to ext remely large graphs. Sketching the sampled subgraphs (instead of the original graph) avoids th e decreasing sketch-ratio when the input graph size grows to extremely large while with a ï¬xed memory c onstraint. G.2 Sketching in GNNs EXACT [ 30] is a recent work which applies random projection to reduce t he memory footprint of non-linear activations in GNNs. In this regard, they also applies sketching techniques to scale up the training of GNNs. However there are three important di fferences between Sketch-GNN and EXACT summarized as follows: (1) Sketch-GNN propagates sketched representations while sketching in EXACT only affects the back-propagation, (2) S ketch-GNN sketches the graph size dimension while EXACT sketches the feature dimension, and ( 3) Sketch-GNN enjoys sub-linear complexity while EXACT does not. W e want to address that Sket ch-GNN and EXACT are aiming for very different goals; Sketch-GNN is sketching the graph to achieve sub-linear complexity, while EXACT is sketching to save the memory footprint of non-linea r activations G.3 Sketching Neural Networks Compression of layers/kernels via sketching methods has been discussed previously, but not on a full-architectural scale. W ang et al. [ 40] utilize a multi-dimensional count sketch to accelerate th e decomposition of a tensorial kernel, at which point the tens or is fully-restored, which is not possible in our memory-limited scenario. Shi and Anandkumar [ 35] utilize the method of W ang et al. [ 40] to compute compressed tensorial operations, such as contra ctions and convolutions, which is more applicable to our setup. Their experiments involve the repl acement of a fully-connected layer at the end of a tensor regression network rather than full archi tectural compression. Furthermore, they guarantee the recovery of a sketched tensor rather than the r ecovery of tensors passing through a nonlinearity such as a ReLU. Kasiviswanathan et al. [ 26] propose layer-to-layer compression via sign sketches, albeit with no guarantees, and their back-pr opagation equations require O(n2) mem- ory complexity when dealing with the nonlinear activations . In contrast to these prior works, we propose a sketching method for nonlinear activation units, which avoids the need to unsketch back to the high dimensional representation in each layer. G.4 LSH in Neural Networks Locality sensitive hashing (LSH) has been widely adopted toaddress the time and memory bot- tlenecks of many large-scale neural networks training syst ems, with applications in computer vi- sion [ 13], natural language processing [ 6] and recommender systems [ 36]. For fully connected neural networks, Chen et al. [ 10] proposes an algorithm, SLIDE, that retrieves the neurons i n each layer with the maximum inner product during the forward pass using an LSH-based data structure. In SLIDE, gradients are only computed for neurons with estim ated large gradients during back- propagation. For transformers, Kitaev et al. [ 28] proposes to mitigate the memory bottleneck of self-attention layers over long sequences using LSH. More r ecently, Chen et al. [ 9] has dealt with the update overheads of LSH during the training of NNs. Chen e t al. [ 9] introduces a scheduling al- gorithm to adaptively perform LSH updates with provable gua rantees and a learnable LSH algorithm to improve the query efï¬ciency. G.5 Graph Sparsiï¬cation for GNNs Graph sparsiï¬cation, i.e., removing task-irrelevant and redundant edges from the large input graph, can be applied to speed up the training of GNNs. Calandriello et al. [ 5] propose fast and scalable graph sparsiï¬cation algorithms for graph-Laplacian-base d learning on large graphs. Zheng et al. [ 47] sparsify the graph using neural networks and applied to the t raining of general GNNs. Srinivasa et al. [38] speciï¬cally considered the graph sparsiï¬cation problem f or graph attention (e.g., graph atten- 20tion networks, GA T). Graph sparsiï¬cation will not reduce th e number of nodes; thus, the memory reduction of node feature representation is limited. Howev er, some carefully designed graph spar- siï¬cation may enjoy small approximation error (thus smalle r performance drops) and improve the robustness of learned models. H Implementation Details This appendix lists the implementation details and hyper-p arameter setups for the experiments in Section 5. Datasets. Dataset ogbn-arxiv and ogbn-product are obtained from the Open Graph Benchmark (OGB)1 . Dataset Reddit is adopted from [ 45] and downloaded from the PyT orch Geometric library 2, it is a sparser version of the original dataset provided by Ha milton et al. [ 18]. W e conform to the standard data splits deï¬ned by OGB or PyT orch Geometric. Code Frameworks.The implementation of our Sketch-GNN is based on the PyT orch library and the PyT orch Sparse library 3. More speciï¬cally, we implement the Fast Fourier Transform (FFT) and its inverse (used in tensor sketch) using PyT orch. W e imp lement count sketch of node features and convolution matrices as sparse-dense or sparse-sparse matrix multiplications, respectively, using PyT orch Sparse. Our implementations of the standard GNNs ar e based on the PyT orch Geometric library. The implementations of SGC [ 42] and GraphSAINT [ 45] are also adopted from PyT orch Geometric, while the implementations of VQ-GNN 4 [15] and Coarsening 5 [22] are adopted from their ofï¬cial repositories, respectively. All of the above -mentioned libraries (except for PyT orch) and code repositories we used are licensed under the MIT lice nse. Computational Infrastructures. All of the experiments are conducted on Nvidia R TX 2080Ti GPUs with Xeon CPUs. Repeated Experiments.For the efï¬ciency measures in Section 5, the experiments are repeated two times to check the self-consistency. For the performanc e measures in Section 5, we run all the experiments ï¬ve times and report the mean and variance. Setups of GNNs and T raining.On all of the three datasets, unless otherwise speciï¬ed, we a lways train 3-layer GNNs with hidden dimensions set to 128 for all s calable methods and for the oracle â€œfull-graphâ€ baseline. The default learning rate is 0.001. W e apply batch normalization on ogbn- arxiv but not the other two datasets. Dropout is never used. Adam is used as the default optimization algorithm. Setups of Baseline Methods.For SGC, we set the number of propagation steps k in preprocessing to 3 to be comparable to other 3-layer GNNs. For GraphSAINT , we u se the GraphSAINT -R W variant with a random walk length of 3. For VQ-GNN, we set the number of K-means clusters to 256 and use a random walk sampler (walk length is also 3). For Coarsening , we use the V ariation Neighborhood graph coarsening method if not otherwise speciï¬ed. As repor ted in [ 22], this coarsening algorithm has the best performance. W e use the mean aggregator in Graph SAGE and single-head attention in GA T . Setups of Sketch-GNN.If not otherwise mentioned, we always set the polynomial ord er (i.e., the number of sketches) r = 3 . An L2 penalty on the learnable coefï¬cients is applied with coefï¬c ient Î» ranging from 0.01 to 0.1. For the computation of the triplet l oss, we always set Î± to 0.1, but the values of t+ > t âˆ’ > 0 are different across datasets. W e can ï¬nd a suitable startin g point to tune by ï¬nding the smallest inner product of vectors hashed into the same bucket. T o get the sampled subset B, we take the union of 0.01c buckets with the largest gradient norms for each sketch. The LSH hash functions are updated every time for the ï¬rst 5 epochs, a nd then only every TLSH = 10 epochs. W e do not traverse through all pairs of vectors in B to populate P+ and Pâˆ’. Instead, we randomly sample pairs until |P+|, |Pâˆ’|> 1000. 1 https://ogb.stanford.edu/ 2 https://github.com/pyg-team/pytorch_geometric 3 https://github.com/rusty1s/pytorch_sparse 4 https://github.com/devnkong/VQ-GNN 5 https://github.com/szzhang17/Scaling-Up-Graph-Neural-Networks-Via-Graph-Coarsening 21",
      "meta_data": {
        "arxiv_id": "2406.15575v1",
        "authors": [
          "Mucong Ding",
          "Tahseen Rabbani",
          "Bang An",
          "Evan Z Wang",
          "Furong Huang"
        ],
        "published_date": "2024-06-21T18:22:11Z",
        "pdf_url": "https://arxiv.org/pdf/2406.15575v1.pdf"
      }
    },
    {
      "title": "HyperAttention: Long-context Attention in Near-Linear Time",
      "abstract": "We present an approximate attention mechanism named HyperAttention to address\nthe computational challenges posed by the growing complexity of long contexts\nused in Large Language Models (LLMs). Recent work suggests that in the\nworst-case scenario, quadratic time is necessary unless the entries of the\nattention matrix are bounded or the matrix has low stable rank. We introduce\ntwo parameters which measure: (1) the max column norm in the normalized\nattention matrix, and (2) the ratio of row norms in the unnormalized attention\nmatrix after detecting and removing large entries. We use these fine-grained\nparameters to capture the hardness of the problem. Despite previous lower\nbounds, we are able to achieve a linear time sampling algorithm even when the\nmatrix has unbounded entries or a large stable rank, provided the above\nparameters are small. HyperAttention features a modular design that easily\naccommodates integration of other fast low-level implementations, particularly\nFlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to\nidentify large entries, HyperAttention outperforms existing methods, giving\nsignificant speed improvements compared to state-of-the-art solutions like\nFlashAttention. We validate the empirical performance of HyperAttention on a\nvariety of different long-context length datasets. For example, HyperAttention\nmakes the inference time of ChatGLM2 50\\% faster on 32k context length while\nperplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k,\nwith causal masking, HyperAttention offers 5-fold speedup on a single attention\nlayer.",
      "full_text": "HyperAttention: Long-context Attention in Near-Linear Time Insu Han Yale University insu.han@yale.edu Rajesh Jayaram Google Research rkjayaram@google.com Amin Karbasi Yale University, Google Research amin.karbasi@yale.edu Vahab Mirrokni Google Research mirrokni@google.com David P. Woodruff CMU, Google Research dwoodruf@cs.cmu.edu Amir Zandieh Independent Researcher amir.zed512@gmail.com Abstract We present an approximate attention mechanism named â€œHyperAttentionâ€ to address the computational challenges posed by the growing complexity of long contexts used in Large Lan- guage Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after de- tecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates in- tegration of other fast low-level implementations, particularly FlashAttention. Empirically, em- ploying Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention. We validate the empirical performance of HyperAttention on a variety of different long-context length datasets. For example, HyperAttention makes the inference time of ChatGLM2 50% faster on 32k context length while perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k, with causal masking, HyperAttention offers 5-fold speedup on a single attention layer. 1 Introduction Transformers [29] have been successfully applied to a wide variety of learning tasks in areas such as natural language processing [13, 30, 3, 26], computer vision [4, 15], and time series forecast- ing [34]. Despite their success, these models face serious scalability limitations because naÂ¨ Ä±ve exact computation of their attention layers incurs quadratic (in the sequence length) runtime and mem- ory complexities. This presents a fundamental challenge for scaling transformer models to longer context lengths. 1Empirical studies are conducted by I. Han and A. Zandieh. 2Codes are available athttps://github.com/insuhan/hyper-attn. 1 arXiv:2310.05869v3  [cs.LG]  1 Dec 2023Various approaches have been explored to tackle the quadratic-time attention layer, with one notable direction focusing on approximating intermediate matrices in attention layers. Methods for doing this include approximations by sparse matrices [20, 11, 27, 28, 14, 18], low-rank matrices [9, 19], or a combination of both [8, 32, 7, 12]. These methods aim to provide faster approximation to various components of attention, but none of them provide end-to-end approximations of the full dot-product attention. Moreover, none of these works support the use of causal masking, which is a crucial part of modern transformer architectures. On the negative side, recent theoretical bounds suggest that entry-wise approximations to the attention matrix are impossible in sub-quadratic time in general [1]. Nevertheless, a recent work, dubbed KDEFormer [33], was shown to provide provable approximation in subquadratic time, under the assumption that the entries of the attention matrix are bounded. Theoretically, KDEFormer runs in roughly ËœO(n1.173) time; it employs kernel density estimation (KDE) to approximate column norms, allowing one to compute probabilities with which to sample columns of the attention matrix. However, the current algorithms for KDE are lacking practical efficiency [6], and even in theory, there is a gap between the runtime of KDEFormer and the theoretically feasible O(n) time algorithms. In [1], the authors demonstrated that under the same assumption of bounded entries, a nearly linear time O(n1+o(1)) algorithm is possible. However, their algorithm also involves using the polynomial method to approximate the softmax and is likely impractical (e.g., it was not empirically evaluated by the authors). In this work, we provide an algorithm which achieves the best of both worlds, being both a (1) practically efficient algorithm that (2) achieves the best possible near-linear time guarantee. Additionally, our approach supports casual masking, which was not possible via previous works. 1.1 Problem Statement The dot-product attention [29] involves processing three input matrices: Q (queries), K (keys), V (values), all of size n Ã— d, where n is the number of tokens in the input sequence and d is the dimension of latent representations. This process outputs the following: Att = Dâˆ’1AV Here, matrix A := exp \u0000 QKâŠ¤\u0001 is defined as the element-wise exponential of QKâŠ¤. Additionally, D is an n Ã— n diagonal matrix derived from the sum of rows of A, Di,i = âˆ¥Ai,:âˆ¥1 for i âˆˆ [n]. In this context, matrix A is referred to as the â€œattention matrixâ€, and Dâˆ’1A is called the â€œsoftmax matrixâ€. It is important to note that calculating the attention matrix A directly requires Î˜( n2d) operations, and storing it consumes Î˜( n2) memory. Consequently, a straightforward computation of Att demands a runtime of â„¦( n2d) and â„¦(n2) memory. Our objective is to efficiently approximate the output matrix Att while retaining its spectral prop- erties. Our strategy involves designing an efficient estimator for the diagonal scaling matrix D in near-linear time. Additionally, we aim to quickly approximate the matrix product of the softmax matrix Dâˆ’1A and value matrix V through subsampling. To be more specific, our objective is to find a sampling matrix S âˆˆ RmÃ—n with a limited number m = no(1) of rows, along with a diagonal matrix eD âˆˆ RnÃ—n, such that the following bound on the operator norm of the error is met: \r\r\rAtt âˆ’ eDâˆ’1ASâŠ¤ Â· SV \r\r\r op â‰¤ Îµ Â· \r\rDâˆ’1A \r\r op âˆ¥V âˆ¥op . (1) 21.2 Our Contributions We show that efficiently solving the matrix multiplication component of the attention approximation problem in Eq. (1) can be achieved by defining the sampling matrix S based on the row norms of V . The more challenging aspect lies in obtaining a reliable spectral approximation for the diagonal matrix D. In a recent result, Zandieh et al. [33] effectively leverages fast KDE solvers to attain a high-quality approximation of D. However, we streamline the KDEformer procedure and demonstrate that uniform sampling is sufficient to achieve the desired spectral guarantee, eliminating the need for importance sampling based on kernel densities. This significant simplification allows us to develop a practical and provably linear time algorithm. In contrast to prior work [1, 33], our approach does not necessitate bounded entries or bounded stable rank. Furthermore, the fine-grained parameters we introduce to analyze the time complexity may remain small even when the entries in the attention matrix or the stable rank are large. Our work is inspired by the hard instance of Alman & Song [1] for showing quadratic time lower bounds. Such instances have one randomly placed large entry in each row of the attention matrix. Our algorithm has an initial phase where we find large entries of the attention matrix in a black box manner, such as by using Locality Sensitive Hashing [20], or a possibly learned CountSketch applied to the attention matrix [5, 23], or just a known heavy entry pattern [7]. We assume these procedures are fast, and that after removing the heavy entries, two parameters in the resulting attention matrix are small: (1) the max column â„“2-norm, and (2) the ratio of row norms in the un-normalized attention matrix. Prior work of Zandieh et al. [33] used KDE to identify columns in the attention matrix with large norm and to perform approximate matrix product with the value matrix by sampling such columns. As mentioned, finding such columns requires at least O(n1.173) time. Instead, we observe that by doing a one-sided sampling from the squared row norms of V , we can avoid the use of KDEs and achieve the same spectral norm guarantee in terms of the stable rank. Although our algorithm is simple and just samples by the row norms of the value matrix (or even samples uniformly in practice), the main technical challenge is that we do not know the row norms of the attention matrix needed in order to normalize it and produce a proper factorization of it. This is reminiscent of the quadratic time hard instance of [1] where we may not be able to find a heavy entry in a row easily, and thus cannot normalize by its norm in the attention matrix. Our parameters (1) and (2) above allow us to argue that the heavy entries, if they exist, are not distributed in the worst possible way. Empirically, HyperAttention demonstrates significant speed improvements, achieving over a 50 Ã— acceleration in forward and backward propagation for sequence lengths of n = 131k. When dealing with causal masking, the method still delivers a substantial 5 Ã— speedup. Moreover, when our approach is applied to pretrained LLMs, e.g., chatglm2-6b-32k [17] and evaluated on long-context benchmark datasets, so-called LongBench [2], it maintains performance levels that closely match those of the original models, even without the need for fine-tuning. Furthermore, we investigate task-specific evaluations and discover summarization and code completion tasks are more robust to approximate attention layers than question answerings. 32 Preliminaries We make use of the Hamming sorted LSH, a variant of angular locality-sensitive hashing introduced in the work by Zandieh et al. [33]. In this variant, the hash buckets are arranged in order of their Hamming distances. This LSH variant is particularly well-suited for designing GPU-friendly algorithms aimed at identifying dominant entries within the attention matrix A. In the context of Hamming sorted LSH, if we let H : Rd â†’ [B] be a hash function with B buckets drawn from an LSH family, then the collision probability Pr H[H(q) = H(k)] is â€œroughlyâ€ proportional to âŸ¨q, kâŸ©. A very useful property of this LSH variant is that its buckets are ordered in such a way that geometrically adjacent buckets have consecutive buckets. We provide the following definition. Definition 1 (Hamming sorted LSH, Definition 7.3 of [33]) . For positive integer r, there exists an LSH function H : Rd â†’ [2r], such that for any x, yâˆˆ Rd its collision probability is Pr[H(x) = H(y)] = \u0010 1 âˆ’ Î¸(x,y) Ï€ \u0011r where Î¸(x, y) := cos âˆ’1 \u0010 xâŠ¤y âˆ¥xâˆ¥âˆ¥yâˆ¥ \u0011 . Furthermore, this LSH function hashes similar points to adjacent buckets. Specifically, the probability that two points end up in adjacent buckets is given by Pr [H(x) = H(y) Â± 1 (mod 2 r)] = 2Î¸(x,y) Ï€ Â· \u0010 1 âˆ’ Î¸(x,y) Ï€ \u0011râˆ’1 . Using this LSH function, as demonstrated by Zandieh et al. [33], we can sort keys and queries within an attention layer in such a way that large entries get shifted towards the diagonal of the attention matrix. Subsequently, these significant entries in the attention matrix can be captured by computing equal-sized blocks along the diagonal. This approach aligns with the block-memory access patterns of modern hardware and can be efficiently parallelized through batching across blocks. 3 Algorithm To obtain a spectral guarantee when approximating Att, our initial step involves producing a 1 Â±Îµ approximation of the diagonal entries in the matrix D. Subsequently, we approximate the matrix product between Dâˆ’1A and V via sampling according to the squared row â„“2-norms of V . Estimating D. Our procedure for approximating D consists of two steps. Initially, we identify the dominant entries within the attention matrix using an algorithm rooted in the Hamming sorted LSH, as defined in Definition 1. The second step revolves around randomly selecting a small subset of keys K. We will demonstrate that under certain mild assumptions about matrices A and D, this simple approach allows us to establish spectral bounds on the estimated matrix. Our aim is to find a sufficiently precise approximate matrix eD that satisfies: \r\r\r \u0010 eDâˆ’1 âˆ’ Dâˆ’1 \u0011 A \r\r\r op â‰¤ Îµ 2 \r\rDâˆ’1A \r\r op (2) Our assumption is that the column norms of the softmax matrix exhibit a relatively uniform dis- tribution. To be more precise, we assume that for any i âˆˆ [n] there exists some Î± = no(1) such that\r\rDâˆ’1A Â· e(i)\r\r2 2 â‰¤ Î± n . Itâ€™s worth noting that our assumption is more general in comparison to the bounded input entries assumption made in [1]. In fact, if their assumption holds, it implies that 4A q1 q2 ... q9 k1 k2 . . . k9 â‡’ PK PQ APQ,PK q2 q8 q5 q4 q3 q6 q7 q9 q1 k2 k8 k4 k7 k3 k6 k9 k1 k5 â‡’ Pâˆ’1 K Pâˆ’1 Q MH âŠ™ A Figure 1: How sortLSH finds large entries of A: (Left) Keys and queries undergo hashing using the Hamming ordered LSH H(Â·). (Middle) Keys and queries are rearranged based on their hash buckets. Attention matrix after applying these row and column permutations is denoted asAPQ,PK. Large entries of APQ,PK are concentrated around the diagonal blocks. (Right) rows and columns permutations are reversed on the attention matrix and MH âŠ™ A is highlighted. Algorithm 1: sortLSH for locating large entries of A 1: input: matrices Q, K âˆˆ RnÃ—d, and block size b 2: Let H(Â·) be a Hamming sorted LSH as per Definition 1 and hash rows of Q, K 3: Let PK, PQ âˆˆ Sym(n) be permutations satisfying PK(i) < PK(j) if H(Ki,:) â‰¤ H(Kj,:) and PQ(i) < PQ(j) if H(Qi,:) â‰¤ H(Qj,:) 4: return Mask matrix MH âˆˆ {0, 1}nÃ—n defined as MH i,j = 1{âŒŠPQ(i)/bâŒ‹=âŒŠPK(j)/bâŒ‹} \r\rDâˆ’1A Â· e(i)\r\r2 2 â‰¤ no(1) n for all i âˆˆ [n]. In Section 4.3, we empirically compute Î± to be the maximum of the squared â„“2-norms of the columns in Dâˆ’1A and verify that it is indeed sublinear in n. The first step of our empirical algorithm involves identifying large entries of the attention matrix A through hashing keys and queries into uniformly-sized buckets using the Hamming sorted LSH, which we refer to as sortLSH. This process is detailed in Algorithm 1 and is visually illustrated in Fig. 1. Note that we also mention other was of identifying large patterns, such as checking for a known heavy hitter pattern, or using CountSketch which we describe more below. Algorithm 1 returns a sparse mask designed to isolate the dominant entries of the attention matrix. Given this mask, we compute an approximation of the matrix D in Algorithm 2 that satisfies the spectral guarantee in Eq. (2). This algorithm accomplishes this by combining the attention values corresponding to the mask with a randomly chosen subset of columns from the attention matrix. The assumptions of Lemma 1 are used to ensure that the variance of the estimator is small, and the same complexity of the algorithm increases as a function of the parameters Î±, Îº. We remark that our algorithm is versatile and can function effectively with a predefined mask that specifies the positions of dominant entries within the attention matrix, mirroring the approach taken in [7]. The main guarantee provided by this algorithm is given in Lemma 1. Lemma 1 (Approximating D). For any Q, K âˆˆ RnÃ—d, let A = exp(QKâŠ¤). Also let D âˆˆ RnÃ—n be the diagonal matrix with Di,i = âˆ¥Ai,:âˆ¥1. Additionally, suppose that Î± = n Â·maxiâˆˆ[n] \r\rDâˆ’1A Â· e(i)\r\r2 2. For any mask matrix MH âˆˆ {0, 1}nÃ—n let us define the condition number Îº := maxiâˆˆ[n]âŸ¨1âˆ’MH i,:,Ai,:âŸ© minjâˆˆ[n]âŸ¨1âˆ’MH j,:,Aj,:âŸ©. If m = â„¦ \u0010 Îº7Â·Î±2 Îµ6 log n \u0011 , the output eD of Algorithm 2 satisfies Eq. (2) with probability at least 1âˆ’ 1 poly(n) . 5Algorithm 2: ApproxD for estimating diagonal matrix D 1: input: matrices Q, K âˆˆ RnÃ—d, large entries mask MH âˆˆ {0, 1}nÃ—n, parameters Îº >0, Îµ > 1 Îº4 , Î± > Îµ2Îº, and integer m 2: Randomly choose a subset T âŠ† [n] with cardinality |T| = m 3: Ï„ â† maxjâˆˆT D 1 âˆ’ MH j,:, exp(KQâŠ¤ j,:) E // estimate of maximum unmasked row sum 4: Generate m i.i.d. sample â„“1, . . . â„“m âˆ¼ Unif([n]) 5: for i âˆˆ [n] do 6: Ci â† Î˜ \u0010 Îµ2m n log n \u0010 âŸ¨MH i,: , exp(KQâŠ¤ i,:)âŸ© + Ï„ Îº \u0011\u0011 // row-sum of masked entries 7: di â† n m P jâˆˆ[m](1 âˆ’ MH i,â„“j ) min \u0000 exp \u0000 âŸ¨Qi,:, Kâ„“j,:âŸ© \u0001 , Ci \u0001 // row-sum of unmasked entries 8: Ëœdi â† âŸ¨MH i,: , exp(KQâŠ¤ i,:)âŸ© + max \u0000 di, Ï„ Îº \u0001 // full row-sum estimate 9: return diagonal matrix eD = diag({ Ëœdi}n i=1) Algorithm 3: HyperAttention: attention mechanism in near-linear time 1: input: matrices Q, K, V âˆˆ RnÃ—d, mask matrix MH âˆˆ {0, 1}nÃ—n, and parameter Îµ > 1 no(1) 2: Run Algorithm 2 and let eD â† ApproxD \u0000 Q, K, MH, no(1), Îµ, no(1), dÂ· no(1)\u0001 3: Let S âˆˆ RmÃ—n be an i.i.d. sampling matrix based on squared row norms of V as in Lemma 2 4: return eD and S Approximating the product of softmax matrix Dâˆ’1A and values matrix V . Given a eD that meets the spectral approximation conditions as in Eq. (2), we can achieve the spectral constraint in Eq. (1), by finding a sampling matrix that satisfies the following condition, \r\r\r eDâˆ’1ASâŠ¤ Â· SV âˆ’ eDâˆ’1AV \r\r\r op â‰¤ Îµ 2 Â· \r\rDâˆ’1A \r\r op âˆ¥V âˆ¥op (3) We can efficiently find a sampling matrix S âˆˆ RmÃ—n with a small number m of rows that satisfies Eq. (3) by leveraging well-established techniques in Approximate Matrix Multiplication (AMM). Lemma 2. For any matrices eD, A âˆˆ RnÃ—n, V âˆˆ RnÃ—d consider a sampling matrix S âˆˆ RmÃ—n con- structed as follows: first generate m i.i.d. samples â„“1, . . . â„“m âˆˆ [n] according to squared row norms of matrix V , i.e., âˆ¥Vi,:âˆ¥2 2 âˆ¥V âˆ¥2 F , then let the rth row of S be âˆ¥V âˆ¥FâˆšmÂ·âˆ¥Vâ„“r,:âˆ¥2 Â·e(â„“r). If m = â„¦ \u0010 Îµâˆ’2d Â· srank( eDâˆ’1A) \u0011 for some Îµ >0, the following holds with probability at least 0.99: \r\r\r eDâˆ’1ASâŠ¤ Â· SV âˆ’ eDâˆ’1AV \r\r\r op â‰¤ Îµ Â· \r\r\r eDâˆ’1A \r\r\r op âˆ¥V âˆ¥op . The above result is standard and for proof refer to [16]. Main Theorem. Now, we can integrate the subroutines for approximating the diagonal eD and approximating the matrix product between eDâˆ’1A and values matrix V . With this, we introduce the HyperAttention, an efficient algorithm that can approximate the attention mechanism with spectral guarantees as per Eq. (1) in near-linear time. Our Algorithm 3 takes as input a mask 6MH that defines the positions of dominant entries within the attention matrix. This mask can be generated using the sortLSH algorithm (Algorithm 1), or it can be a predefined mask similar to the approach taken in [7]. The large entries mask MH is assumed to be sparse by design and its number of nonzero entries is bounded nnz( MH) = n1+o(1). We now introduce our main theorem which will be proved in Appendix A. Theorem 1 (HyperAttention guarantee). For any matrices Q, K, V âˆˆ RnÃ—d, any mask matrix MH âˆˆ {0, 1}nÃ—n, and parameter Îµ > 1 no(1) , let A = exp(QKâŠ¤) and let D âˆˆ RnÃ—n be the diagonal matrix with Di,i = âˆ¥Ai,:âˆ¥1. If maxiâˆˆ[n] \r\rDâˆ’1A Â· e(i)\r\r2 2 â‰¤ no(1) n and maxiâˆˆ[n]âŸ¨1âˆ’MH i,:,Ai,:âŸ© minjâˆˆ[n]âŸ¨1âˆ’MH j,:,Aj,:âŸ© â‰¤ no(1) then with probability at least 0.98 the outputs S, eD of Algorithm 3 satisfy the spectral condition as in Eq. (1). Moreover, this algorithmâ€™s runtime is O(d Â· n1+o(1) + d Â· nnz(MH)). Note that even if MH is not given to us, but MH can be found in dÂ·n1+o(1) time, the theorem holds. We also give examples when this is possible by using Hamming sorted LSH, which our experiments are based on, or using the ExpanderSketch of [21] which is based on CountSketch [5] but also gives a fast recovery time. In the supplementary we show: Corollary 1 (HyperAttention with sortLSH). Suppose all preconditions of Theorem 1 hold, where the mask matrix MH is defined as follows. Suppose MH âˆˆ {0, 1}nÃ—n is generated as in Algorithm 1 with block size b = no(1) and r = log2 n in Definition 1. We further assume there are at most n1+o(1) pairs (i, j) with Î¸(Qi,âˆ—, Kj,âˆ—) â‰¤ Ï€ 2 (1 âˆ’ o(1)), where Î¸ is as in Definition 1. Then with probability 1âˆ’1/no(1), the MH we find in Algorithm 1 has at most n1+o(1) non-zero entries and with probability at least .98, the outputs S, eD of Algorithm 3 satisfy Eq. (1) and the overall runtime is O(dÂ·n1+o(1)). We note the assumption on the angles of the rows of Q and K in Corollary 1 is satisfied if most rows are drawn uniformly at random from a d-dimensional sphere, since in this case they will be nearly orthogonal, i.e., have angle at most Ï€ 2 (1âˆ’o(1)) with high probability. However, the corollarly also allows n1+o(1) pairs of rows to have arbitrary angle, which may be more realistic. Corollary 2 (HyperAttention with ExpanderSketch). Suppose all preconditions of Theorem 1 hold, where the mask matrix MH is defined as follows. Suppose MH âˆˆ {0, 1}nÃ—n is defined such that there is a threshold Ï„ = no(1) such that MH i,j = 1 if and only if (QKâŠ¤)2 i,j â‰¥ âˆ¥QKâŠ¤ejâˆ¥2 2 Ï„ . Then we can find MH exactly with probability 1 âˆ’ O(1/n2), and with probability at least .98, the outputs S, eD of Algorithm 3 satisfy Eq. (1). The runtime is O(d Â· n1+o(1)). The key idea behind the proof of Corollary 2 is to first sketch Q by an ExpanderSketch T, which is efficient since T has a small number of rows. Then compute ( T Â· Q) Â· KâŠ¤ which is again efficient since (T Â· Q) has a small number of rows. Thus, we never form the matrix Q Â· KâŠ¤. 3.1 Causal Masking Language models commonly employ causal masking. The causal mask is a lower triangular binary square matrix denoted as MC where MC i,j = 1{iâ‰¥j}. The causal attention mechanism is defined as: AttC = Dâˆ’1 C (MC âŠ™ A)V , 7M C âŠ™ A â‡’ ApproxDâ†’ CausalApproxDâ†’ A 21 M C 1 âŠ™ A 11 M C 2 âŠ™ A 22 (recursive call) Figure 2: Causal attention matrix can be divided into three equal-sized non-zero sections: MC 1 âŠ™A11 and MC 2 âŠ™ A22 are both causal attention matrices, and A21 is an unmasked attention matrix. Algorithm 4: CausalApproxD, recursive approximation of DC for causal masking 1: input: matrices Q, K âˆˆ RnÃ—d 2: Split Q and K into equal sized sub-matrices: Q = [QâŠ¤ 1 , QâŠ¤ 2 ]âŠ¤ and K = [KâŠ¤ 1 , KâŠ¤ 2 ]âŠ¤ 3: eDC11 â† CausalApproxD(Q1, K1) and eDC22 â† CausalApproxD(Q2, K2) 4: Run the unmasked algorithm ApproxD (Algorithm 2) on Q2, K1 to get eD21 5: return eDC = \" eDC11, 0 0, eD21 + eDC22 # where A := exp \u0000 QKâŠ¤\u0001 is defined as before and DC is an n Ã— n diagonal matrix derived from the sum of rows of the masked attention MC âŠ™ A, specifically [ DC]i,i = âŸ¨MC i,:, Ai,:âŸ© for i âˆˆ [n]. To approximate causal attention with a spectral guarantee, we require two components. First, we need a spectral approximation for the diagonal matrix DC. Second, we need to approximate the matrix product between Dâˆ’1 C (MC âŠ™ A) and V , which can be achieved using the same sampling technique as described in Algorithm 3 and Lemma 2. The first component is more intricate, and we employ a recursive method to address it. So we focus on how to efficiently approximate the diagonal DC. Our approach is based on a key observation, as depicted in Fig. 2. The masked attentionMCâŠ™A can be decomposed into three non-zero matrices, each of which has half the size of the original attention matrix. The block A21, located entirely below the diagonal is unmasked attention. Consequently, we can approximate its row sums using Algorithm 2. The two diagonal blocks MC 1 âŠ™ A11 and MC 2 âŠ™ A22 shown in Fig. 2 are causal attentions with half the original size. To handle these, we apply a recursive approach and further partition them into smaller blocks, and repeat this procedure. We present a pseudocode for this procedure in Algorithm 4. 4 Experiments In this section, we benchmark our algorithms by scaling up existing large language models to handle long-range sequences. All experiments are performed on a single A100 GPU with 40 GB memory and we use FlashAttention 2 [10] for the exact attention computation. 81.0 1.5 2.0 speedup 0 4 8 12 16 20 24 28 number of replaced layers 4 6 8 10 12perplexity (a) chatglm2-6b-32k 1.0 1.2 1.4 speedup 0 4 8 12 16 20 24 number of replaced layers 36 37 38 39 40 41perplexity (b) phi-1.5 Figure 3: Perplexity and speedup of chatglm2-6b-32k (left) and phi-1.5 (right) monkey patched with HyperAttention. We vary the number of replaced attention layers in the final order. Implementation Detail. We implement HyperAttention based on sortLSH and uniform column sampling. Specifically, we first apply sortLSH to all rows in Q, V âˆˆ RnÃ—d. Then, each set of rows is partitioned into b groups where b is the block size as in Fig. 1. The i-th set of rows in Q is multiplied by the corresponding set in K, resulting in a block-diagonal approximation of APQ,PK. Next, we optimize Algorithm 2 for approximating D by sharing random indices {â„“j}m j=1 with all rows in Q. This corresponds to uniformly sampling m rows in V . To further simplify, we reuse indices {â„“j}m j=1 for the Approximate Matrix Multiplication (AMM) in Lemma 2. The required operations involve permuting n rows, reshaping tensors, and small matrix multiplications. Since every batch, head and block has the same configurations, the implementation can be parallelized using GPUs. 4.1 Monkey Patching Self-attention We first evaluate HyperAttention on two pre-trained LLMs. We choose three models with different architectures that are widely used in practical applications: chatglm2-6b-32k [17], and phi-1.5 [24]. We patch their final â„“ attention layers by replacing with HyperAttentions where â„“ can vary from 0 to the number of all attention layers in each LLM. Note that attentions in both models requires causal masking and we make use of Algorithm 4 by recursively applying it until the input sequence lengths n are less than 4 ,096. We set both bucket size b and the number of sampled columns m to 256 for all sequence lengths. We evaluate the performance of such monkey patched models in terms of perplexity and speedup. We use LongBench [2], a collection of long context benchmark datasets, which contains 6 differ- ent tasks ranging from single and multiple-document question answering, summarization, few-shot learning, synthetic tasks, and code completion. We select a subset of dataset whose encoded se- quence lengths are larger than 32,768 and trim them if the length is over 32,768 so that all data have sequence lengths of 32 ,768. Then, we compute the perplexity (i.e., loss on next tokens prediction) of each model. To highlight the scalability on the long sequences, we calculate the total speedup on all attention layers whether performed by HyperAttention or FlashAttention. The results are summarized in Fig. 3. Observe that chatglm2-6b-32k shows a reasonable perplexity even after monkey patched by HyperAttention, e.g., after replacing 20 layers the perplexity increases 9Number of Replaced Layers Task single-qa multi-qa summarization few-shot synthetic code 0 (exact) 80.63 68.14 53.12 186.58 84.00 99.57 7 72.34 65.37 52.54 182.12 82.00 102.06 14 71.32 62.97 52.13 169.02 80.00 92.12 21 44.75 48.91 50.90 150.44 31.00 82.58 28 27.07 22.65 46.28 65.74 8.33 73.68 Table 1: Performance evaluation ofchatglm2-6b-32k equipped with HyperAttentions on LongBench datasets [2]. They contain 6 different tasks and we evaluate each of them with its own metric where higher value indicates better performance. approximately by 1 and it slowly goes up until 24 layers. But it improves runtimes in attention layers about 50%. If all the layers are replaced then the perplexity goes to up 12 but it runs about 2.3Ã— faster. For phi-1.5, similar happens but the perplexities are linearly increasing as the number of HyperAttention grows. In addition, we evaluate the performances of monkey patched chatglm2-6b-32k on LongBench datasets and compute task-specific evaluation scores on each task including single-document ques- tion answering, multiple-document question answering, summarization, few-shot learning, synthetic tasks and code completion. Results are provided in Table 1. While replacing HyperAttention gen- erally leads to performance degradation, we observe that its role can vary depending on the task at hand. For example, summarization and code completion are more robust to other tasks. Notably, when half of all attention layers are patched (i.e., 14 layers), we verify that most of the tasks do not degrade more than 13%. In particular, the performance of the summarization task remained almost unchanged, suggesting that this task may be more robust to partial alterations in the attention mechanism. We recall that computations in attention layers can be 1 .5Ã— faster when n = 32k. 4.2 Single Self Attention Layer We further explore the speedup of HyperAttention with varying sequence lengths from 4 ,096 to 131,072. We measure wall-clock times of both forward and forward+backward operations when they are computed with FlashAttention or are accelerated by HyperAttention. We measure the times with and without causal masking. All inputs Q, K, V have the same length and their dimensions are fixed to d = 64 and the number of attention heads is set by 12. We chose the same parameters in HyperAttention as described in the previous section. In Fig. 4, we observe that HyperAttention runs to up 54Ã— faster without causal masking and 5.4Ã— when the causal masking applies. Although time complexities of both causal masking and non-masking are the same, a practical algorithm for causal masking (Algorithm 1) requires additional operations such as partitioning Q, K, V , and merging attention outputs which result in an increase of practical runtime. However, those speedups will increase when the sequence lengthn grows. We believe this opens the door to scale self-attention not only for inference but also for training or fine-tuning the LLMs to fit in significantly long sequences. 104k 8k 16k 32k 65k 131k sequence length n 0 10 20 30 40 50speedup Forward Forward+Backward (a) Without causal masking 4k 8k 16k 32k 65k 131k sequence length n 0 1 2 3 4 5speedup Forward Forward+Backward (b) With causal masking Figure 4: Speedup of the exact computation using FlashAttention [10] and HyperAttention (this work) in single self-attention layer during forward and backward operations. For n =131k, Hyper- Attention runs up to 54 Ã— faster without causal masking and 5 .4Ã— with causal masking. 4.3 Empirical Verification of Assumption In addition, we empirically verify our assumption in Theorem 1, i.e., squaredâ„“2-norm of columns in Dâˆ’1A is upper bounded by no(1) n . We investigate pretrained transformers with and without causal masking. For non-causal masking attention, we use the T2T-ViT model [31] and take Q, K, V from its first attention layer on the ImageNet test data set as it is the main computational bottleneck. For each image, we compute Î± to be the largest of the squared â„“2-norms of the columns in \r\rDâˆ’1A Â· ei \r\r2 2 and collect the value over 50k images. The sequence length of the model is given by n = 3,136 and the averaged values of Î± is observed to be 8.1801. This is much smaller than n and can be possibly sublinear in n. To further investigate the dependence on n, we utilize the chatglm2-6b-32k and LongBench narrative-qa dataset, changing the sequence length n from 1k to 9k. We trim or pad the input context so that its length is strictly n. Unlike the vision model, we notice that the first columns in Dâˆ’1A often contain heavy entries; hence we compute Î± as the largest squared norm excluding the first 32 columns. We collect these values for all heads and layers and compute their average. Fig. 5 plots the value of Î± n with various sequence length n. It is observed that the value of Î± n decreases as n grows, supporting the claim that our assumption Î± = no(1) holds in practice. 5 Conclusion In this work, we propose a simple linear time attention approximation algorithm by simplifying the existing algorithm based on kernel density estimation (KDE). We introduce a more general parameterization for a spectral approximation guarantee based on the condition number, which does not require assumptions used in prior work. Our algorithm makes use of sortLSH to find large entries and we adopt fast matrix multiplication via row norm sampling. We additionally study how our algorithm is used for causal masking by recursive partitioning. Empirically, we illustrate that pre trained LLMs using our algorithm can enhance both inference and training speeds with only minimal performance degradation. 111k 2k 3k 4k 5k 6k 7k 8k 9k sequence length n 0.04 0.06 0.08 (1/n) maxi âˆ¥Dâˆ’1A Â· eiâˆ¥2 2 Figure 5: Empirical estimations on Î± (i.e., the largest squared â„“2-norms of the columns in Dâˆ’1A) with various n. In Theorem 1, we assume that Î± = no(1) and the plot supports the claim that our assumption holds in practice. References [1] Josh Alman and Zhao Song. Fast attention requires bounded entries. arXiv preprint arXiv:2302.13214, 2023. [2] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding, 2023. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Neural Information Processing Systems (NeurIPS) , 2020. [4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Proceedings of the Eu- ropean Conference on Computer Vision(ECCV) , 2020. [5] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In International Colloquium on Automata, Languages, and Programming (ICAMP) , 2002. [6] Moses Charikar, Michael Kapralov, Navid Nouri, and Paris Siminelakis. Kernel density esti- mation through density constrained near neighbor search. In Foundations of Computer Science (FOCS), 2020. [7] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models. In International Conference on Learning Representations (ICLR) , 2021. [8] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Re. Scatterbrain: Unifying sparse and low-rank attention. Neural Information Processing Systems (NeurIPS) , 2021. 12[9] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking Attention with Performers. In International Conference on Learning Repre- sentations (ICLR), 2021. [10] Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. 2023. [11] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf-efficient attention using asymmetric clustering. Neural Information Processing Systems (NeurIPS) , 2020. [12] Jyotikrishna Dass, Shang Wu, Huihong Shi, Chaojian Li, Zhifan Ye, Zhongfeng Wang, and Yingyan Lin. Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with a linear taylor attention. arXiv preprint arXiv:2211.05109 , 2022. [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Association for Computational Linguistics (NAACL) , 2018. [14] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023. [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations (ICLR) , 2021. [16] Petros Drineas and Ravi Kannan. Fast monte-carlo algorithms for approximate matrix mul- tiplication. In Proceedings 42nd IEEE Symposium on Foundations of Computer Science , pp. 452â€“459. IEEE, 2001. [17] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. 2022. [18] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Sim- ple on-the-fly length generalization for large language models.arXiv preprint arXiv:2308.16137, 2023. [19] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning (ICML) , 2020. [20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The Efficient Transformer. In International Conference on Learning Representations (ICLR) , 2020. [21] Kasper Green Larsen, Jelani Nelson, Huy L. Nguyen, and Mikkel Thorup. Heavy hitters via cluster-preserving clustering. CoRR, abs/1604.01357, 2016. 13[22] FranÂ¸ cois Le Gall. Faster algorithms for rectangular matrix multiplication. In 2012 IEEE 53rd annual symposium on foundations of computer science , pp. 514â€“523. IEEE, 2012. [23] Yi Li, Honghao Lin, Simin Liu, Ali Vakilian, and David P. Woodruff. Learning the positions in countsketch. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023. [24] Yuanzhi Li, SÂ´ ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks Are All You Need II: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023. [25] Colin McDiarmid. Concentration. In Probabilistic methods for algorithmic discrete mathemat- ics, pp. 195â€“248. Springer, 1998. [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research (JMLR) , 2020. [27] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics (ACL), 2021. [28] Zhiqing Sun, Yiming Yang, and Shinjae Yoo. Sparse Attention with Learning to Hash. In International Conference on Learning Representations (ICLR) , 2021. [29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Neural Information Processing Systems (NeurIPS), 2017. [30] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Neural Infor- mation Processing Systems (NeurIPS) , 2019. [31] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In International Conference on Computer Vision (ICCV) , 2021. [32] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, San- tiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Trans- formers for longer sequences. In Neural Information Processing Systems (NeurIPS) , 2020. [33] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating transform- ers via kernel density estimation. In International Conference on Machine Learning (ICML) , 2023. [34] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Conference on Artificial Intelligence (AAAI) , 2021. 14A Omitted proofs Here we include the proofs that were omitted in the main body of the paper. First, we present the proof of Lemma 1. Proof of Lemma 1: First, we show that Ï„ calculated in line 3 of Algorithm 2 is close to the maxi- mum row sum of the matrix (1n âˆ’MH)âŠ™A. It is easy to check that Ï„ Îº â‰¤ D 1 âˆ’ MH i,: , exp(KQâŠ¤ i,:) E â‰¤ Ï„Îº for all i âˆˆ [n] because of the definition of Îº in the lemma statement. Furthermore, if we define the set: S0 := n i âˆˆ [n] : âŸ¨1 âˆ’ MH i,: , exp(KQâŠ¤ i,:)âŸ© > Ï„ o , (4) then we can show that |S0| is small. Recall that Ï„ is the maximum of the row sums of a random subset of rows, denoted by T where |T| = m. Hence, Pr[ |S0| â‰¥t] â‰¤ (1 âˆ’ t/n)m. Choosing m = â„¦(Îº7Î±2Îµâˆ’6 log n) and t = O(Îºâˆ’7Î±âˆ’2Îµ6n) gives that with probability at least 1 âˆ’ 1 poly(n) |S0| â‰¤O \u0000 Îºâˆ’7 Â· Î±âˆ’2 Â· Îµ6 Â· n \u0001 (5) Next, let us define the upper-capped version of matrix A where entries of i-th row on positions where the mask MH value is equal to zero are capped at value Ci (line 6 of the algorithm) as: eA âˆˆ RnÃ—n : eAi,j := ( min (Ai,j, Ci) if MH i,j = 0 Ai,j otherwise , for every i, jâˆˆ [n]. We proceed by bounding the total mass of large entries of matrix Dâˆ’1A lost through capping (i.e., entries of A that are larger than thresholds Ci). If we define constant bC := Îµ2m Îº2n log n, we can write, \r\r\rDâˆ’1(A âˆ’ eA) \r\r\r 1 = X i,jâˆˆ[n] (1 âˆ’ MH i,j) Â· (Ai,j âˆ’ min (Ai,j, Ci)) /Di,i = âˆX t=0 X i,jâˆˆ[n] (2tâˆ’1) bCDi,i<Ai,jâˆ’Ciâ‰¤(2t+1âˆ’1) bCDi,i 1{MH i,j=0} Â· Ai,j âˆ’ min (Ai,j, Ci) Di,i â‰¤ log2 Îº2 bCX t=0 2t+1 bC Â· \f\f\f n i, jâˆˆ [n] : MH i,j = 0 & Ai,j > Ci + (2t âˆ’ 1) bCDi,i o\f\f\f â‰¤ log2 Îº2 bCX t=0 2t+1 bC Â· Î± (2t bC)2 = O \u0012 Îµ4 Îº5 Â· Î± Â· n \u0013 (6) The inequality in Eq. (6) follows because, for every i âˆˆ [n], the cardinality of the set n i, jâˆˆ [n] : MH i,j = 0 ^ Ai,j > Ci + (2t âˆ’ 1) bCDi,i o must be bounded by Î± (2t bC)2 . The proof of this is by contradiction because otherwise, there must be an l âˆˆ [n] such that the cardinality of the set Hl := n i âˆˆ [n] : MH i,l = 0 V Ai,l > Ci + (2t âˆ’ 1) bCDi,i o 15is at least |Hl| > Î± nÂ·(2t bC)2 . This implies that \r\r\rDâˆ’1A Â· e(l) \r\r\r 2 2 â‰¥ X iâˆˆHl \u0012Ai,l Di,i \u00132 = X iâˆˆHl \u0012 Ci Di,i + (2t âˆ’ 1) bC \u00132 â‰¥ |Hl| Â· \u0012 Îµ2m Îº2n log n + (2t âˆ’ 1) bC \u00132 > Î± n, however, this contradicts with the precondition of the lemma about Î± = nÂ·maxiâˆˆ[n] \r\rDâˆ’1A Â· e(i)\r\r2 2. Now, if we defined the sets S1, S2 âŠ† [n] as S1 = \u001a i âˆˆ [n] : ÎµDii 3 < âˆ¥Ai,: âˆ’ eAi,:âˆ¥1 â‰¤ Dii 3 \u001b , S2 = \u001a i âˆˆ [n] : âˆ¥Ai,: âˆ’ eAi,:âˆ¥1 > Dii 3 \u001b (7) then it follows from Eq. (6) that the cardinalities of S1 and S2 are bounded by |S1| â‰¤O \u0000 Îºâˆ’4 Â· Î±âˆ’1 Â· Îµ3n \u0001 , |S2| â‰¤O \u0000 Îºâˆ’4 Â· Î±âˆ’1 Â· Îµ4n \u0001 . (8) Next note that di computed in line 7 of the algorithm is an estimator for i-th row norm of the capped and masked matrix ( 1n âˆ’ MH) âŠ™ eA. Let us define an estimator for the unmasked capped matrix eA as bdi := di + âŸ¨MH i,: , Ai,:âŸ©. By invoking Chernoff-Hoeffding inequality (see e.g., [25]) along with union bound, because the lemma statement assumes that m = â„¦ \u0010 Îº7Â·Î±2 Îµ6 log n \u0011 the following holds simultaneously for all i âˆˆ [n] \\ S2 with probability 1 âˆ’ 1 poly(n) : \r\r\r eAi,: \r\r\r 1 1 + Îµ/6 â‰¤ bdi â‰¤ \r\r\r eAi,: \r\r\r 1 1 âˆ’ Îµ/6. (9) This inequality combined with definition of S1, S2 in Eq. (7) implies that for any i âˆˆ [n] \\ (S1 âˆª S2), (1 âˆ’ Îµ/2) Â· Dâˆ’1 i,i â‰¤ bdâˆ’1 i â‰¤ (1 +Îµ/2) Â· Dâˆ’1 i,i . Now we bound the operator norm of the error as follows: \r\r\r( eDâˆ’1 âˆ’ Dâˆ’1)A \r\r\r op â‰¤ \r\r\r\r \u0010 eDâˆ’1 âˆ’ Dâˆ’1 \u0011 S1âˆªS2 A \r\r\r\r op + \r\r\r\r \u0010 eDâˆ’1 âˆ’ Dâˆ’1 \u0011 [n]\\(S1âˆªS2) A \r\r\r\r op â‰¤ 3 2 \r\r\rDâˆ’1 S1 A \r\r\r op + \r\r\r\r \u0010 eDâˆ’1 âˆ’ Dâˆ’1 \u0011 S2 A \r\r\r\r op + Îµ 2 \r\r\rDâˆ’1 [n]\\(S1âˆªS2)A \r\r\r op â‰¤ 3 2 \r\r\rDâˆ’1 S1 A \r\r\r op + Îº2 \r\r\rDâˆ’1 S2âˆ©S0A \r\r\r op + Îº \r\r\rDâˆ’1 S2\\S0 A \r\r\r op + Îµ 2 \r\r\rDâˆ’1 [n]\\(S1âˆªS2)A \r\r\r op â‰¤ 3 2 \r\r\rDâˆ’1 S1 A \r\r\r op + Îº2 \r\r\rDâˆ’1 S0 A \r\r\r op + Îº \r\r\rDâˆ’1 S2 A \r\r\r op + Îµ 2 \r\r\rDâˆ’1 [n]\\(S1âˆªS2)A \r\r\r op , (10) where the second inequality above follows from the inequality in Eq. (9) and also because the definition of S1 ensures that eDi,i â‰¥ (1 âˆ’ 1/3) Â· Di,i for any i âˆˆ S1. The third inequality above follows because the lower capping in line 8 of the algorithm ensures that eDj,j â‰¥ âŸ¨MH j,:, Aj,:âŸ© + Ï„/Îº for any j âˆˆ S2 while Dj,j â‰¤ âŸ¨MH j,:, Aj,:âŸ© + Ï„ for any j /âˆˆ S0 by definition of set S0 and we know that Dj,j â‰¤ âŸ¨MH j,:, Aj,:âŸ© + Ï„Îº for j âˆˆ S0. Finally we conclude the proof by bounding the terms \r\rDâˆ’1 Sr A \r\r op for r âˆˆ {0, 1, 2} in Eq. (10). Fix some r âˆˆ {0, 1, 2}. Let v be the unit-normed vector that realizes the operator norm of Dâˆ’1 Sr A. Since 16Dâˆ’1 Sr A is a non-negative matrix, w.l.o.g., we can assume that v is a non-negative vector. More precisely v := arg max xâˆˆRn + âˆ¥xâˆ¥2=1 \r\rDâˆ’1 Sr A Â· x \r\r 2. One has that \r\rDâˆ’1 Sr A Â· v \r\r 2 = \r\rDâˆ’1 Sr A \r\r op. We define the sequence of binary matrices B0, B1, B2 . . .which have same shape as Dâˆ’1 Sr A as follows: Bt i,j := 1n 2âˆ’tâˆ’1 âˆš Î±/n<[Dâˆ’1 Sr A]i,jâ‰¤2âˆ’t âˆš Î±/n o for every integers t â‰¥ 0. (11) Note that because of the precondition of the lemma about Î± = n Â· maxiâˆˆ[n] \r\rDâˆ’1A Â· e(i)\r\r2 2 which implies [Dâˆ’1A]i,j â‰¤ p Î±/n, we have the following inequality on each entry of the matrix Dâˆ’1 Sr A: \u0002 Dâˆ’1 Sr A \u0003 i,j â‰¤ p Î±/n Â· âˆX t=0 2âˆ’t Â· \u0002 Bt\u0003 i,j . Since Dâˆ’1 Sr A and v both have non-negative entries, the above inequality implies the following: \r\rDâˆ’1 Sr A Â· v \r\r 2 â‰¤ p Î±/n Â· \r\r\r\r\r âˆX t=0 2âˆ’t Â· Bt Â· v \r\r\r\r\r 2 â‰¤ p Î±/n Â· âˆX t=0 2âˆ’t Â· \r\rBt Â· v \r\r 2 . (12) Now to bound \r\rBt Â· v \r\r 2 we first find bounds on the number of 1â€™s in rows and columns of Bt. Using the definition of Bt in Eq. (11) and the fact that row sums in matrix Dâˆ’1A are equal to 1, we have: \r\rBt i,: \r\r 0 â‰¤ min(2t+1p n/Î±, n). (13) Additionally, using the precondition of the lemma about Î± = n Â· maxiâˆˆ[n] \r\rDâˆ’1A Â· e(i)\r\r2 2, we have: \r\rBt :,j \r\r 0 â‰¤ min(22t+2, |Sr|). (14) Now we bound the norm \r\rBt Â· v \r\r 2 for an arbitrary integer t â‰¥ 0 as follows: \r\rBt Â· v \r\r2 2 â‰¤ |Sr|X i=1 \r\rBt i,: \r\r 0 Â· \r\rBt i,: âŠ™ v \r\r2 2 (Cauchyâ€“Schwarz inequality) â‰¤ 2t+1p n/Î± Â· |Sr|X i=1 \r\rBt i,: âŠ™ v \r\r2 2 = 2t+1p n/Î± Â· X jâˆˆ[n] |Sr|X i=1 Bt i,j Â· v2 j = 2t+1p n/Î± Â· X jâˆˆ[n] \r\rBt :,j \r\r 0 Â· v2 j â‰¤ 2t+1p n/Î± Â· min(22t+2, |Sr|) Â· X jâˆˆ[n] v2 j = 2t+1p n/Î± Â· min(22t+2, |Sr|), where the inequality in second line above follows from Eq. (13) and the inequality in the last line follows from Eq. (14). The last equality follows from the assumption that âˆ¥vâˆ¥2 = 1. Therefore, \r\rBt Â· v \r\r 2 â‰¤ ( 2 3t+3 2 Â· (n/Î±)1/4 if 2t+1 â‰¤ p |Sr| 2 t+1 2 Â· (n/Î±)1/4 Â· p |Sr| otherwise . 17Now by plugging the above inequalities into Eq. (12) we find that: \r\rDâˆ’1 Sr A \r\r op â‰¤ p Î±/n Â· ï£« ï£¬ï£­ log2 âˆš |Sr|âˆ’1X t=0 2âˆ’t Â· \r\rBt Â· v \r\r 2 + âˆX t=log2 âˆš |Sr| 2âˆ’t Â· \r\rBt Â· v \r\r 2 ï£¶ ï£·ï£¸ â‰¤ 12 \u0012Î± Â· |Sr| n \u00131/4 â‰¤ ï£± ï£´ï£² ï£´ï£³ Îµ3/2 6Îº7/4Î±1/4 if r = 0 Îµ3/4 9Îº if r = 1 Îµ 6Îº if r = 2 , where the last line above follows from the upper bound on the size of sets Sr we obtained in Eq. (5) and Eq. (8). Finally, by plugging the above inequality into Eq. (10) and using the fact that Dâˆ’1A is a row-stochastic matrix and thus \r\rDâˆ’1A \r\r op â‰¥ 1 the lemma follows. Next, we prove the main theorem. Proof of Theorem 1: The diagonal matrix eD in line 2 is computed by invoking Algorithm 2. By Lemma 1 with probability at least 1âˆ’ 1 poly(n) , it holds that \r\r\r \u0010 eDâˆ’1 âˆ’ Dâˆ’1 \u0011 A \r\r\r op â‰¤ Îµ/2Â· \r\rDâˆ’1A \r\r op. Furthermore, in line 3 of the algorithm S is defined as the sampling matrix according to the row norms of V . To invoke Lemma 2, we need to have a bound on the stable rank of eDâˆ’1A. First, from Lemma 1 we know that \r\r\r eDâˆ’1A \r\r\r op â‰¥ (1âˆ’Îµ/2)Â· \r\rDâˆ’1A \r\r op â‰¥ 1/2, where the second in- equality follows because Dâˆ’1A is a row-stochastic matrix. Therefore we have srank( eDâˆ’1A) â‰¤ 4 \r\r\r eDâˆ’1A \r\r\r 2 F . Second, the lower capping in line 8 of Algorithm 2 ensures that \r\r\r eDâˆ’1A \r\r\r 2 F â‰¤ Îº2 \r\rDâˆ’1A \r\r2 F â‰¤ no(1) Â· \r\rDâˆ’1A \r\r2 F â‰¤ no(1), thus srank( eDâˆ’1A) â‰¤ no(1). With this bound on the stable rank and since m = d Â· no(1) = â„¦ \u0010 Îµâˆ’2d Â· srank( eDâˆ’1A) \u0011 , Lemma 2 implies that \r\r\r eDâˆ’1ASâŠ¤ Â· SV âˆ’ eDâˆ’1AV \r\r\r op â‰¤ Îµ/3 Â· \r\r\r eDâˆ’1A \r\r\r op âˆ¥V âˆ¥op â‰¤ Îµ/2 Â· \r\rDâˆ’1A \r\r op âˆ¥V âˆ¥op with probability 0.99. By combining these two inequalities, we obtain the spectral approximation guarantee in Eq. (1). The runtime of this algorithm is primarily determined by the time it takes to invoke Algorithm 2 in line 2, which is dominated by the time to multiply two matrices of sizes n Ã— d and d Ã— m and the time to calculate attention matrix entries at the positions defined by MH. Using the matrix multiplication result from [22], the first computation can be done in time O(dn1+o(1)) and the latter can be done in nnz( MH). Proof of Corollary 1: Because r = log2 n, we have Pr[H(Qi,âˆ—) = H(Kj,âˆ—)] â‰¤ 1/n1âˆ’o(1) whenever Î¸(Qi,âˆ—, Kj,âˆ—) â‰¥ Ï€ 2 (1 âˆ’ o(1)). As there are at most n2 total pairs, the expected number of such pairs that collide under H is at most n1+o(1) and so by a Markov bound is at most n1+o(1) with failure probability 1/no(1). Since we also assume there are at most n1+o(1) pairs (i, j) with Î¸(Qi,âˆ—, Kj,âˆ—) < Ï€ 2 (1 âˆ’ o(1)), there can be at most n1+o(1) additional pairs that collide. Thus, in total we have n1+o(1) collisions, and consequently the number of non-zero entries in MH 18is at most n1+o(1) with failure probability 1/no(1). The proof now follows by the assumptions of the corollary statement as well as Theorem 1. Proof of Corollary 2: Let T be an ExpanderSketch matrix with O(Ï„ log n) rows. By the guar- antees [21] of T, we have that with probability at least 1 âˆ’ 1 n3 , for any fixed vector x âˆˆ Rn, that from T Â· x, one can recover a set S of indices i âˆˆ [n] such that if x2 i â‰¥ âˆ¥xâˆ¥2 2 Ï„ , then i âˆˆ S, whereas if x2 i â‰¤ âˆ¥xâˆ¥2 2 2Ï„ , then i /âˆˆ S. Further, S can be found from Tx in no(1) time. We compute T Â·Q, followed by (T Â·Q)Â·KâŠ¤. Note that the time for this computation is O(Ï„n log n). Next, for each column j of QKâŠ¤ this allows us to construct a set Sj with the property that if (Q Â· KâŠ¤)i,j â‰¥ âˆ¥QÂ·KâŠ¤ejâˆ¥2 2 Ï„ , then i âˆˆ Sj. This holds simultaneously for all columns with probability at least 1 âˆ’ 1 n2 by a union bound. The time for constructing all the sets Sj is n1+o(1)d Note that |Sj| â‰¤2Ï„ for all j, and we can explicitly compute the exact value of ( Q Â· KâŠ¤)i,j for all i âˆˆ Sj and all j, in O(nÏ„d) time. By the assumptions of the corollary, we have that Sj contains a superset of the support of the j-th column of MH, and since we can compute the values exactly, we can exactly construct the mask MH matrix that the corollary requires, and in n1+o(1)d time. The proof now follows by the assumptions of the statement as well as Theorem 1. 19",
      "meta_data": {
        "arxiv_id": "2310.05869v3",
        "authors": [
          "Insu Han",
          "Rajesh Jayaram",
          "Amin Karbasi",
          "Vahab Mirrokni",
          "David P. Woodruff",
          "Amir Zandieh"
        ],
        "published_date": "2023-10-09T17:05:25Z",
        "pdf_url": "https://arxiv.org/pdf/2310.05869v3.pdf"
      }
    },
    {
      "title": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations",
      "abstract": "The training of graph neural networks (GNNs) is extremely time consuming\nbecause sparse graph-based operations are hard to be accelerated by hardware.\nPrior art explores trading off the computational precision to reduce the time\ncomplexity via sampling-based approximation. Based on the idea, previous works\nsuccessfully accelerate the dense matrix based operations (e.g., convolution\nand linear) with negligible accuracy drop. However, unlike dense matrices,\nsparse matrices are stored in the irregular data format such that each\nrow/column may have different number of non-zero entries. Thus, compared to the\ndense counterpart, approximating sparse operations has two unique challenges\n(1) we cannot directly control the efficiency of approximated sparse operation\nsince the computation is only executed on non-zero entries; (2) sub-sampling\nsparse matrices is much more inefficient due to the irregular data format. To\naddress the issues, our key idea is to control the accuracy-efficiency trade\noff by optimizing computation resource allocation layer-wisely and\nepoch-wisely. Specifically, for the first challenge, we customize the\ncomputation resource to different sparse operations, while limit the total used\nresource below a certain budget. For the second challenge, we cache previous\nsampled sparse matrices to reduce the epoch-wise sampling overhead. Finally, we\npropose a switching mechanisms to improve the generalization of GNNs trained\nwith approximated operations. To this end, we propose Randomized Sparse\nComputation, which for the first time demonstrate the potential of training\nGNNs with approximated operations. In practice, rsc can achieve up to\n$11.6\\times$ speedup for a single sparse operation and a $1.6\\times$ end-to-end\nwall-clock time speedup with negligible accuracy drop.",
      "full_text": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Zirui Liu 1 Shengyuan Chen 2 Kaixiong Zhou 1 Daochen Zha 1 Xiao Huang 2 Xia Hu 1 Abstract Training graph neural networks (GNNs) is ex- tremely time-consuming because sparse graph- based operations are hard to be accelerated by community hardware. Prior art successfully reduces the computation cost of dense matrix based operations (e.g., convolution and linear) via sampling-based approximation. However, unlike dense matrices, sparse matrices are stored in an irregular data format such that each row/column may have a different number of non-zero entries. Thus, compared to the dense counterpart, approx- imating sparse operations has two unique chal- lenges (1) we cannot directly control the effi- ciency of approximated sparse operation since the computation is only executed on non-zero entries; (2) sampling sparse matrices is much more ineffi- cient due to the irregular data format. To address the issues, our key idea is to control the accuracy- efficiency trade-off by optimizing computation re- source allocation layer-wisely and epoch-wisely. For the first challenge, we customize the com- putation resource to different sparse operations, while limiting the total used resource below a cer- tain budget. For the second challenge, we cache previously sampled sparse matrices to reduce the epoch-wise sampling overhead. To this end, we propose Randomized Sparse Computation. In practice, RSC can achieve up to 11.6Ã— speedup for a single sparse operation and 1.6Ã— end-to- end wall-clock time speedup with almost no ac- curacy drop. Codes are available at https:// github.com/warai-0toko/RSC-ICML. 1Department of Computer Science, Rice University, Houston, TX, USA 2Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong SAR. Correspondence to: Xia Hu <xia.hu@rice.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). 1. Introductions Graph Neural Networks (GNNs) have achieved great suc- cess across different graph-related tasks (Hamilton et al., 2017; Hu et al., 2020; Ying et al., 2018; Jiang et al., 2022; Zhou et al., 2022; 2023). However, despite its effective- ness, the training of GNNs is very time-consuming. Specifi- cally, GNNs are characterized by an interleaved execution that switches between the aggregation and update phases. Namely, in the aggregation phase, every node aggregates messages from its neighborhoods at each layer, which is implemented based on sparse matrix-based operations (Fey & Lenssen, 2019; Wang et al., 2019). In the update phase, each node will update its embedding based on the aggre- gated messages, where the update function is implemented with dense matrix-based operations (Fey & Lenssen, 2019; Wang et al., 2019). In Figure 1, SpMM and MatMul are the sparse and dense operations in the aggregation and update phases, respectively. Through profiling, we found that the aggregation phase may take more than 90% running time for GNN training. This is because the sparse matrix opera- tions in the aggregation phase have many random memory accesses and limited data reuse, which is hard to be acceler- ated by community hardwares (e.g., CPUs and GPUs) (Duan et al., 2022b; Han et al., 2016; Duan et al., 2022a). Thus, training GNNs with large graphs is often time-inefficient. ognb-proteins Reddit ogbn-product0 20 40 60 80 100Percentage of Time Consumption Other MatMul(forward) MatMul(backward) SpMM(forward) SpMM(backward) Figure 1: The time profiling of a two-layer GCNs on dif- ferent datasets. SpMM may take 70% âˆ¼ 90% of the total time. We measure the time on a single NVIDIA RTX3090 (24GB). The detailed software and hardware information can be found in Appendix D. Existing works towards this problem can be roughly divided into three categories. First, some works propose distributed GNNs training systems, which focus on minimizing the 1 arXiv:2210.10737v2  [cs.LG]  2 Jul 2023RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations communication cost among hardware (Zheng et al., 2020; Ramezani et al., 2022; Wan et al., 2022b; Md et al., 2021; Wan et al., 2022a). Second, another research line optimizes the memory access pattern of sparse operations via coalesc- ing the memory access and fusing consecutive operations (Zhang et al., 2022; Huang et al., 2020a; Rahman et al., 2021; Wang et al., 2021). Third, some other works try to accelerate the training process from the optimization aspect, i.e., using fewer iterations to converge (Narayanan et al., 2022; Cong et al., 2020; Xu et al., 2021; Cai et al., 2021). In parallel, an orthogonal direction is to replace the ex- pensive operations with their faster-approximated versions (Adelman et al., 2021; Drineas et al., 2006b). The key idea is to sub-sample tensors onto low dimensional spaces and perform the original operations here. For example, for the linear operation between two matrices A âˆˆ RnÃ—m and B âˆˆ RmÃ—q, we first obtain Aâ€² âˆˆ RnÃ—k and Bâ€² âˆˆ RkÃ—q (k < m) by picking k representative columns of A and the corresponding rows of B (Drineas et al., 2006b). Then we approximate AB â‰ˆ Aâ€²Bâ€². With this procedure, the number of floating-point operations (FLOPs) and memory access are both reduced. Based on the idea, previous work success- fully accelerates the dense matrix based operations, such as convolution and linear operations (Adelman et al., 2021). The approximated operation can plug-and-play replace the exact operation to improve per-operation efficiency, and thus is compatible with most of the efficient training methods. Despite the potential, this perspective however has not been explored for the sparse operations in GNNs. The approximation method reduces the computational com- plexity at the cost of giving noisy outputs. Thus, there naturally exists an accuracy-efficiency trade-off. Com- pared to approximating dense matrix operations, there are two unique challenges to optimizing the trade-off for ap- proximated sparse operations. First, unlike the previous example of approximating linear operation, k cannot di- rectly control the efficiency (FLOPs) for sparse operations. This is because, for dense matrices, each row/column has the same amount of parameters. Thus the reduction of FLOPs in approximated dense operations is determined by the dimensions of the sub-sampled matrices (i.e., k). How- ever, in sparse operations, each row/column in the sparse adjacency matrix has different numbers of non-zero en- tries, and the computation is only executed on non-zero entries (i.e., irregular data format). Thus, the reduction of FLOPs in the sparse operations is decided by the selection of representative rows/columns. It lacks a mechanism to directly control the efficiency-accuracy trade-off for each sparse operation. Second, compared to the dense counter- part, sub-sampling (i.e., slicing) the sparse matrix is much more time-consuming due to its irregular data format (Han et al., 2016; Fey & Lenssen, 2019), which counteracts the acceleration from the FLOPs reduction. To this end, we propose Randomized Sparse Computation, dubbed RSC , the first approximation framework tailored for efficient GNN training. Our core idea is to control the trade-off by optimizing the computation resource alloca- tion at the â€œglobalâ€ level. Specifically, to tackle the first challenge, at the layer-wise level, we propose to customize the FLOPs of each sparse operation while limiting the total FLOPs under a certain budget. The rationale behind this strategy is that each operation may have a different contribu- tion to the model accuracy. Thus, we could to assign more computational resources to â€œimportantâ€ operations under a certain budget. More concretely, we frame it as a constraint optimization problem. Then we propose a greedy algorithm to solve it efficiently. To tackle the second challenge, at the epoch-wise level, we found that the selection of represen- tative row/columns tends to remain similar across nearby iterations. Based on this finding, we develop a caching mechanism to reuse the previously sampled sparse matrix across nearby iterations to reduce per-epoch sampling time. Finally, inspired by the recent finding that the final stage of training usually needs smaller noise to help convergence (Li et al., 2019; Dao et al., 2022), we propose to use approxi- mated sparse operation during most of the training process, while switching back to the original sparse operation at the final stage. This switching mechanism significantly reduces the accuracy drop, at the cost of slightly less speedup. We summarize our contributions as follows: â€¢ We accelerate the training of GNNs from a new perspec- tive, namely, replacing the expensive sparse operations with their faster-approximated versions. â€¢ Instead of focusing on balancing the efficiency-accuracy trade-off at the operation level, we control the trade-off through optimizing resource allocation at the layer-wise and epoch-wise levels. â€¢ We propose a caching mechanism to reduce the cost of sampling sparse matrices by reusing previous results. â€¢ Extensive experiments have demonstrated the effective- ness of the proposed method. Particularly, RSC can achieve up to 11.6Ã— speedup for a single sparse opera- tion and a 1.6Ã— end-to-end wall-clock time speedup with negligible (â‰ˆ 0.3%) accuracy drop. 2. Background and Preliminary 2.1. Graph Neural Networks Let G = ( V, E) be an undirected graph with V = (v1, Â·Â·Â· , v|V|) and E = (e1, Â·Â·Â· , e|E|) being the set of nodes and edges, respectively. Let X âˆˆ R|V|Ã—d be the node feature matrix. A âˆˆ R|V|Ã—|V| is the graph adjacency matrix, where Ai,j = 1 if (vi, vj) âˆˆ Eelse Ai,j = 0. 2RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations ËœA = ËœDâˆ’1 2 (A + I) ËœDâˆ’1 2 is the normalized adjacency ma- trix, where ËœD is the degree matrix of A + I. GNNs re- cursively update the embedding of a node by aggregating embeddings of its neighbors. For example, the forward pass of the lth Graph Convolutional Network (GCN) layer (Kipf & Welling, 2017) can be defined as: H(l+1) = ReLU( ËœAH(l)Î˜(l)), (1) where H(l) is the node embedding matrix at the lth layer and H(0) = X. Î˜(l) is the weight matrix of the lth layer. In practice, ËœA is often stored in the sparse matrix format, e.g., compressed sparse row (CSR) (Fey & Lenssen, 2019). From the implementation aspect, the computation of Equa- tion (1) can be described as: H(l+1) = ReLU   SpMM \u0012 ËœA, MatMul(H(l), Î˜(l)) \u0013! , where SpMM(Â·, Â·) is the Sparse-Dense Matrix Multiplica- tion and MatMul(Â·, Â·) is the Dense Matrix Multiplication. Sparse operations, such as SpMM , have many random mem- ory accesses and limited data reuse. Thus they are much slower than the dense counterpart (Han et al., 2016; Duan et al., 2022b). To get a sense of the scale, we show in Figure 1 that for GCNs, SpMM may take roughly 70% âˆ¼ 90% of the total training time. 2.2. Fast Approximated MatMul with Sampling Let X âˆˆ RnÃ—m, Y âˆˆ RmÃ—q. The goal is to efficiently esti- mate the matrix production XY . Truncated Singular Value Decomposition (SVD) outputs provably optimal low-rank estimation of XY (Adelman et al., 2021). However, SVD is almost as expensive as matrix production itself. Instead, the sampling algorithm is proposed to approximate the matrix product XY by sampling k columns of X and correspond- ing rows of Y to form smaller matrices, which are then multiplied as usual (Drineas et al., 2006b). This algorithm reduces the computational complexity from O(mnq) to O(knq). Specifically, XY = mX i=1 X:,iYi,: â‰ˆ kX t=1 1 st X:,itYit,: = approx(XY ), (2) where X:,i âˆˆ RnÃ—1 and Yi,: âˆˆ R1Ã—q are the ith column and row of X and Y , respectively. In this paper, we call (X:,i, Yi,:) the ith column-row pair. k is the number of sam- ples (1 â‰¤ k â‰¤ m). {pi}m i=1 is a probability distribution over the column-row pairs. it âˆˆ {1, Â·Â·Â· m} is the index of the sampled column-row pair at the tth trial. st is the scale fac- tor. Theoretically, (Drineas et al., 2006b) shows that if we set st = 1 kpit , then we have E[approx(XY )] =XY . Fur- ther, the approximation errorE[||XY âˆ’approx(XY )||F ] is minimized when the sampling probabilities {pi}m i=1 are proportional to the product of the column-row Euclidean norms (Drineas et al., 2006b): pi = ||X:,i||2 ||Yi,:||2Pm j=1 ||X:,j||2 ||Yj,:||2 . (3) 2.2.1. T OP-k SAMPLING The above sampling-based method is originally developed for accelerating the general application ofMatMul (Drineas et al., 2006b). Directly applying it to neural networks may be sub-optimal since it does not consider the characteristic of neural network weights. Based on the empirical observation that the distribution of weights remains centered around zero during training (Glorot & Bengio, 2010; Han et al., 2015), (Adelman et al., 2021) proposes a top-k sampling algorithm: Picking k column-row pairs with the largest ||X:,i||2 ||Yi,:||2Pm j=1 ||X:,j||2 ||Yj,:||2 deterministically without scaling. Equivalently, it means pi of column-row pairs with the k- largest value in Equation (3) equals 1, otherwise it equals 0. And sit is a constant 1. Albeit without the scaling while sampling column-row pairs deterministically, under on the assumption of zero-centered weight distribution, (Adelman et al., 2021) theoretically show that top-k sampling still yields an unbiased estimation of XY with minimal approx- imation error. Consequently, the top-k sampling algorithm empirically shows a significantly lower accuracy drop when approximating the convolution and linear operations in the neural networks (Adelman et al., 2021). In the next section, we explore how to approximate the expensive sparse operation via the top-k sampling. 3. The Proposed Framework The overview of RSC is shown in Figure 2, where we use the computation graph of GCN as an example. We first explore which SpMM in the computation graph can be re- placed with its approximated version (Section 3.1). Then since GNNs have multiple SpMM and each of them may have different importance to the model performance, we then automatically allocate computation resources to dif- ferent SpMM (Section 3.2). Finally, we explore two simple and effective tricks for improvingRSC , including a caching mechanism to reduce the overhead of sampling sparse ma- trices (Section 3.3.1) and a switching mechanism to reduce the accuracy drop (Section 3.3.2). 3.1. Where to Apply the Approximation 3.1.1. E XPERIMENTAL ANALYSIS Each sparse operation is executed twice at each train- ing step, i.e., one in the forward pass and the other one 3RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations SPMM MatMul Approx SpMM MatMul Forward Pass Backward Pass Down Sampl ing Cache Caching (Sec 3.3.1) Down Sampl ing Constraint  Optimization Eq. 5 ğ‘˜!  Resource Allocation (Sec 3.2) Î˜(!) ğ‘¯(!$%) ğ›ğ‘¯(!$%) ğ›Î˜(!) ğ›ğ‘¯(!)ğ‘¯(!) ğ‘±(!) ğ›ğ‘±(!) ğ‘¨' Figure 2: Overview of RSC . For convenience, ReLU is ignored. RSC only replace the SpMM in the backward pass with its approximated version using top-k sampling (Section 3.1). kl is the number of samples for top-k sampling at the lth layer, which is automatically allocated (Section 3.2). To reduce the overhead of sampling, we also cache the sampled graph and reuse it across nearby iterations (Section 3.3). in the backward pass. As shown in Figure 2, here we take SpMM in the lth GCN layer as an example, the for- ward one is H(l+1) = ReLU(SpMM( ËœA, J(l))), where J(l) = MatMul(H(l), Î˜(l)) is the intermediate node representations. And the backward one is âˆ‡J(l) = SpMM( ËœAâŠ¤, âˆ‡H(l+1)). âˆ‡J(l) and âˆ‡H(l) are the gradient with respect to J(l) and H(l), respectively. Even though the approximation method itself is statisti- cally unbiased, replacing the exact sparse operation with their faster-approximated versions still injects noise to the computation graph. As we analyzed above, each SpMM is executed twice in the training step. Below we first exper- imentally analyze the impact of the injected noise in the forward pass and the backward pass. As shown in Table 1, we apply top-k sampling to approximate the SpMM in the forward pass, backward pass, or both, respectively. Table 1: Preliminary results on approximatingSpMM via top- k sampling. The model is a two-layer GCN, and the dataset is Reddit. Here we set thek as 0.1|V| across different layers. Method Reddit without approximation 95.39Â±0.04 only forward 16.45Â±0.39 only backward 95.25Â±0.03 forward and backward 80.74Â±1.00 From Table 1, the accuracy drop is negligible if we only replace SpMM in the backward pass. Notably, if we apply ap- proximation in both the forward and backward pass, the re- sult is significantly better than only applying top-k sampling in the forward pass. The reason is that when only apply- ing approximation in the forward pass, some row/columns are not included in the computation graph, so intuitively these row/columns should be excluded in the backward pass. â€œforward and backwardâ€ result in Table 1 is built based on this intuition such that in the backward pass, we use the column-row pairs sampled in the forward pass to compute the gradient (Adelman et al., 2021). However, it is still not comparable to the result of applying approximation only in the backward pass. Below we mathematically analyze the reason behind the results in Table 1. 3.1.2. T HEORETICAL ANALYSIS We first analyze the case of approximating the sparse opera- tions in the forward pass. Namely, replacingSpMM( ËœA, J(l)) with approx( ËœAJ(l)). We note that we have E[f(x)] Ì¸= f(E[x]) for any non-linear function f(Â·), e.g., E[x2] Ì¸= E2[x]. Thus, even when the approximation method gives an unbiased estimation, i.e., E[approx( ËœAJ(l))] = ËœAJ(l), the node embeddings H(l+1) are still biased since the acti- vation function is non-linear. To see this, E[H(l+1)] =E[ReLU(approx( ËœAJ(l))]) Ì¸= ReLU(E[approx( ËœAJ(l))]) =H(l+1). Thus, if we apply the approximation for the SpMM in the forward pass, the bias will be propagated layer-by-layer and cause significantly worse results. For the case of only approximating the sparse operation in the backward pass, we have the following proposition: Proposition 3.1 (Proof in Appendix A). If the approxima- tion method is itself unbiased, and we only replace theSpMM in the backward pass with its approximated version, while leaving the forward one unchanged, then the calculated gradient is provably unbiased. The high-level idea is that the gradient of the activation function in the backward pass is only related to the pre- activations in the forward pass, and thus is independent of the approximation error introduced in the backward pass. Due to the page limit, we also discuss why sampling-based approximation is suitable for accelerating GNNs in Ap- pendix A. As suggested by our theoretical and empirical analysis, as shown in Figure 2, we only approximate the sparse operations in the backward pass, while leaving all other operations unchanged. 3.2. How to Apply the Approximation As we mentioned, for sparse operations, the acceleration is decided by the selection of sampled column-row pairs. To see this, as shown in Figure 3, suppose we use top- k sampling to approximate SpMM( ËœAâŠ¤, âˆ‡H). Since the computations are only executed on the non-zero entries, so selecting the orange pairs (i.e., pair 1 and 3) will result in 3 7 Ã— less computational cost (FLOPs) compared to selecting 4RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations 1 11 111111132100123 3210âˆ‡ğ»!!âˆ‡ğ»!\"âˆ‡ğ»!#âˆ‡ğ»\"! âˆ‡ğ»#!âˆ‡ğ»$! âˆ‡ğ»\"\" âˆ‡ğ»#\"âˆ‡ğ»$\" âˆ‡ğ»\"# âˆ‡ğ»##âˆ‡ğ»$# Nodeembeddinggradientsâˆ‡ğ»âˆˆâ„!Ã—#,withğ‘‘=3Sparseadjacencymatrixğ´$âˆˆâ„!Ã—!,withğ‘=4 Ã— Figure 3: For approximated sparse operations, the accelera- tion is decided by the selection of column-row pairs. the blue pair (i.e., pair 0 and 2). For both the orange and blue cases, we have k = 2. Thus, the number of samples k cannot directly constrain the FLOPs for each individual operation. Moreover, a GNN has multiple operations (or layers), and the model accuracy has a different sensitivity to the approximation error at different layers. To optimize the accuracy-efficiency trade-off, our key idea is to customize the computation resources (i.e., FLOPs) for each layer by adjusting the number of samples kl in the l-th layer. In this way, we minimize the impact of approximation, while limiting the overall FLOPs under a certain budget. Based on the idea, we frame the resource allocation problem as the following constrained optimization problem: min {kl} âˆ’ LX l=1 X iâˆˆTopkl âˆ¥ ËœAâŠ¤ :,iâˆ¥2âˆ¥âˆ‡H(l+1) i,: âˆ¥2 âˆ¥ ËœAâˆ¥F âˆ¥âˆ‡H(l+1)âˆ¥F , (4a) s.t. LX l=1 X iâˆˆTopkl #nnzi âˆ— dl â‰¤ C LX i=1 |E|dl, (4b) where C is the budget (0 < C <1) that controls the overall reduced FLOPs. kl is the number of samples for the top-k sampling at the l-th layer. dl is the hidden dimensions ofl-th layer, and #nnzi is the number of non-zero entries at the i-th column of ËœAâŠ¤. Topkl is the set of indices associated with the kl largest âˆ¥ ËœAâŠ¤ :,iâˆ¥2âˆ¥âˆ‡H(l+1) i,: âˆ¥2. Equation (4a) is equivalent to minimizing the relative ap- proximation error E[|| ËœAâŠ¤âˆ‡H(l+1)âˆ’approx( ËœAâŠ¤âˆ‡H(l+1))||F âˆ¥ ËœAâˆ¥F âˆ¥âˆ‡H(l+1)||F ] summarized over all layers (Adelman et al., 2021). Also, different sparse operations are weighted summation by the magnitude of gradient âˆ¥âˆ‡H(l+1)âˆ¥2, which implicitly en- codes the importance of different operations. Equation (4b) is the constraint that controls the overall FLOPs. Specifically, the FLOPs of SpMM between ËœA and the gradient âˆ‡H âˆˆ RNÃ—d is O(|E|d) and P jâˆˆV #nnzj = |E|. We note that Equation (4b) also bounds the number of memory access of SpMM . 3.2.1. GREEDY SOLUTION The above combination optimization objective is NP-hard, albeit it can be solved by dynamic programming. However, dynamic programming is very slow, which somehow con- tradicts our purpose of being efficient. Thus, we propose to use a greedy algorithm to solve it. Specifically, it starts with the highest kl = |V| for all layers. In each move, it chooses a kl among {kl}L l=1 to reduce by a step size (e.g., 0.02|V|), such that the increment of errors in Equation (4a) is mini- mal. The greedy algorithm will stop when the current total FLOPs fits in the budget in Equation (4b). This algorithm runs super fast, and we found that it has minimal impact on efficiency. We provide the pseudo-code of our greedy algorithm in Algorithm 1 of Appendix B. 3.3. When to Apply the Approximation 3.3.1. CACHE THE SAMPLED SPARSE MATRICES We first give the details about the Compressed Sparse Row (CSR) format for representing the sparse matrix here. CSR stores nonzero values in a matrix and their position in three arrays: index array Rowptr, column array Col, and value array Val. The elements in Rowptr act as the starting indices of the elements in Col and Val that correspond to each row. Specifically, the elements of row i are stored in indices Rowptr[i] to Rowptr[i+ 1] âˆ’ 1 of Col and Val . The elements in Col and Val are the column index and value in that column, respectively. Figure 5 shows the CSR format of the matrix shown in Figure 3. We ignore the Val array here for illustration convenience. Executing the top- k sampling contains two steps: First, it decides the indices corresponding to the top- k largest column row norms in Equation (3). Second, slicing the matrices according to the indices. In practice, the overhead of the first step can be ignored. However, unlike dense matrices, slicing the adjacency matrix is much slower due to its irregular data format. To see this, suppose the top-k indices of the sparse matrix in Figure 3 correspond to the orange column-row pairs. Figure 5 shows the process of slicing the adjacency matrix in CSR format by reserving only the orange columns. Slicing sparse matrices requires to re-process the graph to build the new Rowptr and Col (Fey & Lenssen, 2019), which introduces significant time overhead, especially for large graphs. For the full graph training, we use the same adjacency matrix across different epochs1. We made a crucial observation that the top-k indices in the adjacency matrix tend to be the same across iterations. In Figure 4, we plot the AUC score of top- k indices between every iteration t and iteration t + 10for 1For sub-graph based training, we can first sample all of the sub-graphs offline. Then during the training, we apply the caching mechanism to each sampled graph. 5RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations 0 50 100 150 200 250 300 350 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Matching score Reddit GCN 1st layer GCN 2nd layer GraphSAGE 1st layer GraphSAGE 2nd layer 0 50 100 150 200 250 300 350 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Matching score Yelp GCN 1st layer GCN 2nd layer GraphSAGE 1st layer GraphSAGE 2nd layer 0 50 100 150 200 250 300 350 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Matching score ogbn-proteins GCN 1st layer GCN 2nd layer GraphSAGE 1st layer GraphSAGE 2nd layer Figure 4: For each layer, the selected column-row pairs tend to be very similar across iterations. Models here are two-layer GCN and GraphSAGE. Here we show the matching scores (AUC) of top-k indices between every 10 steps. Figure 5: The process of slicing the sparse matrix in Figure 3 by only reserving orange columns (in CSR format). each layer throughout the whole training process. Here we note that AUC score is a commonly used ranking measure and a 1.0 AUC score means the ranking of column-row pairs is identical across iterations. The results in Figure 4 indicate that the top-k indices wonâ€™t change significantly within a few iterations. Thus, as shown in Figure 2, we propose to reuse the sampled adjacency matrix for each layer across nearby iterations. Discussion. The rationale behind the success of caching is the slow rate of change in the learned embeddings within GNNs (Fey et al., 2021; Wan et al., 2022a). Prior research has leveraged this â€œstalenessâ€ of embeddings to enhance the efficiency of GNN training [1, 2]. The success of caching can also be explained by the staleness: if embeddings (and their gradients) across consecutive steps remain nearly iden- tical, the sampled sparse matrix will also exhibit minimal variation. Later we experimentally show that the caching mechanism does not impact the model performance a lot, but leads to a significant speedup. 3.3.2. SWITCH BACK AT THE END When training neural networks, the common practice is to use a large learning rate for exploration and anneal to a small one for final convergence (Li et al., 2019). The ratio- nale behind this strategy is that, at the end of the training process, we need to fine-tune our model with small noise for convergence. Since our approximation sparse operations will bring extra noise to the gradient, intuitively, we can switch back to the original sparse operations to help con- vergence. More formally, we propose to use approximated sparse operation during most of the training process, while switching back to the original sparse operation at the final stage. We experimentally show that this switching mecha- nism significantly reduces the accuracy drop at the cost of slightly less acceleration effect. We note that the switching mechanism is not proposed in this paper. The switching mechanism takes inspiration from previous work Dao et al. (2022), and both our work and Dao et al. (2022) utilize the switching mechanism to minimize the impact of approximation. 4. Related work and Discussion Due to the page limit, we first discuss the related work on approximated matrix multiplication. Other related topics, i.e., subgraph-based training, randomized GNN training, and non-approximated GNN acceleration, can be found in Appendix C. Approximated Matrix Multiplication.The approximated matrix production can be roughly divided into three cat- egories. However, only a few of them can be used for accelerating GNN training. Specifically, (1) Random walk- based methods (Cohen & Lewis, 1999) performs random walks on a graph representation of the dense matrices, but is only applicable to non-negative matrices; (2) Butterfly- based methods (Chen et al., 2021; Dao et al., 2022) replace dense matrices with butterfly matrices. It is not applicable to SpMM in GNNs because the adjacency matrix often cannot be reduced to a butterfly matrix. (3) Column-row sampling methods(Drineas et al., 2006a; Drineas & Kannan, 2001) sample the input matrices with important rows and columns, then perform the production on the sampled matrix as usual. 5. Limitations First, to guarantee the model accuracy, we only replace the sparse operation in the backward pass. Thus the upper bound of RSC â€™s speedup is limited. However, we note that the backward pass usually is more time-consuming than the forward pass, which is also empirically shown in Table 2. Second, some GNNs rely on the scatter-and-gather instead of SpMM (and its variant) to perform the aggregation, such as GAT (VeliË‡ckoviÂ´c et al., 2017). They are not covered in this paper. However, scatter-and-gather based GNNs can also 6RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations be accelerated by RSC because the column-row sampling is also applicable to scatter and gather operation. Similarly, the caching and switching mechanisms are also applicable to them. However, for the resource allocation Algorithm 1, the scatter and gather operations require tailored error bound and the computation cost modeling in Equation (4). We leave it as future work. 6. Experiments We verify the effectiveness of our proposed framework via answering the following research questions: Q1: How ef- fective is RSC in terms of accuracy with reduced training time? Q2: How effective is our proposed allocation strategy compared to the uniform allocation strategy? Q3: What is the layer-wise ratio assigned by RSC ? Q4: How effec- tive is the caching and switching mechanism in terms of the trade-off between efficiency and accuracy? If without explicitly mentioned, all reported results are averaged over ten random trials 6.1. Experimental Settings Datasets and Baselines. To evaluateRSC , we adopt four common large-scale graph benchmarks from different do- mains, i.e., Reddit (Hamilton et al., 2017), Yelp (Zeng et al., 2020), ogbn-proteins (Hu et al., 2020), and ogbn- products (Hu et al., 2020). We evaluate RSC under both the mini-batch training and full-batch training settings. For the mini-batch training setting, we integrate RSC with one of the state-of-the-art sampling methods, GraphSAINT (Zeng et al., 2020). For the full-batch training setting, we inte- grate RSC with three popular models: two commonly used shallow models, namely, GCN (Kipf & Welling, 2017) and GraphSAGE (Hamilton et al., 2017), and one deep model GCNII (Chen et al., 2020). To avoid creating confusion, GCN, GraphSAGE, and GCNII are all trained with the whole graph at each step. For a fair comparison, we use the MEAN aggregator for GraphSAGE and GraphSAINT throughout the paper. Details about the hyperparameters and datasets are in Appendix D. Hyperparameter settings. RSC contains three parts. First, the allocation strategy. We choose the overall budget C in Equation (4b) from {0.1, 0.3, 0.5}. We run the resource allocation strategy every ten steps. The step size Î± in Algo- rithm 1 is set as 0.02|V|. Second, the caching mechanism. According to Figure 4, we sample the adjacency matrix every ten steps and reuse the sampled matrices for nearby steps. Third, the switching mechanism, where we apply RSC for 80% of the total epochs, while switching back to the original operations for the rest of the 20% epochs. Due to the page limit, We present a detailed hyperparameter study in Appendix E Figure 11 and Figure 12. Evaluation metrics. To evaluate the practical usage of RSC , we report the wall clock time speedup measured on GPUs. Specifically, the speedup equalsTbaseline/Trsc, where Tbaseline and Trsc are the wall clock training time of baseline and RSC , respectively. We note that the Trsc includes the running time of the greedy algorithm, and the effects of caching and switching. 6.2. Performance Analysis 6.2.1. A CCURACY -EFFICIENCY TRADE -OFF To answer Q1, we summarize the speedup and the test accuracy/F1-micro/AUC of different methods in Table 3. Since RSC accelerates the sparse operation in the backward pass, we also provide the detailed efficiency analysis in Table 2. In summary, we observe: â¶ At the operation level, RSC can accelerate the sparse operation in the backward pass by up to 11.6Ã—. For end- to-end training, the accuracy drop of applying RSC over baselines is negligible (0.3%) across different models and datasets, while achieving up to 1.6Ã— end-to-end wall clock time speedup. The gap between the operation speedup and the end-to-end speedup is due to the following two reasons. First, we focus on accelerating the sparse computations in GNNs, which is the unique bottleneck to GNNs. The other dense computations can certainly be accelerated by approximation methods, but this is beyond the scope of this paper. Second, we only accelerate the sparse computation in the backward pass instead of the forward one to guaran- tee performance. We note that for approximation methods that accelerate the training process at operation level, a 1.2 â‰ˆ 1.3Ã— wall-clock speedup with negligible accuracy drop can be regarded as non-trivial (for details, please see Table 1 in (Adelman et al., 2021)), especially considering that these approximation methods are orthogonal to most of the existing efficient training methods. For GraphSAINT, the speedup of RSC is around 1.1Ã—, which is smaller than the full graph training. This is because for subgraph-based training, the equivalent â€œbatch sizeâ€ is much smaller than the full graph counterparts. As a result, the GPU utility is low since it does not assign each processor a sufficient amount of work and the bottleneck is the mini-batch transfer time (Kaler et al., 2022). We note that the mini-batch sampling and transfer time can be optimized from the system perspec- tive (Kaler et al., 2022), which is orthogonal to our work. The speedup is expected to be larger when the mini-batch sampling time is optimized. 6.2.2. A BLATION ON RESOURCE ALLOCATION . Due to the page limit, we first show the running time of the greedy algorithm in Appendix E Table 11. We conclude that the overhead of the greedy algorithm is negligible com- pared to the acceleration effect of RSC . To answer Q2, we 7RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Table 2: Comparison on the efficiency at the operation level. fwd/bwd is the wall-clock time for a single forward/backward pass (ms). SpMM MEAN corresponds to the MEAN aggregator used in GraphSAGE (Appendix A.3). Reddit Yelp ogbn- proteins ogbn- products fwd bwd fwd bwd fwd bwd fwd bwd SpMM Baseline 36.28 44.23 26.88 34.38 31.72 42.99 261.03 316.80 +RSC - 3.81 (11.6 Ã—) - 9.86 (3.49 Ã—) - 14.87 (2.89 Ã—) - 35.28 (8.98 Ã—) SpMMMEAN (Appendix A.3) Baseline 36.21 44.27 26.78 34.38 31.80 43.11 261.03 316.84 +RSC - 7.47 (5.92 Ã—) - 19.62 (1.75 Ã—) - 5.22 (8.26 Ã—) - 71.59 (4.43 Ã—) Table 3: Comparison on the test accuracy/F1-micro/AUC and speedup on four datasets. Bold faces indicate the accuracy drop is negligible (â‰ˆ 0.3%) or the result is better compared to the baseline.The hardware here is a RTX3090 (24GB). # nodes # edges 230K 11.6M 717K 7.9M 132K 39.5M 2.4M 61.9M Model Methods Reddit Yelp ogbn- proteins ogbn- products Acc. Budget C Speedup F1-microBudget C Speedup AUC Budget C Speedup Acc. Budget C Speedup Graph- SAINT Baseline 96.40Â±0.03 1 1 Ã— 63.30Â±0.14 1 1 Ã— â€” â€” â€” 79.01Â±0.21 1 1 Ã— +RSC 96.24Â±0.030.1 1.11 Ã— 63.34Â±0.180.1 1.09 Ã— â€” â€” â€” 78.99Â±0.32 0.3 1.04 Ã— GCN Baseline 95.33Â±0.03 1 1 Ã— 44.28Â±1.04 1 1 Ã— 71.99Â±0.66 1 1 Ã— 75.74Â±0.11 1 1 Ã— +RSC 95.13Â±0.050.1 1.47 Ã— 46.09Â±0.540.1 1.17 Ã— 71.60Â±0.450.3 1.51 Ã— 75.44Â±0.21 0.3 1.35 Ã— GraphSAGE (full batch) Baseline 96.61Â±0.05 1 1 Ã— 63.06Â±0.18 1 1 Ã— 76.09Â±0.77 1 1 Ã— 78.73 Â± 0.12 1 1 Ã— +RSC 96.52Â±0.040.1 1.32 Ã— 62.89Â±0.190.1 1.13 Ã— 76.30Â±0.420.3 1.60 Ã— 78.50Â± 0.090.1 1.53 Ã— GCNII Baseline 96.71Â±0.07 1 1 Ã— 63.45Â±0.17 1 1 Ã— 73.79Â±1.32 1 1 Ã— â€” â€” â€” +RSC 96.50Â±0.120.3 1.45 Ã— 63.57Â±0.210.1 1.19 Ã— 75.20Â±0.540.5 1.41 Ã— â€” â€” â€” /uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000016/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000018 /uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000058/uni00000053 /uni0000001a/uni00000018 /uni0000001b/uni00000013 /uni0000001b/uni00000018 /uni0000001c/uni00000013 /uni0000001c/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000026/uni00000031/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000024/uni0000004f/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000036/uni00000026 /uni00000014/uni00000011/uni00000014/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000015/uni00000013/uni00000013/uni00000014/uni00000011/uni00000015/uni00000015/uni00000018/uni00000014/uni00000011/uni00000015/uni00000018/uni00000013/uni00000014/uni00000011/uni00000015/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000016/uni00000013/uni00000013/uni00000014/uni00000011/uni00000016/uni00000015/uni00000018 /uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000058/uni00000053 /uni0000001c/uni00000019/uni00000011/uni00000018/uni0000001b /uni0000001c/uni00000019/uni00000011/uni00000019/uni00000013 /uni0000001c/uni00000019/uni00000011/uni00000019/uni00000015 /uni0000001c/uni00000019/uni00000011/uni00000019/uni00000017/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000024/uni0000004f/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000036/uni00000026 /uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000016/uni00000014/uni00000011/uni00000017 /uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000058/uni00000053 /uni0000001c/uni00000018/uni00000011/uni00000018 /uni0000001c/uni00000019/uni00000011/uni00000013 /uni0000001c/uni00000019/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000024/uni0000004f/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000036/uni00000026 Figure 6: The Pareto frontier of the accuracy-efficiency trade-off for RSC and the uniform allocation. Here we disabled the caching and switch mechanism for a fair comparison. More results can be found in Appendix E Table 4: Ablation on the caching and switching mechanism. Experiments are conducted on ogbn-proteins. All results are averaged over five random trials. Ablation on Caching Switching AUC Speedup GCN âœ— âœ— 71.60 Â± 0.66 1.19 Ã— âœ— âœ“ 72.19 Â± 0.79 1.14 Ã— âœ“ âœ— 69.80 Â± 0.60 1.60 Ã— âœ“ âœ“ 71.60 Â± 0.45 1.51 Ã— GraphSAGE âœ— âœ— 75.23 Â± 0.79 1.37 Ã— âœ— âœ“ 76.39 Â± 0.39 1.32 Ã— âœ“ âœ— 75.53 Â± 0.60 1.78 Ã— âœ“ âœ“ 76.30 Â± 0.42 1.60 Ã— GCNII âœ— âœ— 74.07 Â± 0.83 1.10 Ã— âœ— âœ“ 74.50 Â± 0.52 1.04 Ã— âœ“ âœ— 72.47 Â± 0.75 1.46 Ã— âœ“ âœ“ 75.20 Â± 0.54 1.41 Ã— compare RSC with the uniform allocation strategy, i.e., set- ting kl = C|V| for all sparse operations in the backward pass. As shown in Figure 6, we plot the Pareto frontier of the accuracy-efficiency trade-off on the Reddit dataset for RSC and the uniform strategy with different C. For a fair comparison, we disabled the caching and switching mechanism. Due to page limit, more results are shown in Appendix E. We observe that: â· RSC exhibits a supe- rior trade-off between accuracy and efficiency compared to the uniform allocation, especially under high speedup regime. Namely, compared to the uniform allocation, RSC can achieve higher model accuracy under the same speedup. This can be explained by the fact that each operation has a different importance to the model performance. RSC can au- tomatically allocate more resources to important operations under a given total budget. To answer Q3, due to the page limit, we visualize the al- located kl for each layer across iterations in Appendix E Figure 7, and the degree of picked nodes in Appendix E Figure 8. We observe: â¸ The kl assigned by RSC evolves along with the training. 6.2.3. A BLATION ON CACHING AND SWITCHING . In section 6.2.2, we have shown the superior results of the proposed resource allocation strategy. As we mentioned in Section 3.3, we also introduce two simple tricks to for improving RSC , i.e., the caching and switching mechanism. To verify the effect of each of them (Q4), we conduct incre- mental evaluations on GCN, GraphSAGE and GCNII with 8RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations ogbn-proteins, which are summarized in Table 4. The row without caching and switching in Table 4 corresponds to the results with the proposed resource allocation strategy. We observe: â¹ Switching mechanism significantly improves the model performance, at the cost of slightly less acceleration effect. As we analyzed in Section 3.3.2, the improvement can be explained by the fact that the final training stage requires smaller gradient noise to help convergence. âº Caching mechanism significantly improves the wall-clock time speedup, at the cost of worse model performance. Al- though caching mechanism can reduce the overhead of sam- pling, the performance drop is too large (> 1%). Intuitively, the accuracy drop of caching also implies that we could not use a â€œstaticâ€ down-sampled graph throughout the training process. â» Surprisingly, jointly applying the caching and switching, the performance drop can be minimized. 7. Acknowledgements The authors thank the anonymous reviewers for their helpful comments. The work is in part supported by NSF grants NSF IIS-2224843. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. 8. Conclusions and Future work We propose RSC , which replaces the sparse computations in GNNs with their fast approximated versions. RSC can be plugged into most of the existing training frameworks to improve their efficiency. Future work includes exploring RSC for GNNs that rely on scatter-and-gather operations. References Adelman, M., Levy, K., Hakimi, I., and Silberstein, M. Faster neural network training with approximate tensor operations. Advances in Neural Information Processing Systems, 34:27877â€“27889, 2021. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-y., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, pp. 1204â€“1215. PMLR, 2021. Chen, B., Dao, T., Liang, K., Yang, J., Song, Z., Rudra, A., and Re, C. Pixelated butterfly: Simple and efficient sparse training for neural network models. arXiv preprint arXiv:2112.00029, 2021. Chen, J., Zhu, J., and Song, L. Stochastic training of graph convolutional networks with variance reduction. In Inter- national conference on machine learning. PMLR, 2017. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Sim- ple and deep graph convolutional networks. In Interna- tional Conference on Machine Learning, pp. 1725â€“1735. PMLR, 2020. Chiang, W.-L., Liu, X., Si, S., Li, Y ., Bengio, S., and Hsieh, C.-J. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceed- ings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 257â€“266, 2019. Cohen, E. and Lewis, D. D. Approximating matrix multi- plication for pattern recognition tasks. Journal of Algo- rithms, 30(2):211â€“252, 1999. Cong, W., Forsati, R., Kandemir, M., and Mahdavi, M. Minimal variance sampling with provable guarantees for fast training of graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pp. 1393â€“1403, 2020. Dao, T., Chen, B., Sohoni, N. S., Desai, A., Poli, M., Gro- gan, J., Liu, A., Rao, A., Rudra, A., and RÂ´e, C. Monarch: Expressive structured matrices for efficient and accurate training. In International Conference on Machine Learn- ing, pp. 4690â€“4721. PMLR, 2022. Drineas, P. and Kannan, R. Fast monte-carlo algorithms for approximate matrix multiplication. In Proceedings 42nd IEEE Symposium on Foundations of Computer Science, pp. 452â€“459. IEEE, 2001. Drineas, P., Kannan, R., and Mahoney, M. W. Fast monte carlo algorithms for matrices i: Approximating matrix multiplication. SIAM Journal on Computing, 36(1):132â€“ 157, 2006a. Drineas, P., Kannan, R., and Mahoney, M. W. Fast monte carlo algorithms for matrices i: Approximating matrix multiplication. SIAM Journal on Computing, 36(1):132â€“ 157, 2006b. Duan, K., Liu, Z., Wang, P., Zheng, W., Zhou, K., Chen, T., Hu, X., and Wang, Z. A comprehensive study on large-scale graph training: Benchmarking and rethink- ing. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2022a. URL https://openreview.net/forum? id=2QrFr_U782Z. Duan, K., Liu, Z., Wang, P., Zheng, W., Zhou, K., Chen, T., Hu, X., and Wang, Z. A comprehensive study on large-scale graph training: Benchmarking and rethinking. 2022b. Feng, W., Zhang, J., Dong, Y ., Han, Y ., Luan, H., Xu, Q., Yang, Q., Kharlamov, E., and Tang, J. Graph random neural networks for semi-supervised learning on graphs. 9RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Advances in neural information processing systems, 33: 22092â€“22103, 2020. Feng, W., Dong, Y ., Huang, T., Yin, Z., Cheng, X., Khar- lamov, E., and Tang, J. Grand+: Scalable graph random neural networks. In Proceedings of the ACM Web Confer- ence 2022, pp. 3248â€“3258, 2022. Fey, M. and Lenssen, J. E. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Repre- sentation Learning on Graphs and Manifolds, 2019. Fey, M., Lenssen, J. E., Weichert, F., and Leskovec, J. Gn- nautoscale: Scalable and expressive graph neural net- works via historical embeddings. In International confer- ence on machine learning, 2021. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Pro- ceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249â€“256. JMLR Workshop and Conference Proceedings, 2010. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. InProceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025â€“1035, 2017. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., and Dally, W. J. Eie: Efficient inference engine on compressed deep neural network. ACM SIGARCH Computer Architecture News, 44(3):243â€“254, 2016. Han, X., Jiang, Z., Liu, N., and Hu, X. G-mixup: Graph data augmentation for graph classification. arXiv preprint arXiv:2202.07179, 2022. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. Huang, G., Dai, G., Wang, Y ., and Yang, H. Ge-spmm: General-purpose sparse matrix-matrix multiplication on gpus for graph neural networks. In SC20: International Conference for High Performance Computing, Network- ing, Storage and Analysis, pp. 1â€“12. IEEE, 2020a. Huang, Q., He, H., Singh, A., Lim, S.-N., and Benson, A. R. Combining label propagation and simple models out-performs graph neural networks. In International Conference on Learning Representations, 2020b. Huang, W., Zhang, T., Rong, Y ., and Huang, J. Adap- tive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, 2018. Jiang, Z., Han, X., Fan, C., Liu, Z., Zou, N., Mostafavi, A., and Hu, X. Fmp: Toward fair graph message passing against topology bias. arXiv preprint arXiv:2202.04187, 2022. Jin, W., Ma, Y ., Liu, X., Tang, X., Wang, S., and Tang, J. Graph structure learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 66â€“74, 2020. Kaler, T., Stathas, N., Ouyang, A., Iliopoulos, A.-S., Schardl, T., Leiserson, C. E., and Chen, J. Accelerating training and inference of graph neural networks with fast sampling and pipelining. Proceedings of Machine Learning and Systems, 4:172â€“189, 2022. Kipf, T. N. and Welling, M. Semi-supervised classi- fication with graph convolutional networks. In In- ternational Conference on Learning Representations , 2017. URL https://openreview.net/forum? id=SJU4ayYgl. Klicpera, J., Bojchevski, A., and G Â¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2018. Li, Y ., Wei, C., and Ma, T. Towards explaining the regu- larization effect of initial large learning rate in training neural networks. Advances in Neural Information Pro- cessing Systems, 32, 2019. Liu, Z., Jin, H., Wang, T.-H., Zhou, K., and Hu, X. Di- vaug: Plug-in automated data augmentation with explicit diversity maximization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4762â€“ 4770, 2021. Martinsson, P.-G. and Tropp, J. Randomized numerical linear algebra: foundations & algorithms (2020). arXiv preprint arXiv:2002.01387, 2020. Md, V ., Misra, S., Ma, G., Mohanty, R., Georganas, E., Heinecke, A., Kalamkar, D., Ahmed, N. K., and Avancha, S. Distgnn: Scalable distributed training for large-scale graph neural networks. In Proceedings of the Interna- tional Conference for High Performance Computing, Net- working, Storage and Analysis, pp. 1â€“14, 2021. Narayanan, S. D., Sinha, A., Jain, P., Kar, P., and SEL- LAMANICKAM, S. Iglu: Efficient GCN training via lazy updates. In International Conference on Learning 10RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Representations, 2022. URL https://openreview. net/forum?id=5kq11Tl1z4. Qiu, J., Dhulipala, L., Tang, J., Peng, R., and Wang, C. Lightne: A lightweight graph processing system for net- work embedding. In Proceedings of the 2021 interna- tional conference on management of data, pp. 2281â€“2289, 2021. Rahman, M. K., Sujon, M. H., and Azad, A. Fusedmm: A unified sddmm-spmm kernel for graph embedding and graph neural networks. In 2021 IEEE International Par- allel and Distributed Processing Symposium (IPDPS), pp. 256â€“266. IEEE, 2021. Ramezani, M., Cong, W., Mahdavi, M., Kandemir, M., and Sivasubramaniam, A. Learn locally, correct globally: A distributed algorithm for training graph neural networks. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=FndDxSz3LxQ. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: Towards deep graph convolutional networks on node clas- sification. arXiv preprint arXiv:1907.10903, 2019. Savas, B. and Dhillon, I. S. Clustered low rank approxi- mation of graphs in information science applications. In Proceedings of the 2011 SIAM International Conference on Data Mining, pp. 164â€“175. SIAM, 2011. VeliË‡ckoviÂ´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations, 2017. Wan, C., Li, Y ., Kim, N. S., and Lin, Y . {BDS}- {gcn}: Efficient full-graph training of graph convolu- tional nets with partition-parallelism and boundary sam- pling, 2021. URL https://openreview.net/ forum?id=uFA24r7v4wL. Wan, C., Li, Y ., Li, A., Kim, N. S., and Lin, Y . Bns-gcn: Efficient full-graph training of graph convolutional net- works with partition-parallelism and random boundary node sampling. Proceedings of Machine Learning and Systems, 4:673â€“693, 2022a. Wan, C., Li, Y ., Wolfe, C. R., Kyrillidis, A., Kim, N. S., and Lin, Y . Pipegcn: Efficient full-graph training of graph convolutional networks with pipelined feature communi- cation. arXiv preprint arXiv:2203.10428, 2022b. Wang, M., Zheng, D., Ye, Z., Gan, Q., Li, M., Song, X., Zhou, J., Ma, C., Yu, L., Gai, Y ., Xiao, T., He, T., Karypis, G., Li, J., and Zhang, Z. Deep graph library: A graph- centric, highly-performant package for graph neural net- works. arXiv preprint arXiv:1909.01315, 2019. Wang, Y ., Feng, B., and Ding, Y . Tc-gnn: Accelerating sparse graph neural network computation via dense tensor core on gpus. arXiv preprint arXiv:2112.02052, 2021. Wang, Z., Wu, X. C., Xu, Z., and Ng, T. E. Cupcake: Acom- pression optimizer for scalable communication-efficient distributed training. Wang, Z., Xu, Z., Wu, X., Shrivastava, A., and Ng, T. E. Dragonn: Distributed randomized approximate gradients of neural networks. In International Conference on Ma- chine Learning, pp. 23274â€“23291. PMLR, 2022. Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Wein- berger, K. Simplifying graph convolutional networks. In International conference on machine learning, pp. 6861â€“ 6871. PMLR, 2019. Xu, K., Zhang, M., Jegelka, S., and Kawaguchi, K. Op- timization of graph neural networks: Implicit acceler- ation by skip connections and more depth. In Inter- national Conference on Machine Learning, pp. 11592â€“ 11602. PMLR, 2021. Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W. L., and Leskovec, J. Graph convolutional neural net- works for web-scale recommender systems. In Proceed- ings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974â€“983, 2018. Yu, L., Shen, J., Li, J., and Lerer, A. Scalable graph neu- ral networks for heterogeneous graphs. arXiv preprint arXiv:2011.09679, 2020. Yuan, B., Wolfe, C. R., Dun, C., Tang, Y ., Kyril- lidis, A., and Jermaine, C. Distributed learning of fully connected neural networks using independent sub- net training. Proc. VLDB Endow. , 15(8):1581â€“1590, 2022. URL https://www.vldb.org/pvldb/ vol15/p1581-wolfe.pdf. Zeng, H., Zhou, H., Srivastava, A., Kannan, R., and Prasanna, V . Graphsaint: Graph sampling based in- ductive learning method. In International Conference on Learning Representations, 2020. URL https:// openreview.net/forum?id=BJe8pkHFwS. Zha, D., Feng, L., Tan, Q., Liu, Z., Lai, K.-H., Bhushanam, B., Tian, Y ., Kejariwal, A., and Hu, X. Dreamshard: Gen- eralizable embedding table placement for recommender systems. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Pro- cessing Systems, 2022. URL https://openreview. net/forum?id=_atSgd9Np52. 11RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Zha, D., Feng, L., Luo, L., Bhushanam, B., Liu, Z., Hu, Y ., Nie, J., Huang, Y ., Tian, Y ., Kejariwal, A., and Hu, X. Pre- train and search: Efficient embedding table sharding with pre-trained neural cost models. CoRR, abs/2305.01868, 2023. doi: 10.48550/arXiv.2305.01868. URL https: //doi.org/10.48550/arXiv.2305.01868. Zhang, H., Yu, Z., Dai, G., Huang, G., Ding, Y ., Xie, Y ., and Wang, Y . Understanding gnn computational graph: A coordinated computation, io, and memory perspective. Proceedings of Machine Learning and Systems, 4:467â€“ 484, 2022. Zheng, D., Ma, C., Wang, M., Zhou, J., Su, Q., Song, X., Gan, Q., Zhang, Z., and Karypis, G. Distdgl: distributed graph neural network training for billion-scale graphs. In 2020 IEEE/ACM 10th Workshop on Irregular Appli- cations: Architectures and Algorithms (IA3), pp. 36â€“44. IEEE, 2020. Zhong, S., Zhang, G., Huang, N., and Xu, S. Revisit kernel pruning with lottery regulated grouped convolutions. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=LdEhiMG9WLO. Zhou, K., Liu, Z., Chen, R., Li, L., Choi, S., and Hu, X. Table2graph: Transforming tabular data to unified weighted graph. In Raedt, L. D. (ed.), Proceedings of the Thirty-First International Joint Conference on Ar- tificial Intelligence, IJCAI 2022, Vienna, Austria, 23- 29 July 2022 , pp. 2420â€“2426. ijcai.org, 2022. doi: 10.24963/ijcai.2022/336. URL https://doi.org/ 10.24963/ijcai.2022/336. Zhou, K., Choi, S.-H., Liu, Z., Liu, N., Yang, F., Chen, R., Li, L., and Hu, X. Adaptive label smoothing to regularize large-scale graph training. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM), pp. 55â€“63. SIAM, 2023. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. arXiv preprint arXiv:1911.07323, 2019. 12RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations A. Mathematical Analysis A.1. Why Sampling-based Approximation for GNN? In the main text, we mentioned SpMM is the main speed bottleneck for GNNs. Below we illustrate why the column- row sampling is suitable for accelerating SpMM in GNNs, from the approximation error perspective. Here we analyze ËœAJ(l) = SpMM( ËœA, J(l)) for illustration convenience. For the backward pass of SpMM , the analysis is similar, except that we are approximating âˆ‡J(l) = SpMM( ËœAâŠ¤, âˆ‡H(l+1)). Column-row sampling approximates the matrix production by excluding some â€œunimportantâ€ columns and rows in the original matrix. So intuitively, the approximation error E[|| ËœAJ(l) âˆ’ approx( ËœAJ(l))||F ] is low if the â€œunimportantâ€ columns/rows are correlated in the selected one. Namely, ËœA and J(l) are low-rank. Formally, we have the following theorem: Theorem A.1 ((Martinsson & Tropp, 2020)) . Suppose we approximate ËœAJ(l) using column-row sampling, and pi is obtained by Equation (3). Then for any positive number Ïµ, if the number of samples k satisfies k â‰¥ Ïµâˆ’2(srank( ËœA) + srank(J(l))) log(|V| + d), we have E[|| ËœAJ(l) âˆ’ approx( ËœAJ(l))||F ] â‰¤ 2Ïµ, where srank in Theorem A.1 is called the stable rank, which is the continuous surrogate measure for the rank that is largely unaffected by tiny singular values. Formally for any matrix Y , srank(Y ) =||Y ||2 F ||Y ||2 â‰¤ rank(Y ). Fortunately, most real-world graphs are cluster-structured, which means the adjacency matrix ËœA is low-rank (Qiu et al., 2021; Savas & Dhillon, 2011). The low-rank property of real-world graphs is also wildly reported in previous work (Jin et al., 2020; Qiu et al., 2021). Moreover, the intermediate activations J(l) and the activation gradients are also low-rank, due to the aggregation. Namely, low-rank means â€œcorrelationâ€ in the row/column space. The embedding (i.e., rows in the activation matrix) of connected nodes tend to close due to the graph propagation, which resulting in the low-rank property of the activation matrix. Thus for GNNs, the approximation error is low with a relatively small number of sample k. This perspective is also experimentally verified in the experiment section. A.2. Proof of Proposition 1 Proposition A.2 (Proof in Appendix A). If the approximation method is itself unbiased, and we only replace the SpMM in the backward pass with its approximated version, while leaving the forward one unchanged, then the calculated gradient is provably unbiased. Here we note that in the main text, for the notation convenience, we ignore the backward pass of ReLU. However, the proof here will consider the non-linear activation function to prove the unbiasedness. Let H(l+1) pre = SpMM( ËœA, J(l)) be the pre-activation. The backward pass of ReLU is: E[âˆ‡H(l+1) pre ] =E[1 H(l+1) pre >0 âŠ™ âˆ‡H(l+1)] = 1 H(l+1) pre >0 âŠ™ E[âˆ‡H(l+1)], (5) where âŠ™ is the element-wise product and 1 is the indicator function. The element-wise product is linear operation and 1 H(l+1) pre >0 is only related to the pre-activation in the forward pass, we only apply the approximation during the backward pass so 1 H(l+1) pre >0 can be extracted from the expectation. We know that for the last layer, we have E[âˆ‡H(L)] = H(L) since we do not apply ReLU at the output layer. We then can prove by induction that E[âˆ‡H(l+1)] = H(l+1) and E[âˆ‡J(l)] =E[approx( ËœAâŠ¤âˆ‡H(l+1) pre )] =âˆ‡J(l) for any layer l. A.3. Analysis of MEAN aggregator For GraphSAGE, one commonly used aggregator is the MEAN aggregator, which can be expressed as follows: H(l+1) = W1H(l) + W2SpMM MEAN(A, H(l)), (6) 13RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations where SpMM MEAN is one variant of the vanilla SpMM , which replace the reducer function from sum(Â·) to mean(Â·). We note that in popular GNN packages, the MEAN aggregator usually is implemented based on SpMM MEAN (Fey & Lenssen, 2019; Wang et al., 2019) to reduce the memory usage. Here we give an example of SpMM MEAN to illustrate how it works: SpMM MEAN( ï£® ï£° 1 0 0 4 5 6 ï£¹ ï£», \u00147 8 9 10 \u0015 ) = \"1 2 (1 Ã— 7 + 0Ã— 9) 1 2 (1 Ã— 8 + 0Ã— 10) 1 2 (0 Ã— 7 + 4Ã— 9) 1 2 (0 Ã— 8 + 4Ã— 10) 1 2 (5 Ã— 7 + 6Ã— 9) 1 2 (5 Ã— 8 + 6Ã— 10) # , Equivalently, the SpMM MEAN can also be expressed as: SpMM MEAN(A, H(l)) =Dâˆ’1AH(l), where D is the degree matrix ofA. Thus, although we did not normalize the adjacency matrix in GraphSAGE, when applying the top-k sampling to approximate SpMM MEAN, the column norm of A:,ji is actually 1âˆš Degji due to the normalization. Also, for GraphSAGE, the inputs to the first SpMM MEAN operation are A and X. They do not require gradient since they are not trainable. Thus, the first SAGE layer is not presented in Figure 8 and Figure 7. B. Pseudo code of the greedy algorithm Algorithm 1 The greedy algorithm Inputs: Gradients of node embeddings{âˆ‡H(1), Â·Â·Â·âˆ‡ H(L)}, adjacency matrix A, graph G = (V, E), hidden dimensions {d1, Â·Â·Â· dL}. Parameters: The step size Î±, the overall budget C. Outputs: The layer-wise {k1, Â·Â·Â· kL} associated with the top-k sampling. B â† PL l=1 |E|dl. âˆ€i, kl â† |V|, Topkl â† {1, Â·Â·Â·|V|} . while B â‰¥ C PL l=1 |E|dl do m â† arg minlâˆˆ{1,Â·Â·Â·L}(P iâˆˆTopkl âˆ¥AâŠ¤ :,iâˆ¥2âˆ¥âˆ‡H(l+1) i,: âˆ¥2 âˆ¥Aâˆ¥F âˆ¥âˆ‡H(l+1)âˆ¥F âˆ’ P iâˆˆTopklâˆ’Î±|V| âˆ¥AâŠ¤ :,iâˆ¥2âˆ¥âˆ‡H(l+1) i,: âˆ¥2) âˆ¥Aâˆ¥F âˆ¥âˆ‡H(l+1)âˆ¥F /* Choose the layer m to reduce by a step size Î±|V|, such that the increment of errors is minimal. */ B â† B âˆ’ dm P iâˆˆTopkmâˆ©i/âˆˆTopkmâˆ’Î±|V| #nnzi /*Since we exclude some column-row pairs for layer m, here we reduce the budget B accordingly. */ km â† km âˆ’ Î±|V| /* Update km accordingly. */ Topkm â† the set of indices i associated with km largest âˆ¥AâŠ¤ :,iâˆ¥2âˆ¥âˆ‡H(l+1) i,: âˆ¥2 âˆ¥Aâˆ¥F âˆ¥âˆ‡H(l+1)âˆ¥F /* Update Topkm accordingly. */ end while Return {k1, Â·Â·Â· , kL} In algorithm 1, here we provide the pseudo code of our greedy algorithm for solving the constrained optimization problem. In Table 11, we show the run time of the greedy algorithm, which is negligible compared to the acceleration effect. C. Extended Related works Connections to Graph Data Augmentation Data augmentation (Liu et al., 2021; Han et al., 2022) is wildly adopted in the graph learning for improving model generalization, including dropping nodes (Feng et al., 2020), dropping edges (Rong et al., 2019), and graph mixup (Han et al., 2022). As shown in Figure 5, the top- k sampling drops the entire columns in the adjacency matrix, while keeping the number of rows unchanged. That means RSC drops all of the out edges for a set of nodes. This can be viewed as the â€œstructural dropedgeâ€ for improving the efficiency. Since we only apply the top-k sampling in the backward pass and top- k indices are different for each operation, RSC essentially forward pass with the whole graph, backward pass with different subgraphs at each layer. This structural dropedge and heterogeneous backward propagation introduce the regularization effect. Thus as shown in the experiment section, RSC may also improve the model accuracy over the baseline. 14RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Subgraph-based GNN training. The key idea of this line of work is to improve the scalability of GNNs by separating the graph into overlapped small batches, then training models with sampled subgraphs (Hamilton et al., 2017; Huang et al., 2018; Zou et al., 2019; Chiang et al., 2019; Zeng et al., 2020). Based on this idea, various sampling techniques have been proposed, including the node-wise sampling (Hamilton et al., 2017; Chen et al., 2017), layer-wise sampling (Huang et al., 2018; Zou et al., 2019), and subgraph sampling (Chiang et al., 2019; Zeng et al., 2020). However, this approach reduces the memory footprint but results in extra time cost to compute the overlapping nodes between batches. Generally, methods in this category are orthogonal to RSC , and they can be combined. Graph precomputation. The graph precomputation methods decouple the message passing from the model training, either as a preprocessing step (Wu et al., 2019; Klicpera et al., 2018; Yu et al., 2020) or post-processing step (Huang et al., 2020b), where the model is simplified as the Multi-Layer Perceptron (MLP). We did consider this line of work in this paper since the backbone model is not GNN anymore. Distributed GNN training. The distributed training leverages extra hardwares to increase the memory capacity and training efficiency (Zha et al., 2023; 2022; Yuan et al., 2022; Wang et al., 2022; Wang et al.). However, the graph data cannot be trivially divided into independent partitions due to the node connectivity. Thus, the graph distributed training frameworks propose to split graph into related partitions and minimize the communication overhead (Wan et al., 2021; 2022b; Ramezani et al., 2022). Our methods are orthogonal to this line of work. Other randomized GNN training. Dropedge (Rong et al., 2019) randomly drops edges to avoid the over-smoothing problem. Graph Random Neural Networks (Grand) (Feng et al., 2020) randomly drop nodes to generate data augmentation for improving model generalization. Grand+ improves the scalability over Grand by pre-computing a general propagation matrix and employ it to perform data augmentation (Feng et al., 2022). As shown in Section C, the key difference between GRAND(+) and RSC is that RSC does not drop any node. Instead RSC drops all of the out edges for a set of nodes only during backward pass. Moreover, the drop pattern are evolving during the training process. This can be viewed as the â€œstructural dropedgeâ€. However, unlike Dropedge (Rong et al., 2019), RSC drop the column-row pairs according to the euclidean norm instead of uniformly dropping. D. Experimental Settings D.1. Software and Hardware Descriptions All experiments are conducted on a server with four NVIDIA 3090 GPUs, four AMD EPYC 7282 CPUs, and 252GB host memory. We implement all models based on Pytorch and Pytorch Geometric. During our experiments, we found that the version of Pytorch, Pytorch Sparse, and Pytorch Scatter can significantly impact the running speed of the baseline. Here we list the details of our used packages in all experiments in Table 5. Table 5: Package configurations of our experiments. Package Version CUDA 11.1 pytorch sparse 0.6.12 pytorch scatter 2.0.8 pytorch geometric 1.7.2 pytorch 1.9.0 OGB 1.3.2 D.2. Statistics of benchmark datasets The statistics for all used datasets are shown in Table 6. We follow the standard data splits and all datasets are directly downloaded from Pytorch Geometric or the protocol of OGB (Hu et al., 2020). D.3. Hyperparameter Settings Regarding Reddit and Yelp dataset, we follow the hyperparameter reported in the respective papers as closely as possible. Regarding ogbn-proteins and ogbn-products dataset, we follow the hyperparameter configurations and codebases provided 15RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Table 6: Dataset Statistics. Dataset Task Nodes Edges Classes Label Rates Reddit multi-class 232,965 11,606,919 41 65.86% Yelp multi-label 716,847 6,977,409 100 75.00% ogbn-proteins binary-Class 132,534 39,561,252 2 65.00% ogbn-products multi-class 2,449,029 61,859,076 47 8.03% on the OGB (Hu et al., 2020) leader-board. Please refer to the OGB website for more details. The optimizer is Adam for all these models. All methods terminate after a fixed number of epochs. We report the test accuracy associated with the highest validation score. Table 10 summarize the hyperparameter configuration of GraphSAINT. Table 7, Table 8, and Table 9 summarize the hyperparameter configuration of full-Batch GCN, GraphSAGE, and GCNII, respectively. Table 7: Configuration of Full-Batch GCN. Dataset Training Archtecture Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 0.01 400 0.5 Yes 3 256 Yelp 0.01 500 0.1 Yes 3 512 ogbn- proteins 0.01 1000 0.5 No 3 256 ogbn- products 0.001 500 0.5 No 3 256 Table 8: Configuration of Full-Batch GraphSAGE. Dataset Training Archtecture Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 0.01 400 0.5 Yes 3 256 Yelp 0.01 500 0.1 Yes 3 512 ogbn- proteins 0.01 1000 0.5 No 3 256 ogbn- products 0.001 500 0.5 No 3 256 Table 9: Configuration of Full-Batch GCNII. Dataset Training Archtecture Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 0.01 400 0.5 Yes 4 256 Yelp 0.01 500 0.1 Yes 4 256 ogbn- proteins 0.01 1000 0.5 No 4 256 E. More experiment results The running time of the greedy algorithm is shown in 11. We also visualize the allocated kl for each layer across iterations in Figure 7, and the degree of picked nodes in Figure 8. Here we use Reddit dataset for the case study. We observe that the kl assigned by RSC evolves along with the training. 16RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Table 10: Training configuration of GraphSAINT. Dataset RandomWalk Sampler Training Archtecture Walk length Roots Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 4 8000 0.01 40 0.1 Yes 3 128 Yelp 2 8000 0.01 75 0.1 Yes 3 512 ogbn- products 3 60000 0.01 20 0.5 No 3 256 Table 11: The running time (second) of the greedy algorithm. Reddit Yelp ogbn- proteins ogbn- products GCN 0.03 0.03 0.03 0.03 GraphSAGE 0.02 0.02 0.03 0.03 GCNII 0.05 0.05 0.06 - /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000037/uni0000004b/uni00000048/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003ki | | /uni0000002a/uni00000026/uni00000031/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni0000002a/uni00000026/uni00000031/uni00000003/uni00000014/uni00000056/uni00000057/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni00000003/uni00000015/uni00000051/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni00000003/uni00000016/uni00000055/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056 /uni00000013/uni00000011/uni00000014/uni00000013 /uni00000013/uni00000011/uni00000014/uni00000018 /uni00000013/uni00000011/uni00000015/uni00000013 /uni00000037/uni0000004b/uni00000048/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003ki | | /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni00000015/uni00000051/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni00000016/uni00000055/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000037/uni0000004b/uni00000048/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003ki | | /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000014/uni00000056/uni00000057/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000015/uni00000051/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000016/uni00000055/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000017/uni00000057/uni0000004b/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 Figure 7: The allocated layer-wise kl for GCN, GraphSAGE and GCNII on Reddit, where budget C is set as 0.1. The input of the SpMM in the first GraphSAGE layer does not require gradient and thus absent in the Figure (Appendix A.3). 0 50 100 150 200 250 300 350 400 Epochs 50 100 150Node degrees GCN 1st layer GCN 2nd layer GCN 3rd layer 0 50 100 150 200 250 300 350 400 Epochs 20 40 60 80 100 120Node degrees GraphSAGE 2nd layer GraphSAGE 3rd layer 0 50 100 150 200 250 300 350 400 Epochs 50 100 150Node degrees GCNII 1st layer GCNII 2nd layer GCNII 3rd layer GCNII 4th layer Figure 8: The averaged degrees of nodes picked by top-k sampling along the whole training process, where the applied dataset is Reddit and overall budget C is set as 0.1. E.1. Additional Ablation Results to the Resource Allocation Algorithm (Figure 6) Due to the page limit, we present more ablation study on the resource allocation algorithm here. Specifically, in Figure 9, we compare RSC to the uniform allocation on ogbn-proteins dataset with GCN, GraphSAGE, and GCNII, respectively. In Figure 10, we compare RSC to the uniform allocation on Yelp dataset with GCN, GraphSAGE, and GCNII, respectively. We conclude that RSC generally outperforms the uniform allocation strategy. E.2. Hyperparameter Sensitivity Analysis Here we analyze the impacts of the main hyperparameters of RSC : (1) the budget C, which controls the efficiency-accuracy trade-off; (2) the step size Î± in the greedy Algorithm 1; (3) when switching back to the original sparse operations. In Figure 12, we vary only one of them with the others fixed. We conclude (1) larger budget C leads to better accuracy with smaller speedup, since we are using more computational resources to approximate the full operation. (2) larger step size Î± leads 17RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations 1.01.21.41.61.8 Speedup 67 68 69 70 71 72Accuracy (%) GCN (ogbn-proteins) Uniform Allocation RSC 1.21.41.61.82.02.2 Speedup 70 71 72 73 74 75 76Accuracy (%) GraphSAGE (ogbn-proteins) Uniform Allocation RSC 1.01.21.41.61.8 Speedup 64 66 68 70 72 74Accuracy (%) GCNII (ogbn-proteins) Uniform Allocation RSC Figure 9: The Pareto frontier of the accuracy-efficiency trade-off for RSC and the uniform allocation. The dataset is ogbn-proteins. Here we disabled the caching and switch mechanism for a fair comparison. 0.960.981.001.021.041.061.081.10 Speedup 42 44 46 48Accuracy (%) GCN (Yelp) Uniform Allocation RSC 1.0001.0251.0501.0751.1001.1251.1501.175 Speedup 63.0 63.2 63.4 63.6 63.8 64.0Accuracy (%) GraphSAGE (Yelp) Uniform Allocation RSC 1.101.121.141.161.181.201.221.241.26 Speedup 64.0 64.1 64.2 64.3 64.4Accuracy (%) GCNII (Yelp) Uniform Allocation RSC Figure 10: The Pareto frontier of the accuracy-efficiency trade-off for RSC and the uniform allocation. The dataset is Yelp. Here we disabled the caching and switch mechanism for a fair comparison. to marginally larger speedup since the greedy algorithm will terminate earlier. Also the step size Î± does not affect the model accuracy a lot. In practice, we set Î± = 0.02|V|. (3) The later we switch back to the original operation, the larger the accuracy drop and the smaller the speedup, it is equivalent to using less resources to approximate the full operation epoch-wisely. Thus, we apply RSC for 80% of the total epochs to balance the trade-off. 0 100 200 300 400 Epochs 20 40 60 80Validation Accuracy GCN (Reddit) Baseline C=0.1 C=0.2 C=0.3 0 100 200 300 400 Epochs 20 40 60 80 100Validation Accuracy GCNII (Reddit) Baseline C=0.1 C=0.2 C=0.3 Figure 11: Learning curves for validation accuracy under different overall budget C on Reddit dataset. Here we disabled the caching and switching mechanism for ablating the effect of C. 0.1 0.2 0.3 0.4 0.5 (a) Budget C 73 74 75 76Test AUC Baseline AUC RSC AUC RSC Speedup 1.5 1.6 1.7 1.8 Speedup GraphSAGE (ogbn-proteins) 0.01| |  0.02| |  0.05| |  0.1| |  0.2| | (b) step size  75.9 76.0 76.1 76.2 76.3Test AUC Baseline AUC RSC AUC RSC Speedup 1.58 1.60 1.62 1.64 Speedup GraphSAGE (ogbn-proteins) At 60%  total epochs At 70%  total epochs At 80%  total epochs At 90%  total epochs At 95%  total epochs (c) When switching back to the original 75.50 75.75 76.00 76.25 76.50Test AUC Baseline AUC RSC AUC RSC Speedup 1.45 1.50 1.55 1.60 1.65 1.70 Speedup GraphSAGE (ogbn-proteins) Figure 12: Hyperparameter analysis w.r.t. the budget C, the step size Î± in Algorithm 1, and when switching back to the original operations. The model is GraphSAGE and the applied dataset is ogbn-proteins. 18",
      "meta_data": {
        "arxiv_id": "2210.10737v2",
        "authors": [
          "Zirui Liu",
          "Shengyuan Chen",
          "Kaixiong Zhou",
          "Daochen Zha",
          "Xiao Huang",
          "Xia Hu"
        ],
        "published_date": "2022-10-19T17:25:33Z",
        "pdf_url": "https://arxiv.org/pdf/2210.10737v2.pdf"
      }
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liuâˆ— Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wuâˆ— Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ï¬xed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efï¬ciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiï¬cation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ï¬‚oating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (Lâˆ’1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover âˆ—Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such â€œneighbor explosionâ€ problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above â€œneighbor explosionâ€ problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], qâ‹† ij = Î±ijâˆ¥h(l) j âˆ¥2 âˆ‘ kâˆˆNi Î±ikâˆ¥h(l) k âˆ¥2 for vertex vi, to minimize the variance of the estimator Ë†h(l+1) i involves all its neighborsâ€™ hidden embeddings, i.e.{Ë†h(l) j |vj âˆˆNi}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsÎ±ijâ€™s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulï¬l this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deï¬ne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ï¬rst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi âˆˆV, and edges (vi,vj) âˆˆE. Let the adjacency matrix denote as AâˆˆRNÃ—N. Assuming the feature matrix H(0) âˆˆRNÃ—D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = Ïƒ ( Nâˆ‘ j=1 Î±(vi,vj) h(l) j W(l) ) , l = 0,...,L âˆ’1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, Î±Î±Î±= (Î±(vi,vj)) âˆˆRNÃ—N is a kernel or weight matrix, W(l) âˆˆRD(l)Ã—D(l+1) is the transform parameter on the l-th layer, and Ïƒ(Â·) is the activation function. The weight Î±(vi,vj), or Î±ij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deï¬ne ï¬xed weights as Î±Î±Î±= ËœDâˆ’1 ËœAor Î±Î±Î±= ËœDâˆ’1 2 ËœAËœDâˆ’1 2 respectively, where ËœA= A+I, and ËœDis the diagonal node degree matrix of ËœA. (2) The attentive GNNs [22, 17] deï¬ne a learned weight Î±(vi,vj) by attention functions: Î±(vi,vj) = ËœÎ±(vi,vj;Î¸)âˆ‘ vkâˆˆNi ËœÎ±(vi,vk;Î¸) , where the unnormalized attentions ËœÎ±(vi,vj; Î¸) = exp(ReLU(aT[Whiâˆ¥Whj])), are parameterized by Î¸= {a,W}. Different 2from GCNs, the learned weights Î±ij âˆËœÎ±ij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as Ë†h(l+1) i = Ïƒ ( N(i) Epij [ Ë†h(l) j ] W(l) ) , (2) where pij âˆÎ±ij, and N(i) =âˆ‘ jÎ±ij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = Î±ij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling Ë†h(l+1) i = ÏƒW(l) ( Ë†Âµ(l) i ) = ÏƒW(l) ( Eqij [Î±ij qij Ë†h(l) j ]) , (3) where we use ÏƒW(l) (Â·) to include transform parameter W(l) into the function Ïƒ(Â·) for conciseness. As such, one can ï¬nd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator Ë†Âµ(l) i = 1 k âˆ‘k s=1 Î±ijs qijs Ë†h(l) js , where js âˆ¼qi. Take expectation overqi, we deï¬ne the variance of Ë†Âµ(l) i = Î±ijs qijs Ë†h(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [îµ¹îµ¹îµ¹Ë†Âµ(l) i (t) âˆ’Âµ(l) i (t) îµ¹îµ¹îµ¹ 2] = E [îµ¹îµ¹îµ¹Î±ijs(t) qijs h(l) js (t) âˆ’ âˆ‘ jâˆˆNi Î±ij(t)h(l) j (t) îµ¹îµ¹îµ¹ 2] . (4) Note that Î±ij and h(vj) that are inferred during training may vary over steps tâ€™s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ï¬rst is a function of qi, which we refer to as the effective variance: Ve(qi) = âˆ‘ jâˆˆNi 1 qij Î±2 ijâˆ¥hjâˆ¥2 , (5) while the second does not depend on qi, and we denote it by Vc = îµ¹îµ¹îµ¹âˆ‘ jâˆˆNi Î±ijhj îµ¹îµ¹îµ¹ 2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: qâ‹† ij = Î±ijâˆ¥h(l) j âˆ¥2 âˆ‘ kâˆˆNi Î±ikâˆ¥h(l) k âˆ¥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighborsâ€™ embeddings in the denominator of Eq.(6). Moreover, the Î±ijâ€™s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theÎ±ijâ€™s are ï¬xed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ï¬xed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several â€œlayer samplingâ€ approaches [11, 6, 14, 25] have been proposed to alleviate the â€œneigh- bor explosionâ€ problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efï¬cient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two â€œgraph samplingâ€ approaches [7, 24] ï¬rst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with â€œlayer samplingâ€ approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 â‰¤tâ‰¤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Qâ‹† i = argmin Qi âˆ‘T t=1 Vt e(Qi), such that âˆ‘T t=1 Vt e(Qt i) â‰¤câˆ‘T t=1 Vt e(Qâ‹† i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si âŠ‚Ni where Si âˆ¼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) âˆ’Vt e(Qâ‹† i) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Qâ‹† i we have the upper bound derived on right hand of Eq. (7). We deï¬ne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Qâ‹† i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = âˆ’âˆ‡Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ï¬xed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deï¬ne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs âŠ‚Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =âˆ’âˆ‡qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efï¬cient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si âŠ‚Ni that satisï¬es âˆ‘ Si:jâˆˆSi Qi,Si = qij,âˆ€vj âˆˆNi, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward âˆ’âˆ‡Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ï¬nd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j âˆˆNi else 0, wij(1) = 1if j âˆˆNi else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deï¬ned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect viâ€™sksampled neighbors vj âˆˆSt i, and rewards rt i = {rij(t) :vj âˆˆSt i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ï¬nally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deï¬ned only at the (l+ 1)-th layer, hence we should maintain multiple qiâ€™s at each layer. In practice, we ï¬nd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qiâ€™s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ï¬rst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ï¬rst assume the weights Î±ijâ€™s are ï¬xed, then extend to attentive GNNs that Î±ij(t)â€™s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator Ë†Âµi = 1 k kâˆ‘ s=1 Î±ijs qijs Ë†hjs, js âˆ¼qi. (8) This yields the variance V(qi) = 1 k Eqi [îµ¹îµ¹îµ¹ Î±ijs qijs hjs âˆ’âˆ‘ jâˆˆNi Î±ijhj îµ¹îµ¹îµ¹ 2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =âˆ’âˆ‡qij(t)Vt e(qt i) = Î±2 ij kÂ·qij(t)2 âˆ¥hj(t)âˆ¥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisï¬es âˆ‘ Si:jâˆˆSi Qi,Si = qij,âˆ€vj âˆˆ Ni, where Si âŠ‚Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. Ë†Âµi = âˆ‘ jsâˆˆSi Î±ijs qijs hjs is the unbiased estimator of Âµi = âˆ‘ jâˆˆNi Î±ijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =âˆ‘ SiâŠ‚Ni Qi,Siâˆ¥âˆ‘ jsâˆˆSi Î±ijs qijs hjsâˆ¥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensenâ€™s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) â‰¤âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2. 5Table 1: Dataset summary. â€œsâ€ dontes multi-class task, and â€œmâ€ denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =âˆ‘ jsâˆˆSi Î±ijs qijs(t)2 âˆ¥hjs(t)âˆ¥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = Î±ij qij(t)2 âˆ¥hj(t)âˆ¥2,âˆ€j âˆˆSi. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value Î±ij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ËœÎ±ij. We deï¬ne the adjusted feedback attention values as follows: Î±â€² ij = âˆ‘ jâˆˆSi qij Â· ËœÎ±ijâˆ‘ jâˆˆSi ËœÎ±ij , (10) where ËœÎ±ijâ€™s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use âˆ‘ jâˆˆSi qij as a surrogate of âˆ‘ jâˆˆSi ËœÎ±ij âˆ‘ jâˆˆNi ËœÎ±ij so that we can approximate the truth attention values Î±ij by our adjusted attention values Î±â€² ij. 6 Regret Analysis As we described in section 4, the regret is deï¬ned as âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with Î·= 0.4 and Î´= âˆš (1âˆ’Î·)Î·4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1â‰¤tâ‰¤T, we have Tâˆ‘ t=1 Vt e(Qt i) â‰¤3 Tâˆ‘ t=1 Vt e(Qâ‹† i) + 10 âˆš Tn4 ln(n/k) k3 (11) where T â‰¥ln(n/k)n2(1 âˆ’Î·)/(kÎ·2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability qâ‹† i and using the reward deï¬nition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ï¬xed variance given the speciï¬c estimators, this is the ï¬rst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ï¬x the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(Â±0.014) 0.890(Â±0.002) 0.689(Â±0.005) 0.949(Â±0.001) 0.494(Â±0.001) FastGCN 0.827(Â±0.001) 0.895(Â±0.005) 0.502(Â±0.003) 0.825(Â±0.006) 0.500(Â±0.001) LADIES 0.843(Â±0.003) 0.880(Â±0.006) 0.574(Â±0.003) 0.932(Â±0.001) 0.465(Â±0.007) AS-GCN 0.830(Â±0.001) 0.888(Â±0.006) 0.599(Â±0.004) 0.890(Â±0.013) 0.506(Â±0.012) S-GCN 0.828(Â±0.001) 0.893(Â±0.001) 0.744(Â±0.003) 0.943(Â±0.001) 0.501(Â±0.002) ClusterGCN 0.807(Â±0.006) 0.887(Â±0.001) 0.853(Â±0.001) 0.938(Â±0.002) 0.418(Â±0.002) GraphSAINT 0.815(Â±0.012) 0.899(Â±0.002) 0.787(Â±0.003) 0.965(Â±0.001) 0.507(Â±0.001) GCN-BS 0.855(Â±0.005) 0.903(Â±0.001) 0.905(Â±0.003) 0.957(Â±0.000) 0.513(Â±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(Â±0.001) 0.884(Â±0.003) 0.566(Â±0.002) NA 0.472(Â±0.012) GraphSAINT-GAT0.773(Â±0.036) 0.886(Â±0.016) 0.789(Â±0.001) 0.933(Â±0.012) 0.470(Â±0.002) GAT-BS 0.857(Â±0.003) 0.894(Â±0.001) 0.841(Â±0.001) 0.962(Â±0.001) 0.513(Â±0.001) GAT-BS.M 0.857(Â±0.003) 0.894(Â±0.000) 0.867(Â±0.003) 0.962(Â±0.000) 0.513(Â±0.001) GP-BS 0.811(Â±0.002) 0.890(Â±0.003) 0.958(Â±0.001) 0.964(Â±0.000) 0.507(Â±0.000) GP-BS.M 0.811(Â±0.001) 0.892(Â±0.001) 0.965(Â±0.001) 0.964(Â±0.000) 0.507(Â±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are â€œgraph samplingâ€ techniques that ï¬rst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theâ„“2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ï¬rst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ï¬‚ickr as 200 by doing grid search. We set the architecture of GraphSAINT as â€œ0-1-1â€3 which means MLP layer followed by two graph convolution layers. We use the â€œrwâ€ sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiï¬cantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciï¬ed. As a result, their variances are simply ï¬xed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ï¬nd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighborsâ€™ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48â€“77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efï¬cient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257â€“266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702â€“2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530â€“6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324â€“360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024â€“1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 33, pages 946â€“953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558â€“4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiï¬cation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077â€“2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 33, pages 4424â€“4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593â€“607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiï¬ca- tion in network data. AI magazine, 29(3):93â€“93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375â€“389. Springer, 2010. [22] P. VeliË‡ckoviÂ´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247â€“11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: Î·= 0.4, sample size k, neighbor size n= |Ni|, Î´= âˆš (1 âˆ’Î·)Î·4k5 ln(n/k)/(Tn4). 1: Set Ë†rij(t) =rij(t)/qij(t) if j âˆˆSt i else 0 wij(t+ 1) =wij(t) exp(Î´Ë†rij(t)/n) 2: Set qij(t+ 1)â†(1 âˆ’Î·) wij(t+1)âˆ‘ jâˆˆNi wij(t+1) + Î· n, for j âˆˆNi Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: Î· = 0.4, sample size k, neighbor size n = |Ni|, Î´ = âˆš (1 âˆ’Î·)Î·4k5 ln(n/k)/(Tn4), Ut i = âˆ…. 1: For j âˆˆNi set Ë†rij(t) = {rij(t)/qij(t) if j âˆˆSt i 0 otherwise wij(t+ 1) = {wij(t) exp(Î´Ë†rij(t)/n) if j /âˆˆUt i wij(t) otherwise 2: if maxjâˆˆNi wij(t+ 1)â‰¥( 1 k âˆ’Î· n) âˆ‘ jâˆˆNi wij(t+ 1)/(1 âˆ’Î·) then 3: Decide at so as to satisfy atâˆ‘ wij(t+1)â‰¥at at + âˆ‘ wij(t+1)<at wij(t+ 1)= (1 k âˆ’Î· n)/(1 âˆ’Î·) 4: Set Ut+1 i = {j : wij(t+ 1)â‰¥at} 5: else 6: Set Ut+1 i = âˆ… 7: end if 8: Set wâ€² ij(t+ 1) = {wij(t+ 1) if j âˆˆNi\\Ut+1 i at if j âˆˆUt i 9: Set qij(t+ 1) =k ( (1 âˆ’Î·) wâ€² ij(t+1)âˆ‘ jâˆˆNi wâ€² ij(t+1) + Î· n ) for j âˆˆNi Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with âˆ‘K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set Î² = min{1 âˆ’qi,qj}and Î³ = min{qi,1 âˆ’qj} 6: Update qi and qj as (qi,qj) = { (qi + Î²,qj âˆ’Î²) with probability Î³ Î²+Î³ (qi âˆ’Î³,qj + Î³) with probability Î² Î²+Î³ 7: end while 8: return {i: qi = 1,1 â‰¤iâ‰¤K} 12B Proofs Proposition 1. Ë†Âµi = âˆ‘ jsâˆˆSi Î±ijs qijs hjs is the unbiased estimator of Âµi = âˆ‘ jâˆˆNi Î±ijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si âŠ‚Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy âˆ‘ Si:jâˆˆSi Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[Ë†Âµi] =E ï£® ï£°âˆ‘ jsâˆˆSi Î±ijs qijs hjs ï£¹ ï£» (12) = âˆ‘ SiâŠ‚Ni Qi,Si âˆ‘ jsâˆˆSi Î±ijs qijs hjs (13) = âˆ‘ jâˆˆNi âˆ‘ Si:jâˆˆSi Qi,Si Î±ij qij hj (14) = âˆ‘ jâˆˆNi Î±ij qij hj âˆ‘ Si:jâˆˆSi Qi,Si (15) = âˆ‘ jâˆˆNi Î±ij qij hjqij (16) = âˆ‘ jâˆˆNi Î±ijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) â‰¤âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2. Proof. The variance is V(Qi) =E ï£® ï£¯ï£° îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ jsâˆˆSi Î±ijs qijs hjs âˆ’ âˆ‘ jâˆˆNi Î±ijhj îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2ï£¹ ï£ºï£» = âˆ‘ SiâŠ‚Ni Qi,Si îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ jsâˆˆSi Î±ijs qijs hjs îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2 âˆ’ îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ jâˆˆNi Î±ijhj îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2 . Therefore the effective variance has following upper bound: Ve(Qi) = âˆ‘ SiâŠ‚Ni Qi,Si îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ âˆ‘ jsâˆˆSi Î±ijs qijs hjs îµ¹îµ¹îµ¹îµ¹îµ¹îµ¹ 2 â‰¤ âˆ‘ SiâŠ‚Ni Qi,Si âˆ‘ jsâˆˆSi Î±ijs îµ¹îµ¹îµ¹îµ¹ hjs qijs îµ¹îµ¹îµ¹îµ¹ 2 (Jensenâ€²sInequality ) = âˆ‘ jsâˆˆNi âˆ‘ Si:jsâˆˆSi Qi,SiÎ±ijs îµ¹îµ¹îµ¹îµ¹ hjs qijs îµ¹îµ¹îµ¹îµ¹ 2 = âˆ‘ jsâˆˆNi Î±ijs q2 ijs âˆ¥hjsâˆ¥2 âˆ‘ Si:jsâˆˆSi Qi,Si = âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 13Proposition 3. The negative derivative of the approximated effective variance âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =âˆ‘ jsâˆˆSi Î±ijs qijs(t)2 âˆ¥hjs(t)âˆ¥2. Proof. Deï¬ne the upper bound as Ë†Ve(Qi) =âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2, then its derivative is âˆ‡Qi,Si Ë†Ve(Qi) =âˆ‡Qi,Si âˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 = âˆ‡Qi,Si âˆ‘ jsâˆˆNi Î±ijsâˆ‘ Sâ€² i:jsâˆˆSâ€² i Qi,Sâ€² i âˆ¥hjsâˆ¥2 = âˆ‡Qi,Si âˆ‘ jsâˆˆSi Î±ijsâˆ‘ Sâ€² i:jsâˆˆSâ€² i Qi,Sâ€² i âˆ¥hjsâˆ¥2 = âˆ’ âˆ‘ jsâˆˆSi Î±js q2 ijs âˆ¥hjsâˆ¥2 (chainrule) Before we give the proof of Theorem 1, we ï¬rst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantÎ·â‰¤1 and any valid distributions Qt i and Qâ‹† i we have (1 âˆ’2Î·)Vt e(Qt i) âˆ’(1 âˆ’Î·)Vt e(Qâ‹† i) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©+ Î·âŸ¨Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ© (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Qâ‹† i we have Vt e(Qt i) âˆ’Vt e(Qâ‹† i) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©. (19) Multiplying both sides of this inequality by 1 âˆ’Î·, we have (1 âˆ’Î·)Vt e(Qt i) âˆ’(1 âˆ’Î·)Vt e(Qâ‹† i) (20) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©âˆ’Î·âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have âŸ¨Qt i,âˆ‡Qt i Vt e(Qt i)âŸ©= âˆ’ âˆ‘ jâˆˆNi qij(t) Î±2 ij kÂ·qij(t)2 âˆ¥hj(t)âˆ¥2 (22) = âˆ’Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective varianceâˆ‘ jsâˆˆNi Î±ijs qijs âˆ¥hjsâˆ¥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have âŸ¨Qt i,âˆ‡Qt i Vt e(Qt i)âŸ©= âˆ’ âˆ‘ SiâŠ‚Ni Qt i,Si âˆ‘ jsâˆˆSi Î±ijs qijs(t)2 âˆ¥hjsâˆ¥2 (24) = âˆ’ âˆ‘ jsâˆˆNi Î±ijs qijs(t)2 âˆ¥hjsâˆ¥2 âˆ‘ Si:jsâˆˆSi Qt i,Si (25) = âˆ’ âˆ‘ jsâˆˆNi Î±ijs qijs(t)âˆ¥hjsâˆ¥2 (26) = âˆ’Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 âˆ’Î·)Vt e(Qt i) âˆ’(1 âˆ’Î·)Vt e(Qâ‹† i) (28) â‰¤âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©âˆ’Î·âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ© (29) = âŸ¨Qt i âˆ’Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©+ Î·âŸ¨Qâ‹† i,âˆ‡Qt i Vt e(Qt i)âŸ©+ Î·Vt e(Qt i). (30) Theorem 1. Using Algorithm 1 with Î·= 0.4 and Î´ = âˆš (1 âˆ’Î·)Î·4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1â‰¤tâ‰¤T, we have Tâˆ‘ t=1 Vt e(Qt i) â‰¤3 Tâˆ‘ t=1 Vt e(Qâ‹† i) + 10 âˆš Tn4 ln(n/k) k3 (31) where T â‰¥ln(n/k)n2(1 âˆ’Î·)/(kÎ·2) and n= |Ni|. Proof. First we explain why condition T â‰¥ln(n/k)n2(1 âˆ’Î·)/(kÎ·2) ensures that Î´Ë†rij(t) â‰¤1, Î´Ë†rij(t) = âˆš (1 âˆ’Î·)Î·4k5 ln(n/k) Tn4 Â·Î±ij(t) q3 ij(t) âˆ¥hj(t)âˆ¥2 (32) â‰¤ âˆš (1 âˆ’Î·)Î·4k5 ln(n/k) Tn4 Â· n3 k3Î·3 (33) â‰¤1 (34) Assuming âˆ¥hj(t)âˆ¥â‰¤ 1, inequality (33) holds because Î±ij(t) â‰¤1 and qij(t) â‰¥kÎ·/n. Then replace T by the condition, we get Î´Ë†rij(t) â‰¤1. Let Wi(t), Wâ€² i(t) denote âˆ‘ jâˆˆNi wij(t), âˆ‘ jâˆˆNi wâ€² ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = âˆ‘ jâˆˆNi\\Ut i wij(t+ 1) Wi(t) + âˆ‘ jâˆˆUt i wij(t+ 1) Wi(t) (35) = âˆ‘ jâˆˆNi\\Ut i wij(t) Wi(t) Â·exp(Î´Ë†rij(t)) + âˆ‘ jâˆˆUt i wij(t) Wi(t) (36) â‰¤ âˆ‘ jâˆˆNi\\Ut i wij(t) Wi(t) [ 1 +Î´Ë†rij(t) + (Î´Ë†rij(t))2] + âˆ‘ jâˆˆUt i wij(t) Wi(t) (37) = 1 +Wâ€² i (t) Wi(t) âˆ‘ jâˆˆNi\\Ut i wij(t) Wâ€² i (t) [ Î´Ë†rij(t) + (Î´Ë†rij(t))2] (38) = 1 +Wâ€² i (t) Wi(t) âˆ‘ jâˆˆNi\\Ut i qij(t)/kâˆ’Î·/n 1 âˆ’Î· [ Î´Ë†rij(t) + (Î´Ë†rij(t))2] (39) â‰¤1 + Î´ k(1 âˆ’Î·) âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†r2 ij(t) (40) Inequality (37) uses ea â‰¤1 +a+ a2 for aâ‰¤1. Equality (39) holds because of update equation of qij(t) deï¬ned in EXP3.M. Inequality (40) holds because Wâ€² i (t) Wi(t) â‰¤1. Since 1 +xâ‰¤ex for xâ‰¥0, we have ln Wi(t+ 1) Wi(t) â‰¤ Î´ k(1 âˆ’Î·) âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†r2 ij(t) (41) 15If we sum, for 1 â‰¤tâ‰¤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = Tâˆ‘ t=1 ln Wi(t+ 1) Wi(t) (42) â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†r2 ij(t) (43) â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)Ë†r2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) â‰¥ln âˆ‘ jâˆˆS wij(T + 1) Wi(1) (45) â‰¥ âˆ‘ jâˆˆS ln wij(T + 1) k âˆ’ln n k (46) â‰¥Î´ k âˆ‘ jâˆˆS âˆ‘ t:j /âˆˆUt i Ë†rij(t) âˆ’ln n k (47) The inequality (46) uses the fact that âˆ‘ jâˆˆS wij(T + 1)â‰¥k( âˆ jâˆˆS wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(Î´ âˆ‘ t:j /âˆˆUt i Ë†rij(t)) From (44) and (47), we get Î´ k âˆ‘ jâˆˆS âˆ‘ t:j /âˆˆUt i Ë†rij(t) âˆ’ln n k â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi\\Ut i qij(t)Ë†r2 ij(t) (48) And we have the following inequality Î´ k âˆ‘ jâˆˆS âˆ‘ t:jâˆˆUt i rij(t) = Î´ k âˆ‘ jâˆˆS âˆ‘ t:jâˆˆUt i qij(t)Ë†rij(t) (49) â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆUt i qij(t)Ë†rij(t) (50) The equality (49) holds beacuse rij(t) =qijË†rij(t) when j âˆˆSt i and Ut i âŠ†St i bacause qt ij = 1for all j âˆˆUt i. Then add inequality (50) in (48) we have Î´ k âˆ‘ jâˆˆS âˆ‘ t:jâˆˆUt i rij(t) +Î´ k âˆ‘ jâˆˆS âˆ‘ t:j /âˆˆUt i Ë†rij(t) âˆ’ln n k (51) â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)Ë†rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)Ë†r2 ij(t) (52) Given qij(t) we have E[Ë†r2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that Î´ k Tâˆ‘ t=1 âˆ‘ jâˆˆS rij(t) âˆ’ln n k â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi r2 ij(t) (53) 16By multiplying (53) by Qâ‹† i,S and summing over S, we get Î´ k Tâˆ‘ t=1 âˆ‘ SâŠ‚Ni Qâ‹† i,S âˆ‘ jâˆˆS rij(t) âˆ’ln n k â‰¤ Î´ k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi qij(t)rij(t) + Î´2 k(1 âˆ’Î·) Tâˆ‘ t=1 âˆ‘ jâˆˆNi r2 ij(t) (54) As âˆ‘ jâˆˆNi qij(t)rij(t) = âˆ‘ jâˆˆNi âˆ‘ Si:jâˆˆSi Qt i,Sirij(t) (55) = âˆ‘ SiâŠ‚Ni Qt i,Si âˆ‘ jâˆˆSi rij(t) (56) = âˆ’ âˆ‘ SiâŠ‚Ni Qt i,Siâˆ‡Qt i,Si Vt e(Qt i,Si) (57) = âˆ’âŸ¨Qt i,âˆ‡Qt i Vt e(Qt i)âŸ© (58) By plugging (58) in (54) and rearranging it, we ï¬nd Tâˆ‘ t=1 âŸ¨Qt i âˆ’Qâ‹† i ,âˆ‡Qt i Vt e(Qt i)âŸ©+ Î· Tâˆ‘ t=1 âŸ¨Qâ‹† i ,âˆ‡Qt i Vt e(Qt i)âŸ© (59) â‰¤Î´ Tâˆ‘ t=1 âˆ‘ jâˆˆNi r2 ij(t) +(1 âˆ’Î·)k Î´ ln(n/k) Using Lemma 1, we have (1 âˆ’2Î·) Tâˆ‘ t=1 Vt e(Qt i) âˆ’(1 âˆ’Î·) Tâˆ‘ t=1 Vt e(Qâ‹† i ) â‰¤Î´ Tâˆ‘ t=1 âˆ‘ jâˆˆNi r2 ij(t) +(1 âˆ’Î·)k Î´ ln(n/k) (60) Finally, we know that âˆ‘ jâˆˆNi r2 ij(t) = âˆ‘ jâˆˆNi Î±ij(t)2 qij(t)4 (61) â‰¤ âˆ‘ jâˆˆNi Î±ij(t) n4 k4Î·4 (becauseqij(t) â‰¥kÎ·/n) (62) = n4 k4Î·4 (63) By setting Î·= 0.4 and Î´= âˆš (1 âˆ’Î·)Î·4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other â€œlayer samplingâ€ approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between â€œgraph samplingâ€ and â€œlayer samplingâ€ paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with â€œgraph samplingâ€ approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ï¬‚oating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the â€œlayer samplingâ€ paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. â€œGraph samplingâ€ approaches are not applicable to cases where only partial vertices have labels. To summarize, the â€œlayer samplingâ€ approaches are more ï¬‚exible and general compared with â€œgraph samplingâ€ approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ï¬nd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      }
    },
    {
      "title": "KDEformer: Accelerating Transformers via Kernel Density Estimation",
      "abstract": "Dot-product attention mechanism plays a crucial role in modern deep\narchitectures (e.g., Transformer) for sequence modeling, however, na\\\"ive exact\ncomputation of this model incurs quadratic time and memory complexities in\nsequence length, hindering the training of long-sequence models. Critical\nbottlenecks are due to the computation of partition functions in the\ndenominator of softmax function as well as the multiplication of the softmax\nmatrix with the matrix of values. Our key observation is that the former can be\nreduced to a variant of the kernel density estimation (KDE) problem, and an\nefficient KDE solver can be further utilized to accelerate the latter via\nsubsampling-based fast matrix products. Our proposed KDEformer can approximate\nthe attention in sub-quadratic time with provable spectral norm bounds, while\nall prior results merely provide entry-wise error bounds. Empirically, we\nverify that KDEformer outperforms other attention approximations in terms of\naccuracy, memory, and runtime on various pre-trained models. On BigGAN image\ngeneration, we achieve better generative scores than the exact computation with\nover $4\\times$ speedup. For ImageNet classification with T2T-ViT, KDEformer\nshows over $18\\times$ speedup while the accuracy drop is less than $0.5\\%$.",
      "full_text": "KDEformer: Accelerating Transformers via Kernel Density Estimation Amir Zandieh1 Insu Hanâ€ 2 Majid Daliriâ€ 3 Amin Karbasi2 1Max-Planck-Institut fÂ¨ ur Informatik 2Yale University 3New York University June 30, 2023 Abstract Dot-product attention mechanism plays a crucial role in modern deep architectures (e.g., Transformer) for sequence modeling, however, naÂ¨ Ä±ve exact computation of this model incurs quadratic time and memory complexities in sequence length, hindering the training of long- sequence models. Critical bottlenecks are due to the computation of partition functions in the denominator of softmax function as well as the multiplication of the softmax matrix with the matrix of values. Our key observation is that the former can be reduced to a variant of the kernel density estimation (KDE) problem, and an efficient KDE solver can be further utilized to accelerate the latter via subsampling-based fast matrix products. Our proposed KDEformer can approximate the attention in sub-quadratic time with provable spectral norm bounds, while all prior results merely provide entry-wise error bounds. Empirically, we verify that KDEformer outperforms other attention approximations in terms of accuracy, memory, and runtime on various pre-trained models. On BigGAN image generation, we achieve better generative scores than the exact computation with over 4 Ã— speedup. For ImageNet classification with T2T-ViT, KDEformer shows over 18Ã— speedup while the accuracy drop is less than 0 .5%. 1 Introduction Transformers [31] have been successfully applied to a wide variety of learning tasks in areas such as natural language processing [ 15, 32, 4, 22], computer vision [ 5, 16], and time series forecasting [ 35]. Although popular, these models face serious scalability limitations because naÂ¨ Ä±ve exact computation of their attention layers incurs quadratic (in sequence length) runtime and memory complexities. This can inhibit the training of large-scale long-sequence models. Several algorithms have been proposed to improve Transformersâ€™ efficiency via approximating the softmax matrices in their attention layers with either sparse matrices [ 20, 13, 23, 27] or low-rank matrices [ 12, 19], or a combination of both [ 10, 34, 9, 14]. However, all prior advances solely focused on point-wise approximating the entries of the softmax matrix and fail to provide rigorous approximation guarantees on the final output of the attention mechanism. In this work, we design algorithms to approximate the output matrix of attention layers with provable spectral norm guarantees. â€ Equal contribution. 1 arXiv:2302.02451v2  [cs.LG]  29 Jun 20231.1 Problem Formulation and Setting. Let n be the number of tokens in the input sequence and d be the dimension of latent representations. The dot-product attention [31] is a mapping which takes inputs Q , K, V âˆˆ RnÃ—d (interpreted as queries, keys, and values of a dictionary) and outputs the following matrix: Att(Q, K, V) := Dâˆ’1AV A := exp \u0010 QKâŠ¤/ âˆš d \u0011 , D := diag(A1n), where exp(Â·) is applied in an element-wise manner, 1n is the ones vector in Rn, and diag(Â·) maps its input vector to a diagonal matrix. We refer to A âˆˆ RnÃ—n as the attention matrix and to Dâˆ’1A as the softmax matrix. Exact computation of the attention matrix A takes Î˜( n2d) operations and storing it requires Î˜(n2) memory. Thus, naÂ¨ Ä±ve computation ofAtt(Q, K, V) requires â„¦(n2d) runtime and â„¦(n2) memory. Our aim is to approximate the output matrix Att(Q, K, V) efficiently while preserving its spectral structure. Our approach is based on reducing the number of columns of matrix A using importance sampling. We also devise an efficient estimator for the diagonal scaling matrix D, which bypasses exact and explicit computation of matrix A. Formally, for any given Îµ >0 and any Q , K, V âˆˆ RnÃ—d, we want to quickly find a sampling matrix Î  âˆˆ RmÃ—n with a small number m = n1âˆ’â„¦(1) of rows along with a diagonal matrix eD âˆˆ RnÃ—n, such that the following bound on the operator norm of the error is satisfied: \r\r\rAtt(Q, K, V) âˆ’ eDâˆ’1AÎ âŠ¤ Â· Î V \r\r\r op â‰¤ Îµ Â· \r\rDâˆ’1A \r\r op âˆ¥Vâˆ¥op . (1) Note that Dâˆ’1A is a row-stochastic (transition) matrix, so its operator norm is \r\rDâˆ’1A \r\r op âˆˆ [1, âˆšn]. Given a sampling matrix Î  with m rows, we can compute the matrix product AÎ  âŠ¤ Â· Î V in O(nmd) total runtime and O(nm) memory because we only need to compute the m sampled columns of A. Therefore, our main goal is to generate a sampling matrix Î  with a small number of samples along with a diagonal matrix eD which satisfy Equation (1) using a sub-quadratic runtime in n. All prior approximate attention methods have solely focused on finding an approximate attention matrix eA such that \r\r\rA âˆ’ eA \r\r\r F is small, even though A is not the ultimate output of attention and the output depends on V in addition to A. In contrast, we propose the first efficient algorithm for approximating the output matrix Att(Q, K, V) with spectral bounds as per Equation (1) (see Section 3.3). 1.2 Our Techniques and Results We leverage the line of work on efficient Kernel Density Estimation (KDE) [25, 18, 6, 1, 2, 26]. In the KDE problem, we are given a dataset X = {x1, x2, . . . xn} and a kernel function k(Â·, Â·) and aim to compute the kernel density ÂµX(q) = 1 n Pn i=1 k(q, xi) for an arbitrary query point q. The goal of existing methods in the literature is to estimate this value to (1 + Îµ) relative error in time O \u0000 Îµâˆ’2d/eÂµÏ„ \u0001 for some Ï„ > 0, where eÂµ is a lower bound on ÂµX(q). Particularly, the best-known algorithm for the Gaussian kernel, due to Charikar et al. [7], achieves Ï„ = 0.173 + o(1). We show that finding the sampling matrix Î  and diagonal scaling eD which satisfy Equation (1) can be reduced to a generalization of the KDE problem. First note that the ith diagonal entry of the scaling matrix D is D i,i = Pn j=1 exp \u0010âŸ¨qi,kjâŸ©âˆš d \u0011 , which is indeed the kernel density corresponding 2to exponential kernel function k(x, y) = exp(âŸ¨x, yâŸ©) and dataset 1 d1/4 Â· K at query point 1 d1/4 Â· qi. Thus, if we had an efficient KDE procedure for estimating the exponential kernel density up to a multiplicative (1 Â± Îµ) factor, we could compute a scaling eD that satisfies the spectral guarantee of Equation (1). Additionally, to design an efficient sampling matrix Î  that satisfies Equation (1) with small number of rows, the sampling probabilities need to be proportional to the column norms of the softmax matrix D âˆ’1A [36]. One can see that the squared norm of the ith column of D âˆ’1A isP jâˆˆ[n] Dâˆ’2 j,j exp \u0010 2âˆš dâŸ¨qj, kiâŸ© \u0011 , which is a weighted exponential kernel density with weights n Dâˆ’2 i,i o iâˆˆ[n] and dataset âˆš 2 d1/4 Â·Q at query point âˆš 2 d1/4 Â·ki. Therefore, if we could estimate this weighted exponential kernel density up to some constant multiplicative factor, we could generate a sampling matrix Î  with small number of samples that satisfies Equation (1). Thus, having a generalized KDE procedure for efficiently evaluating the weighted exponential kernel density, enables us to approximate Att(Q, K, V) as per Equation (1). While there is no prior solution for this problem, we show how to translate it to the Gaussian KDE problem, which has witnessed significant recent progress, by applying appropriate transformations on K and Q (see Algorithm 2 and Theorem 3.4). Our Theoretical Results. We give an algorithm that outputs a diagonal eD âˆˆ RnÃ—n and a sampling matrix Î  âˆˆ RmÃ—n with m = O \u0000 Îµâˆ’2 log n Â· srank(Dâˆ’1A) \u0001 samples which satisfy the spectral bound of Equation (1) with high probability in n, where srank(Dâˆ’1A) denotes the stable rank of the softmax matrix. Our method reduces the memory of attention layers to mn = O \u0000 Îµâˆ’2n log n Â· srank(Dâˆ’1A) \u0001 . Furthermore, if the Gaussian KDE is supported by an algorithm with runtime O \u0000 Îµâˆ’2d/eÂµÏ„ \u0001 for relative error 1 + Îµ, and density lower bound eÂµ, then our algorithmâ€™s runtime is bounded by O \u0000 Îµâˆ’2d Â· n1+Ï„ \u0001 for any datasets of queries Q and keys K with diameter maxi,jâˆˆ[n] âˆ¥ki âˆ’ qjâˆ¥2 2 = o \u0010âˆš d Â· log n \u0011 , which is strongly sub-quadratic in n. The current best value for Ï„ is Ï„ = 0.173+o(1) due to [7] and any future progress on Gaussian density evaluation immediately improves our methodâ€™s runtime. This result applies to a wide range of practical scenarios where the dimensiond is not too large. To see why, note that entries of K, Q are typically constant, thus, the diameter is maxi,jâˆˆ[n] âˆ¥ki âˆ’ qjâˆ¥2 2 = O(d). Therefore, for any dimension d = o(log2 n), e.g., d â‰ˆ log2 n log logn, our method needs only O \u0000 m + Îµâˆ’2d Â· n1+Ï„ \u0001 operations, which is significantly faster than exact computation of Att(Q, K, V). Our Practical Results. Our necessary number m of samples depends on the stable rank of the softmax matrix. To reduce m, we employ Locality Sensitive Hashing (LSH) to extract the heavy elements of Dâˆ’1A and then show that, in practice, the residual has a significantly smaller stable rank than the original matrix (see Section 3.4). With this heuristic improvement, we verify that our proposed algorithm outperforms popular attention approximations. In particular, it can save memory space up to 19 .06Ã— when the sequence length n is 16,394. We apply our method to image generation with BigGAN [ 3] and observe that our images, shown in Figure 1, look more natural than others and our generative score is even better than the exact attention. Furthermore, for ImageNet classification with Vision Transformer [33], KDEformer shows 18 Ã— speedup and 82.08% accuracy which is only 0 .5% lower than the exact attention (see Section 4). Finally, we demonstrate our method on end-to-end training under the Long Range Arena benchmark [ 28] and observe up to 3Exact KDEformer Performer Reformer Figure 1: Image generations by the pre-trained BigGAN using exact and approximate attention without fine-tuning. 8Ã— speedup on wall-clock time than the exact attention (see Section 4.4). 1.3 Prior Work Several popular methods try to approximate the heavy entries of the attention matrix A by restricting the attention to local neighbors of queries using Locality Sensitive Hashing (LSH) [ 20, 8, 27] or k-means clustering [13, 23]. Such approaches, however, only provide error bounds on the attention matrix, e.g., guarantees of the form âˆ¥A âˆ’ eAâˆ¥F < Îµn, and cannot provide any provable guarantees for the final output matrix Att(Q, K, V). Remarkably, at the core of our algorithm, there are invocations of the Gaussian KDE primitive from Charikar et al. [7], which heavily employs LSH to estimate kernel densities. In contrast to previous works, our algorithm uses LSH in a more subtle way, that is for estimating the right sampling probabilities in order to generate Î  and also to approximate the scaling D. This difference of approach allows us to approximate Att(Q, K, V) with spectral norm guarantees. Another recent line of work is based on approximating the attention matrix A via random feature maps of the Gaussian or exponential kernels [ 12, 19]. Chen et al. [10] has recently shown that using a combination of both LSH-based and random features based methods works better at approximating the attention matrix A. See [29] for a survey. 2 Preliminaries and Notations For any matrix A, we let ai be its ith row vector and its stable rank is defined as srank(A) := âˆ¥Aâˆ¥2 F âˆ¥Aâˆ¥2 op which is always upper bounded by the algebraic rank. We denote e1, e2, . . . en by the standard basis vectors in Rn and 1n and 0n by the all-ones and all-zeros vectors in Rn. For vectors x, ytheir direct sum is denoted by x âŠ• y := [xâŠ¤, yâŠ¤]âŠ¤. 4Gaussian KDE. Our main algorithm is tightly related to the Gaussian KDE, where one is given a dataset X âˆˆ RnÃ—d and wants to build a data-structure (DS) such that given this DS one can estimate the following kernel density value up to (1 + Îµ) relative error for any query point q âˆˆ Rd: ÂµX(q) := 1 n X iâˆˆ[n] exp(âˆ’âˆ¥q âˆ’ xiâˆ¥2 2 /2). (2) The naÂ¨ Ä±ve method without any DS requires Î˜(nd) time and memory complexities. The aim is to minimize the memory needed to store the DS and the query time, ultimately being sublinear in n. The pre-processing time which is needed to construct the DS is also desired to be small. There have been significant advances on this problem and the current best result was proposed by Charikar et al. [7] as follows: Theorem 2.1 (Fast Gaussian KDE, Theorem 2 in [ 7]). Let Ï„ = 0.173 + o(1). For any dataset X âˆˆ RnÃ—d and any Îµ, eÂµ âˆˆ (0, 1), there exist the following procedures: 1. PreprocessKDE(X, Îµ,eÂµ) constructs a data-structure named DSkde in time O \u0000 Îµâˆ’2dn/eÂµÏ„ \u0001 . 2. Given DSkde, any query q âˆˆ Rd, and ÂµX(q) defined as in Equation (2), QueryKDE(DSkde, q) approximates the quantity ÂµX(q)Â·1 {eÂµâ‰¤ÂµX(q)} up to (1+Îµ) relative error in O(Îµâˆ’2d/ (eÂµ + ÂµX(q))Ï„ ) runtime. The density lower bound eÂµ required by Theorem 2.1 is unknown to us in advance and we learn this quantity adaptively in Algorithm 2. We show in Section 3.3 that for datasets with bounded diameter eÂµ = nâˆ’1âˆ’o(1). 3 Efficient Attention with Spectral Bounds In this section, we design KDEformer which can efficiently compute a sampling matrix Î  and a diagonal scaling eD satisfying Equation (1). We start by showing that this can be done very efficiently given access to a primitive for estimating the row-norms of the attention matrix A as well as the column-norms of the softmax matrix D âˆ’1A. Next, in Section 3.2, we present a reduction from norm estimators for A and D âˆ’1A to the Gaussian KDE problem which has an efficient solution. Finally, we prove our main result in Section 3.3 3.1 High-level Architecture of the Algorithm Here, we assume that we have access to an oracle, which can estimate theweighted linear combination of n exponential kernels at arbitrary query points, and given this oracle, we design an algorithm that can output Î  and eD which satisfy Equation (1). In other words, we translate and reduce the problem of spectrally approximating Att(Q, K, V) to a weighted KDE problem corresponding to the exponential dot-product kernel. The precise interface and desired properties of this oracle are presented in the following definition, Definition 3.1 (Weighted Exponential KDE). Let X, Y âˆˆ RnÃ—d be arbitrary datasets and let v âˆˆ Rn + be an arbitrary vector with positive coordinates. For any Îµ >0, primitive WExpKDE(X, Y, v, Îµ) outputs a non-negative vector Î± âˆˆ Rn + such that: Î±j âˆˆ (1 Â± Îµ) Â· X iâˆˆ[n] vi exp(âŸ¨xi, yjâŸ©) âˆ€j âˆˆ [n]. (3) 5Now we show how to generate Î  and eD that satisfy Equation (1), given access to WExpKDE as per Definition 3.1. Estimating D = diag \u0010 exp \u0010 QKâŠ¤/ âˆš d \u0011 1n \u0011 . One can easily see that the jth diagonal entry of D equals: Dj,j = X iâˆˆ[n] exp \u0010 âŸ¨ki, qjâŸ©/ âˆš d \u0011 âˆ€j âˆˆ [n]. (4) Therefore, if we let Î± = WExpKDE \u0010 K d1/4 , Q d1/4 , 1n, Îµ 3 \u0011 and define eD = diag(Î±), then by Defini- tion 3.1 and using the fact that entries of D are positive, we have (1 âˆ’ Îµ/3)D âª¯ eD âª¯ (1 + Îµ/3)D where âª¯ is the Loewner order. So, \r\r\rAtt(Q, K, V) âˆ’ eDâˆ’1AV \r\r\r op â‰¤ Îµ 2 Â· \r\rDâˆ’1AV \r\r op . (5) Hence, we can estimate D to sufficient precision by invoking WExpKDE \u0010 K d1/4 , Q d1/4 , 1n, Îµ 3 \u0011 . Generating the Sampling Matrix Î . Given a diagonal matrix eD which satisfies Equation (5), by triangle inequality, in order to satisfy the spectral bound of Equation (1), it suffices to find a sampling matrix for which the following holds, \r\r\reDâˆ’1AÎ âŠ¤ Â· Î V âˆ’ eDâˆ’1AV \r\r\r op â‰¤ Îµ 2 Â· \r\rDâˆ’1A \r\r op âˆ¥Vâˆ¥op (6) So, our goal is to design a sampling matrix Î  âˆˆ RmÃ—n with a small number m of rows that satisfies Equation (6). This problem is in fact well studied in the randomized numerical linear algebra literature and is known as the Approximate Matrix Multiplication (AMM) with respect to the spectral norm. It is known how to achieve the above guarantee using a sampling matrix with m = O \u0000 Îµâˆ’2 log n Â· (srank(Dâˆ’1A) + srank(V)) \u0001 i.i.d. rows. More formally, we have the following result which is a slight modification of Theorem 2.1 from [36] and is proved in Section 8.1. Lemma 3.2 (AMM). For any matrices X âˆˆ RnÃ—q, Y âˆˆ RnÃ—d and any probability distribution {pi}iâˆˆ[n] satisfying pi â‰¥ 1 4 Â· âˆ¥xiâˆ¥2 2+Î³Â·âˆ¥yiâˆ¥2 2 âˆ¥Xâˆ¥2 F +Î³Â·âˆ¥Yâˆ¥2 F for all i âˆˆ [n] and Î³ = âˆ¥Xâˆ¥2 op / âˆ¥Yâˆ¥2 op, a sampling matrix Î  âˆˆ RmÃ—n constructed by first generating m i.i.d. samples â„“1, . . . â„“m âˆˆ [n] according to {pâ„“}â„“âˆˆ[n] and then letting the rth row of Î  be 1âˆšmÂ·pâ„“r Â· eâŠ¤ â„“r , if m = â„¦ \u0000 Îµâˆ’2 log n Â· (srank(X) + srank(Y)) \u0001 for some Îµ >0, the following holds, Pr \u0014\r\r\rXâŠ¤Î âŠ¤Î Y âˆ’ XâŠ¤Y \r\r\r op > Îµâˆ¥Xâˆ¥op âˆ¥Yâˆ¥op \u0015 â‰¤ 1 poly(n). So, by invoking Lemma 3.2 with X âŠ¤ = eDâˆ’1A and Y = V and error parameter Îµ/2, we can find a random sampling matrix Î  which satisfies Equation (6) with high probability in n, as long as the number of samples is at least m = â„¦ \u0010 Îµâˆ’2 log n(srank(eDâˆ’1A) + srank(V)) \u0011 . The only catch is that, to apply Lemma 3.2, we need to compute the distribution {pi}iâˆˆ[n] as per this lemma. In other 6Algorithm 1 KDEformer 1: input: matrices Q , K, V âˆˆ RnÃ—d, integer m, and Îµ >0 2: Î³ â† âˆ¥Vâˆ¥âˆ’2 op via power method 3: Î± â† WExpKDE \u0010 K d1/4 , Q d1/4 , 1n, Îµ 3 \u0011 in Definition 3.1 4: Î² â† WExpKDE \u0010âˆš 2Â·Q d1/4 , âˆš 2Â·K d1/4 , u,1/3 \u0011 , where ui â† 1/Î±2 i for every i âˆˆ [n] 5: pi â† Î²i + Î³ Â· âˆ¥viâˆ¥2 2 for every i âˆˆ [n] then normalize pâ„“ â† pâ„“P jâˆˆ[n] pj for every â„“ âˆˆ [n] 6: generate i.i.d. samples â„“1, â„“2, . . . â„“m âˆˆ [n] from distribution {pâ„“}â„“âˆˆ[n] 7: let rth row of Î  be 1âˆšmÂ·pâ„“r Â· eâŠ¤ â„“r for every r âˆˆ [m] 8: return eD = diag(Î±) and Î  words, we need to compute the row norms of V as well as the column norms of eDâˆ’1A. All row norms of V can be computed in O(nd) time. However, naively computing the column norms of eDâˆ’1A would require Î˜(n2d) operations. Fortunately, the column norms of eDâˆ’1A can be approximated via the primitive WExpKDE from Definition 3.1. The procedure for computing eD and sampler Î  is presented in Algorithm 1. We state the correctness of Algorithm 1 in the following theorem and prove it in Section 8.2. Theorem 3.3 (Correctness of Algorithm 1) . For any matrices Q, K, V âˆˆ RnÃ—d, any Îµ >0, and number of samples m = â„¦ \u0000 Îµâˆ’2 log n Â· (srank(Dâˆ’1A) + srank(V)) \u0001 , given access to a primitive WExpKDE as per Definition 3.1, Algorithm 1 outputs a diagonal matrix eD âˆˆ RnÃ—n and a sampling matrix Î  âˆˆ RmÃ—n which satisfy Equation (1) with probability at least 1 âˆ’ 1 poly(n) . So, to spectrally approximate Att(Q, K, V), it is enough to run Algorithm 1. This algorithm relies on the existence of primitive WExpKDE as per Definition 3.1, therefore, we focus on efficient implementation of WExpKDE. 3.2 Weighted Exponential KDE Here, we devise an efficient algorithm that satisfies the desired properties of WExpKDE as per Definition 3.1. We show that this procedure is tightly related to and can be translated to an instance of the Gaussian KDE. First note that if all data-points in dataset X were on a sphere, i.e., âˆ¥xiâˆ¥2 = r for all i âˆˆ [n] and some r >0, then the weighted exponential kernel density corresponding to the weights v = 1 n Â· 1n would be equal to e(âˆ¥qâˆ¥2 2+r2)/2 Â· ÂµX(q), where ÂµX(q) is defined as in Equation (2). Our proposed WExpKDE primitive employs a fast Gaussian KDE method as per Theorem 2.1. The weighted exponential kernel density for a query point q and weight vector v âˆˆ Rn + can be written as, X iâˆˆ[n] vieâŸ¨xi,qâŸ© = e âˆ¥qâˆ¥2 2 2 X iâˆˆ[n] vie âˆ¥xiâˆ¥2 2 2 Â· eâˆ’âˆ¥xiâˆ’qâˆ¥2 2 2 . (7) Let us define wi := r 2 log P jâˆˆ[n] vj exp(âˆ¥xjâˆ¥2 2/2) viÂ·exp(âˆ¥xiâˆ¥2 2/2) for every i âˆˆ [n] and define the augmented dataset Xâ€² âˆˆ RnÃ—(d+1) as xâ€² i := xi âŠ•[wi] for every i âˆˆ [n]. Also let the augmented query point be qâ€² := q âŠ•[0]. 7Algorithm 2 Weighted Exponential KDE (WExpKDE) 1: input: matrices X , Y âˆˆ RnÃ—d, vector v âˆˆ Rn +, error parameter Îµ >0, and Ï„ >0 2: Âµ â† 1/n and S â† [n] and Î± â† 0n 3: N â† P jâˆˆ[n] vje âˆ¥xjâˆ¥ 2 2 2 4: wi â† q 2 log N viÂ·exp(âˆ¥xiâˆ¥2 2/2) for every i âˆˆ [n] 5: Xâ€² â† [X; w] âˆˆ RnÃ—(d+1), Yâ€² â† [Y; 0n] âˆˆ RnÃ—(d+1) 6: while Âµâˆ’Ï„ â‰¤ Îµ2 Â· |S| do 7: DSkde â† PreprocessKDE(Xâ€², Îµ, Âµ) 8: Î±i â† n Â· N Â· e âˆ¥yiâˆ¥2 2 2 Â· QueryKDE(DSkde, yâ€² i) for every i âˆˆ S 9: Âµ â† Âµ/2 and S â† {i âˆˆ [n] : Î±i = 0} 10: Î±j â† P iâˆˆ[n] vi Â· exp(âŸ¨xi, yjâŸ©) for every j âˆˆ S 11: return Î± Then, the r.h.s. in Equation (7) can be written as e âˆ¥qâˆ¥2 2 2 X iâˆˆ[n] vie âˆ¥xiâˆ¥2 2 2 Â· exp   âˆ’âˆ¥xâ€² i âˆ’ qâ€²âˆ¥2 2 2 + w2 i 2 ! = n Â· e âˆ¥qâˆ¥2 2 2 X jâˆˆ[n] vje âˆ¥xjâˆ¥ 2 2 2 Â· ÂµXâ€²(qâ€²). (8) Therefore, the weighted exponential kernel density can be obtained from the Gaussian kernel density corresponding to the augmented dataset X â€² and augmented query qâ€², i.e., ÂµXâ€²(qâ€²). The augmented dataset can be constructed very efficiently in time O(nd), so given a fast Gaussian KDE as per Theorem 2.1, Equation (8) shows us an efficient way to implement the WExpKDE procedure. Our proposed procedure is presented in Algorithm 2. Note that, fast Gaussian KDE requires a lower bound eÂµ on the kernel density value ÂµXâ€²(qâ€²), and we show how to adaptively learn eÂµ in Algorithm 2 using the fact that if QueryKDE(DSkde, qâ€²) outputs zero we can infer that our lower bound was too high. We analyze Algorithm 2 in the following theorem. Theorem 3.4 (Analysis of Algorithm 2) . For every matrices X, Y âˆˆ RnÃ—d, any non-negative vector v âˆˆ Rn +, and any Îµ âˆˆ (0, 1), and given a fast Gaussian KDE as per Theorem 2.1, Algorithm 2 outputs a vector Î± âˆˆ Rn which satisfies the desired conditions of Definition 3.1 (i.e., Equation (3)). Furthermore, this procedureâ€™s runtime is O (nd Â· CX,Y,v,Îµ,Ï„ ), where CX,Y,v,Îµ,Ï„ := min Âµ>0 1 Îµ2ÂµÏ„ + \f\f\f\f\f\f\f\f ï£± ï£´ï£´ï£² ï£´ï£´ï£³ i âˆˆ [n] : Pn j=1 vjeâŸ¨xj,yiâŸ© Pn j=1 vje âˆ¥xjâˆ¥ 2 2+âˆ¥yiâˆ¥2 2 2 < nÂµ ï£¼ ï£´ï£´ï£½ ï£´ï£´ï£¾ \f\f\f\f\f\f\f\f (9) Proof. First, we prove the correctness. Let us index the iterations of the algorithmâ€™s while loop by t = 0, 1, 2, . . .and let Âµt, Î±t, and St denote the value of Âµ, the vector Î±, and set S at tth iteration. We have |St| â‰¤n and Âµt = 1 nÂ·2t for every t, thus, the algorithm must terminate inT = O(log n) iterations. Also, by Theorem 2.1, the set St+1 computed in line 9 equals St+1 = {i âˆˆ [n] : ÂµXâ€²(yâ€² i) < Âµt}, because the fast Gaussian KDE procedure outputs zero if and only if ÂµXâ€²(yâ€² i) < Âµt. Next, we show by induction that at every iteration t, Î±t(i) is within (1 Â± Îµ) factor of nNe âˆ¥yiâˆ¥2 2 2 Â· ÂµXâ€²(yâ€² i) for all i âˆˆ [n]\\St. Base of induction is trivial because S0 = [n]. For proving the inductive 8step, note that in lines 7-8 Î±t+1(i) is updated for every i âˆˆ St by invoking the fast Gaussian KDE procedure and Î±t+1(i) = Î±t(i) for i âˆˆ [n] \\ St. Thus, by the inductive hypothesis and Theorem 2.1 as well as definition of St+1 in line 9, Î±t+1(i) is within (1 Â± Îµ) factor of nNe âˆ¥yiâˆ¥2 2 2 Â· ÂµXâ€²(yâ€² i) for all i âˆˆ [n] \\St+1, which completes the inductive proof. Using the definition of N in line 3 and definition of Xâ€², Yâ€² in line 5 along with Equation (8), the invariant that we proved implies that for every t = 0, 1, . . . T, Î±t(i) is within (1 Â± Îµ) factor of P jâˆˆ[n] vj Â· exp(âŸ¨xj, yiâŸ©) for all i âˆˆ [n] \\ St. After exiting the while loop, Î±(i) is updated at all i âˆˆ ST+1 in line 2 as Î±(i) = P jâˆˆ[n] vj Â· exp(âŸ¨xj, yiâŸ©), and Î±(i) = Î±T (i) for every i âˆˆ [n] \\ ST . This proves that the output vector Î± satisfies Equation (3), which completes the correctness proof. Runtime Analysis. The runtime has three components; 1. Time to run PreprocessKDE in line 7. The total time of running this primitive in all iterations t = 0, 1, . . . Tis O \u0010PT t=0 dÂ·n Îµ2 Âµâˆ’Ï„ t \u0011 , by Theorem 2.1. Since Âµt = 1 nÂ·2t , this runtime is bounded by O \u0000dÂ·n Îµ2 Âµâˆ’Ï„ T \u0001 . 2. Time to run QueryKDE in line 8. By Theorem 2.1, the total time to run this procedure in all iterations is O \u0010 d Îµ2 Â· PT t=0 P iâˆˆSt (Âµt + ÂµXâ€²(yâ€² i))âˆ’Ï„ \u0011 . Because |St| â‰¤n, this runtime complexity is completely dominated by (1). 3. Time to exactly compute the weighted exponential densities of the points with very small ÂµXâ€²(yâ€² i) value in line 10. This runtime is bounded by O(nd Â· |ST+1|). Now we combine these bounds. Using the assumption that the algorithm terminated at iteration t = T, the while loop condition at iteration T +1 must fail. Therefore, |ST+1| < Âµâˆ’Ï„ T+1/Îµ2 < 2Âµâˆ’Ï„ T /Îµ2. This shows that the first component of the runtime must dominate the third component. So the total time is bounded by O \u0000dÂ·n Îµ2 Âµâˆ’Ï„ T \u0001 . Recall that the while loop terminates at iteration T meaning that Îµâˆ’2Âµâˆ’Ï„ t â‰¤ |St| for every t = 0, 1, . . . Tand Îµâˆ’2Âµâˆ’Ï„ T+1 > |ST+1|. So, T is the largest integer that satisfies Îµâˆ’2Âµâˆ’Ï„ T â‰¤ |ST |. Also recall that St = {i âˆˆ [n] : ÂµXâ€²(yâ€² i) < Âµtâˆ’1} and Âµt = 1 nÂ·2t . Thus, the runtime of the procedure can be expressed as, O(nd) Â· min Âµ>0 Îµâˆ’2Âµâˆ’Ï„ + \f\f\b i âˆˆ [n] : ÂµXâ€²(yâ€² i) < Âµ \t\f\f. The definition of X â€², Yâ€² in line 5 along with Equation (8) gives the claimed runtime bound in Equation (9). To get a better understanding of the runtime bound in Theorem 3.4, suppose that datasets X , Y are such that cardinality of set ï£± ï£´ï£´ï£´ï£² ï£´ï£´ï£´ï£³ i âˆˆ [n] : P jâˆˆ[n] vj exp(âŸ¨xj,yiâŸ©) P jâˆˆ[n] vj exp ï£« ï£­âˆ¥xjâˆ¥ 2 2+âˆ¥yiâˆ¥2 2 2 ï£¶ ï£¸ â‰¤ nâˆ’o(1) ï£¼ ï£´ï£´ï£´ï£½ ï£´ï£´ï£´ï£¾ is upper bounded by O \u0000 Îµâˆ’2 Â· nÏ„ \u0001 . For such datasets, the runtime of Theorem 3.4 is bounded by O \u0000 Îµâˆ’2d Â· n1+Ï„+o(1)\u0001 , which is strongly sub-quadratic in n. 93.3 Main Result Now we are in a position to prove our main result, i.e., an efficient algorithm that can approximate the attention mechanism with spectral guarantees as per Equation (1). Theorem 3.5 (Approximate Attention with Spectral Norm Bound) . For any matrices Q, K, V âˆˆ RnÃ—d, any Îµ > 0, and given a fast Gaussian KDE as per Theorem 2.1, there exists an algo- rithm that outputs a diagonal matrix eD âˆˆ RnÃ—n and a sampling matrix Î  âˆˆ RmÃ—n with m = O \u0000 Îµâˆ’2 log n Â· (srank(Dâˆ’1A) + srank(V)) \u0001 samples which satisfy Equation (1) with probability at least 1âˆ’ 1 poly(n) . The runtime of this algorithm is O \u0012 m + nd Â· \u0012 C K d1/4 , Q d1/4 ,1n,Îµ,Ï„ + Câˆš 2Â·Q d1/4 , âˆš 2Â·K d1/4 ,v,1,Ï„ \u0013\u0013 , where vj = \u0010P â„“âˆˆ[n] exp \u0010 1âˆš dâŸ¨qj, kâ„“âŸ© \u0011\u0011âˆ’2 for j âˆˆ [n] and C K d1/4 , Q d1/4 ,1n,Îµ,Ï„ , Câˆš 2Â·Q d1/4 , âˆš 2Â·K d1/4 ,v,1,Ï„ are defined as in Equation (9). We prove this theorem in Section 8.3. The runtime bound in Theorem 3.5 can be simplified for datasets Q, K with bounded diameter as follows, Corollary 3.6 (Simplified Runtime for Bounded Diameter Datasets) . For any datasets Q, K with diameter maxi,jâˆˆ[n] âˆ¥ki âˆ’ qjâˆ¥2 2 = Î³ âˆš d log n for some Î³ > 0, the runtime of Theorem 3.5 is upper bounded by O \u0000 m + nd Â· \u0000 nÏ„(1+Î³) + Îµâˆ’2nÏ„(1+Î³/2)\u0001\u0001 , which is strongly sub-quadratic in n. In particular, if Î³ = o(1), the runtime is bounded by O \u0000 m + Îµâˆ’2d Â· n1+Ï„+o(1)\u0001 . We prove Corollary 3.6 in Section 8.4. The current best value for Ï„ is Ï„ = 0.173 + o(1) due to Charikar et al.[7], thus, for any datasets of queries Q and keys K with diametermaxi,jâˆˆ[n] âˆ¥ki âˆ’ qjâˆ¥2 2 = o( âˆš d log n), our algorithmâ€™s runtime is O \u0000 m + Îµâˆ’2d Â· n1.173+o(1)\u0001 . 3.4 Practical Improvements by Exploiting Sparsity Our method relies on a sampling-based AMM (Lemma 3.2) and the number of samples m is proportional to srank(Dâˆ’1A) by Theorem 3.5. Here, we propose a practical technique for reducing the stable rank of D âˆ’1A by finding and subtracting off its â€œheavyâ€ elements. Specifically, recall that srank(Dâˆ’1A) = âˆ¥Dâˆ’1Aâˆ¥2 F âˆ¥Dâˆ’1Aâˆ¥2 op and the softmax matrix D âˆ’1A is dominated by its largest elements which correspond to the nearest pairs of queries qi and keys kj. Therefore, subtracting off the heavy elements of Dâˆ’1A reduces \r\rDâˆ’1A \r\r2 F which in turn can reduce srank(Dâˆ’1A). Similar to Reformer [20], we employ a Locality Sensitive Hashing (LSH) scheme to find dominant entries of the attention matrix A. Specifically, let H : Rd â†’ [B] be an LSH function with B buckets such that the collision probability Pr[H(qi) = H(kj)] is â€œroughlyâ€ proportional to âŸ¨qi, kjâŸ©. Given such LSH function, we define the sparse approximation to A as well as the residual attention matrix as: âˆ€i, jâˆˆ [n] : [A spar]i,j := e âŸ¨qi,kjâŸ©âˆš d Â· 1 {H(qi)=H(kj)} Ares := A âˆ’ Aspar. (10) Intuitively, the stable rank of D âˆ’1Ares is expected to be smaller than that of D âˆ’1A because the former has a considerably smaller Frobenius norm. We verify this intuition by plotting the singular values distributions of the softmax matrix D âˆ’1A and the residual D âˆ’1Ares for two real-world 100 512 1024 1536 2048 index (i) 0.0 0.5 1.0 singular values (Ïƒi) Ïƒi ( Dâˆ’1A) ) srank ( Dâˆ’1A ) Ïƒi ( Dâˆ’1Ares ) srank ( Dâˆ’1Ares ) (a) GloVe dataset 0 784 1568 2352 3136 index (i) 0.0 0.5 1.0 singular values (Ïƒi) Ïƒi ( Dâˆ’1A) ) srank ( Dâˆ’1A ) Ïƒi ( Dâˆ’1Ares ) srank ( Dâˆ’1Ares ) (b) T2T-ViT on ImageNet Figure 2: Singular values distribution and stable rank of the softmax matrix D âˆ’1A versus those of the residual Dâˆ’1Ares. The stable rank of the residual matrix is significantly smaller. Algorithm 3 Practical Improvement of KDEformer 1: input: matrices Q , K, V âˆˆ RnÃ—d, integer m, Îµ >0, and LSH function H : Rd â†’ [B] 2: compute Î±, Î², Î³as per lines 2-4 of Algorithm 1 3: pj â† Î²j âˆ’ Pn i=1 Î±âˆ’2 j e 2âŸ¨qi,kjâŸ©âˆš d Â· 1 {H(qi)=H(kj)} + Î³ âˆ¥vjâˆ¥2 2 for every j âˆˆ [n] then normalize pâ„“ â† pâ„“P jâˆˆ[n] pj for every â„“ âˆˆ [n] 4: generate the sampling matrix Î  res as per lines 6-7 of Algorithm 1 using distribution {pj}jâˆˆ[n] computed above 5: return eD = diag(Î±) and Î res instances in Figure 2. Figure 2(a) corresponds to when keys and queries are the first n = 2,048 vectors from GloVe word embedding dataset [ 21]. In Figure 2(b), we focused on the first attention layer in Tokens-to-token Vision Transformer (T2T-ViT) [33] and an arbitrary batch of images from ImageNet dataset. In both instances, the singular values of the residual D âˆ’1Ares decay faster than that of Dâˆ’1A while the largest singular value (spectral norm) of both matrices are equal to one. Thus, as shown in Figure 2, subtracting off the sparse component D âˆ’1Aspar reduces the stable rank significantly. Building upon this observation, we propose a new version of Algorithm 1 with improved practical performance. We start by using Equation (10) to write: Att(Q, K, V) = Dâˆ’1AsparV + Dâˆ’1AresV. (11) Given D, the first term above can be computed in time O(d Â· nnz(Aspar)), where nnz(Â·) denotes the number of nonzero entries of a matrix. By choosing an appropriate LSH we can ensure that nnz(Aspar) is almost linear in n. The second term in Equation (11) can be approximated via AMM, similar to what was done in Algorithm 1, however, we need to be able to estimate the column norms of D âˆ’1Ares. Fortunately, by Equation (10), we have \r\r\rDâˆ’1Aj res \r\r\r 2 2 = \r\rDâˆ’1Aj\r\r2 2 âˆ’ \r\r\rDâˆ’1Aj sparse \r\r\r 2 2 , where Aj res, Aj, Aj sparse denote the jth columns of Ares, A, Aspar, respectively. Since we can estimate the column norms of D âˆ’1A efficiently using WExpKDE and all column norms of Dâˆ’1Aspar can be computed in total nnz(Aspar) time, the AMM sampling matrix for residual Î  res can be generated quickly. 11D âˆ’1 A â‡’ D âˆ’1 A spar + D âˆ’1 A res Î  âŠ¤ res Figure 3: The softmax matrix D âˆ’1A decomposes into its sparse approximation D âˆ’1Aspar, which captures large entries (coded with darker colors), and the residual D âˆ’1Ares, where black cells represent entries captured by Dâˆ’1Aspar. Blank colors in D âˆ’1Ares represent columns not sampled by AMM sampling matrix Î  res. 2 8 32 128 512 GFLOPS 0.0 0.2 0.4 relative spectral error 2 8 32 memory usage (GiB) 0.0 0.2 0.4 relative spectral error 64 256 1024 4096 feature dimension 101 103 CPU-clock time (sec) 256 1024 4096 16384 sequence length n 100 101 memory usage (GiB) Reformer Performer ScatterBrain KDEformer Exact Figure 4: Performance evaluations of various self-attention approximations on approximating under the GloVe word embeddings. Putting everything together, we first choose an appropriate LSH function H and compute the sparse approximation to the attention matrix as per Equation (10). We show how to design a GPU-friendly LSH whose collision probability Pr[H(qi) = H(kj)] is roughly proportional to âŸ¨qi, kjâŸ© in Section 7. Next, we compute a spectral proxy eD for D, as was done efficiently in Algorithm 1. Finally, we perform AMM on matrices eDâˆ’1Ares and V via a sampling matrix Î  res. The resulting estimator is: gAtt = eDâˆ’1AsparV + eDâˆ’1AresÎ âŠ¤ res Â· Î resV. We illustrate this procedure in Figure 3 and present the pseudocode for computing eD and Î res in Algorithm 3. By an analysis similar to Corollary 3.6, we find that the runtime of Algorithm 3 is O(m + Îµâˆ’2dn1+Ï„+o(1) + nnz(Aspar)) with some m = O \u0000 Îµâˆ’2 log n Â· srank(Dâˆ’1Ares) \u0001 . 4 Experiments 4.1 Single Self-attention Layer Approximation We first benchmark our algorithm on approximating a single self-attention layer, i.e., Att(Q, K, V). We randomly select a pair of matrices Q , V âˆˆ RnÃ—d from the GloVe word embeddings [ 21] with sequence length n = 8,192 and dimension d = 100 and set K = Q. We compare our KDEformer to other attention approximations including Reformer [ 20], Performer [12], and ScatterBrain [ 10]. We compute the relative error under the operator norm, i.e., âˆ¥Att(Q,K,V)âˆ’gAttâˆ¥op âˆ¥Att(Q,K,V)âˆ¥op where gAtt âˆˆ RnÃ—d is an 12Table 1: Results on image generation using BigGAN with the exact attention and its approximations. Bold values indicate the best within the standard deviation. Method FID ( â†“) IS ( â†‘) GFLOPS Exact 32.17 58.38 Â± 4.23 10.738 âˆ’ Reformer 72.39 19.04 Â± 2.32 10.872 (0.99 Ã—) Performer 33.39 37.32 Â± 2.91 1.682 (6.38Ã—) ScatterBrain 38.55 36.43 Â± 3.34 2.891 (3.71 Ã—) KDEformer 31.41 58.16 Â± 4.04 2.596 (4.14 Ã—) approximate attention, and measure the peak memory usage, FLOP count and CPU-clock time while varying hyperparameters of algorithms which affect both the runtime and memory space. In Figure 4, we observe that our proposed algorithm achieves the lowest error with minimal FLOP count and memory usage. In particular, our approximation error can be about 9% with 3.06 Ã— memory reduction and 5.11Ã— lower FLOPS. In addition, we plot CPU-clock time for various choices of hyperparameters that determine peak memory usage. Specifically, if the approximation requires at most nk memory space for computing gAtt and we call k as the feature dimension. Given the same feature dimension, our algorithm and Performer are the fastest methods, but Performer has significantly larger errors than the others. We fix the feature dimension k = 128 and measure the peak memory usage while the sequence length n is changing from 256 and 16 ,384. For n = 16,384, our method can save up to 19 .62Ã— memory space compared to the exact computation. 4.2 Image generation with BigGAN We next apply above-mentioned attention approximations to generate synthetic images with Big- GAN [3]. The model contains a single attention layer where the corresponding inputs have different dimensions: Q âˆˆ R4,096Ã—64, K âˆˆ R1,024Ã—64 and V âˆˆ R1,024Ã—256. Following the experiments in [ 10], we use the pre-trained BigGAN 1 on ImageNet at 512 Ã— 512 resolution and replace the exact attention with its approximations. We generate 5,000 fake images and compute the Frechet Inception Distance (FID) with ImageNet validation set as ground truth and Inception Scores (IS) [ 24]. Note that lower FID and higher IS values imply better generation quality. We also calculate FLOPS for operations in the attention layer. We set the hyperparameters (i.e., feature dimensions) so that all approximation methods have the same peak memory usage. The results are reported in Table 1. Interestingly, our algorithm shows a lower FID value than the exact attention with 4 .14Ã— fewer FLOPs. Although Performer is the fastest algorithm, its generated images are unnatural compared while our attention can generate more realistic images. A number of generated images by various methods can be found in the Section 9. 4.3 ImageNet classification with Vision Transformer Finally, we evaluate the attention approximations on image classification with Tokens-to-Token Vision Transformer2 [33]. The model consists of Tokens-to-Token (T2T) module and the Vision Transformer (ViT) backbone where the computational bottleneck comes from the T2T module. 1https://github.com/huggingface/pytorch-pretrained-BigGAN 2https://github.com/yitu-opensource/T2T-ViT 13Table 2: Results on ImageNet classification using T2T-ViT with the exact attention and its approximations. Method Top-1 Accuracy (%) GFLOPS Exact 82.55 161.10 âˆ’ Reformer 81.44 11.71 (13.75 Ã—) Performer 80.50 5.06 (31.87 Ã—) ScatterBrain 81.95 7.18 (22.43 Ã—) KDEformer 82.08 8.80 (18.30 Ã—) Again, we use the pre-trained model with 24 layers in ViT backbone and apply our method to 2 attention layers in the T2T module as a drop-in replacement. The dimensions of Q , K, V are all the same, n = 3,136, d= 64 in the first layer and n = 784, d= 64 in the second layer. We compute top-1 accuracy on ImageNet validation dataset and measure FLOPS in the first attention layer, which requires the most resources. The results are shown in Table 2. Observe that our method is the best among all approximate methods with 82 .08% test accuracy. In particular, it leads to less than 1% performance drop compared to the exact computation but the required operations are 18 .3Ã— fewer. Such performance gains would increase when token sequence lengths are larger. 4.4 End-to-end Training with Long Range Arena Benckmark Finally, to demonstrate the power of our method in reducing the training time of transformer models, we run end-to-end training on the Long Range Arena benchmark [ 28], which contains 5 classification datasets, i.e., ListOps, Text, Image, Retrieval and Pathfinder. The maximum sequence lengths of these datasets are 2 ,048, 4,096, 1,024, 4,096 and 1,024, respectively. We follow the same settings from [11]; model is a 2-layer transformer with 64 embedding dimension, 128 hidden dimension, 2 attention heads, and mean pooling is used for the classification task. Learning rate is set to 10 âˆ’4 for Text, ListOps, Image and 2 Ã— 10âˆ’4 for the rest. All models are trained for 50 ,000 steps. Similar to Section 4.1, we choose hyperparameters of all methods having equal feature dimensions to 128. In Table 3, we provide results on (a) test accuracy, (b) peak memory and (c) wall-clock time per batch of single training step (including forward and backward propagations). As a result, we observe that the proposed KDEformer achieves the second-best test accuracy in average followed by Reformer, but it requires much less memory as well as faster wall-clock time than other competitors. For example, KDEformer with Text dataset runs about 8 Ã— faster than the exact attention. 5 Conclusion We propose a fast attention approximation based on recent advances in KDE solvers. The proposed algorithm can run in strongly sub-quadratic time in sequence length and provide an error bound under the spectral norm. It shows promising performances under various practical applications involving long-sequence attention. We believe this can have a significant impact on other practical problems as well. 14Table 3: Results on end-to-end training on 5 Long Range Arena (LRA) benchmark datasets. ListOps Text Image Retrieval Pathfinder Average Exact 33.32 60.22 37.41 81.07 70.25 56.45 Reformer 36.74 61.39 43.59 78.15 66.25 57.22 Performer 37.75 58.81 35.74 80.39 62.84 55.11 KDEformer 36.64 62.00 45.45 73.52 68.13 57.15 (a) Test accuracy (%) ListOps Text Image Retrieval Pathfinder Average Exact 6.53 16.71 9.41 8.72 4.70 9.21 Reformer 1.59 3.18 6.36 2.94 3.18 3.45 Performer 1.07 2.13 4.28 2.15 2.14 2.35 KDEformer 1.02 2.03 4.08 2.38 1.87 2.28 (b) Peak memory (GB) ListOps Text Image Retrieval Pathfinder Average Exact 0.133 0.479 0.276 0.478 0.141 0.301 Reformer 0.041 0.081 0.155 0.092 0.082 0.090 Performer 0.036 0.067 0.127 0.074 0.068 0.074 KDEformer 0.034 0.058 0.110 0.073 0.063 0.068 (c) Wall-clock time (sec) per batch 6 Acknowledgement We would like to thank Navid Nouri for his helpful ideas and discussions about new advancements in kernel density estimation and their potential application. Amir Zandieh was supported by the Swiss NSF grant No. P2ELP2 195140. Amin Karbasi acknowledges funding in direct support of this work from NSF (IIS-1845032), ONR (N00014- 19-1-2406), and the AI Institute for Learning-Enabled Optimization at Scale (TILOS). References [1] Arturs Backurs, Moses Charikar, Piotr Indyk, and Paris Siminelakis. Efficient density evaluation for smooth kernels. In Foundations of Computer Science (FOCS) , 2018. [2] Arturs Backurs, Piotr Indyk, and Tal Wagner. Space and time efficient kernel density estimation in high dimensions. Neural Information Processing Systems (NeurIPS) , 2019. [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large Scale GAN Training for High Fidelity Natural Image Synthesis. In International Conference on Learning Representations (ICLR), 2019. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, 15Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Neural Information Processing Systems (NeurIPS) , 2020. [5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Proceedings of the European Conference on Computer Vision(ECCV) , 2020. [6] Moses Charikar and Paris Siminelakis. Hashing-based-estimators for kernel density in high dimensions. In Foundations of Computer Science (FOCS) , 2017. [7] Moses Charikar, Michael Kapralov, Navid Nouri, and Paris Siminelakis. Kernel density estimation through density constrained near neighbor search. In Foundations of Computer Science (FOCS), 2020. [8] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. MONGOOSE: A learnable LSH framework for efficient neural network training. In International Conference on Learning Representations (ICLR), 2020. [9] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models. In International Conference on Learning Representations (ICLR) , 2021. [10] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Re. Scatterbrain: Unifying sparse and low-rank attention. Neural Information Processing Systems (NeurIPS) , 2021. [11] Yifan Chen, Qi Zeng, Heng Ji, and Yun Yang. Skyformer: Remodel self-attention with gaussian kernel and nystr\\â€ om method. Neural Information Processing Systems (NeurIPS) , 2021. [12] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking Attention with Performers. In International Conference on Learning Representations (ICLR), 2021. [13] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf-efficient attention using asymmetric clustering. Neural Information Processing Systems (NeurIPS) , 2020. [14] Jyotikrishna Dass, Shang Wu, Huihong Shi, Chaojian Li, Zhifan Ye, Zhongfeng Wang, and Yingyan Lin. Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with a linear taylor attention. arXiv preprint arXiv:2211.05109 , 2022. [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Association for Computational Linguistics (NAACL) , 2018. [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations (ICLR) , 2021. 16[17] Torben Hagerup, Kurt Mehlhorn, and J Ian Munro. Maintaining discrete probability distri- butions optimally. In International Colloquium on Automata, Languages, and Programming , 1993. [18] Sarang Joshi, Raj Varma Kommaraji, Jeff M Phillips, and Suresh Venkatasubramanian. Comparing distributions and shapes using the kernel distance. In Symposium on Computational Geometry (SOCG), 2011. [19] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning (ICML) , 2020. [20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The Efficient Transformer. In International Conference on Learning Representations (ICLR) , 2020. [21] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP) , 2014. [22] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research (JMLR) , 2020. [23] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics (ACL), 2021. [24] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Neural Information Processing Systems (NeurIPS) , 2016. [25] Bernhard SchÂ¨ olkopf, Alexander J Smola, Francis Bach, et al.Learning with kernels: support vector machines, regularization, optimization, and beyond . MIT press, 2002. [26] Paris Siminelakis, Kexin Rong, Peter Bailis, Moses Charikar, and Philip Levis. Rehashing kernel evaluation in high dimensions. In International Conference on Machine Learning (ICML) , 2019. [27] Zhiqing Sun, Yiming Yang, and Shinjae Yoo. Sparse Attention with Learning to Hash. In International Conference on Learning Representations (ICLR) , 2021. [28] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference on Learning Representations (ICLR) , 2021. [29] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys , 2022. [30] Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and TrendsÂ® in Machine Learning, 2015. 17[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Neural Information Processing Systems (NeurIPS), 2017. [32] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Neural Information Processing Systems (NeurIPS), 2019. [33] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In International Conference on Computer Vision (ICCV) , 2021. [34] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Neural Information Processing Systems (NeurIPS) , 2020. [35] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Conference on Artificial Intelligence (AAAI) , 2021. [36] Anastasios Zouzias. Randomized primitives for linear algebra and applications . University of Toronto, 2013. 7 Practical Angular LSH with Fixed Bucket Sizes The practical version of our algorithm that we presented in Section 3.4 requires a locality sensitive hashing H : Rd â†’ [B] for identifying the dominant entries of the attention matrix A, which correspond to pairs of keys and queries whose â€œangular distancesâ€ are small. In this section, we develop a simple yet effective and practical LSH function whose collision probability is related to the angular distance between hashed points. While the lsh allows computing a very sparse approximation to the attention matrix, uneven bucket sizes hinder batching of the computations across lsh buckets. In fact, if we parallelize the computation across buckets, the largest bucket determines the runtime [ 20]. Our proposed lsh function has equal-sized buckets, thus, it aligns with modern hardwareâ€™s block-memory access and can be efficiently parallelized by batching across buckets. We start by defining a simple LSH function whose collision probability is roughly proportional to the angle between the hashed points. Definition 7.1 (Angular LSH). For positive integers d, r, let w1, w2, . . . wr be i.i.d. random samples from the tropical Gaussian distribution N(0, Id). We define the rank-r angular LSH h : Rd â†’ {0, 1}r as follows: h(x) := \u0010 1 {wâŠ¤ 1 x}, 1 {wâŠ¤ 2 x}, . . .1 {wâŠ¤r x} \u0011 for any x âˆˆ Rd. Note that the buckets are labeled by r-bit binary numbers and if r â‰¤ d then almost surely the total number of buckets is 2r. It is easy to calculate the collision probability of the angular lsh defined in Definition 7.1. 18Claim 1. For positive integers r, dlet h(Â·) be an instance of rank-r angular LSH as per Definition 7.1. For any x, yâˆˆ Rd the collision probability of h(x) and h(y) is: Pr[h(x) = h(y)] = \u0012 1 âˆ’ Î¸x,y Ï€ \u0013r , where Î¸x,y = cosâˆ’1 \u0010 xâŠ¤y âˆ¥xâˆ¥Â·âˆ¥yâˆ¥ \u0011 denotes the angle between x and y. Therefore, the points with small angular distances are likely to be hashed to the same buckets while points with large angular distances are unlikely to be hashed to the same buckets. So, if we hash keys kj and queries qi using the angular lsh given in Definition 7.1 then the entries of the attention matrix A which correspond to colliding pairs of keys and queries will likely have very large values. As we mentioned earlier, the main efficiency bottleneck in this lsh-based approach for computing the dominant entries of the attention matrix is the unevenness of hash bucket sizes. If we try to compute the sparse approximation to A, as defined in Equation (10), using the lsh function from Definition 7.1 by parallelizing the computation across buckets, the runtime will be dominated by the time to compute entries in the largest bucket. One solution for increasing efficiency, which was proposed in [ 20], is to truncate the lsh buckets and force them to contain equal number of keys and queries. However, truncation can degrade the quality of approximation drastically because there will be spillover from one bucket to another, and some points can be forced into far-away buckets. The reason for this spillover effect is the fact that consecutive buckets in a hash table do not necessarily represent areas of the Rd space which are geometrically close to each other. We show that in fact, it is possible to sort the buckets of the angular lsh from Definition 7.1 such that the order of buckets reflects their geometrical position, thus, consecutive buckets actually represent neighboring partitions of Rd. It turns out that the geometric distance between two buckets of this lsh function translates into the Hamming distance between their binary labels. To be precise, for any binary numbers b1, b2 âˆˆ {0, 1}r let dH(b1, b2) âˆˆ [r + 1] represent the Hamming distance between the two, i.e., the number of bits where b1 and b2 differ. Now note that the lsh buckets in Definition 7.1 are labeled with r-bit binary numbers. Each bit in the binary representations of buckets corresponds to a partitioning of the Rd into two sides of a random hyperplane whose normal vector is sampled from a tropical Gaussian. Therefore, if we have two buckets b1 and b2 with hamming distance dH(b1, b2) = 1 then these buckets are positioned on the same sides of all random hyperplanes except for one, thus, they represent neighboring regions in Rd and the hyperplanes corresponding to the differing bit of b1 and b2 is the boundary between two regions. We show this fact in Figure 5(a), which illustrates the space partitions corresponding to the buckets of a rank-2 angular lsh in dimension d = 2. It is clearly visible that the bucket labels of neighboring partitions have unit Hamming distance. In Figure 5(b) we hash an example dataset using this LSH function and as can be seen, the buckets have uneven sizes. Because of the relationship between the Hamming distance of bucket labels and the distance between space partitions, if we order the dataset according to the Hamming ordering of their buckets and then truncate them we get new buckets with even sizes and minimal spillover effect. In particular, in Figure 5(c) we order the dataset such that the points from buckets 00 , 01, 11, 10 come in this specific order and then we bin the data points by partitioning the ordered dataset into equal-sized parts. The resulting bins show no spillover effect. 19âˆ’1 âˆ’0.5 0 0.5 1âˆ’1 âˆ’0.5 0 0.5 1 y=x/3 y=âˆ’x 00 11 0110 (a) Space partitions by angular LSH âˆ’1 âˆ’0.5 0 0.5 1âˆ’1 âˆ’0.5 0 0.5 1 y=x/3 y=âˆ’x (b) Hashing an example dataset âˆ’1 âˆ’0.5 0 0.5 1âˆ’1 âˆ’0.5 0 0.5 1 y=x/3 y=âˆ’x (c) Buckets truncation in Hamming dis- tance order Figure 5: Rank-2 Angular LSH in action (in dimension d = 2). The space partitions corresponding to buckets with unit Hamming distance are neighbors in Rd. In Figure 5(b) we hash an example dataset and we get uneven buckets. Figure 5(c) show that if we order the dataset according to the Hamming distance of their buckets and then truncate the buckets we get new equal-sized buckets with minimal spillover effect. In the following lemma we show how to order r-bit binary numbers {0, 1}r such that all consecutive numbers have unit Hamming distance: Lemma 7.2 (Ordering of binary numbers according to their Hamming distance) . For any positive integer r it is possible to order the set of binary numbers {0, 1}r as a sequence b1, b2, . . . b2r such that for any j âˆˆ [2r âˆ’ 1]: dH(bj, bj+1) = 1. Proof. The proof is by induction. For r = 1 the base of induction follows trivially. Now suppose that we have the sequence of ( r âˆ’ 1)-bit numbers bâ€² 1, bâ€² 2, . . . bâ€² 2râˆ’1 such that dH(bâ€² j, bâ€² j+1) = 1 for any j âˆˆ [2râˆ’1 âˆ’ 1]. Then the sequence of r-bit numbers will be as follows: bj := ( (bâ€² j, 0) if j â‰¤ 2râˆ’1 (bâ€² 2r+1âˆ’j, 1) if j >2râˆ’1 for j âˆˆ [2r]. One can verify that this sequence satisfies the desired property and the proof is complete. Therefore, we can use the angular LSH together with the ordering of binary numbers from Lemma 7.2 to construct an effective hash function with equal-sized buckets. Definition 7.3 (Equal-sized LSH with Minimal Spillover). Suppose that we want to hash a dataset x1, x2, . . . xn âˆˆ Rd. 1. Hash these points using a rank- r Angular LSH h(Â·) as per Definition 7.1. 2. Then, using Lemma 7.2, produce an ordering of r-bit binary numbers such that consecutive numbers have unit Hamming distance; let b1, b2, . . . b2r be such ordering. 3. Next, define a permutation P âˆˆSym(n) which orders the dataset according to the Hamming ordering of their buckets. More specifically, P satisfies: P(i) < P(j) iff h(xi) â‰¤âˆ— h(xj), where the inequality â‰¤âˆ— acts with respect to the ordering b1, b2, . . . b2r . 20A q1 q2 q3 q4 q5 q6 q7 q8 q9 k1 k2 k3 k4 k5 k6 k7 k8 k9 â‡’ P AP q2 q8 q5 q4 q3 q6 q7 q9 q1 k2 k8 k4 k7 k3 k6 k9 k1 k5 â‡’ Pâˆ’1 Asparb1 b2 b3 b4 Figure 6: An example of how A spar can be computed efficiently. (Left) keys and queries are hashed using the angular lsh function. buckets are represented by shades of violet. (Middle) keys and queries are permuted such that their buckets are sorted according to the Hamming distance ordering. Large entries of the permuted attention matrix A P are concentrated around the diagonal blocks, so we compute the diagonal blocks. (Right) the block diagonal approximation to A P is reverse permuted to obtain A spar. 4. Permute x1, x2, . . . xn according to P and then partition the sequence into equal-sized chunks. These chunks are the buckets. Now we explain how we can use the lsh procedure given in Definition 7.3 to compute A spar as per Equation (10) through an example shown in Figure 6. We first hash keys kj and queries qi via the angular lsh. We represent the buckets of this hashing via different shades of violet in Figure 6. Clearly, the bucket sizes are uneven. Then we permute keys and queries via P which orders the points such that their buckets are sorted according to the ordering b1, b2, b3, b4 obtained from Lemma 7.2. Then we truncate the sorted points which is in fact equivalent to selecting blocks along the diagonal of the permuted attention matrix. The selected diagonal blocks in Figure 6 illustrate this. Finally, we can reverse the permutation on the rows and columns of the block diagonal attention which gives us the final A spar. 8 Omitted Proofs 8.1 Proof of Lemma 3.2: Approximate Matrix Multiplication via Sampling In this section, we analyze the random sampling method for approximately computing the product of two rectangular matrices, presented in Lemma 3.2. The proof of this lemma is based on the following version of the matrix Bernstein inequality. Lemma 8.1 (Matrix Approximation by Random Sampling, Corollary 6.2.1 from [ 30]). Let B be a fixed q Ã— d matrix. Construct a q Ã— d random matrix R that satisfies E[R] = B, and âˆ¥Râˆ¥op â‰¤ L. 21Compute the per-sample second moment: m2(R) = max{âˆ¥E[Râˆ—R]âˆ¥op , âˆ¥E[RRâˆ—]âˆ¥op}. Form the matrix sampling estimator Rm = 1 m mX i=1 Ri where each Ri is an independent copy of R. Then for every t >0, the estimator satisfies Pr h\r\rRm âˆ’ B \r\r op â‰¥ t i â‰¤ (q + d) Â· exp \u0012 âˆ’mt2/2 m2(R) + 2Lt/3 \u0013 . Now we prove Lemma 3.2 by invoking the above matrix Bernstein inequality. Lemma 3.2 (Approximate Matrix Multiplication (AMM)). For any matrices X âˆˆ RnÃ—q, Y âˆˆ RnÃ—d and any probability distribution {pi}iâˆˆ[n] which satisfies pi â‰¥ 1 4 Â· âˆ¥xiâˆ¥2 2+Î³Â·âˆ¥yiâˆ¥2 2 âˆ¥Xâˆ¥2 F +Î³Â·âˆ¥Yâˆ¥2 F for all i âˆˆ [n] and Î³ = âˆ¥Xâˆ¥2 op / âˆ¥Yâˆ¥2 op, a sampling matrix Î  âˆˆ RmÃ—n constructed by first generating m i.i.d. samples â„“1, â„“2, . . . â„“m âˆˆ [n] according to {pâ„“}â„“âˆˆ[n] and then letting the rth row of Î  be 1âˆšmÂ·pâ„“r Â· eâŠ¤ â„“r , if m = â„¦ \u0000 Îµâˆ’2 log n Â· (srank(X) + srank(Y)) \u0001 for some Îµ >0, the following holds, Pr \u0014\r\r\rXâŠ¤Î âŠ¤Î Y âˆ’ XâŠ¤Y \r\r\r op > Îµâˆ¥Xâˆ¥op âˆ¥Yâˆ¥op \u0015 â‰¤ 1 poly(n). Proof. First we let B := XâŠ¤Y. Then we let the random matrix R have the following distribution Pr \u0014 R = xâŠ¤ i Â· yi pi \u0015 = pi for i âˆˆ [n] where xi and yi are ith row vector in X and Y, respectively. With this definition we have, E[R] = X iâˆˆ[n] xâŠ¤ i Â· yi pi Â· pi = X iâˆˆ[n] xâŠ¤ i Â· yi = XâŠ¤Y = B. Furthermore, we can bound the operator norm of R as follows, âˆ¥Râˆ¥op â‰¤ max iâˆˆ[n] \r\rxâŠ¤ i Â· yi \r\r op pi = max iâˆˆ[n] âˆ¥xiâˆ¥2 âˆ¥yiâˆ¥2 pi â‰¤ 4 Â· max iâˆˆ[n] âˆ¥xiâˆ¥2 âˆ¥yiâˆ¥2 Â· \u0010 âˆ¥Xâˆ¥2 F + Î³ Â· âˆ¥Yâˆ¥2 F \u0011 âˆ¥xiâˆ¥2 2 + Î³ Â· âˆ¥yiâˆ¥2 2 â‰¤ 2 Â· max iâˆˆ[n] 1âˆšÎ³ Â· âˆ¥Xâˆ¥2 F + âˆšÎ³ Â· âˆ¥Yâˆ¥2 F = 2 âˆ¥Xâˆ¥op Â· âˆ¥Yâˆ¥op Â· (srank(X) + srank(Y)) â‰¡ L, 22where the third line above follows from the precondition of Lemma 3.2 about the distribution {pi}iâˆˆ[n] and the fourth line follows from AM-GM inequality. The last line follows from the definition of Î³ and definition of stable rank. Next, we will compute the per-sample second moment as follows, E[Râˆ—R] = X iâˆˆ[n] âˆ¥xiâˆ¥2 2 Â· yâŠ¤ i Â· yi p2 i Â· pi = X iâˆˆ[n] âˆ¥xiâˆ¥2 2 Â· yâŠ¤ i Â· yi pi âª¯ 4 Â· \u0010 âˆ¥Xâˆ¥2 F + Î³ Â· âˆ¥Yâˆ¥2 F \u0011 Â· X iâˆˆ[n] âˆ¥xiâˆ¥2 2 âˆ¥xiâˆ¥2 2 + Î³ Â· âˆ¥yiâˆ¥2 2 Â· yâŠ¤ i yi âª¯ 4 Â· \u0010 âˆ¥Xâˆ¥2 F + Î³ Â· âˆ¥Yâˆ¥2 F \u0011 Â· X iâˆˆ[n] yâŠ¤ i yi = 4 Â· \u0010 âˆ¥Xâˆ¥2 F + Î³ Â· âˆ¥Yâˆ¥2 F \u0011 Â· YâŠ¤Y. Similarly, E[RRâˆ—] âª¯ 4 Â· \u0010 âˆ¥Xâˆ¥2 F /Î³ + âˆ¥Yâˆ¥2 F \u0011 Â· XâŠ¤X. In summary, m2(R) = max{âˆ¥E[Râˆ—R]âˆ¥op , âˆ¥E[RRâˆ—]âˆ¥op} â‰¤ 4 Â· max \u001a\u0010 âˆ¥Xâˆ¥2 F + Î³ Â· âˆ¥Yâˆ¥2 F \u0011 Â· \r\r\rYâŠ¤Y \r\r\r op , \u0010 âˆ¥Xâˆ¥2 F /Î³ + âˆ¥Yâˆ¥2 F \u0011 Â· \r\r\rXXâŠ¤ \r\r\r op \u001b = 4 Â· âˆ¥Xâˆ¥2 op âˆ¥Yâˆ¥2 op Â· (srank(X) + srank(Y)) . Finally, we note that, from the way the sampling matrix was constructed we have X âŠ¤Î âŠ¤Î Y = 1 m P râˆˆ[m] xâ„“r Â·yâ„“r pir = Rm. Thus, by invoking Lemma 8.1 we find that for t = Îµ Â· âˆ¥Xâˆ¥op âˆ¥Yâˆ¥op we have, Pr h\r\rRm âˆ’ B \r\r op â‰¥ Îµ Â· âˆ¥Xâˆ¥op âˆ¥Yâˆ¥op i â‰¤ (q + d) Â· exp \u0012 âˆ’mt2/2 m2(R) + 2Lt/3 \u0013 â‰¤ 1 poly(n). This completes the proof of Lemma 3.2. 8.2 Proof of Theorem 3.3 Theorem 3.3 (Correctness of Algorithm 1) . For any matrices Q, K, V âˆˆ RnÃ—d, any Îµ >0, and number of samples m = â„¦ \u0000 Îµâˆ’2 log n Â· (srank(Dâˆ’1A) + srank(V)) \u0001 , given access to a primitive WExpKDE as per Definition 3.1, Algorithm 1 outputs a diagonal matrix eD âˆˆ RnÃ—n and a sampling matrix Î  âˆˆ RmÃ—n which satisfy Equation (1) with probability at least 1 âˆ’ 1 poly(n) . Proof. First, note that all entries of D âˆ’1A are positive and the sum of entries of each row of this matrix equals 1, so by the Gershgorin circle theorem \r\rDâˆ’1A \r\r op â‰¤ 1. On the other hand, Dâˆ’1A Â· 1n = 1n, so we have \r\rDâˆ’1A \r\r op = 1. We will use this fact in the rest of the proof. Now note that Algorithm 1 computes Î± = WExpKDE \u0010 K d1/4 , Q d1/4 , 1n, Îµ 3 \u0011 in line 3 and lets eD = diag(Î±). Thus, as we showed earlier, by Definition 3.1 and using the fact that entries of D are positive, we have (1 âˆ’ Îµ/3)D âª¯ eD âª¯ (1 + Îµ/3)D. So, using this inequality along with the fact that\r\rDâˆ’1A \r\r op = 1, the diagonal matrix eD satisfies Equation (5). 23Next, let us consider the vector Î² = WExpKDE \u0010âˆš 2Â·Q d1/4 , âˆš 2Â·K d1/4 , u,1/3 \u0011 computed in line 4. For ease of notation, let X âŠ¤ := eDâˆ’1A. By Definition 3.1 and using the definition of ui = 1/Î±2 i in line 3, we have, Î²j âˆˆ (1 Â± 1/3) Â· X iâˆˆ[n] ui Â· exp \u0012 2âˆš d âŸ¨qi, kjâŸ© \u0013 = (1 Â± 1/3) Â· âˆ¥xjâˆ¥2 2 for any j âˆˆ [n]. Also, note that Î³ which is computed in line 2 of the algorithm is equal to Î³ = âˆ¥Dâˆ’1Aâˆ¥ 2 op âˆ¥Vâˆ¥2 op . Because (1 âˆ’Îµ/3)D âª¯ eD âª¯ (1 +Îµ/3)D, we have Î³ âˆˆ (1 Â±Îµ/3)âˆ’1 Â· ËœÎ³, where ËœÎ³ := \r\r\reDâˆ’1A \r\r\r 2 op / âˆ¥Vâˆ¥2 op. Therefore, the distribution {pi}iâˆˆ[n] computed in line 5 satisfies, pâ„“ = Î²â„“ + Î³ Â· âˆ¥vâ„“âˆ¥2 2P jâˆˆ[n] Î²j + Î³ Â· âˆ¥Vâˆ¥2 F â‰¥ 1 4 Â· âˆ¥xâ„“âˆ¥2 2 + ËœÎ³ Â· âˆ¥vâ„“âˆ¥2 2 âˆ¥Xâˆ¥2 F + ËœÎ³ Â· âˆ¥Vâˆ¥2 F . Furthermore, note that srank(eDâˆ’1A) â‰¤ 2 Â· srank(Dâˆ’1A). Therefore, we can invoke the AMM result from Lemma 3.2 with matrices X âŠ¤ = eDâˆ’1A and Y = V and use the precondition of Theorem 3.3 about the number of samples m = â„¦ \u0000 Îµâˆ’2 log n Â· (srank(Dâˆ’1A) + srank(V)) \u0001 = â„¦ \u0010 Îµâˆ’2 log n Â· (srank(eDâˆ’1A) + srank(V)) \u0011 to conclude that the sampling matrix Î  computed in lines 6-7 satisfies the following with high probability in n: \r\r\reDâˆ’1AÎ âŠ¤ Â· Î V âˆ’ eDâˆ’1AV \r\r\r op â‰¤ Îµ 4 \r\r\reDâˆ’1A \r\r\r op âˆ¥Vâˆ¥op â‰¤ Îµ 2 \r\rDâˆ’1A \r\r op âˆ¥Vâˆ¥op , where the second inequality above follows from the fact that \r\r\reDâˆ’1A \r\r\r op â‰¤ 2 Â· \r\rDâˆ’1A \r\r op. The above inequality shows that Equation (6) holds with high probability in n. Thus the theorem follows from combining Equation (5) and Equation (6) using triangle inequality. 8.3 Proof of Theorem 3.5 Theorem 3.5 (Approximate Attention with Spectral Norm Bound) . For any matrices Q, K, V âˆˆ RnÃ—d, any Îµ > 0, and given a fast Gaussian KDE as per Theorem 2.1, there exists an algo- rithm that outputs a diagonal matrix eD âˆˆ RnÃ—n and a sampling matrix Î  âˆˆ RmÃ—n with m = O \u0000 Îµâˆ’2 log n Â· (srank(Dâˆ’1A) + srank(V)) \u0001 samples which satisfy Equation (1) with probability at least 1âˆ’ 1 poly(n) . The runtime of this algorithm is O \u0012 m + nd Â· \u0012 C K d1/4 , Q d1/4 ,1n,Îµ,Ï„ + Câˆš 2Â·Q d1/4 , âˆš 2Â·K d1/4 ,v,1,Ï„ \u0013\u0013 , where vj = \u0010P â„“âˆˆ[n] exp \u0010 1âˆš dâŸ¨qj, kâ„“âŸ© \u0011\u0011âˆ’2 for j âˆˆ [n] and C K d1/4 , Q d1/4 ,1n,Îµ,Ï„ , Câˆš 2Â·Q d1/4 , âˆš 2Â·K d1/4 ,v,1,Ï„ are defined as in Equation (9). Proof. It suffices to run Algorithm 1 with some m = O \u0000 Îµâˆ’2 log n(srank(Dâˆ’1A) + srank(V)) \u0001 samples and invoke Algorithm 2 for the calls to WExpKDE made in lines 3-4. By Theorem 3.3 and Theorem 3.4 along with union bound, the outputs Î  and eD of this procedure satisfy the desired condition of Equation (1) with probability â‰¥ 1 âˆ’ 1 poly(n) . 24Runtime Analysis. By Theorem 3.4, the time to compute eD through invoking WExpKDE (i.e., Algorithm 2) in line 3 of Algorithm 1 is O \u0012 nd Â· CK d1/4 , Q d1/4 ,1n,Îµ,Ï„ \u0013 . Furthermore, time to run WExpKDE in line 4 is O \u0012 nd Â· Câˆš 2Â·Q d1/4 , âˆš 2Â·K d1/4 ,u,1,Ï„ \u0013 , where u is the vector computed in lines 3-4 of Algorithm 1. On the other hand, by Theorem 3.4, vector u satisfies 1 2 vj â‰¤ uj â‰¤ 3 2 vj for all j âˆˆ [n] with probability at least 1 âˆ’ 1 poly(n) , where v is the vector defined in the theorem statement. Thus, using the definition of Câˆš 2Â·Q d1/4 , âˆš 2Â·K d1/4 ,u,1,Ï„ in Equation (9) we can show that the aforementioned runtime is bounded by O \u0012 nd Â· Câˆš 2Â·Q d1/4 , âˆš 2Â·K d1/4 ,v,1,Ï„ \u0013 . Finally, the time to generate m samples in line 6 of Algorithm 1 is O(m + n), using the sampling method developed by Hagerup et al. [17]. The total runtime is obtained by summing up these terms. 8.4 Proof of Corollary 3.6 Corollary 3.6 (Simplified Runtime for Bounded Diameter Datasets) . For any datasets Q, K with diameter maxi,jâˆˆ[n] âˆ¥ki âˆ’ qjâˆ¥2 2 = Î³ âˆš d log n for some Î³ > 0, the runtime of Theorem 3.5 is upper bounded by O \u0000 m + nd Â· \u0000 nÏ„(1+Î³) + Îµâˆ’2nÏ„(1+Î³/2)\u0001\u0001 , which is strongly sub-quadratic in n. In particular, if Î³ = o(1), the runtime is bounded by O \u0000 m + Îµâˆ’2d Â· n1+Ï„+o(1)\u0001 . Proof. First recall that the diameter of the datasets Q , K is maxi,jâˆˆ[n] âˆ¥ki âˆ’ qjâˆ¥2 2 = Î³ âˆš d log n for some Î³ >0. For any i, jâˆˆ [n], using the fact that âˆ¥ki âˆ’ qjâˆ¥2 2 â‰¤ Î³ âˆš d log n, we have, exp \u0012 1âˆš d âŸ¨kj, qiâŸ© \u0013 = exp \u0012 âˆ’1 2 âˆš d âˆ¥kj âˆ’ qiâˆ¥2 2 \u0013 Â· exp   âˆ¥kjâˆ¥2 + âˆ¥qiâˆ¥2 2 âˆš d ! â‰¥ nâˆ’Î³/2 Â· exp   âˆ¥kjâˆ¥2 + âˆ¥qiâˆ¥2 2 âˆš d ! . Therefore, summing the above inequality over all j âˆˆ [n] gives, X jâˆˆ[n] exp \u0012 1âˆš d âŸ¨kj, qiâŸ© \u0013 â‰¥ nâˆ’Î³/2 Â· X jâˆˆ[n] exp   âˆ¥kjâˆ¥2 + âˆ¥qiâˆ¥2 2 âˆš d ! . The above inequality holds for every i âˆˆ [n]. This inequality implies that the following set is empty for any Âµ â‰¤ nâˆ’1âˆ’Î³/2, ï£± ï£² ï£³i âˆˆ [n] : P jâˆˆ[n] exp \u0010 1âˆš dâŸ¨kj, qiâŸ© \u0011 P jâˆˆ[n] exp \u0010âˆ¥kjâˆ¥2+âˆ¥qiâˆ¥2 2 âˆš d \u0011 < nÂ· Âµ ï£¼ ï£½ ï£¾ = âˆ…. Thus, C K d1/4 , Q d1/4 ,1n,Îµ,Ï„ defined as per Equation (9) is bounded as follows, C K d1/4 , Q d1/4 ,1n,Îµ,Ï„ = min Âµ>0 Îµâˆ’2Âµâˆ’Ï„ + \f\f\f\f\f\f ï£± ï£² ï£³i âˆˆ [n] : P jâˆˆ[n] exp \u0010 1âˆš dâŸ¨kj, qiâŸ© \u0011 P jâˆˆ[n] exp \u0010âˆ¥kjâˆ¥2+âˆ¥qiâˆ¥2 2 âˆš d \u0011 < nÂµ ï£¼ ï£½ ï£¾ \f\f\f\f\f\f â‰¤ Îµâˆ’2 Â· nÏ„(1+Î³/2). 25Similarly, because vj > 0 for every j âˆˆ [n], we can show that, for any i âˆˆ [n], X jâˆˆ[n] vj exp \u0012 2âˆš d âŸ¨qj, kiâŸ© \u0013 â‰¥ nâˆ’Î³ Â· X jâˆˆ[n] vj exp   âˆ¥qjâˆ¥2 + âˆ¥kiâˆ¥2 âˆš d ! . As a result, the following set is empty for any Âµ â‰¤ nâˆ’1âˆ’Î³, ï£± ï£² ï£³i âˆˆ [n] : P jâˆˆ[n] vj Â· exp \u0010 2âˆš dâŸ¨qj, kiâŸ© \u0011 P jâˆˆ[n] vj exp \u0010âˆ¥qjâˆ¥2+âˆ¥kiâˆ¥2 âˆš d \u0011 < nÂ· Âµ ï£¼ ï£½ ï£¾ = âˆ…. So, Câˆš 2Â·Q d1/4 , âˆš 2Â·K d1/4 ,v,1,Ï„ defined as per Equation (9) is bounded as follows, Câˆš 2Â·Q d1/4 , âˆš 2Â·K d1/4 ,v,1,Ï„ = min Âµ>0 Âµâˆ’Ï„ + \f\f\f\f\f\f ï£± ï£² ï£³i âˆˆ [n] : P jâˆˆ[n] vj Â· exp \u0010 2âˆš dâŸ¨qj, kiâŸ© \u0011 P jâˆˆ[n] vj exp \u0010âˆ¥qjâˆ¥2+âˆ¥kiâˆ¥2 âˆš d \u0011 < nÂ· Âµ ï£¼ ï£½ ï£¾ \f\f\f\f\f\f â‰¤ nÏ„(1+Î³). Therefore, the total runtime of Theorem 3.5 is bounded by O \u0012 m + nd Â· \u0012 C K d1/4 , Q d1/4 ,1n,Îµ,Ï„ + Câˆš 2Â·Q d1/4 , âˆš 2Â·K d1/4 ,v,1,Ï„ \u0013\u0013 = O \u0010 m + nd Â· \u0010 nÏ„(1+Î³) + nÏ„(1+Î³/2)/Îµ2 \u0011\u0011 , which completes the proof. 9 Additional Results on BigGAN Image Generations Images in Figure 7 are randomly subset from 2 , 000 generations from BigGAN [ 33]3 with the exact attention computation and its various approximations including KDEformer (our), Performer [ 12], Reformer [20] and ScatterBrain [ 10]. One can observe that our KDEformer generates more natural and realistic images than other methods by a large margin, and in many cases it is even better than the exact computation. This means that it has much less running time and memory, but it has produced a higher quality and more realistic image in the end. Also, note that the hyperparameters of our approach were not fine-tuned. 3https://github.com/huggingface/pytorch-pretrained-BigGAN 26Exact KDEformer Performer Reformer ScatterBrain Figure 7: Images generations from the pre-trained BigGAN with the exact attention (top) and drop-in replacement with its approximations including our KDEformer (second row), Performer (third row), Reformer (fourth row) and ScatterBrain (bottom). 27",
      "meta_data": {
        "arxiv_id": "2302.02451v2",
        "authors": [
          "Amir Zandieh",
          "Insu Han",
          "Majid Daliri",
          "Amin Karbasi"
        ],
        "published_date": "2023-02-05T18:23:49Z",
        "pdf_url": "https://arxiv.org/pdf/2302.02451v2.pdf"
      }
    }
  ]
}